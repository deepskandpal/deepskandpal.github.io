<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents></nav></nav></aside><article class="post-single page-my-work-history"><h1>My experience</h1><span class=reading-time><em>18 min read</em></span><div class=post-content><p>I&rsquo;ll start with Jio
in jio i joined as GET (graduate engineer traineer) i was quickly placed in their flag ship app Myjio <a href=https://www.jio.com/apps/myjio/>https://www.jio.com/apps/myjio/</a> since i had coding acumen especially in languages like swift . Here i worked initially as a myjio developer but i was soon asked to join a new and upcoming team <a href=https://www.jio.com/help/helpful-tips/mobile/hellojio-article/>https://www.jio.com/help/helpful-tips/mobile/hellojio-article/</a> . The goal was to design an in house chat bot using machine learning that will solve simple user queries like &ldquo;internet not working&rdquo; &ldquo;issues with recharge&rdquo; while the user is inside myjio. in this team i wore multiple hats and worked as an ios developer , backend developer , machine learning engineer , data scientist. in each of these roles i have some situations that i took up a task which i am proud of</p><p>iOS developer: we had a usecase where we wanted to give user text/audio response with a pre recorded video. the pre recorded video had an actor answer the question behind a green screen. now the challenge was due to some constrainted the playback server couldn&rsquo;t remove the green screen and replace it with a custom background. So it had to be done at the client side ie on device ios. So i wrote the system where before the video is played while streaming , a buffer would always keep track of the next frame that is about to be played. then for each frame i would set the value of green channel to 0 this will make the background transperent. In the IOS layer then I would add a background image for that video sourced from the backend which will automatically appear as its the only layer behind the video playback layer that has the background transperent. It requried coding the logic in Metal the GPU language for ios
backend developer.: We had a simple training and inference pipeline setup in production. we used to train the model which was a shallow deep learning model used for multi class intent classification problem . the prediction pipeline will take use query and return a label (intent) which will then be processed by the client for appropriate action. the problem was whenever we use to train a new model. the file had to be manually placed in the inference server and a reboot was required everytime this happened, it was tedious and it became a pain for future as we went for Horizontal scaling with k8s as now we have more than 1 prediction server. In order to solve this i came up with an idea where we would use redis to keep track of the model version . How this works is for training. once the training is completed the training job will make an entry in redis for the version update and push it to shared file system with the inference servers. During prediction before answering any request the predicton server will first check the entry of the model version in redis and compare it with the model version its using. if its different it will pull the new model and auto reload it without shutting down the sever. this may cause a delayed response for that query but overall this was not a major issue and a tradeoff we lived with as this reduced in frequency significantly once we added more inference servers
ML engineer / data scietist. I learned whatever data science and Machine learning i know today on my work . I used to read books blogs and did courses especially google developers entry level course on machine learning. For the chat bot we had made a simple intent classifier based on fixed number of classes and the problem was to classifiy user query in 1 of those classes ( multi class) we started with very simple model hashing user query for features along with Logistic regression model for prediction. then we moved to tf-idf bow , LR and finally we started using deep learning models and embeddings for our use case especially fasttext for pre trained embeddings paired with a deeplearning classifiier designed using LSTM layers. The core insight that i was able to draw was using transfer learning to solve a very interesing issue Multi language queries ie a query with more than 1 language words. for eg Hinglish words like kirana store which would mean grocery store now kirana is not part of the english vocab of fasttext so it was causing problems for those kind of queries . in order to solve this we thought rather than using the pre training fastext word embeddings directly what if we add an embedding layer on top of LSTM layers and train those embeddings using a cross entropy loss for our multi class classification problem , and use that embedding layer as features for a better discriminator model like Logisitic regression. this approach helped us significantly as now we had fine tuned embeddings for our use case the problem of kirana = gocery store was solved .</p><p>This was my jio experience</p><p>Lets move to accenture</p><p>So the move to accenture was 2 fold i wanted to have more &ldquo;data science experience apart from NLP&rdquo; and to also move towards functional roles as i wanted to do an MBA back then. I was working as a data scientist in Accenture strategy and consulting under marketing analytics helping CPG clients with RGM ( revenue growth management) and RTM ( Route to market) strategies. This was a completely new domain i had come to we had econometric models , area prioritization models based on sales data prediction , outlet level scoring with geo spatial features. everything was pretty new and tbh not at all in my comfort zone. For the start few weeks i was stuck not knowing what to do then slowly i picked up and i worked on some pretty interesting problems</p><p>Enhancing the outlet prioritization score.: We used mostly numerical features such as ratings and orders placed along with geo spatial features like number of connected roads Places of interests near by and had a weighted equation in order to determine the potential outlet score which the client can then priortize to target for expansion. Now here i added the learnings from jio where were classifying user intent and we had issues with mixed language queries. Something similar was a problem here as well as whatever potential outlets list we used to get it used to have 100k to 200k stores based on the size of the territory and not all will be relevant from cpg perspective eg hardware , salons etc we were mostly interested in so since there was no way before to use this information ( store name and attributes like longitude and latitude) correctly without inviting additional noise we mostly used to ignore this signal. now when i saw this i thought maybe i can use my learnings from jio and help us get an accurate list of stores from this master list by classifying them as groceries and non groceries using the same technique that way we will be able to use this list as well for our out let prioritization. IT did wonders not only we were able to use it accurately as the classifier was so well trained to manage mix words stores as well for different languages ( thai vietnamise hindi english) those store level geo attributes were really helpful for more accurate ranking. this was my major achievement in accenture which really helped bring a new dimension to our CPG insights
the other problem that i was able to find and solve was more of an engineering problem. SO we had multiple data sources like global data euro monitor which used to provide us with demographics and econometric attributes of a location across different cpg categories like F&amp;B Cosmetics CLeaners etc carefully curated at continent country district city level , we used to create a quarterly report in tableau which used to give is market sizing at geography vs cpg category level. now this used to be a huge effort as analysts would download reports manually for each geography 1 by 1 and then vet them for consistency do aggregation and apply business logic in excel to create derived features which used to then go to a proprietrier market sizing model which was based on consumption curves,ALL of it manual and in many excel. It used to take 2-3 weeks easily to do this with the help of 2-4 analysts , I saw that there were apis also provided by these data providers and i decided to use use them to make this manual effort automated using python. I did this on my own time ( as i was supposed to do it in the next quater and i was dreading it ) and came up with scripts that would reduce that manual effort. I added the validation and the business logic and voila within 2 weeks i had a script that could give me the final datasets that would just need to be fed on the market size model and it would update it automatically. All this within 2 hours of running it on my machine so a 2-3 weeks effort was brought down to few hours with little to no manual intervention. These were my achievements in accenture</p><p>now we will do farfetch
At farfetch it was the first time i cam across scale and data intensive applications , Here i was part of the experimentation science team which took responsibility of stats engine the final component in a big complex very insightlful experimentation platform which helped product managers and other stakeholder run A/B tests in production for validating whether their new change is gonna have a good or a bad impact. The main purpose of this system was to make sure that atleast the bad impact is caught that too as soon as possible. to this the team had build a peaking based test called msprt ( mixed sequential probability ratio test) but the problem was the deployment cycle for any new change was very slow. so slow in fact that it would take months to push any changes , build failures , python env locking failures deployment failures it was semi cloud native batch processing service which would every day check for experiments new data from big query table using data partition , filter out those experiments that have new data and then start processing it. sometimes the service would get stuck during processing and then it would have to be manually restarted ( docker down docker up) This was my first task to make the service easy to deploy and to make it stable. I first check the python configuration and for build failures the problem had an easy fix , move the project from pipenv to poetry which would give us more deterministic build which pipenv was not able to do. this solved any build failures. next was there were some integration tests which were heavy as they depended upon big query to run , these tests used to fail which would not let new build pipeline to succeed and avoid deployments of new features. I removed those and instead added mock tests. which means now the service was having no issues. To have more control over the pipelines i migrated it to declarative pipelines where we decided our own build and test env rather that us being depended upon some down stream setup for it. That way any kind of build test or deploy issues were resolved almost instantly. Now the service was deployable. The next was the stability part. the service used to get &ldquo;stuck&rdquo; nobody knew why it happened , there were theories but the code base was vast and since the issue was not reproducible as well as there was no fixed pattern to its occurance it was decided that this 1 instance batch processor is too risky. if its used to get stuck then the overhead of the experiment processing used to be huge resulting in delayed results and even further delayed decision making. Since there was no solution i was asked if we could horizontally scale this . if we had more than 1 instance of the batch job even if one instance is &ldquo;stuck&rdquo; the others would process the experiments and it would not hamper our velocity. To this we decided to use K8s and locking mechanism for experiments so that no 2 instances process the same experiments. The idea was k8s will have the batch job running. there will be a separate code which would prepare the master list of experiments that have new data and need to be processed today. it will store it in a bq table usually 100 odd experiments were there per day. Each instance will check the list for experiments to process. and then check in redis if the name is already present or not. if the redis has and entry which means some other instance is already processing the experiment so this instance will skip it , till it finds an experiment hat as no entry in redis that means that experiment is available for processing. it will pick it up and make an entry for that experiment in redis for others to view. after the processing would be completed that instance would delete the entry in redis for future processing. Redis itself had a TTL for 24 hrs for any &ldquo;hanging experiments&rdquo; the ones that got stuck.
afterwards after few months i did find the issue for the &ldquo;stuck experiments&rdquo; you see deep down in the code somewhere were the big query calls were getting made, there was this condition in the big query client that was written that if after making an sql request you don&rsquo;t recieve the data within 1 hour , you move ahead in the code base. the problem was sometimes it used to take more than an hour to get data as the tables were very heavy ( 100s to terabytes to process) and the condition was not written correctly. the code used to NOT move ahead but rather wait for the data to come. The other problem was there was no cancellation at big query side. it would queue the request forever till the slots to process it were available there WAS no timeout at its end to cancel a request and send that information to client stating that unable to process the request. hence sometimes the client in the batch job would wait indefinietly now at any given point each batch job used to process 3 experiments simultaneously, if by coincidence this happened for all 3 at the same time or during the same day the whole batch service would stop as all the 3 workers would be marked as busy and no new experiment could be selected for processing.
this was an important discovery.</p><p>After we fixed this I was also involved in developing MAB multi arm bandit support as well for our experiment platform here we worked on selecting the best algorithm from epsilon greedy to thompson sampling to finally settling for UCB</p><p>this was my farfetch experience where i worked on doing data science at scale</p><p>now lets do my current company knowbe4 where i was exposed to everything GENAI</p><p>In knowbe4 i got my chance to fully immerse myself in the world of gen ai here i was working on aws ecosystem. This was also my introduction to terraform lambdas all things serverless but as a senior ML engineer i interacted most with aws bedrock where i used anthropic models for developing production ready gen AI applications. since we are cyber security awareness spaces we have 2 verticals were we work across. one is red team tools like phisihing simulation test where in admins can give users phishing emails to test their knowledge on identifying red flags , if the user fails then they get training assinged to them if the user reports them that means they have passed. or blue team products like trainings for security awareness. they were also assigned manually by the admins. Now the ask was can we use LLMs to make these 2 pillars automated so that the admins have little to no involvement in them. Now for phishing simulation test the idea was simple rather than admins crafting the phish email what if the admins send some attributes that would like to include ( user details, attack vector, red flags) and we generate a phish email for them .This is a use case designed for GenAI , we would need to write the prompt manage the user inputs validate the results ( is it in correct format) and give the option of re generation if the admin wants to try variations. This was the first GenAI application i got a chance to work on it was also new as prompt engineering had just come up and the model of choice back then was claude sonnet 3. This was a big hit as this service which was deployed using terraform and docker lambda really reduce the time it took to make new phish emails . The next usecase we tackled were in compliance ( blue team) . Our customers have policies of various compliances such as GDPR HIPAA PII that their employees would just read and mark as done. there was no way to actively test them as creating quizz for them manually would be very cumbersome and also after a point the questions would become stale. the ask was if we can use GenAI to take in the policy pdf content and generate quizzes from that document. this was the next step in prompt engineering where we are adding context in the form of document and asking the LLM to generate based on that. This was a huge it again because A it made sure the questions are relevant , and new as well as they can be easily refreshed if someone had to update them. Also we supported multiple lanugages which means people could select in which language they want the quiz in regardless in which language the policy is. this involved coming up with custom logic for chunking the pdf for different languages so that the right content is passed without overflowing the context windows of the LLM. as most languages had longer text because of the script that can cause an overflow of the tokens, this was also well recieved and both the use cases have a greatly reduced customer headaches and have been a great add on to our product suite. Now we are moving towards autonomous agents where we would want a Pishing email agent that would decide when to trigger phish test and also what attributes would be used to decide the level of difficulty of the phish based on it it will generate a user specific phish and deliver it to the user. this has a combination of algorithms like UCB for deciding the users difficulty rating , orchestration setup based on phishing budget ( 1 phish per 2 weeks) and then a bunch of LLM work flows to decide the right subject for the phish and generating it as an internal email ( from hr or any other department of your company) or as an external email ( some account you have either of some company vendor like slack or your personal account say of netflix) it will have options of adding a new red flag ( spoofed logo) which it will generate by inputing a companies logo in stable diffusion and asking it to make some cosmetic changes that will be subtle but identifiable. And it will also have memory of the past users interactions of phish email to decide what is the best set of attributes that will challegene the user</p><p>And now we are working towards a platfrom that will help MAS ( multi agent system) from build to deploy to monitor to interaction Agent to Agent or Agent to MCP</p><p>that has been my knowbe4 journey</p><p>I also forgot to add one more work i did in knowbe4 using generative models for text classification. so in policy quiz we wanted to make sure that whatever the user did in the quizz its impact showed in a global metric that we call &ldquo;RISK SCORE&rdquo; which is a number between 0 to 100 ( 100 being extremely risky user) that gets influenced negatively by risky events ( clicking on a simulated phishing test) and positively by secure events ( by completing a training) so in order to that there are buckets like Email security data security web security account hygiene compliance electives and physical security that the events fall into but we wanted the policies to be also mapped to them and if the user passes the quiz it treats it as secure event and failing it treats it like a risky event to do this we used claude sonnet 3.5 which would take the pdf content and map it to 1 or more buckets with. score between 0 to 100 and any score above 80 we would treat it as valid mapping to the bucket. now we are exploring options of fine tuning inhouse some open source LLMS for this task so that we can reduce cost and get better results</p></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>