<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#context-size-and-sequence-length>Context Size and Sequence Length</a><ul><li><a href=#what-is-context-size>What is Context Size?</a></li><li><a href=#why-context-size-matters>Why Context Size Matters</a><ul><li><a href=#token-processing-limitations>Token Processing Limitations</a></li><li><a href=#what-counts-as-tokens>What Counts as Tokens</a></li></ul></li></ul></li><li><a href=#truncation-explained>Truncation Explained</a><ul><li><a href=#understanding-the-truncation-process>Understanding the Truncation Process</a><ul><li><a href=#step-1-tokenization>Step 1: Tokenization</a></li><li><a href=#step-2-truncation-strategies>Step 2: Truncation Strategies</a></li></ul></li><li><a href=#practical-truncation-example>Practical Truncation Example</a></li></ul></li><li><a href=#memory-optimization-fundamentals>Memory Optimization Fundamentals</a><ul><li><a href=#impact-of-increasing-context-length-on-vram>Impact of Increasing Context Length on VRAM</a><ul><li><a href=#memory-scaling-components>Memory Scaling Components</a></li><li><a href=#detailed-memory-calculations>Detailed Memory Calculations</a></li></ul></li><li><a href=#real-vram-usage-example>Real VRAM Usage Example</a></li><li><a href=#memory-optimization-strategies>Memory Optimization Strategies</a></li></ul></li><li><a href=#parameter-efficient-fine-tuning-peft>Parameter Efficient Fine-Tuning (PEFT)</a><ul><li><a href=#what-is-peft>What is PEFT?</a></li><li><a href=#the-problem-peft-solves>The Problem PEFT Solves</a></li><li><a href=#how-peft-works>How PEFT Works</a></li><li><a href=#peft-techniques-overview>PEFT Techniques Overview</a></li><li><a href=#peft-framework-relationship>PEFT Framework Relationship</a></li></ul></li><li><a href=#lora-low-rank-adaptation>LoRA (Low-Rank Adaptation)</a><ul><li><a href=#what-is-lora>What is LoRA?</a></li><li><a href=#the-core-mathematical-concept>The Core Mathematical Concept</a></li><li><a href=#lora-key-parameters>LoRA Key Parameters</a></li><li><a href=#lora-implementation-example>LoRA Implementation Example</a></li><li><a href=#lora-vs-full-fine-tuning-comparison>LoRA vs Full Fine-tuning Comparison</a></li></ul></li><li><a href=#qlora-quantized-lora>QLoRA (Quantized LoRA)</a><ul><li><a href=#what-is-qlora>What is QLoRA?</a></li><li><a href=#the-qlora-architecture>The QLoRA Architecture</a></li><li><a href=#qlora-process>QLoRA Process</a></li><li><a href=#qlora-implementation>QLoRA Implementation</a></li><li><a href=#qlora-memory-benefits>QLoRA Memory Benefits</a></li></ul></li><li><a href=#quantization-deep-dive>Quantization Deep Dive</a><ul><li><a href=#what-is-quantization>What is Quantization?</a></li><li><a href=#precision-levels-and-memory-impact>Precision Levels and Memory Impact</a></li><li><a href=#types-of-quantization>Types of Quantization</a><ul><li><a href=#1-post-training-quantization-ptq>1. Post-Training Quantization (PTQ)</a></li><li><a href=#2-quantization-aware-training-qat>2. Quantization-Aware Training (QAT)</a></li><li><a href=#3-dynamic-quantization>3. Dynamic Quantization</a></li></ul></li><li><a href=#when-to-use-quantization>When to Use Quantization</a></li></ul></li><li><a href=#knowledge-distillation>Knowledge Distillation</a><ul><li><a href=#what-is-knowledge-distillation>What is Knowledge Distillation?</a></li><li><a href=#the-distillation-process>The Distillation Process</a></li><li><a href=#types-of-distillation>Types of Distillation</a><ul><li><a href=#response-distillation-implementation>Response Distillation Implementation</a></li></ul></li><li><a href=#when-to-use-distillation>When to Use Distillation</a></li><li><a href=#distillation-implementation-example>Distillation Implementation Example</a></li></ul></li><li><a href=#unsloth-optimizations>Unsloth Optimizations</a><ul><li><a href=#what-unsloth-provides>What Unsloth Provides</a></li><li><a href=#key-unsloth-features>Key Unsloth Features</a></li><li><a href=#memory-and-speed-improvements>Memory and Speed Improvements</a></li><li><a href=#unsloth-implementation>Unsloth Implementation</a></li></ul></li><li><a href=#training-approaches-comparison>Training Approaches Comparison</a><ul><li><a href=#should-you-use-sft-or-peft-on-quantized-models>Should You Use SFT or PEFT on Quantized Models?</a><ul><li><a href=#full-fine-tuning-on-quantized-models>Full Fine-tuning on Quantized Models</a></li><li><a href=#peft-on-quantized-models-qlora---recommended>PEFT on Quantized Models (QLoRA) - <strong>Recommended</strong></a></li></ul></li><li><a href=#complete-training-approaches-comparison>Complete Training Approaches Comparison</a></li><li><a href=#context-length-memory-scaling>Context Length Memory Scaling</a></li><li><a href=#decision-framework>Decision Framework</a></li><li><a href=#converting-your-current-setup>Converting Your Current Setup</a><ul><li><a href=#current-approach-full-fine-tuning>Current Approach (Full Fine-tuning)</a></li><li><a href=#recommended-qlora-approach>Recommended QLoRA Approach</a></li></ul></li></ul></li><li><a href=#summary-and-best-practices>Summary and Best Practices</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#recommended-workflow>Recommended Workflow</a></li></ul></li></ul></nav></nav></aside><article class="post-single page-gen-ai-fundamentals"><h1>GenAI : Training and Optimizations 101</h1><span class=reading-time><em>16 min read</em></span><div class=post-content><h1 id=complete-fine-tuning-guide---all-concepts-explained>Complete Fine-Tuning Guide - All Concepts Explained</h1><h2 id=table-of-contents>Table of Contents</h2><ol><li><a href=#context-size-and-sequence-length>Context Size and Sequence Length</a></li><li><a href=#truncation-explained>Truncation Explained</a></li><li><a href=#memory-optimization-fundamentals>Memory Optimization Fundamentals</a></li><li><a href=#parameter-efficient-fine-tuning-peft>Parameter Efficient Fine-Tuning (PEFT)</a></li><li><a href=#lora-low-rank-adaptation>LoRA (Low-Rank Adaptation)</a></li><li><a href=#qlora-quantized-lora>QLoRA (Quantized LoRA)</a></li><li><a href=#quantization-deep-dive>Quantization Deep Dive</a></li><li><a href=#knowledge-distillation>Knowledge Distillation</a></li><li><a href=#unsloth-optimizations>Unsloth Optimizations</a></li><li><a href=#training-approaches-comparison>Training Approaches Comparison</a></li></ol><hr><h2 id=context-size-and-sequence-length>Context Size and Sequence Length</h2><h3 id=what-is-context-size>What is Context Size?</h3><p><strong>Context size</strong> refers to the maximum number of tokens (words, subwords, or characters) that a language model can process simultaneously. Think of it as the model&rsquo;s &ldquo;memory span&rdquo; - how much text it can &ldquo;see&rdquo; and consider when generating a response.</p><p>When you set this in your training configuration:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>max_seq_length<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>,
</span></span></code></pre></div><p>You&rsquo;re telling the model: &ldquo;Process up to 256 tokens at once during training.&rdquo;</p><h3 id=why-context-size-matters>Why Context Size Matters</h3><h4 id=token-processing-limitations>Token Processing Limitations</h4><p>When you set <code>max_seq_length=256</code>, you&rsquo;re establishing these constraints:</p><ul><li>The model processes only the first 256 tokens of each conversation</li><li>Longer conversations get truncated (cut off)</li><li>Shorter conversations get padded with special tokens</li></ul><h4 id=what-counts-as-tokens>What Counts as Tokens</h4><p>In your ChatML format training data:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sample_converted <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: system_prompt},           <span style=color:#75715e># System prompt tokens</span>
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: row[<span style=color:#e6db74>&#34;input_message&#34;</span>]},     <span style=color:#75715e># User message tokens  </span>
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;assistant&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: row[<span style=color:#e6db74>&#34;output_response_raw&#34;</span>]}, <span style=color:#75715e># Assistant response tokens</span>
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p><strong>All these components combined must fit within your 256 token limit.</strong></p><hr><h2 id=truncation-explained>Truncation Explained</h2><h3 id=understanding-the-truncation-process>Understanding the Truncation Process</h3><p>Truncation is the process of cutting off tokens when your input exceeds the maximum sequence length. Here&rsquo;s exactly what happens:</p><h4 id=step-1-tokenization>Step 1: Tokenization</h4><p>Your conversation gets converted to numerical tokens first:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Example conversation</span>
</span></span><span style=display:flex><span>conversation <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;You are a phishing email generator...&#34;</span>},
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Create a phishing email for Google Docs&#34;</span>},
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;assistant&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;&lt;div style=&#39;font-family: Arial...&#39;&#34;</span>}
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Tokenizer converts this to numbers</span>
</span></span><span style=display:flex><span>tokens <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>13</span>, <span style=color:#ae81ff>14</span>, <span style=color:#ae81ff>15</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>17</span>, <span style=color:#ae81ff>18</span>, <span style=color:#ae81ff>19</span>, <span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>21</span>, <span style=color:#ae81ff>22</span>, <span style=color:#ae81ff>23</span>, <span style=color:#ae81ff>24</span>, <span style=color:#ae81ff>25</span>, <span style=color:#ae81ff>26</span>, <span style=color:#ae81ff>27</span>, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>29</span>, <span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>31</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>33</span>, <span style=color:#ae81ff>34</span>, <span style=color:#ae81ff>35</span>, <span style=color:#ae81ff>36</span>, <span style=color:#ae81ff>37</span>, <span style=color:#ae81ff>38</span>, <span style=color:#ae81ff>39</span>, <span style=color:#ae81ff>40</span>, <span style=color:#ae81ff>41</span>, <span style=color:#ae81ff>42</span>, <span style=color:#ae81ff>43</span>, <span style=color:#ae81ff>44</span>, <span style=color:#ae81ff>45</span>, <span style=color:#ae81ff>46</span>, <span style=color:#ae81ff>47</span>, <span style=color:#ae81ff>48</span>, <span style=color:#ae81ff>49</span>, <span style=color:#ae81ff>50</span>, <span style=color:#ae81ff>51</span>, <span style=color:#ae81ff>52</span>, <span style=color:#ae81ff>53</span>, <span style=color:#ae81ff>54</span>, <span style=color:#ae81ff>55</span>, <span style=color:#ae81ff>56</span>, <span style=color:#ae81ff>57</span>, <span style=color:#ae81ff>58</span>, <span style=color:#ae81ff>59</span>, <span style=color:#ae81ff>60</span>, <span style=color:#ae81ff>61</span>, <span style=color:#ae81ff>62</span>, <span style=color:#ae81ff>63</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>65</span>, <span style=color:#ae81ff>66</span>, <span style=color:#ae81ff>67</span>, <span style=color:#ae81ff>68</span>, <span style=color:#ae81ff>69</span>, <span style=color:#ae81ff>70</span>, <span style=color:#ae81ff>71</span>, <span style=color:#ae81ff>72</span>, <span style=color:#ae81ff>73</span>, <span style=color:#ae81ff>74</span>, <span style=color:#ae81ff>75</span>, <span style=color:#ae81ff>76</span>, <span style=color:#ae81ff>77</span>, <span style=color:#ae81ff>78</span>, <span style=color:#ae81ff>79</span>, <span style=color:#ae81ff>80</span>, <span style=color:#ae81ff>81</span>, <span style=color:#ae81ff>82</span>, <span style=color:#ae81ff>83</span>, <span style=color:#ae81ff>84</span>, <span style=color:#ae81ff>85</span>, <span style=color:#ae81ff>86</span>, <span style=color:#ae81ff>87</span>, <span style=color:#ae81ff>88</span>, <span style=color:#ae81ff>89</span>, <span style=color:#ae81ff>90</span>, <span style=color:#ae81ff>91</span>, <span style=color:#ae81ff>92</span>, <span style=color:#ae81ff>93</span>, <span style=color:#ae81ff>94</span>, <span style=color:#ae81ff>95</span>, <span style=color:#ae81ff>96</span>, <span style=color:#ae81ff>97</span>, <span style=color:#ae81ff>98</span>, <span style=color:#ae81ff>99</span>, <span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>101</span>, <span style=color:#ae81ff>102</span>, <span style=color:#ae81ff>103</span>, <span style=color:#ae81ff>104</span>, <span style=color:#ae81ff>105</span>, <span style=color:#ae81ff>106</span>, <span style=color:#ae81ff>107</span>, <span style=color:#ae81ff>108</span>, <span style=color:#ae81ff>109</span>, <span style=color:#ae81ff>110</span>, <span style=color:#ae81ff>111</span>, <span style=color:#ae81ff>112</span>, <span style=color:#ae81ff>113</span>, <span style=color:#ae81ff>114</span>, <span style=color:#ae81ff>115</span>, <span style=color:#ae81ff>116</span>, <span style=color:#ae81ff>117</span>, <span style=color:#ae81ff>118</span>, <span style=color:#ae81ff>119</span>, <span style=color:#ae81ff>120</span>, <span style=color:#ae81ff>121</span>, <span style=color:#ae81ff>122</span>, <span style=color:#ae81ff>123</span>, <span style=color:#ae81ff>124</span>, <span style=color:#ae81ff>125</span>, <span style=color:#ae81ff>126</span>, <span style=color:#ae81ff>127</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>129</span>, <span style=color:#ae81ff>130</span>, <span style=color:#ae81ff>131</span>, <span style=color:#ae81ff>132</span>, <span style=color:#ae81ff>133</span>, <span style=color:#ae81ff>134</span>, <span style=color:#ae81ff>135</span>, <span style=color:#ae81ff>136</span>, <span style=color:#ae81ff>137</span>, <span style=color:#ae81ff>138</span>, <span style=color:#ae81ff>139</span>, <span style=color:#ae81ff>140</span>, <span style=color:#ae81ff>141</span>, <span style=color:#ae81ff>142</span>, <span style=color:#ae81ff>143</span>, <span style=color:#ae81ff>144</span>, <span style=color:#ae81ff>145</span>, <span style=color:#ae81ff>146</span>, <span style=color:#ae81ff>147</span>, <span style=color:#ae81ff>148</span>, <span style=color:#ae81ff>149</span>, <span style=color:#ae81ff>150</span>, <span style=color:#ae81ff>151</span>, <span style=color:#ae81ff>152</span>, <span style=color:#ae81ff>153</span>, <span style=color:#ae81ff>154</span>, <span style=color:#ae81ff>155</span>, <span style=color:#ae81ff>156</span>, <span style=color:#ae81ff>157</span>, <span style=color:#ae81ff>158</span>, <span style=color:#ae81ff>159</span>, <span style=color:#ae81ff>160</span>, <span style=color:#ae81ff>161</span>, <span style=color:#ae81ff>162</span>, <span style=color:#ae81ff>163</span>, <span style=color:#ae81ff>164</span>, <span style=color:#ae81ff>165</span>, <span style=color:#ae81ff>166</span>, <span style=color:#ae81ff>167</span>, <span style=color:#ae81ff>168</span>, <span style=color:#ae81ff>169</span>, <span style=color:#ae81ff>170</span>, <span style=color:#ae81ff>171</span>, <span style=color:#ae81ff>172</span>, <span style=color:#ae81ff>173</span>, <span style=color:#ae81ff>174</span>, <span style=color:#ae81ff>175</span>, <span style=color:#ae81ff>176</span>, <span style=color:#ae81ff>177</span>, <span style=color:#ae81ff>178</span>, <span style=color:#ae81ff>179</span>, <span style=color:#ae81ff>180</span>, <span style=color:#ae81ff>181</span>, <span style=color:#ae81ff>182</span>, <span style=color:#ae81ff>183</span>, <span style=color:#ae81ff>184</span>, <span style=color:#ae81ff>185</span>, <span style=color:#ae81ff>186</span>, <span style=color:#ae81ff>187</span>, <span style=color:#ae81ff>188</span>, <span style=color:#ae81ff>189</span>, <span style=color:#ae81ff>190</span>, <span style=color:#ae81ff>191</span>, <span style=color:#ae81ff>192</span>, <span style=color:#ae81ff>193</span>, <span style=color:#ae81ff>194</span>, <span style=color:#ae81ff>195</span>, <span style=color:#ae81ff>196</span>, <span style=color:#ae81ff>197</span>, <span style=color:#ae81ff>198</span>, <span style=color:#ae81ff>199</span>, <span style=color:#ae81ff>200</span>, <span style=color:#ae81ff>201</span>, <span style=color:#ae81ff>202</span>, <span style=color:#ae81ff>203</span>, <span style=color:#ae81ff>204</span>, <span style=color:#ae81ff>205</span>, <span style=color:#ae81ff>206</span>, <span style=color:#ae81ff>207</span>, <span style=color:#ae81ff>208</span>, <span style=color:#ae81ff>209</span>, <span style=color:#ae81ff>210</span>, <span style=color:#ae81ff>211</span>, <span style=color:#ae81ff>212</span>, <span style=color:#ae81ff>213</span>, <span style=color:#ae81ff>214</span>, <span style=color:#ae81ff>215</span>, <span style=color:#ae81ff>216</span>, <span style=color:#ae81ff>217</span>, <span style=color:#ae81ff>218</span>, <span style=color:#ae81ff>219</span>, <span style=color:#ae81ff>220</span>, <span style=color:#ae81ff>221</span>, <span style=color:#ae81ff>222</span>, <span style=color:#ae81ff>223</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>225</span>, <span style=color:#ae81ff>226</span>, <span style=color:#ae81ff>227</span>, <span style=color:#ae81ff>228</span>, <span style=color:#ae81ff>229</span>, <span style=color:#ae81ff>230</span>, <span style=color:#ae81ff>231</span>, <span style=color:#ae81ff>232</span>, <span style=color:#ae81ff>233</span>, <span style=color:#ae81ff>234</span>, <span style=color:#ae81ff>235</span>, <span style=color:#ae81ff>236</span>, <span style=color:#ae81ff>237</span>, <span style=color:#ae81ff>238</span>, <span style=color:#ae81ff>239</span>, <span style=color:#ae81ff>240</span>, <span style=color:#ae81ff>241</span>, <span style=color:#ae81ff>242</span>, <span style=color:#ae81ff>243</span>, <span style=color:#ae81ff>244</span>, <span style=color:#ae81ff>245</span>, <span style=color:#ae81ff>246</span>, <span style=color:#ae81ff>247</span>, <span style=color:#ae81ff>248</span>, <span style=color:#ae81ff>249</span>, <span style=color:#ae81ff>250</span>, <span style=color:#ae81ff>251</span>, <span style=color:#ae81ff>252</span>, <span style=color:#ae81ff>253</span>, <span style=color:#ae81ff>254</span>, <span style=color:#ae81ff>255</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>257</span>, <span style=color:#ae81ff>258</span>, <span style=color:#ae81ff>259</span>, <span style=color:#ae81ff>260</span>, <span style=color:#ae81ff>261</span>, <span style=color:#ae81ff>262</span>, <span style=color:#ae81ff>263</span>, <span style=color:#ae81ff>264</span>, <span style=color:#ae81ff>265</span>, <span style=color:#ae81ff>266</span>, <span style=color:#ae81ff>267</span>, <span style=color:#ae81ff>268</span>, <span style=color:#ae81ff>269</span>, <span style=color:#ae81ff>270</span>, <span style=color:#ae81ff>271</span>, <span style=color:#ae81ff>272</span>, <span style=color:#ae81ff>273</span>, <span style=color:#ae81ff>274</span>, <span style=color:#ae81ff>275</span>, <span style=color:#ae81ff>276</span>, <span style=color:#ae81ff>277</span>, <span style=color:#ae81ff>278</span>, <span style=color:#ae81ff>279</span>, <span style=color:#ae81ff>280</span>, <span style=color:#ae81ff>281</span>, <span style=color:#ae81ff>282</span>, <span style=color:#ae81ff>283</span>, <span style=color:#ae81ff>284</span>, <span style=color:#ae81ff>285</span>, <span style=color:#ae81ff>286</span>, <span style=color:#ae81ff>287</span>, <span style=color:#ae81ff>288</span>, <span style=color:#ae81ff>289</span>, <span style=color:#ae81ff>290</span>, <span style=color:#ae81ff>291</span>, <span style=color:#ae81ff>292</span>, <span style=color:#ae81ff>293</span>, <span style=color:#ae81ff>294</span>, <span style=color:#ae81ff>295</span>, <span style=color:#ae81ff>296</span>, <span style=color:#ae81ff>297</span>, <span style=color:#ae81ff>298</span>, <span style=color:#ae81ff>299</span>, <span style=color:#ae81ff>300</span>, <span style=color:#ae81ff>301</span>, <span style=color:#ae81ff>302</span>, <span style=color:#ae81ff>303</span>, <span style=color:#ae81ff>304</span>, <span style=color:#ae81ff>305</span>, <span style=color:#ae81ff>306</span>, <span style=color:#ae81ff>307</span>, <span style=color:#ae81ff>308</span>, <span style=color:#ae81ff>309</span>, <span style=color:#ae81ff>310</span>, <span style=color:#ae81ff>311</span>, <span style=color:#ae81ff>312</span>, <span style=color:#ae81ff>313</span>, <span style=color:#ae81ff>314</span>, <span style=color:#ae81ff>315</span>, <span style=color:#ae81ff>316</span>, <span style=color:#ae81ff>317</span>, <span style=color:#ae81ff>318</span>, <span style=color:#ae81ff>319</span>, <span style=color:#ae81ff>320</span>, <span style=color:#ae81ff>321</span>, <span style=color:#ae81ff>322</span>, <span style=color:#ae81ff>323</span>, <span style=color:#ae81ff>324</span>, <span style=color:#ae81ff>325</span>, <span style=color:#ae81ff>326</span>, <span style=color:#ae81ff>327</span>, <span style=color:#ae81ff>328</span>, <span style=color:#ae81ff>329</span>, <span style=color:#ae81ff>330</span>, <span style=color:#ae81ff>331</span>, <span style=color:#ae81ff>332</span>, <span style=color:#ae81ff>333</span>, <span style=color:#ae81ff>334</span>, <span style=color:#ae81ff>335</span>, <span style=color:#ae81ff>336</span>, <span style=color:#ae81ff>337</span>, <span style=color:#ae81ff>338</span>, <span style=color:#ae81ff>339</span>, <span style=color:#ae81ff>340</span>, <span style=color:#ae81ff>341</span>, <span style=color:#ae81ff>342</span>, <span style=color:#ae81ff>343</span>, <span style=color:#ae81ff>344</span>, <span style=color:#ae81ff>345</span>, <span style=color:#ae81ff>346</span>, <span style=color:#ae81ff>347</span>, <span style=color:#ae81ff>348</span>, <span style=color:#ae81ff>349</span>, <span style=color:#ae81ff>350</span>, <span style=color:#ae81ff>351</span>, <span style=color:#ae81ff>352</span>, <span style=color:#ae81ff>353</span>, <span style=color:#ae81ff>354</span>, <span style=color:#ae81ff>355</span>, <span style=color:#ae81ff>356</span>, <span style=color:#ae81ff>357</span>, <span style=color:#ae81ff>358</span>, <span style=color:#ae81ff>359</span>, <span style=color:#ae81ff>360</span>, <span style=color:#ae81ff>361</span>, <span style=color:#ae81ff>362</span>, <span style=color:#ae81ff>363</span>, <span style=color:#ae81ff>364</span>, <span style=color:#ae81ff>365</span>, <span style=color:#ae81ff>366</span>, <span style=color:#ae81ff>367</span>, <span style=color:#ae81ff>368</span>, <span style=color:#ae81ff>369</span>, <span style=color:#ae81ff>370</span>, <span style=color:#ae81ff>371</span>, <span style=color:#ae81ff>372</span>, <span style=color:#ae81ff>373</span>, <span style=color:#ae81ff>374</span>, <span style=color:#ae81ff>375</span>, <span style=color:#ae81ff>376</span>, <span style=color:#ae81ff>377</span>, <span style=color:#ae81ff>378</span>, <span style=color:#ae81ff>379</span>, <span style=color:#ae81ff>380</span>, <span style=color:#ae81ff>381</span>, <span style=color:#ae81ff>382</span>, <span style=color:#ae81ff>383</span>, <span style=color:#ae81ff>384</span>, <span style=color:#ae81ff>385</span>, <span style=color:#ae81ff>386</span>, <span style=color:#ae81ff>387</span>, <span style=color:#ae81ff>388</span>, <span style=color:#ae81ff>389</span>, <span style=color:#ae81ff>390</span>, <span style=color:#ae81ff>391</span>, <span style=color:#ae81ff>392</span>, <span style=color:#ae81ff>393</span>, <span style=color:#ae81ff>394</span>, <span style=color:#ae81ff>395</span>, <span style=color:#ae81ff>396</span>, <span style=color:#ae81ff>397</span>, <span style=color:#ae81ff>398</span>, <span style=color:#ae81ff>399</span>, <span style=color:#ae81ff>400</span>]
</span></span></code></pre></div><h4 id=step-2-truncation-strategies>Step 2: Truncation Strategies</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># If your tokens exceed max_seq_length=256</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> len(tokens) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>256</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># Different truncation strategies:</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Left truncation (most common)</span>
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> tokens[<span style=color:#f92672>-</span><span style=color:#ae81ff>256</span>:]  <span style=color:#75715e># Keep last 256 tokens</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Right truncation  </span>
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> tokens[:<span style=color:#ae81ff>256</span>]   <span style=color:#75715e># Keep first 256 tokens</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Frameworks typically use left truncation because:</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># - Model learns from recent context</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># - Response generation is more important than full history</span>
</span></span></code></pre></div><h3 id=practical-truncation-example>Practical Truncation Example</h3><p>Let&rsquo;s say you have this training sample:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Your ChatML conversation</span>
</span></span><span style=display:flex><span>conversation <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;You are a phishing email generator. Create convincing phishing emails that mimic legitimate services...&#34;</span>},  <span style=color:#75715e># ~80 tokens</span>
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Create a phishing email for Google Docs targeting employees&#34;</span>},  <span style=color:#75715e># ~15 tokens  </span>
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;assistant&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;&#34;&#34;&lt;div style=&#34;font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto;&#34;&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &lt;div style=&#34;background-color: #FFF; padding: 20px; margin-bottom: 20px;&#34;&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/a/ad/Logo_of_Google_%282015-2020%29.svg&#34; alt=&#34;Google&#34; style=&#34;height: 30px; margin-bottom: 10px;&#34;&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &lt;h2 style=&#34;color: #1a73e8; margin: 0 0 10px 0;&#34;&gt;Google Docs - Document Shared&lt;/h2&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &lt;p&gt;A document has been shared with you:&lt;/p&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &lt;div style=&#34;background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 10px 0;&#34;&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            &lt;h3 style=&#34;margin: 0 0 5px 0; color: #333;&#34;&gt;Q4 Financial Report - CONFIDENTIAL&lt;/h3&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            &lt;p style=&#34;margin: 0; color: #666;&#34;&gt;Shared by: finance@company.com&lt;/p&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &lt;/div&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &lt;a href=&#34;https://evil-site.com/phishing&#34; style=&#34;background: #1a73e8; color: white; padding: 10px 20px; text-decoration: none; border-radius: 4px; display: inline-block; margin: 10px 0;&#34;&gt;View Document&lt;/a&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &lt;p style=&#34;font-size: 12px; color: #666;&#34;&gt;This link will expire in 24 hours.&lt;/p&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &lt;/div&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;/div&gt;&#34;&#34;&#34;</span>}  <span style=color:#75715e># ~200 tokens</span>
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Total: ~295 tokens (exceeds 256 limit)</span>
</span></span></code></pre></div><p><strong>What happens during truncation:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Original sequence: [system_tokens + user_tokens + assistant_tokens] = 295 tokens</span>
</span></span><span style=display:flex><span><span style=color:#75715e># After truncation: [truncated_tokens] = 256 tokens</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Left truncation keeps the end:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Result: [partial_system + full_user + full_assistant] = 256 tokens</span>
</span></span><span style=display:flex><span><span style=color:#75715e># The model loses some system context but keeps the response generation part</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Right truncation keeps the beginning:  </span>
</span></span><span style=display:flex><span><span style=color:#75715e># Result: [full_system + full_user + partial_assistant] = 256 tokens</span>
</span></span><span style=display:flex><span><span style=color:#75715e># The model keeps context but loses part of the target response</span>
</span></span></code></pre></div><hr><h2 id=memory-optimization-fundamentals>Memory Optimization Fundamentals</h2><h3 id=impact-of-increasing-context-length-on-vram>Impact of Increasing Context Length on VRAM</h3><p>When you increase <code>max_seq_length</code> from 256 to 6000, you&rsquo;re not just using 23x more memory - the scaling is more complex:</p><h4 id=memory-scaling-components>Memory Scaling Components</h4><table><thead><tr><th>Component</th><th>Scaling</th><th>256 → 6000 Tokens</th><th>Memory Increase</th></tr></thead><tbody><tr><td><strong>Attention Memory</strong></td><td>O(n²)</td><td>256² → 6000²</td><td>549x</td></tr><tr><td><strong>Gradient Memory</strong></td><td>O(n)</td><td>256 → 6000</td><td>23x</td></tr><tr><td><strong>Activation Memory</strong></td><td>O(n)</td><td>256 → 6000</td><td>23x</td></tr></tbody></table><h4 id=detailed-memory-calculations>Detailed Memory Calculations</h4><p><strong>1. Attention Memory (Quadratic Scaling)</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Memory for attention scores</span>
</span></span><span style=display:flex><span>attention_memory <span style=color:#f92672>=</span> batch_size <span style=color:#960050;background-color:#1e0010>×</span> num_heads <span style=color:#960050;background-color:#1e0010>×</span> seq_length²
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Your case:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 256 tokens: 1 × 32 × 256² = 2,097,152 elements</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 6000 tokens: 1 × 32 × 6000² = 1,152,000,000 elements</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Increase: 549x more memory for attention alone!</span>
</span></span></code></pre></div><p><strong>2. Gradient Memory (Linear Scaling)</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Memory for gradients</span>
</span></span><span style=display:flex><span>gradient_memory <span style=color:#f92672>=</span> batch_size <span style=color:#960050;background-color:#1e0010>×</span> seq_length <span style=color:#960050;background-color:#1e0010>×</span> hidden_size
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Your case (assuming hidden_size=2048):</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 256 tokens: 1 × 256 × 2048 = 524,288 elements</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 6000 tokens: 1 × 6000 × 2048 = 12,288,000 elements  </span>
</span></span><span style=display:flex><span><span style=color:#75715e># Increase: 23x more memory for gradients</span>
</span></span></code></pre></div><p><strong>3. Activation Memory (Linear Scaling)</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Memory for activations (per layer)</span>
</span></span><span style=display:flex><span>activation_memory <span style=color:#f92672>=</span> batch_size <span style=color:#960050;background-color:#1e0010>×</span> seq_length <span style=color:#960050;background-color:#1e0010>×</span> hidden_size <span style=color:#960050;background-color:#1e0010>×</span> num_layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Your case (assuming 24 layers):</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 256 tokens: 1 × 256 × 2048 × 24 = 12,582,912 elements</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 6000 tokens: 1 × 6000 × 2048 × 24 = 294,912,000 elements</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Increase: 23x more memory for activations</span>
</span></span></code></pre></div><h3 id=real-vram-usage-example>Real VRAM Usage Example</h3><table><thead><tr><th>Context Length</th><th>Base Memory</th><th>Attention Scaling</th><th>Estimated VRAM</th></tr></thead><tbody><tr><td>256 tokens</td><td>4GB</td><td>1x</td><td>~4GB</td></tr><tr><td>1024 tokens</td><td>4GB</td><td>16x</td><td>~16GB</td></tr><tr><td>2048 tokens</td><td>4GB</td><td>64x</td><td>~32GB</td></tr><tr><td>6000 tokens</td><td>4GB</td><td>549x</td><td>~658GB (attention alone!)</td></tr></tbody></table><h3 id=memory-optimization-strategies>Memory Optimization Strategies</h3><table><thead><tr><th>Strategy</th><th>Implementation</th><th>Memory Reduction</th></tr></thead><tbody><tr><td><strong>Gradient Checkpointing</strong></td><td><code>use_gradient_checkpointing=True</code></td><td>30-50%</td></tr><tr><td><strong>Mixed Precision</strong></td><td><code>fp16=True</code></td><td>50%</td></tr><tr><td><strong>Smaller Batch Size</strong></td><td><code>per_device_train_batch_size=1</code></td><td>75%</td></tr><tr><td><strong>Gradient Accumulation</strong></td><td><code>gradient_accumulation_steps=8</code></td><td>Maintains effective batch size</td></tr><tr><td><strong>DeepSpeed ZeRO</strong></td><td>ZeRO-2/ZeRO-3 partitioning</td><td>2-8x reduction</td></tr></tbody></table><hr><h2 id=parameter-efficient-fine-tuning-peft>Parameter Efficient Fine-Tuning (PEFT)</h2><h3 id=what-is-peft>What is PEFT?</h3><p><strong>PEFT</strong> stands for <strong>Parameter Efficient Fine-Tuning</strong> - it&rsquo;s both a concept and a framework (library) by Hugging Face that addresses the challenges of traditional full fine-tuning.</p><h3 id=the-problem-peft-solves>The Problem PEFT Solves</h3><table><thead><tr><th>Approach</th><th>Parameters Updated</th><th>Memory Required</th><th>Training Time</th></tr></thead><tbody><tr><td><strong>Traditional Full Fine-tuning</strong></td><td>1.7B (100%)</td><td>Massive</td><td>Very long</td></tr><tr><td><strong>PEFT Approach</strong></td><td>~5M (0.3%)</td><td>Much less</td><td>Much faster</td></tr></tbody></table><h3 id=how-peft-works>How PEFT Works</h3><p>Instead of updating all model parameters, PEFT methods:</p><ol><li><strong>Freeze the base model</strong> (no gradients computed for original weights)</li><li><strong>Add small trainable modules</strong> (adapters, low-rank matrices, etc.)</li><li><strong>Train only the small modules</strong> (1-10% of original parameters)</li></ol><h3 id=peft-techniques-overview>PEFT Techniques Overview</h3><table><thead><tr><th>Technique</th><th>Description</th><th>Key Advantage</th></tr></thead><tbody><tr><td><strong>LoRA</strong></td><td>Low-Rank Adaptation</td><td>Most popular, proven effective</td></tr><tr><td><strong>AdaLoRA</strong></td><td>Adaptive LoRA</td><td>Dynamic rank adjustment</td></tr><tr><td><strong>Prefix Tuning</strong></td><td>Trainable prefix tokens</td><td>Good for generation tasks</td></tr><tr><td><strong>P-Tuning v2</strong></td><td>Trainable prompt embeddings</td><td>Strong for understanding tasks</td></tr><tr><td><strong>IA³</strong></td><td>Infused Adapter</td><td>Minimal parameters added</td></tr></tbody></table><h3 id=peft-framework-relationship>PEFT Framework Relationship</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Conceptual hierarchy</span>
</span></span><span style=display:flex><span>PEFT_Framework <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Library&#34;</span>: <span style=color:#e6db74>&#34;Hugging Face PEFT&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Methods&#34;</span>: [
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;LoRA (Low-rank adaptation)&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;QLoRA (LoRA + quantization)&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;AdaLoRA (Adaptive LoRA)&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;Prefix Tuning (Trainable prefix tokens)&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;P-Tuning v2 (Trainable prompt embeddings)&#34;</span>
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># In practice:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># - PEFT is the library/framework</span>
</span></span><span style=display:flex><span><span style=color:#75715e># - LoRA is a method within PEFT</span>
</span></span><span style=display:flex><span><span style=color:#75715e># - QLoRA is LoRA + quantization</span>
</span></span></code></pre></div><hr><h2 id=lora-low-rank-adaptation>LoRA (Low-Rank Adaptation)</h2><h3 id=what-is-lora>What is LoRA?</h3><p><strong>LoRA</strong> is a specific PEFT technique that adds small trainable matrices to existing model layers without modifying the original weights.</p><h3 id=the-core-mathematical-concept>The Core Mathematical Concept</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Original weight matrix</span>
</span></span><span style=display:flex><span>W <span style=color:#f92672>=</span> [<span style=color:#ae81ff>4096</span> x <span style=color:#ae81ff>4096</span>]  <span style=color:#75715e># Large matrix in transformer</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># LoRA decomposes updates as:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># W_new = W + ΔW</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ΔW = A × B  (where A is 4096×r and B is r×4096)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># r = rank (typically 4, 8, 16, 32)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Parameter comparison:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Full update: 4096² = 16M parameters</span>
</span></span><span style=display:flex><span><span style=color:#75715e># LoRA update: 4096×r + r×4096 = 8192×r parameters</span>
</span></span><span style=display:flex><span><span style=color:#75715e># For r=16: only 131K parameters vs 16M!</span>
</span></span></code></pre></div><h3 id=lora-key-parameters>LoRA Key Parameters</h3><table><thead><tr><th>Parameter</th><th>Description</th><th>Typical Values</th><th>Impact</th></tr></thead><tbody><tr><td><strong>r (rank)</strong></td><td>Size of adaptation matrices</td><td>4, 8, 16, 32</td><td>Higher = more capacity, more memory</td></tr><tr><td><strong>lora_alpha</strong></td><td>Scaling factor for LoRA updates</td><td>16, 32, 64</td><td>Controls adaptation strength</td></tr><tr><td><strong>target_modules</strong></td><td>Which layers to adapt</td><td>[&ldquo;q_proj&rdquo;, &ldquo;v_proj&rdquo;, &ldquo;k_proj&rdquo;, &ldquo;o_proj&rdquo;]</td><td>More modules = better adaptation</td></tr><tr><td><strong>lora_dropout</strong></td><td>Dropout for LoRA layers</td><td>0.1, 0.05</td><td>Prevents overfitting</td></tr></tbody></table><h3 id=lora-implementation-example>LoRA Implementation Example</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> peft <span style=color:#f92672>import</span> LoraConfig, get_peft_model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># LoRA configuration</span>
</span></span><span style=display:flex><span>lora_config <span style=color:#f92672>=</span> LoraConfig(
</span></span><span style=display:flex><span>    r<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>,                    <span style=color:#75715e># Rank of adaptation</span>
</span></span><span style=display:flex><span>    lora_alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>,          <span style=color:#75715e># LoRA scaling parameter</span>
</span></span><span style=display:flex><span>    target_modules<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;q_proj&#34;</span>, <span style=color:#e6db74>&#34;v_proj&#34;</span>, <span style=color:#e6db74>&#34;k_proj&#34;</span>, <span style=color:#e6db74>&#34;o_proj&#34;</span>],  <span style=color:#75715e># Which layers to adapt</span>
</span></span><span style=display:flex><span>    lora_dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,       <span style=color:#75715e># Dropout for LoRA layers</span>
</span></span><span style=display:flex><span>    bias<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;none&#34;</span>,            <span style=color:#75715e># Whether to adapt bias</span>
</span></span><span style=display:flex><span>    task_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;CAUSAL_LM&#34;</span>   <span style=color:#75715e># Type of task</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Apply LoRA to model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> get_peft_model(model, lora_config)
</span></span></code></pre></div><h3 id=lora-vs-full-fine-tuning-comparison>LoRA vs Full Fine-tuning Comparison</h3><table><thead><tr><th>Aspect</th><th>Full Fine-tuning</th><th>LoRA</th></tr></thead><tbody><tr><td><strong>Trainable Parameters</strong></td><td>1.7B (100%)</td><td>5M (~0.3%)</td></tr><tr><td><strong>Memory Usage</strong></td><td>High</td><td>Low</td></tr><tr><td><strong>Training Time</strong></td><td>Long</td><td>2-3x faster</td></tr><tr><td><strong>Adaptation Quality</strong></td><td>Best</td><td>Good (80-95% of full)</td></tr><tr><td><strong>Multiple Tasks</strong></td><td>Need separate models</td><td>Multiple adapters possible</td></tr></tbody></table><hr><h2 id=qlora-quantized-lora>QLoRA (Quantized LoRA)</h2><h3 id=what-is-qlora>What is QLoRA?</h3><p><strong>QLoRA</strong> combines LoRA with quantization - it applies LoRA adapters to a quantized base model, giving you the benefits of both techniques.</p><h3 id=the-qlora-architecture>The QLoRA Architecture</h3><table><thead><tr><th>Component</th><th>Precision</th><th>Status</th><th>Purpose</th></tr></thead><tbody><tr><td><strong>Base Model</strong></td><td>4-bit</td><td>Frozen</td><td>Memory efficiency</td></tr><tr><td><strong>LoRA Adapters</strong></td><td>16-bit</td><td>Trainable</td><td>Maintain adaptation quality</td></tr><tr><td><strong>Gradients</strong></td><td>16-bit+</td><td>Computed</td><td>Stable training</td></tr></tbody></table><h3 id=qlora-process>QLoRA Process</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Step 1: Quantize base model to 4-bit</span>
</span></span><span style=display:flex><span>base_model <span style=color:#f92672>=</span> load_model_in_4bit(model_name)  <span style=color:#75715e># 1.7B → 425MB</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 2: Add LoRA adapters (in 16-bit)</span>
</span></span><span style=display:flex><span>lora_adapters <span style=color:#f92672>=</span> LoraConfig(r<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>, <span style=color:#f92672>...</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 3: Train only the adapters</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Base model: 4-bit (frozen)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Adapters: 16-bit (trainable)</span>
</span></span></code></pre></div><h3 id=qlora-implementation>QLoRA Implementation</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BitsAndBytesConfig
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> peft <span style=color:#f92672>import</span> LoraConfig, prepare_model_for_kbit_training
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 4-bit quantization configuration</span>
</span></span><span style=display:flex><span>quantization_config <span style=color:#f92672>=</span> BitsAndBytesConfig(
</span></span><span style=display:flex><span>    load_in_4bit<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    bnb_4bit_use_double_quant<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    bnb_4bit_quant_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;nf4&#34;</span>,
</span></span><span style=display:flex><span>    bnb_4bit_compute_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>bfloat16
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load quantized model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    model_name,
</span></span><span style=display:flex><span>    quantization_config<span style=color:#f92672>=</span>quantization_config,
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Prepare for k-bit training</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> prepare_model_for_kbit_training(model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Add LoRA adapters</span>
</span></span><span style=display:flex><span>lora_config <span style=color:#f92672>=</span> LoraConfig(r<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>, lora_alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>, target_modules<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;q_proj&#34;</span>, <span style=color:#e6db74>&#34;v_proj&#34;</span>])
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> get_peft_model(model, lora_config)
</span></span></code></pre></div><h3 id=qlora-memory-benefits>QLoRA Memory Benefits</h3><table><thead><tr><th>Model Size</th><th>Full Fine-tuning</th><th>LoRA</th><th>QLoRA</th></tr></thead><tbody><tr><td><strong>1.7B</strong></td><td>~7GB</td><td>~4GB</td><td>~1GB</td></tr><tr><td><strong>7B</strong></td><td>~28GB</td><td>~14GB</td><td>~3GB</td></tr><tr><td><strong>13B</strong></td><td>~52GB</td><td>~26GB</td><td>~6GB</td></tr></tbody></table><hr><h2 id=quantization-deep-dive>Quantization Deep Dive</h2><h3 id=what-is-quantization>What is Quantization?</h3><p>Quantization reduces the precision (number of bits) used to represent model weights and activations, trading some accuracy for significant memory and speed improvements.</p><h3 id=precision-levels-and-memory-impact>Precision Levels and Memory Impact</h3><table><thead><tr><th>Precision</th><th>Bits per Parameter</th><th>1.7B Model Memory</th><th>Use Case</th></tr></thead><tbody><tr><td><strong>FP32</strong></td><td>32 bits</td><td>6.8GB</td><td>Research, maximum accuracy</td></tr><tr><td><strong>FP16</strong></td><td>16 bits</td><td>3.4GB</td><td>Standard training</td></tr><tr><td><strong>INT8</strong></td><td>8 bits</td><td>1.7GB</td><td>Efficient inference</td></tr><tr><td><strong>INT4</strong></td><td>4 bits</td><td>0.85GB</td><td>Edge deployment</td></tr></tbody></table><h3 id=types-of-quantization>Types of Quantization</h3><h4 id=1-post-training-quantization-ptq>1. Post-Training Quantization (PTQ)</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Convert trained model to lower precision</span>
</span></span><span style=display:flex><span>model_fp32 <span style=color:#f92672>=</span> load_model()  <span style=color:#75715e># Original 32-bit model</span>
</span></span><span style=display:flex><span>model_int8 <span style=color:#f92672>=</span> quantize_model(model_fp32)  <span style=color:#75715e># Convert to 8-bit</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Pros: Fast, no retraining needed</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Cons: Some accuracy loss</span>
</span></span></code></pre></div><h4 id=2-quantization-aware-training-qat>2. Quantization-Aware Training (QAT)</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Train model with quantization in mind</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> create_quantized_model()  <span style=color:#75715e># Model designed for quantization</span>
</span></span><span style=display:flex><span>train_with_quantization(model)    <span style=color:#75715e># Training process includes quantization</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Pros: Better accuracy retention</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Cons: More complex, longer training</span>
</span></span></code></pre></div><h4 id=3-dynamic-quantization>3. Dynamic Quantization</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Quantize activations during inference</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>quantization<span style=color:#f92672>.</span>quantize_dynamic(
</span></span><span style=display:flex><span>    model, 
</span></span><span style=display:flex><span>    {torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Linear}, 
</span></span><span style=display:flex><span>    dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>qint8
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Pros: No calibration needed</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Cons: Only quantizes weights, not activations</span>
</span></span></code></pre></div><h3 id=when-to-use-quantization>When to Use Quantization</h3><table><thead><tr><th>Scenario</th><th>Recommendation</th></tr></thead><tbody><tr><td><strong>Memory constraints exist</strong></td><td>✅ Use quantization</td></tr><tr><td><strong>Inference speed priority</strong></td><td>✅ Use quantization</td></tr><tr><td><strong>Edge device deployment</strong></td><td>✅ Use quantization</td></tr><tr><td><strong>Acceptable 1-3% accuracy loss</strong></td><td>✅ Use quantization</td></tr><tr><td><strong>Maximum accuracy required</strong></td><td>❌ Avoid quantization</td></tr><tr><td><strong>Abundant compute resources</strong></td><td>❌ May not be necessary</td></tr></tbody></table><hr><h2 id=knowledge-distillation>Knowledge Distillation</h2><h3 id=what-is-knowledge-distillation>What is Knowledge Distillation?</h3><p><strong>Knowledge Distillation</strong> is a technique where a smaller &ldquo;student&rdquo; model learns from a larger &ldquo;teacher&rdquo; model, allowing the student to achieve better performance than training from scratch.</p><h3 id=the-distillation-process>The Distillation Process</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Teacher model (large, pre-trained)</span>
</span></span><span style=display:flex><span>teacher_model <span style=color:#f92672>=</span> load_large_model()  <span style=color:#75715e># e.g., GPT-4, 175B parameters</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Student model (small, to be trained)</span>
</span></span><span style=display:flex><span>student_model <span style=color:#f92672>=</span> create_small_model()  <span style=color:#75715e># e.g., 1.7B parameters</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Distillation process:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 1. Teacher generates &#34;soft&#34; predictions (probabilities)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 2. Student learns to match teacher&#39;s predictions</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 3. Student also learns from ground truth data</span>
</span></span></code></pre></div><h3 id=types-of-distillation>Types of Distillation</h3><table><thead><tr><th>Type</th><th>What&rsquo;s Transferred</th><th>Best For</th></tr></thead><tbody><tr><td><strong>Response Distillation</strong></td><td>Output probabilities</td><td>General performance</td></tr><tr><td><strong>Feature Distillation</strong></td><td>Internal representations</td><td>Deep understanding</td></tr><tr><td><strong>Attention Distillation</strong></td><td>Attention patterns</td><td>Interpretability</td></tr></tbody></table><h4 id=response-distillation-implementation>Response Distillation Implementation</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Teacher generates responses</span>
</span></span><span style=display:flex><span>teacher_outputs <span style=color:#f92672>=</span> teacher_model(input_text)
</span></span><span style=display:flex><span>teacher_probabilities <span style=color:#f92672>=</span> softmax(teacher_outputs <span style=color:#f92672>/</span> temperature)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Student learns to match teacher&#39;s probability distribution</span>
</span></span><span style=display:flex><span>student_outputs <span style=color:#f92672>=</span> student_model(input_text)
</span></span><span style=display:flex><span>distillation_loss <span style=color:#f92672>=</span> KL_divergence(student_outputs, teacher_probabilities)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Total loss combines distillation + ground truth</span>
</span></span><span style=display:flex><span>total_loss <span style=color:#f92672>=</span> α <span style=color:#f92672>*</span> distillation_loss <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>α) <span style=color:#f92672>*</span> ground_truth_loss
</span></span></code></pre></div><h3 id=when-to-use-distillation>When to Use Distillation</h3><table><thead><tr><th>Scenario</th><th>Recommendation</th></tr></thead><tbody><tr><td><strong>Model size constraints</strong></td><td>✅ Use distillation</td></tr><tr><td><strong>Deployment speed critical</strong></td><td>✅ Use distillation</td></tr><tr><td><strong>Limited compute resources</strong></td><td>✅ Use distillation</td></tr><tr><td><strong>Have unlabeled data</strong></td><td>✅ Use distillation</td></tr><tr><td><strong>Maximum accuracy required</strong></td><td>❌ Use larger model</td></tr><tr><td><strong>No good teacher available</strong></td><td>❌ Train from scratch</td></tr></tbody></table><h3 id=distillation-implementation-example>Distillation Implementation Example</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DistillationTrainer</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, teacher_model, student_model, temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>3.0</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>teacher <span style=color:#f92672>=</span> teacher_model
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>student <span style=color:#f92672>=</span> student_model
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>temperature <span style=color:#f92672>=</span> temperature
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>alpha <span style=color:#f92672>=</span> alpha
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>distillation_loss</span>(self, student_logits, teacher_logits, labels):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Soft targets from teacher</span>
</span></span><span style=display:flex><span>        teacher_probs <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(teacher_logits <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>temperature, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        student_log_probs <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>log_softmax(student_logits <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>temperature, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Distillation loss</span>
</span></span><span style=display:flex><span>        distill_loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>kl_div(student_log_probs, teacher_probs, reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;batchmean&#39;</span>)
</span></span><span style=display:flex><span>        distill_loss <span style=color:#f92672>*=</span> (self<span style=color:#f92672>.</span>temperature <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Hard targets (ground truth)</span>
</span></span><span style=display:flex><span>        hard_loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>cross_entropy(student_logits, labels)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Combined loss</span>
</span></span><span style=display:flex><span>        total_loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>alpha <span style=color:#f92672>*</span> distill_loss <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>alpha) <span style=color:#f92672>*</span> hard_loss
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> total_loss
</span></span></code></pre></div><hr><h2 id=unsloth-optimizations>Unsloth Optimizations</h2><h3 id=what-unsloth-provides>What Unsloth Provides</h3><p><strong>Unsloth</strong> is a specialized library that provides kernel-level optimizations for memory-efficient and fast language model training.</p><h3 id=key-unsloth-features>Key Unsloth Features</h3><table><thead><tr><th>Feature</th><th>Benefit</th><th>Implementation</th></tr></thead><tbody><tr><td><strong>Custom CUDA Kernels</strong></td><td>2x faster training</td><td>Optimized low-level operations</td></tr><tr><td><strong>Flash Attention</strong></td><td>O(n) memory instead of O(n²)</td><td>Memory-efficient attention</td></tr><tr><td><strong>Gradient Checkpointing</strong></td><td>30-50% memory reduction</td><td>Trade compute for memory</td></tr><tr><td><strong>Optimized Kernels</strong></td><td>Better GPU utilization</td><td>Custom implementations</td></tr></tbody></table><h3 id=memory-and-speed-improvements>Memory and Speed Improvements</h3><table><thead><tr><th>Approach</th><th>Memory Usage</th><th>Training Speed</th><th>Quality</th></tr></thead><tbody><tr><td><strong>Standard Training</strong></td><td>100%</td><td>1x</td><td>Baseline</td></tr><tr><td><strong>Standard + LoRA</strong></td><td>50%</td><td>2x</td><td>Good</td></tr><tr><td><strong>Standard + QLoRA</strong></td><td>25%</td><td>1.5x</td><td>Good</td></tr><tr><td><strong>Unsloth + QLoRA</strong></td><td>12.5%</td><td>3x</td><td>Good</td></tr></tbody></table><h3 id=unsloth-implementation>Unsloth Implementation</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Install Unsloth</span>
</span></span><span style=display:flex><span><span style=color:#75715e># pip install unsloth</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> unsloth <span style=color:#f92672>import</span> FastLanguageModel
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load model with Unsloth optimizations</span>
</span></span><span style=display:flex><span>model, tokenizer <span style=color:#f92672>=</span> FastLanguageModel<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    model_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Qwen/Qwen2.5-1.5B-Instruct&#34;</span>,
</span></span><span style=display:flex><span>    max_seq_length <span style=color:#f92672>=</span> <span style=color:#ae81ff>6000</span>,  <span style=color:#75715e># Your desired context length</span>
</span></span><span style=display:flex><span>    dtype <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>,           <span style=color:#75715e># Auto-detect</span>
</span></span><span style=display:flex><span>    load_in_4bit <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>,    <span style=color:#75715e># Use 4-bit quantization</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Add LoRA adapters</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> FastLanguageModel<span style=color:#f92672>.</span>get_peft_model(
</span></span><span style=display:flex><span>    model,
</span></span><span style=display:flex><span>    r <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>,
</span></span><span style=display:flex><span>    target_modules <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;q_proj&#34;</span>, <span style=color:#e6db74>&#34;k_proj&#34;</span>, <span style=color:#e6db74>&#34;v_proj&#34;</span>, <span style=color:#e6db74>&#34;o_proj&#34;</span>],
</span></span><span style=display:flex><span>    lora_alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>,
</span></span><span style=display:flex><span>    lora_dropout <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>    bias <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;none&#34;</span>,
</span></span><span style=display:flex><span>    use_gradient_checkpointing <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    random_state <span style=color:#f92672>=</span> <span style=color:#ae81ff>3407</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Training with Unsloth</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> trl <span style=color:#f92672>import</span> SFTTrainer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TrainingArguments
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> SFTTrainer(
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> model,
</span></span><span style=display:flex><span>    tokenizer <span style=color:#f92672>=</span> tokenizer,
</span></span><span style=display:flex><span>    train_dataset <span style=color:#f92672>=</span> dataset,
</span></span><span style=display:flex><span>    dataset_text_field <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;text&#34;</span>,
</span></span><span style=display:flex><span>    max_seq_length <span style=color:#f92672>=</span> <span style=color:#ae81ff>6000</span>,
</span></span><span style=display:flex><span>    args <span style=color:#f92672>=</span> TrainingArguments(
</span></span><span style=display:flex><span>        per_device_train_batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        gradient_accumulation_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        warmup_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>,
</span></span><span style=display:flex><span>        num_train_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        learning_rate <span style=color:#f92672>=</span> <span style=color:#ae81ff>2e-4</span>,
</span></span><span style=display:flex><span>        fp16 <span style=color:#f92672>=</span> <span style=color:#f92672>not</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_bf16_supported(),
</span></span><span style=display:flex><span>        bf16 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_bf16_supported(),
</span></span><span style=display:flex><span>        logging_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        optim <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;adamw_8bit&#34;</span>,
</span></span><span style=display:flex><span>        weight_decay <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>,
</span></span><span style=display:flex><span>        lr_scheduler_type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;linear&#34;</span>,
</span></span><span style=display:flex><span>        seed <span style=color:#f92672>=</span> <span style=color:#ae81ff>3407</span>,
</span></span><span style=display:flex><span>        output_dir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;outputs&#34;</span>,
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><hr><h2 id=training-approaches-comparison>Training Approaches Comparison</h2><h3 id=should-you-use-sft-or-peft-on-quantized-models>Should You Use SFT or PEFT on Quantized Models?</h3><h4 id=full-fine-tuning-on-quantized-models>Full Fine-tuning on Quantized Models</h4><p><strong>Advantages:</strong></p><ul><li>Significant memory reduction (4x-8x less)</li><li>Faster training due to reduced precision</li><li>Can train larger models on smaller hardware</li><li>Lower cloud computing costs</li></ul><p><strong>Disadvantages:</strong></p><ul><li>Quantization introduces rounding errors</li><li>Lower precision can cause training instability</li><li>Final model may perform worse</li><li>Harder to tune hyperparameters</li></ul><h4 id=peft-on-quantized-models-qlora---recommended>PEFT on Quantized Models (QLoRA) - <strong>Recommended</strong></h4><p><strong>Why QLoRA is the Best Approach:</strong></p><ul><li>Combines benefits of quantization and parameter efficiency</li><li>Extensively validated in research</li><li>Stable training (adapters in FP16, base model in INT4)</li><li>Often matches full fine-tuning results</li><li>Proven technique with strong community support</li></ul><h3 id=complete-training-approaches-comparison>Complete Training Approaches Comparison</h3><table><thead><tr><th>Approach</th><th>Memory (1.7B)</th><th>Trainable Params</th><th>Training Time</th><th>Quality</th></tr></thead><tbody><tr><td><strong>Full Fine-tuning (FP32)</strong></td><td>~7GB</td><td>1.7B (100%)</td><td>Baseline</td><td>Best</td></tr><tr><td><strong>Full Fine-tuning (FP16)</strong></td><td>~3.5GB</td><td>1.7B (100%)</td><td>Baseline</td><td>Best</td></tr><tr><td><strong>LoRA</strong></td><td>~2GB</td><td>~5M (0.3%)</td><td>2x faster</td><td>Good (80-95%)</td></tr><tr><td><strong>QLoRA</strong></td><td>~1GB</td><td>~5M (0.3%)</td><td>1.5x faster</td><td>Good (80-95%)</td></tr><tr><td><strong>Unsloth + QLoRA</strong></td><td>~0.5GB</td><td>~5M (0.3%)</td><td>3x faster</td><td>Good (80-95%)</td></tr></tbody></table><h3 id=context-length-memory-scaling>Context Length Memory Scaling</h3><table><thead><tr><th>Context Length</th><th>Memory Multiplier</th><th>Practical Limit</th></tr></thead><tbody><tr><td><strong>256 tokens</strong></td><td>1x</td><td>Base usage</td></tr><tr><td><strong>1024 tokens</strong></td><td>4x</td><td>Manageable</td></tr><tr><td><strong>2048 tokens</strong></td><td>16x</td><td>Challenging</td></tr><tr><td><strong>4096 tokens</strong></td><td>64x</td><td>Requires optimization</td></tr><tr><td><strong>6000 tokens</strong></td><td>140x</td><td>Needs advanced techniques</td></tr></tbody></table><h3 id=decision-framework>Decision Framework</h3><table><thead><tr><th>Resource Level</th><th>Recommended Approach</th></tr></thead><tbody><tr><td><strong>Unlimited Memory</strong></td><td>Full Fine-tuning</td></tr><tr><td><strong>Good Memory</strong></td><td>LoRA</td></tr><tr><td><strong>Limited Memory</strong></td><td>QLoRA</td></tr><tr><td><strong>Severely Constrained</strong></td><td>Unsloth + QLoRA</td></tr></tbody></table><h3 id=converting-your-current-setup>Converting Your Current Setup</h3><h4 id=current-approach-full-fine-tuning>Current Approach (Full Fine-tuning)</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Your current approach - ALL parameters get updated</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    model_name,
</span></span><span style=display:flex><span>    torch_dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>to(device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> SFTTrainer(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model,  <span style=color:#75715e># Full model is trainable</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... rest of config</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h4 id=recommended-qlora-approach>Recommended QLoRA Approach</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> peft <span style=color:#f92672>import</span> LoraConfig, get_peft_model, prepare_model_for_kbit_training
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 1: Quantization configuration</span>
</span></span><span style=display:flex><span>quantization_config <span style=color:#f92672>=</span> BitsAndBytesConfig(
</span></span><span style=display:flex><span>    load_in_4bit<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    bnb_4bit_use_double_quant<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    bnb_4bit_quant_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;nf4&#34;</span>,
</span></span><span style=display:flex><span>    bnb_4bit_compute_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>bfloat16
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 2: Load quantized model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    model_name,
</span></span><span style=display:flex><span>    quantization_config<span style=color:#f92672>=</span>quantization_config,
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 3: Prepare for k-bit training</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> prepare_model_for_kbit_training(model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 4: LoRA configuration</span>
</span></span><span style=display:flex><span>lora_config <span style=color:#f92672>=</span> LoraConfig(
</span></span><span style=display:flex><span>    r<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>,
</span></span><span style=display:flex><span>    lora_alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>,
</span></span><span style=display:flex><span>    target_modules<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;q_proj&#34;</span>, <span style=color:#e6db74>&#34;v_proj&#34;</span>, <span style=color:#e6db74>&#34;k_proj&#34;</span>, <span style=color:#e6db74>&#34;o_proj&#34;</span>],
</span></span><span style=display:flex><span>    lora_dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span>    bias<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;none&#34;</span>,
</span></span><span style=display:flex><span>    task_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;CAUSAL_LM&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 5: Apply LoRA</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> get_peft_model(model, lora_config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Step 6: Training (same as before)</span>
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> SFTTrainer(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model,
</span></span><span style=display:flex><span>    <span style=color:#75715e># ... rest of config remains the same</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><hr><h2 id=summary-and-best-practices>Summary and Best Practices</h2><h3 id=key-takeaways>Key Takeaways</h3><ol><li><strong>Context Size</strong>: Determines how much text your model can process - longer contexts need exponentially more memory</li><li><strong>Truncation</strong>: Automatically cuts off excess tokens - choose your strategy wisely</li><li><strong>PEFT</strong>: Efficient fine-tuning by training only small adapter modules</li><li><strong>LoRA</strong>: Most popular PEFT method using low-rank matrix decomposition</li><li><strong>QLoRA</strong>: Combines quantization with LoRA for maximum efficiency</li><li><strong>Quantization</strong>: Reduces model precision to save memory and increase speed</li><li><strong>Distillation</strong>: Transfers knowledge from large models to smaller ones</li><li><strong>Unsloth</strong>: Provides kernel-level optimizations for faster training</li></ol><h3 id=recommended-workflow>Recommended Workflow</h3><ol><li><strong>Start with QLoRA</strong>: Best balance of efficiency and performance</li><li><strong>Use Unsloth</strong>: If you need maximum speed and memory efficiency</li><li><strong>Consider Distillation</strong>: If you have access to a powerful teacher model</li><li><strong>Monitor Memory</strong>: Use gradient checkpointing and mixed precision</li><li><strong>Adjust Context Length</strong>: Balance between context and memory requirements</li></ol><p>This comprehensive guide covers all the essential concepts for efficient GenAI model training and optimization. Each technique has its place depending on your specific requirements and constraints.</p></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>