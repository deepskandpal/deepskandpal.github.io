<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul></nav></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#the-coffee-shop-analogy--os-concepts>The Coffee Shop Analogy & OS Concepts</a></li><li><a href=#why-didnt-your-attempts-work-for-sdxl>Why Didn&rsquo;t Your Attempts Work for SDXL?</a></li><li><a href=#key-takeaways--what-to-do>Key Takeaways & What To Do</a></li></ul></li></ul></li></ul></nav></nav></aside><article class="post-single page-concurrency-ds"><h1>Multi Processing , Multi Threading, AsyncIO: A Guide to Python Concurrency for Data Scientists</h1><span class=reading-time><em>4 min read</em></span><div class=post-content><p><strong>The problem : Help! My Python SDXL Script Isn&rsquo;t Faster with Asyncio/Threading/Multiprocessing. Why?</strong></p><p>You&rsquo;ve built a cool script, maybe generating image variations with SDXL (<a href=https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0>Stable Diffusion XL model</a>) like one of our engineers. It works, but it&rsquo;s slow. You think, &ldquo;I know! Parallelism!&rdquo; You try asyncio, then multithreading, maybe even multiprocessing. But&mldr; nothing speeds up significantly, or you just hit weird errors, especially in your Jupyter Notebook. Sounds familiar?</p><p>This is a common hurdle when data science tasks meet heavier computation. Let&rsquo;s demystify Python&rsquo;s asyncio, multithreading, and multiprocessing, touching on the underlying Operating System (OS) ideas and Python&rsquo;s infamous GIL.</p><h3 id=the-coffee-shop-analogy--os-concepts>The Coffee Shop Analogy & OS Concepts</h3><p>Imagine you need to make multiple coffees:</p><ul><li><ol><li>Normal Python (Single Worker): One barista makes one coffee start-to-finish, then the next. This is like a standard single process, single thread application. Simple, sequential.</li></ol></li><li><ol start=2><li>AsyncIO (Efficient Single Barista): One barista starts brewing espresso (await), and while it&rsquo;s brewing (an operation it doesn&rsquo;t need to watch constantly), immediately starts steaming milk (await) for the same coffee.<ul><li>OS Concept: Relies on non-blocking I/O. Instead of waiting idly, the task tells the OS &ldquo;let me know when this network/disk operation is done&rdquo; and yields control. An event loop manages these pending operations, running code only when needed. It all happens within a single thread and single process.</li><li>Good for: Tasks with lots of waiting (I/O-bound).</li><li>Bad for: Tasks needing constant active work (CPU/GPU-bound). Still only one &ldquo;worker&rdquo; doing the active computation at any moment.</li></ul></li></ol></li><li><ol start=3><li>Multithreading (Multiple Baristas, ONE Espresso Machine): Multiple baristas share one counter and one main espresso machine. Only one can use the machine at a time.<ul><li>OS Concept: Threads are lightweight execution units within a single process. They share the same memory space. The OS rapidly switches between threads (context switching), giving an illusion of parallelism.</li><li>Python&rsquo;s Catch (The GIL): In standard Python (CPython), the Global Interpreter Lock (GIL) acts like that single espresso machine. It allows only one thread to execute Python bytecode at any given time, even on multi-core CPUs.</li><li>Good for: I/O-bound tasks where threads spend time waiting (releasing the GIL), letting others run Python code.</li><li>Bad for: CPU-bound Python code, as the GIL prevents true parallel Python execution.</li></ul></li></ol></li><li><ol start=4><li>Multiprocessing (Multiple Full Coffee Stations): Several independent stations, each with its own barista and espresso machine. They work truly in parallel.<ul><li>OS Concept: Processes are independent instances of a program, each with its own separate memory space and resources. The OS schedules these processes across different CPU cores for true hardware parallelism.</li><li>Python&rsquo;s Advantage: Since each process has its own memory and Python interpreter, each has its own GIL. They don&rsquo;t block each other&rsquo;s Python execution.</li><li>Good for: CPU-bound tasks that can be run independently.</li><li>Bad for: Tasks needing lots of communication between processes (requires slower Inter-Process Communication - IPC) or where process creation overhead is significant.</li></ul></li></ol></li></ul><h3 id=why-didnt-your-attempts-work-for-sdxl>Why Didn&rsquo;t Your Attempts Work for SDXL?</h3><ul><li><ol><li>AsyncIO: Image generation is CPU/GPU-bound, not I/O-bound. There&rsquo;s no significant &ldquo;waiting&rdquo; for asyncio&rsquo;s non-blocking I/O magic to exploit. Result: No speedup.</li></ol></li><li><ol start=2><li>Multithreading: Image generation needs CPU/GPU work. The GIL prevented parallel Python execution. Even if underlying libraries release the GIL during GPU work, the Python coordination parts likely remained serialized. Result: No significant speedup.</li></ol></li><li><ol start=3><li>Multiprocessing: In Jupyter Notebook: Notebook state conflicts with the OS fork mechanism used to create processes. Result: Weird errors. In a .py Script: This should allow parallel Python execution across CPU cores. The limited speedup suggests:<ul><li>GPU Bottleneck: Most likely! Your multiple processes (running parallel Python on CPU cores) are probably queuing up to use the single GPU resource. The OS can schedule processes in parallel, but hardware contention limits throughput.</li><li>Overhead: Process creation, model loading per-process (heavy on RAM/VRAM), and data transfer (IPC) add costs.</li><li>Not Enough CPU Work: If the GPU does 95% of the work, parallelizing the CPU part has limited impact.</li></ul></li></ol></li></ul><h3 id=key-takeaways--what-to-do>Key Takeaways & What To Do</h3><p><em><strong>Match Tool to Task & OS Behavior:</strong></em></p><p>asyncio: Non-blocking I/O for waiting tasks.</p><p>multithreading: Shared memory, GIL bottleneck for CPU-bound Python, good for I/O waits.</p><p>multiprocessing: Separate memory, true CPU parallelism, best for heavy computation but watch for overhead and resource contention (like a single GPU).</p><p>For the SDXL Task:</p><ul><li><ol><li>Use .py scripts for multiprocessing.</li></ol></li><li><ol start=2><li>Profile! Check CPU and GPU utilization. Is the GPU the bottleneck?</li></ol></li><li><ol start=3><li>Manage Resources: Minimize per-process setup (model loading) and inter-process data transfer.</li></ol></li><li><ol start=4><li>Check Framework specifics: How does MLX handle concurrent access from multiple processes to the same GPU?</li></ol></li></ul><p>We will cover this in detail in coming articles.</p><p>Understanding how Python&rsquo;s concurrency tools interact with OS process/thread management, memory sharing, and the specific limitations like the GIL is crucial. Don&rsquo;t just apply parallelism blindly; diagnose the bottleneck and choose the strategy that addresses it effectively. Good luck!</p></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer></body></html>