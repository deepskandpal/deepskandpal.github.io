<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on 404EngineerNotFound</title><link>https://deepskandpal.github.io/categories/deep-learning/</link><description>Recent content in Deep Learning on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 30 Dec 2024 12:00:00 +0000</lastBuildDate><atom:link href="https://deepskandpal.github.io/categories/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective</title><link>https://deepskandpal.github.io/papershelf/read/wsd-river-valley-landscape/</link><pubDate>Mon, 30 Dec 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/read/wsd-river-valley-landscape/</guid><description>&lt;h2 id="paper">&lt;strong>Paper: &amp;ldquo;Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective&amp;rdquo;&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Authors:&lt;/strong> Kaiyue Wen, David Hall, Zhiyuan Li, Jason Wang, Percy Liang, Tengyu Ma (Stanford University &amp;amp; TTIC)&lt;/p>
&lt;h2 id="abstract--key-contributions">&lt;strong>Abstract &amp;amp; Key Contributions:&lt;/strong>&lt;/h2>
&lt;p>This paper provides a theoretical explanation for the effectiveness of the Warmup-Stable-Decay (WSD) learning rate schedule in language model pretraining. The key contributions include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>River Valley Loss Landscape Theory:&lt;/strong> Introduces a novel conceptual framework where the loss landscape resembles a deep valley with a river at its bottom&lt;/li>
&lt;li>&lt;strong>WSD Mechanism Explanation:&lt;/strong> Shows why WSD&amp;rsquo;s non-traditional loss curve (high loss during stable phase, sharp drop during decay) leads to superior performance&lt;/li>
&lt;li>&lt;strong>WSD-S Algorithm:&lt;/strong> Proposes WSD-Simplified, an improved variant that reuses checkpoints and maintains a single main branch&lt;/li>
&lt;/ul>
&lt;h2 id="1-the-wsd-learning-rate-schedule">&lt;strong>1. The WSD Learning Rate Schedule&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Traditional Problem:&lt;/strong>&lt;br>
Cosine learning rate schedules require pre-determining a fixed compute budget, making it difficult to adapt to additional compute or data.&lt;/p></description></item></channel></rss>