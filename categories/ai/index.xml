<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on 404EngineerNotFound</title><link>https://deepskandpal.github.io/categories/ai/</link><description>Recent content in AI on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 04 Jun 2025 23:24:48 +0530</lastBuildDate><atom:link href="https://deepskandpal.github.io/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Multi-Head Attention: Collaborate Instead of Concatenate</title><link>https://deepskandpal.github.io/papershelf/multihead-attention/</link><pubDate>Wed, 04 Jun 2025 23:24:48 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/multihead-attention/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title><link>https://deepskandpal.github.io/papershelf/flash-attention/</link><pubDate>Wed, 04 Jun 2025 23:24:37 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/flash-attention/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link>https://deepskandpal.github.io/papershelf/bert/</link><pubDate>Wed, 04 Jun 2025 23:24:31 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/bert/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Improving Language Understanding by Generative Pre-Training</title><link>https://deepskandpal.github.io/papershelf/gpt-1/</link><pubDate>Tue, 03 Jun 2025 11:01:51 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/gpt-1/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications</title><link>https://deepskandpal.github.io/bookshelf/design-ml-system/</link><pubDate>Thu, 22 May 2025 11:19:32 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/design-ml-system/</guid><description/></item><item><title>Reliable Machine Learning: Applying SRE Principles to ML in Productionl</title><link>https://deepskandpal.github.io/bookshelf/reliable-ml/</link><pubDate>Tue, 20 May 2025 13:49:22 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/reliable-ml/</guid><description>&lt;h1 id="main-notes-for-hands-on-ml">Main Notes for Hands-On ML&lt;/h1>
&lt;p>This book is a comprehensive guide to machine learning.&lt;/p></description></item><item><title>The Matrix Calculus You Need For Deep Learning</title><link>https://deepskandpal.github.io/papershelf/maths/</link><pubDate>Sun, 27 Apr 2025 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/maths/</guid><description>&lt;h1 id="the-big-picture-goal">&lt;strong>The Big Picture Goal:&lt;/strong>&lt;/h1>
&lt;p>We want our machine learning models to make good predictions. To do this, we define a &lt;strong>loss function&lt;/strong> (or cost function) that measures how &amp;ldquo;bad&amp;rdquo; our model&amp;rsquo;s predictions are compared to the true values.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Training = Minimizing Loss:&lt;/strong> Training a model means finding the model parameters (like weights &lt;code>w&lt;/code> and biases &lt;code>b&lt;/code>) that make this loss function as small as possible.&lt;/li>
&lt;li>&lt;strong>Calculus as a Tool for Minimization:&lt;/strong> Derivatives (and their extensions like gradients) tell us the &lt;strong>rate of change&lt;/strong> or the &lt;strong>slope&lt;/strong> of a function.
&lt;ul>
&lt;li>If we know the slope of our loss function with respect to our model parameters, we know which &amp;ldquo;direction&amp;rdquo; to tweak those parameters to &lt;em>decrease&lt;/em> the loss. This is the essence of &lt;strong>Gradient Descent&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>So, all this calculus is ultimately about finding an efficient way to &amp;ldquo;walk downhill&amp;rdquo; on our loss function surface to find the parameter values that give the lowest error.&lt;/p></description></item><item><title>Attention Is All You Need</title><link>https://deepskandpal.github.io/papershelf/attention-all-u-need/</link><pubDate>Sat, 27 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/attention-all-u-need/</guid><description/></item><item><title>Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing</title><link>https://deepskandpal.github.io/papershelf/infinit-retrevial-attention/</link><pubDate>Sat, 27 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/infinit-retrevial-attention/</guid><description/></item><item><title>One-Minute Video Generation with Test-Time Training</title><link>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</link><pubDate>Mon, 15 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</guid><description>&lt;p>&lt;a href="https://test-time-training.github.io/video-dit/">Website&lt;/a>&lt;/p>
&lt;h2 id="tldr">TLDR;&lt;/h2>
&lt;p>They tackled long video generation by replacing expensive global attention with efficient local attention, and bridging the gaps between local segments using novel TTT layers. These TTT layers act like RNNs but have a much smarter, adaptive hidden state (a neural network that learns on-the-fly during generation). This allows them to capture long-range dependencies and complex dynamics better than traditional RNNs, leading to more coherent minute-long videos, albeit with some remaining artifacts and efficiency challenges.&lt;/p></description></item><item><title>Hands-On Large Language Models: Language Understanding and Generation</title><link>https://deepskandpal.github.io/bookshelf/hands-on-llm/</link><pubDate>Sun, 10 Mar 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-llm/</guid><description/></item><item><title>The Hundred-Page Language Models Book</title><link>https://deepskandpal.github.io/bookshelf/the-lm-book/</link><pubDate>Sun, 10 Mar 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/the-lm-book/</guid><description/></item><item><title>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, 2nd Edition</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/</link><pubDate>Tue, 20 Feb 2024 11:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/</guid><description>&lt;h1 id="this-book-is-a-comprehensive-guide-to-machine-learning">This book is a comprehensive guide to machine learning.&lt;/h1>
&lt;p>System Prompt&lt;/p>
&lt;p>&lt;code>You are a self taught machine learning specialist at a major ivy league college who takes machine learning machine learning 101 elective for stem undergraduate student. Your core competency is in physics in which you have done a PHD from the same college but you are a curious researcher who loves to dabble into different domains of science and engineering. You have a passion for teaching and the students love you because you have the ability to teach hard abstract concepts in the domains of applied sciences mathematics statistics and more recently machine learning into easy to follow intuitive grounded in the philosophy of &amp;quot;what this concept is ultimately trying to achieve &amp;quot; which is very different from usual approaches of how these concepts are taught which have heavy reliance on definition and equations ( you bring realism to them as well which is ultimately your goal) you have written a GOAT book hands on machine learning which students love for its comprehensive depth and breadth and your course is derived from that book&lt;/code>&lt;/p></description></item><item><title>Interpretable Machine Learning</title><link>https://deepskandpal.github.io/bookshelf/interpretable-machine-learning/</link><pubDate>Mon, 15 Jan 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/interpretable-machine-learning/</guid><description/></item></channel></rss>