<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on 404EngineerNotFound</title><link>https://deepskandpal.github.io/categories/ai/</link><description>Recent content in AI on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 12 Jun 2025 18:07:27 +0530</lastBuildDate><atom:link href="https://deepskandpal.github.io/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Qwen3 Technical Report</title><link>https://deepskandpal.github.io/papershelf/qwen3/</link><pubDate>Thu, 12 Jun 2025 18:07:27 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/qwen3/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title><link>https://deepskandpal.github.io/papershelf/illusion-of-thinking/</link><pubDate>Sun, 08 Jun 2025 01:01:23 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/illusion-of-thinking/</guid><description>&lt;p>At its heart, &amp;ldquo;The Illusion of Thinking,&amp;rdquo; is about trying to genuinely understand how well these new &amp;ldquo;Large Reasoning Models&amp;rdquo; (LRMs) actually &lt;em>reason&lt;/em>. You&amp;rsquo;ve probably heard about models that show their &amp;ldquo;thinking steps&amp;rdquo; before giving an answer, like with Chain-of-Thought. They often do better on benchmarks, which is exciting. But we felt that just looking at the final answer on standard math or coding tests wasn&amp;rsquo;t telling the whole story.&lt;/p></description></item><item><title>Understanding the difficulty of training deep feedforward neural networks</title><link>https://deepskandpal.github.io/papershelf/understand-training-diff-deep-nn/</link><pubDate>Fri, 06 Jun 2025 16:11:01 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/understand-training-diff-deep-nn/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 â€“ LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY</title><link>https://deepskandpal.github.io/papershelf/disciplined-nn/</link><pubDate>Fri, 06 Jun 2025 15:57:54 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/disciplined-nn/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Multi-Head Attention: Collaborate Instead of Concatenate</title><link>https://deepskandpal.github.io/papershelf/multihead-attention/</link><pubDate>Wed, 04 Jun 2025 23:24:48 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/multihead-attention/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title><link>https://deepskandpal.github.io/papershelf/flash-attention/</link><pubDate>Wed, 04 Jun 2025 23:24:37 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/flash-attention/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link>https://deepskandpal.github.io/papershelf/bert/</link><pubDate>Wed, 04 Jun 2025 23:24:31 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/bert/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Improving Language Understanding by Generative Pre-Training</title><link>https://deepskandpal.github.io/papershelf/gpt-1/</link><pubDate>Tue, 03 Jun 2025 11:01:51 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/gpt-1/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications</title><link>https://deepskandpal.github.io/bookshelf/design-ml-system/</link><pubDate>Thu, 22 May 2025 11:19:32 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/design-ml-system/</guid><description/></item><item><title>Reliable Machine Learning: Applying SRE Principles to ML in Productionl</title><link>https://deepskandpal.github.io/bookshelf/reliable-ml/</link><pubDate>Tue, 20 May 2025 13:49:22 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/reliable-ml/</guid><description>&lt;h1 id="main-notes-for-hands-on-ml">Main Notes for Hands-On ML&lt;/h1>
&lt;p>This book is a comprehensive guide to machine learning.&lt;/p></description></item><item><title>The Matrix Calculus You Need For Deep Learning</title><link>https://deepskandpal.github.io/papershelf/maths/</link><pubDate>Sun, 27 Apr 2025 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/maths/</guid><description>&lt;h1 id="the-big-picture-goal">&lt;strong>The Big Picture Goal:&lt;/strong>&lt;/h1>
&lt;p>We want our machine learning models to make good predictions. To do this, we define a &lt;strong>loss function&lt;/strong> (or cost function) that measures how &amp;ldquo;bad&amp;rdquo; our model&amp;rsquo;s predictions are compared to the true values.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Training = Minimizing Loss:&lt;/strong> Training a model means finding the model parameters (like weights &lt;code>w&lt;/code> and biases &lt;code>b&lt;/code>) that make this loss function as small as possible.&lt;/li>
&lt;li>&lt;strong>Calculus as a Tool for Minimization:&lt;/strong> Derivatives (and their extensions like gradients) tell us the &lt;strong>rate of change&lt;/strong> or the &lt;strong>slope&lt;/strong> of a function.
&lt;ul>
&lt;li>If we know the slope of our loss function with respect to our model parameters, we know which &amp;ldquo;direction&amp;rdquo; to tweak those parameters to &lt;em>decrease&lt;/em> the loss. This is the essence of &lt;strong>Gradient Descent&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>So, all this calculus is ultimately about finding an efficient way to &amp;ldquo;walk downhill&amp;rdquo; on our loss function surface to find the parameter values that give the lowest error.&lt;/p></description></item><item><title>Attention Is All You Need</title><link>https://deepskandpal.github.io/papershelf/attention-all-u-need/</link><pubDate>Sat, 27 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/attention-all-u-need/</guid><description>&lt;h2 id="paper">&lt;strong>Paper: &amp;ldquo;Attention Is All You Need&amp;rdquo;&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Title Itself:&lt;/strong> The title is a bold claim. It signals a departure from the dominant RNN/LSTM and CNN paradigms for sequence modeling at the time. It suggests that the attention mechanism, previously often an auxiliary component, could be the &lt;em>core&lt;/em> and &lt;em>sufficient&lt;/em> mechanism.&lt;/p>
&lt;h2 id="abstract">&lt;strong>Abstract:&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Introduces the Transformer:&lt;/strong> &amp;ldquo;based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.&amp;rdquo;
&lt;ul>
&lt;li>&lt;strong>Goal:&lt;/strong> Superior quality, more parallelizable, less training time for sequence transduction (e.g., machine translation).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Key Results:&lt;/strong> State-of-the-art (SOTA) on WMT 2014 English-to-German (28.4 BLEU, &amp;gt;2 BLEU improvement) and English-to-French translation tasks, with significantly reduced training costs (e.g., 3.5 days on 8 P100 GPUs for a big model).&lt;/li>
&lt;li>&lt;strong>Generalization:&lt;/strong> Shows it can be applied to other tasks like English constituency parsing.&lt;/li>
&lt;/ul>
&lt;h2 id="1-introduction">&lt;strong>1. Introduction&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Context:&lt;/strong> Recurrent Neural Networks (RNNs), especially LSTMs and Gated Recurrent Units (GRUs), were the SOTA for sequence modeling (language modeling, machine translation).
&lt;ul>
&lt;li>&lt;strong>Problem with RNNs:&lt;/strong> They are inherently sequential. &lt;code>h_t&lt;/code> depends on &lt;code>h_{t-1}&lt;/code>. This &amp;ldquo;sequential nature precludes parallelization within training examples.&amp;rdquo; This becomes a bottleneck for longer sequences, as memory constraints limit batching across examples.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Attention as a Solution (Previously):&lt;/strong> Attention mechanisms were already being used with RNNs, allowing modeling of dependencies regardless of distance. However, they were typically used &lt;em>in conjunction&lt;/em> with a recurrent network.&lt;/li>
&lt;li>&lt;strong>The Paper&amp;rsquo;s Proposal:&lt;/strong> &amp;ldquo;In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.&amp;rdquo;
&lt;ul>
&lt;li>&lt;strong>Key Advantages Claimed:&lt;/strong> More parallelization, can reach new SOTA in translation quality with less training time.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="2-background">&lt;strong>2. Background&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Motivation:&lt;/strong> Reduce sequential computation.&lt;/li>
&lt;li>&lt;strong>Existing Alternatives to RNNs:&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Extended Neural GPU, ByteNet, ConvS2S:&lt;/strong> Used CNNs as basic building blocks, computing hidden representations in parallel.
&lt;ul>
&lt;li>&lt;strong>Limitation:&lt;/strong> The number of operations to relate signals from two arbitrary positions grows with distance (linearly for ConvS2S, logarithmically for ByteNet). This makes learning distant dependencies harder.&lt;/li>
&lt;li>&lt;strong>Transformer&amp;rsquo;s Advantage:&lt;/strong> Reduces this to a &lt;em>constant&lt;/em> number of operations (via self-attention), albeit at the cost of reduced &amp;ldquo;effective resolution&amp;rdquo; due to averaging attention-weighted positions (counteracted by Multi-Head Attention).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Self-Attention (Intra-Attention):&lt;/strong> Defined as an attention mechanism relating different positions of a &lt;em>single sequence&lt;/em> to compute a representation of that sequence. It had been used before successfully.&lt;/li>
&lt;li>&lt;strong>End-to-End Memory Networks:&lt;/strong> Based on recurrent attention, showed good performance on simple QA.&lt;/li>
&lt;li>&lt;strong>Transformer&amp;rsquo;s Novelty Claim:&lt;/strong> &amp;ldquo;the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;h1 id="3-model-architecture">&lt;strong>3. Model Architecture&lt;/strong>&lt;/h1>
&lt;p>This is the core of the paper.&lt;/p></description></item><item><title>Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing</title><link>https://deepskandpal.github.io/papershelf/infinit-retrevial-attention/</link><pubDate>Sat, 27 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/infinit-retrevial-attention/</guid><description/></item><item><title>One-Minute Video Generation with Test-Time Training</title><link>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</link><pubDate>Mon, 15 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</guid><description>&lt;p>&lt;a href="https://test-time-training.github.io/video-dit/">Website&lt;/a>&lt;/p>
&lt;h1 id="tldr">TLDR;&lt;/h1>
&lt;p>They tackled long video generation by replacing expensive global attention with efficient local attention, and bridging the gaps between local segments using novel TTT layers. These TTT layers act like RNNs but have a much smarter, adaptive hidden state (a neural network that learns on-the-fly during generation). This allows them to capture long-range dependencies and complex dynamics better than traditional RNNs, leading to more coherent minute-long videos, albeit with some remaining artifacts and efficiency challenges.&lt;/p></description></item><item><title>Hands-On Large Language Models: Language Understanding and Generation</title><link>https://deepskandpal.github.io/bookshelf/hands-on-llm/</link><pubDate>Sun, 10 Mar 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-llm/</guid><description/></item><item><title>The Hundred-Page Language Models Book</title><link>https://deepskandpal.github.io/bookshelf/the-lm-book/</link><pubDate>Sun, 10 Mar 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/the-lm-book/</guid><description/></item><item><title>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, 2nd Edition</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/</link><pubDate>Tue, 20 Feb 2024 11:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/</guid><description>&lt;h1 id="this-book-is-a-comprehensive-guide-to-machine-learning">This book is a comprehensive guide to machine learning.&lt;/h1>
&lt;p>System Prompt&lt;/p>
&lt;p>&lt;code>You are a self taught machine learning specialist at a major ivy league college who takes machine learning machine learning 101 elective for stem undergraduate student. Your core competency is in physics in which you have done a PHD from the same college but you are a curious researcher who loves to dabble into different domains of science and engineering. You have a passion for teaching and the students love you because you have the ability to teach hard abstract concepts in the domains of applied sciences mathematics statistics and more recently machine learning into easy to follow intuitive grounded in the philosophy of &amp;quot;what this concept is ultimately trying to achieve &amp;quot; which is very different from usual approaches of how these concepts are taught which have heavy reliance on definition and equations ( you bring realism to them as well which is ultimately your goal) you have written a GOAT book hands on machine learning which students love for its comprehensive depth and breadth and your course is derived from that book&lt;/code>&lt;/p></description></item><item><title>Interpretable Machine Learning</title><link>https://deepskandpal.github.io/bookshelf/interpretable-machine-learning/</link><pubDate>Mon, 15 Jan 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/interpretable-machine-learning/</guid><description/></item></channel></rss>