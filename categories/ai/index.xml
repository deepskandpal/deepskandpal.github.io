<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on 404EngineerNotFound</title><link>https://deepskandpal.github.io/categories/ai/</link><description>Recent content in AI on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 17 Jun 2025 09:57:39 +0530</lastBuildDate><atom:link href="https://deepskandpal.github.io/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core</title><link>https://deepskandpal.github.io/papershelf/moe/</link><pubDate>Tue, 17 Jun 2025 09:57:39 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/moe/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Qwen3 Technical Report</title><link>https://deepskandpal.github.io/papershelf/qwen3/</link><pubDate>Thu, 12 Jun 2025 18:07:27 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/qwen3/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title><link>https://deepskandpal.github.io/papershelf/illusion-of-thinking/</link><pubDate>Sun, 08 Jun 2025 01:01:23 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/illusion-of-thinking/</guid><description>&lt;p>At its heart, &amp;ldquo;The Illusion of Thinking,&amp;rdquo; is about trying to genuinely understand how well these new &amp;ldquo;Large Reasoning Models&amp;rdquo; (LRMs) actually &lt;em>reason&lt;/em>. You&amp;rsquo;ve probably heard about models that show their &amp;ldquo;thinking steps&amp;rdquo; before giving an answer, like with Chain-of-Thought. They often do better on benchmarks, which is exciting. But we felt that just looking at the final answer on standard math or coding tests wasn&amp;rsquo;t telling the whole story.&lt;/p></description></item><item><title>Improving Language Understanding by Generative Pre-Training</title><link>https://deepskandpal.github.io/papershelf/gpt-1/</link><pubDate>Tue, 03 Jun 2025 11:01:51 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/gpt-1/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications</title><link>https://deepskandpal.github.io/bookshelf/design-ml-system/</link><pubDate>Thu, 22 May 2025 11:19:32 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/design-ml-system/</guid><description/></item><item><title>Artificial Intelligence: A Modern Approach</title><link>https://deepskandpal.github.io/bookshelf/ai-modern-approach-russell-norvig/</link><pubDate>Sun, 26 Jan 2025 18:25:00 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/ai-modern-approach-russell-norvig/</guid><description>&lt;h1 id="the-comprehensive-foundation-of-artificial-intelligence">The Comprehensive Foundation of Artificial Intelligence&lt;/h1>
&lt;p>&lt;strong>The most widely used AI textbook that provides the broad conceptual foundation for understanding how GenAI fits into the larger landscape of artificial intelligence.&lt;/strong> This book contextualizes all the specialized GenAI techniques within the broader AI paradigm.&lt;/p>
&lt;h2 id="why-this-book-provides-essential-context">Why This Book Provides Essential Context&lt;/h2>
&lt;p>While specialized books cover specific aspects of GenAI, this text provides the overarching framework that makes everything else make sense:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Problem Solving and Search&lt;/strong>: Foundation for understanding how AI agents reason and plan&lt;/li>
&lt;li>&lt;strong>Knowledge Representation&lt;/strong>: How AI systems represent and manipulate information&lt;/li>
&lt;li>&lt;strong>Reasoning Under Uncertainty&lt;/strong>: Probabilistic approaches that underlie AI safety and alignment&lt;/li>
&lt;li>&lt;strong>Machine Learning Integration&lt;/strong>: How GenAI fits into the broader ML landscape&lt;/li>
&lt;li>&lt;strong>AI Safety and Ethics&lt;/strong>: Early treatment of alignment problems and ethical considerations&lt;/li>
&lt;/ul>
&lt;h2 id="connection-to-your-genai-knowledge-tree">Connection to Your GenAI Knowledge Tree&lt;/h2>
&lt;p>This book provides the conceptual bridges between different areas of your GenAI materials:&lt;/p></description></item><item><title>Reinforcement Learning: An Introduction</title><link>https://deepskandpal.github.io/bookshelf/reinforcement-learning-sutton-barto/</link><pubDate>Sun, 26 Jan 2025 18:15:00 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/reinforcement-learning-sutton-barto/</guid><description>&lt;h1 id="the-foundation-of-ai-alignment-and-rlhf">The Foundation of AI Alignment and RLHF&lt;/h1>
&lt;p>&lt;strong>The authoritative text on reinforcement learning that provides the theoretical foundation for modern AI alignment techniques.&lt;/strong> This book is essential for understanding RLHF, Constitutional AI, and human preference learning in GenAI systems.&lt;/p>
&lt;h2 id="why-this-book-is-critical-for-modern-genai">Why This Book is Critical for Modern GenAI&lt;/h2>
&lt;p>RLHF (Reinforcement Learning from Human Feedback) is the key breakthrough that enables models like ChatGPT and Claude to behave helpfully and safely. This book provides the theoretical foundation:&lt;/p></description></item><item><title>Deep Learning</title><link>https://deepskandpal.github.io/bookshelf/deep-learning-goodfellow/</link><pubDate>Sun, 26 Jan 2025 18:00:00 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/deep-learning-goodfellow/</guid><description>&lt;h1 id="the-foundational-text-for-deep-learning">The Foundational Text for Deep Learning&lt;/h1>
&lt;p>&lt;strong>The definitive reference for deep learning theory and practice.&lt;/strong> Written by three of the field&amp;rsquo;s most prominent researchers, this book provides the mathematical foundations and conceptual framework that underlies all modern generative AI systems.&lt;/p>
&lt;h2 id="why-this-book-is-essential-for-genai">Why This Book is Essential for GenAI&lt;/h2>
&lt;p>This text covers all the fundamental concepts referenced throughout your GenAI knowledge tree:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Mathematical Foundations&lt;/strong>: Linear algebra, probability theory, information theory&lt;/li>
&lt;li>&lt;strong>Neural Network Architectures&lt;/strong>: From MLPs to the building blocks of transformers&lt;/li>
&lt;li>&lt;strong>Optimization Theory&lt;/strong>: The mathematical principles behind training large models&lt;/li>
&lt;li>&lt;strong>Regularization&lt;/strong>: Techniques that prevent overfitting in large-scale training&lt;/li>
&lt;li>&lt;strong>Representation Learning&lt;/strong>: How models learn meaningful representations&lt;/li>
&lt;/ul>
&lt;h2 id="connection-to-genai-concepts">Connection to GenAI Concepts&lt;/h2>
&lt;p>Every major concept in your GenAI materials traces back to principles established in this book:&lt;/p></description></item><item><title>Generative AI System Design Interview (Full Colour Edition)</title><link>https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/</link><pubDate>Sun, 26 Jan 2025 12:00:00 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/</guid><description>&lt;h1 id="notes--summary-for-the-book">Notes / Summary for the Book&lt;/h1>
&lt;p>&lt;strong>Generative AI System Design Interview (Full Colour Edition)&lt;/strong> by Alex Xu, Ali Aminian, Hao Sheng&lt;/p>
&lt;p>This book provides an insider&amp;rsquo;s perspective on what interviewers are truly looking for in GenAI system design interviews and why. It includes:&lt;/p>
&lt;ul>
&lt;li>A 7-step framework to help tackle GenAI system design interview questions&lt;/li>
&lt;li>10 real-world GenAI system design questions with in-depth solutions&lt;/li>
&lt;li>280+ diagrams to demystify complex GenAI systems&lt;/li>
&lt;/ul>
&lt;p>The book is particularly helpful for:&lt;/p></description></item><item><title>Training Compute-Optimal Large Language Models</title><link>https://deepskandpal.github.io/papershelf/chinchilla-scaling-laws/</link><pubDate>Mon, 30 Dec 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/chinchilla-scaling-laws/</guid><description>&lt;h2 id="paper">&lt;strong>Paper: &amp;ldquo;Training Compute-Optimal Large Language Models&amp;rdquo;&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Authors:&lt;/strong> Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre&lt;/p>
&lt;p>&lt;strong>Also Known As:&lt;/strong> The &amp;ldquo;Chinchilla&amp;rdquo; paper (named after their 70B parameter model)&lt;/p>
&lt;h2 id="abstract--key-contributions">&lt;strong>Abstract &amp;amp; Key Contributions:&lt;/strong>&lt;/h2>
&lt;p>This seminal paper from DeepMind challenges the conventional wisdom about how to scale large language models optimally. The key findings are:&lt;/p></description></item><item><title>One-Minute Video Generation with Test-Time Training</title><link>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</link><pubDate>Mon, 15 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</guid><description>&lt;p>&lt;a href="https://test-time-training.github.io/video-dit/">Website&lt;/a>&lt;/p>
&lt;h1 id="tldr">TLDR;&lt;/h1>
&lt;p>They tackled long video generation by replacing expensive global attention with efficient local attention, and bridging the gaps between local segments using novel TTT layers. These TTT layers act like RNNs but have a much smarter, adaptive hidden state (a neural network that learns on-the-fly during generation). This allows them to capture long-range dependencies and complex dynamics better than traditional RNNs, leading to more coherent minute-long videos, albeit with some remaining artifacts and efficiency challenges.&lt;/p></description></item><item><title>Hands-On Large Language Models: Language Understanding and Generation</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/</link><pubDate>Sun, 10 Mar 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/</guid><description>&lt;p>&amp;ndash;&lt;/p></description></item><item><title>The Hundred-Page Language Models Book</title><link>https://deepskandpal.github.io/bookshelf/100-page-lm-book/</link><pubDate>Sun, 10 Mar 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/100-page-lm-book/</guid><description/></item><item><title>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, 2nd Edition</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/</link><pubDate>Tue, 20 Feb 2024 11:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/</guid><description>&lt;h1 id="this-book-is-a-comprehensive-guide-to-machine-learning">This book is a comprehensive guide to machine learning.&lt;/h1>
&lt;p>System Prompt&lt;/p>
&lt;p>&lt;code>You are a self taught machine learning specialist at a major ivy league college who takes machine learning machine learning 101 elective for stem undergraduate student. Your core competency is in physics in which you have done a PHD from the same college but you are a curious researcher who loves to dabble into different domains of science and engineering. You have a passion for teaching and the students love you because you have the ability to teach hard abstract concepts in the domains of applied sciences mathematics statistics and more recently machine learning into easy to follow intuitive grounded in the philosophy of &amp;quot;what this concept is ultimately trying to achieve &amp;quot; which is very different from usual approaches of how these concepts are taught which have heavy reliance on definition and equations ( you bring realism to them as well which is ultimately your goal) you have written a GOAT book hands on machine learning which students love for its comprehensive depth and breadth and your course is derived from that book&lt;/code>&lt;/p></description></item><item><title>Interpretable Machine Learning</title><link>https://deepskandpal.github.io/bookshelf/interpretable-machine-learning/</link><pubDate>Mon, 15 Jan 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/interpretable-machine-learning/</guid><description/></item></channel></rss>