<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on 404EngineerNotFound</title><link>https://deepskandpal.github.io/categories/ai/</link><description>Recent content in AI on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 17 Jun 2025 09:57:39 +0530</lastBuildDate><atom:link href="https://deepskandpal.github.io/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core</title><link>https://deepskandpal.github.io/papershelf/moe/</link><pubDate>Tue, 17 Jun 2025 09:57:39 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/moe/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Qwen3 Technical Report</title><link>https://deepskandpal.github.io/papershelf/qwen3/</link><pubDate>Thu, 12 Jun 2025 18:07:27 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/qwen3/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title><link>https://deepskandpal.github.io/papershelf/illusion-of-thinking/</link><pubDate>Sun, 08 Jun 2025 01:01:23 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/illusion-of-thinking/</guid><description>&lt;p>At its heart, &amp;ldquo;The Illusion of Thinking,&amp;rdquo; is about trying to genuinely understand how well these new &amp;ldquo;Large Reasoning Models&amp;rdquo; (LRMs) actually &lt;em>reason&lt;/em>. You&amp;rsquo;ve probably heard about models that show their &amp;ldquo;thinking steps&amp;rdquo; before giving an answer, like with Chain-of-Thought. They often do better on benchmarks, which is exciting. But we felt that just looking at the final answer on standard math or coding tests wasn&amp;rsquo;t telling the whole story.&lt;/p></description></item><item><title>Improving Language Understanding by Generative Pre-Training</title><link>https://deepskandpal.github.io/papershelf/gpt-1/</link><pubDate>Tue, 03 Jun 2025 11:01:51 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/gpt-1/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications</title><link>https://deepskandpal.github.io/bookshelf/design-ml-system/</link><pubDate>Thu, 22 May 2025 11:19:32 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/design-ml-system/</guid><description/></item><item><title>Reliable Machine Learning: Applying SRE Principles to ML in Productionl</title><link>https://deepskandpal.github.io/bookshelf/reliable-ml/</link><pubDate>Tue, 20 May 2025 13:49:22 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/reliable-ml/</guid><description>&lt;h1 id="main-notes-for-hands-on-ml">Main Notes for Hands-On ML&lt;/h1>
&lt;p>This book is a comprehensive guide to machine learning.&lt;/p></description></item><item><title>Generative AI System Design Interview (Full Colour Edition)</title><link>https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/</link><pubDate>Sun, 26 Jan 2025 12:00:00 +0530</pubDate><guid>https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/</guid><description>&lt;h1 id="notes--summary-for-the-book">Notes / Summary for the Book&lt;/h1>
&lt;p>&lt;strong>Generative AI System Design Interview (Full Colour Edition)&lt;/strong> by Alex Xu, Ali Aminian, Hao Sheng&lt;/p>
&lt;p>This book provides an insider&amp;rsquo;s perspective on what interviewers are truly looking for in GenAI system design interviews and why. It includes:&lt;/p>
&lt;ul>
&lt;li>A 7-step framework to help tackle GenAI system design interview questions&lt;/li>
&lt;li>10 real-world GenAI system design questions with in-depth solutions&lt;/li>
&lt;li>280+ diagrams to demystify complex GenAI systems&lt;/li>
&lt;/ul>
&lt;p>The book is particularly helpful for:&lt;/p></description></item><item><title>Training Compute-Optimal Large Language Models</title><link>https://deepskandpal.github.io/papershelf/chinchilla-scaling-laws/</link><pubDate>Mon, 30 Dec 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/chinchilla-scaling-laws/</guid><description>&lt;h2 id="paper">&lt;strong>Paper: &amp;ldquo;Training Compute-Optimal Large Language Models&amp;rdquo;&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Authors:&lt;/strong> Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre&lt;/p>
&lt;p>&lt;strong>Also Known As:&lt;/strong> The &amp;ldquo;Chinchilla&amp;rdquo; paper (named after their 70B parameter model)&lt;/p>
&lt;h2 id="abstract--key-contributions">&lt;strong>Abstract &amp;amp; Key Contributions:&lt;/strong>&lt;/h2>
&lt;p>This seminal paper from DeepMind challenges the conventional wisdom about how to scale large language models optimally. The key findings are:&lt;/p></description></item><item><title>One-Minute Video Generation with Test-Time Training</title><link>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</link><pubDate>Mon, 15 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</guid><description>&lt;p>&lt;a href="https://test-time-training.github.io/video-dit/">Website&lt;/a>&lt;/p>
&lt;h1 id="tldr">TLDR;&lt;/h1>
&lt;p>They tackled long video generation by replacing expensive global attention with efficient local attention, and bridging the gaps between local segments using novel TTT layers. These TTT layers act like RNNs but have a much smarter, adaptive hidden state (a neural network that learns on-the-fly during generation). This allows them to capture long-range dependencies and complex dynamics better than traditional RNNs, leading to more coherent minute-long videos, albeit with some remaining artifacts and efficiency challenges.&lt;/p></description></item><item><title>Hands-On Large Language Models: Language Understanding and Generation</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/</link><pubDate>Sun, 10 Mar 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/</guid><description>&lt;p>&amp;ndash;&lt;/p></description></item><item><title>The Hundred-Page Language Models Book</title><link>https://deepskandpal.github.io/bookshelf/100-page-lm-book/</link><pubDate>Sun, 10 Mar 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/100-page-lm-book/</guid><description/></item><item><title>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, 2nd Edition</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/</link><pubDate>Tue, 20 Feb 2024 11:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/</guid><description>&lt;h1 id="this-book-is-a-comprehensive-guide-to-machine-learning">This book is a comprehensive guide to machine learning.&lt;/h1>
&lt;p>System Prompt&lt;/p>
&lt;p>&lt;code>You are a self taught machine learning specialist at a major ivy league college who takes machine learning machine learning 101 elective for stem undergraduate student. Your core competency is in physics in which you have done a PHD from the same college but you are a curious researcher who loves to dabble into different domains of science and engineering. You have a passion for teaching and the students love you because you have the ability to teach hard abstract concepts in the domains of applied sciences mathematics statistics and more recently machine learning into easy to follow intuitive grounded in the philosophy of &amp;quot;what this concept is ultimately trying to achieve &amp;quot; which is very different from usual approaches of how these concepts are taught which have heavy reliance on definition and equations ( you bring realism to them as well which is ultimately your goal) you have written a GOAT book hands on machine learning which students love for its comprehensive depth and breadth and your course is derived from that book&lt;/code>&lt;/p></description></item><item><title>Interpretable Machine Learning</title><link>https://deepskandpal.github.io/bookshelf/interpretable-machine-learning/</link><pubDate>Mon, 15 Jan 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/interpretable-machine-learning/</guid><description/></item></channel></rss>