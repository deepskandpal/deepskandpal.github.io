<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Maths on 404EngineerNotFound</title><link>https://deepskandpal.github.io/categories/maths/</link><description>Recent content in Maths on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 27 Apr 2025 19:30:00 +0000</lastBuildDate><atom:link href="https://deepskandpal.github.io/categories/maths/index.xml" rel="self" type="application/rss+xml"/><item><title>The Matrix Calculus You Need For Deep Learning</title><link>https://deepskandpal.github.io/papershelf/read/maths/</link><pubDate>Sun, 27 Apr 2025 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/read/maths/</guid><description>&lt;h1 id="the-big-picture-goal"&gt;&lt;strong&gt;The Big Picture Goal:&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;We want our machine learning models to make good predictions. To do this, we define a &lt;strong&gt;loss function&lt;/strong&gt; (or cost function) that measures how &amp;ldquo;bad&amp;rdquo; our model&amp;rsquo;s predictions are compared to the true values.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Training = Minimizing Loss:&lt;/strong&gt; Training a model means finding the model parameters (like weights &lt;code&gt;w&lt;/code&gt; and biases &lt;code&gt;b&lt;/code&gt;) that make this loss function as small as possible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculus as a Tool for Minimization:&lt;/strong&gt; Derivatives (and their extensions like gradients) tell us the &lt;strong&gt;rate of change&lt;/strong&gt; or the &lt;strong&gt;slope&lt;/strong&gt; of a function.
&lt;ul&gt;
&lt;li&gt;If we know the slope of our loss function with respect to our model parameters, we know which &amp;ldquo;direction&amp;rdquo; to tweak those parameters to &lt;em&gt;decrease&lt;/em&gt; the loss. This is the essence of &lt;strong&gt;Gradient Descent&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, all this calculus is ultimately about finding an efficient way to &amp;ldquo;walk downhill&amp;rdquo; on our loss function surface to find the parameter values that give the lowest error.&lt;/p&gt;</description></item></channel></rss>