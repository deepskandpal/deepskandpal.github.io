<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformers on 404EngineerNotFound</title><link>https://deepskandpal.github.io/categories/transformers/</link><description>Recent content in Transformers on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 09 Jun 2025 22:02:04 +0530</lastBuildDate><atom:link href="https://deepskandpal.github.io/categories/transformers/index.xml" rel="self" type="application/rss+xml"/><item><title>Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention</title><link>https://deepskandpal.github.io/papershelf/hardware-attention/</link><pubDate>Mon, 09 Jun 2025 22:02:04 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/hardware-attention/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Multi-Head Attention: Collaborate Instead of Concatenate</title><link>https://deepskandpal.github.io/papershelf/multihead-attention/</link><pubDate>Wed, 04 Jun 2025 23:24:48 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/multihead-attention/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title><link>https://deepskandpal.github.io/papershelf/flash-attention/</link><pubDate>Wed, 04 Jun 2025 23:24:37 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/flash-attention/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link>https://deepskandpal.github.io/papershelf/bert/</link><pubDate>Wed, 04 Jun 2025 23:24:31 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/bert/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Attention Is All You Need</title><link>https://deepskandpal.github.io/papershelf/attention-all-u-need/</link><pubDate>Sat, 27 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/attention-all-u-need/</guid><description>&lt;h2 id="paper">&lt;strong>Paper: &amp;ldquo;Attention Is All You Need&amp;rdquo;&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Title Itself:&lt;/strong> The title is a bold claim. It signals a departure from the dominant RNN/LSTM and CNN paradigms for sequence modeling at the time. It suggests that the attention mechanism, previously often an auxiliary component, could be the &lt;em>core&lt;/em> and &lt;em>sufficient&lt;/em> mechanism.&lt;/p>
&lt;h2 id="abstract">&lt;strong>Abstract:&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Introduces the Transformer:&lt;/strong> &amp;ldquo;based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.&amp;rdquo;
&lt;ul>
&lt;li>&lt;strong>Goal:&lt;/strong> Superior quality, more parallelizable, less training time for sequence transduction (e.g., machine translation).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Key Results:&lt;/strong> State-of-the-art (SOTA) on WMT 2014 English-to-German (28.4 BLEU, &amp;gt;2 BLEU improvement) and English-to-French translation tasks, with significantly reduced training costs (e.g., 3.5 days on 8 P100 GPUs for a big model).&lt;/li>
&lt;li>&lt;strong>Generalization:&lt;/strong> Shows it can be applied to other tasks like English constituency parsing.&lt;/li>
&lt;/ul>
&lt;h2 id="1-introduction">&lt;strong>1. Introduction&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Context:&lt;/strong> Recurrent Neural Networks (RNNs), especially LSTMs and Gated Recurrent Units (GRUs), were the SOTA for sequence modeling (language modeling, machine translation).
&lt;ul>
&lt;li>&lt;strong>Problem with RNNs:&lt;/strong> They are inherently sequential. &lt;code>h_t&lt;/code> depends on &lt;code>h_{t-1}&lt;/code>. This &amp;ldquo;sequential nature precludes parallelization within training examples.&amp;rdquo; This becomes a bottleneck for longer sequences, as memory constraints limit batching across examples.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Attention as a Solution (Previously):&lt;/strong> Attention mechanisms were already being used with RNNs, allowing modeling of dependencies regardless of distance. However, they were typically used &lt;em>in conjunction&lt;/em> with a recurrent network.&lt;/li>
&lt;li>&lt;strong>The Paper&amp;rsquo;s Proposal:&lt;/strong> &amp;ldquo;In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.&amp;rdquo;
&lt;ul>
&lt;li>&lt;strong>Key Advantages Claimed:&lt;/strong> More parallelization, can reach new SOTA in translation quality with less training time.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="2-background">&lt;strong>2. Background&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Motivation:&lt;/strong> Reduce sequential computation.&lt;/li>
&lt;li>&lt;strong>Existing Alternatives to RNNs:&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Extended Neural GPU, ByteNet, ConvS2S:&lt;/strong> Used CNNs as basic building blocks, computing hidden representations in parallel.
&lt;ul>
&lt;li>&lt;strong>Limitation:&lt;/strong> The number of operations to relate signals from two arbitrary positions grows with distance (linearly for ConvS2S, logarithmically for ByteNet). This makes learning distant dependencies harder.&lt;/li>
&lt;li>&lt;strong>Transformer&amp;rsquo;s Advantage:&lt;/strong> Reduces this to a &lt;em>constant&lt;/em> number of operations (via self-attention), albeit at the cost of reduced &amp;ldquo;effective resolution&amp;rdquo; due to averaging attention-weighted positions (counteracted by Multi-Head Attention).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Self-Attention (Intra-Attention):&lt;/strong> Defined as an attention mechanism relating different positions of a &lt;em>single sequence&lt;/em> to compute a representation of that sequence. It had been used before successfully.&lt;/li>
&lt;li>&lt;strong>End-to-End Memory Networks:&lt;/strong> Based on recurrent attention, showed good performance on simple QA.&lt;/li>
&lt;li>&lt;strong>Transformer&amp;rsquo;s Novelty Claim:&lt;/strong> &amp;ldquo;the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;h1 id="3-model-architecture">&lt;strong>3. Model Architecture&lt;/strong>&lt;/h1>
&lt;p>This is the core of the paper.&lt;/p></description></item></channel></rss>