<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><a href=#chapter-1-machine-learning-primer>Chapter 1: Machine Learning Primer</a></li><li><a href=#topic-feature-selection-and-feature-engineering>Topic: Feature Selection and Feature Engineering</a><ul><li><a href=#1-one-hot-encoding-ohe>1. One-Hot Encoding (OHE)</a></li><li><a href=#2-mean-encoding-or-target-encoding>2. Mean Encoding (or Target Encoding)</a></li><li><a href=#3-feature-hashing-the-hashing-trick>3. Feature Hashing (&ldquo;The Hashing Trick&rdquo;)</a></li><li><a href=#4-cross-features>4. Cross Features</a></li><li><a href=#5-embedding>5. Embedding</a></li></ul></li><li><a href=#topic-training-pipeline--data-partitioning-page-33>Topic: Training Pipeline & Data Partitioning (Page 33)</a><ul><li><a href=#mapping-generic-concepts-to-aws-services>Mapping Generic Concepts to AWS Services</a></li><li><a href=#use-case-diagram-daily-re-training-of-a-products-you-may-like-model-on-aws>Use Case Diagram: Daily Re-training of a &ldquo;Products You May Like&rdquo; Model on AWS</a></li><li><a href=#explaining-the-diagram-in-an-interview>Explaining the Diagram in an Interview</a></li></ul></li><li><a href=#topic-handle-imbalance-class-distribution-page-34>Topic: Handle Imbalance Class Distribution (Page 34)</a><ul><li></li></ul></li><li><a href=#topic--loss-functions---the-why-behind-optimization>Topic : Loss Functions - The &ldquo;Why&rdquo; Behind Optimization</a><ul><li><a href=#deep-dive-1-regression-loss-how-wrong-is-our-number>Deep Dive 1: Regression Loss: How wrong is our number?</a></li><li><a href=#deep-dive-2-classification-loss-how-wrong-is-our-label>Deep Dive 2: Classification Loss: How wrong is our label?</a></li><li><a href=#deep-dive-3-huber-loss-the-robust-best-of-both-worlds>Deep Dive 3: Huber Loss: The Robust &ldquo;Best of Both Worlds&rdquo;</a></li><li><a href=#deep-dive-4-how-facebook-uses-normalized-cross-entropy-nce-the-fair-comparison-metric>Deep Dive 4: How Facebook Uses Normalized Cross Entropy (NCE): The &ldquo;Fair Comparison&rdquo; Metric</a></li><li><a href=#deep-dive-4-forecast-metrics-mape-and-smape>Deep Dive 4: Forecast Metrics: MAPE and SMAPE</a></li><li><a href=#deep-dive-5-focal-loss-focusing-on-the-hard-cases>Deep Dive 5: Focal Loss: Focusing on the Hard Cases</a></li><li><a href=#deep-dive-6-hinge-loss-learning-with-a-margin>Deep Dive 6: Hinge Loss: Learning with a Margin</a></li></ul></li><li><a href=#topic-model-evaluation-metrics>Topic: Model Evaluation Metrics</a><ul><li><a href=#area-under-the-curve-auc-the-better-than-chance-metric>Area Under the Curve (AUC): The &ldquo;Better than Chance?&rdquo; Metric</a></li><li><a href=#mean-average-recall-at-k-mark-the-did-we-find-everything-metric>Mean Average Recall at K (MAR@K): The &ldquo;Did we find everything?&rdquo; Metric</a></li><li><a href=#mean-average-precision-map--mean-reciprocal-rank-mrr-is-the-first-right-answer-high-up>Mean Average Precision (MAP) & Mean Reciprocal Rank (MRR): &ldquo;Is the first right answer high up?&rdquo;</a></li><li><a href=#normalized-discounted-cumulative-gain-ndcg--cumulative-gain-cg>Normalized Discounted Cumulative Gain (NDCG) & Cumulative Gain (CG)</a></li><li><a href=#online-metrics-click-through-rate-time-spent>Online Metrics: Click-Through Rate, Time Spent</a></li></ul></li><li><a href=#deep-dive-3-common-sampling-techniques>Deep Dive 3: Common Sampling Techniques</a><ul><li><a href=#random-sampling-the-simplest-baseline>Random Sampling: The Simplest Baseline</a></li></ul></li><li><a href=#deep-dive-4-common-deep-learning-model-architectures>Deep Dive 4: Common Deep Learning Model Architectures</a><ul><li><a href=#1-wide-and-deep-architecture-the-power-of-memorization-and-generalization>1. Wide and Deep Architecture: The Power of Memorization and Generalization</a></li><li><a href=#2-two-tower-architecture-the-scalable-retrieval-champion>2. Two-Tower Architecture: The Scalable Retrieval Champion</a></li><li><a href=#3-deep-cross-network-dcn-explicit-and-bounded-feature-crossing>3. Deep Cross Network (DCN): Explicit and Bounded Feature Crossing</a></li><li><a href=#4-multitask-learning-learning-more-with-less>4. Multitask Learning: Learning More with Less</a></li></ul></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 1: Introduction to ML System Design</h1><span class=reading-time><em>55 min read</em></span><div class=book-details><div class=book-content><p>This chapter introduces the fundamentals of designing machine learning systems at scale. It covers the key differences between traditional software systems and ML systems, and outlines the unique challenges faced when building production ML systems.</p><hr><h2 id=chapter-1-machine-learning-primer>Chapter 1: Machine Learning Primer</h2><p>This chapter covers the building blocks. Mastering these fundamentals is non-negotiable. An interviewer will expect you to be fluent in this language.</p><h2 id=topic-feature-selection-and-feature-engineering>Topic: Feature Selection and Feature Engineering</h2><h3 id=1-one-hot-encoding-ohe>1. One-Hot Encoding (OHE)</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> OHE is used for low-cardinality categorical features (e.g., <code>device_type</code> = [&lsquo;mobile&rsquo;, &lsquo;desktop&rsquo;, &rsquo;tablet&rsquo;]). It prevents the model from assuming a false numerical order (i.e., that <code>tablet</code>(2) is somehow &ldquo;more&rdquo; than <code>mobile</code>(0)). Its major drawback is creating very wide, sparse vectors for high-cardinality features, which is memory-intensive.</li><li><strong>The 2024+ Perspective:</strong> The principle is still valid, but its use is more limited. For any feature with more than a handful of categories (like <code>user_id</code>, <code>product_id</code>), OHE is a non-starter. The industry has almost completely moved to embeddings for these cases.</li><li><strong>Interview Focus & Phrasing:</strong><ul><li><strong>Good:</strong> &ldquo;For a low-cardinality feature like &lsquo;day_of_week&rsquo;, we can use one-hot encoding.&rdquo;</li><li><strong>Senior Level:</strong> &ldquo;We&rsquo;ll need to handle our categorical features. For low-cardinality ones like <code>country_code</code>, one-hot encoding is a simple and effective baseline. However, for high-cardinality features like <code>user_id</code>, OHE would lead to extreme sparsity and high dimensionality. We&rsquo;ll use embeddings for those instead.&rdquo;</li></ul></li></ul><h3 id=2-mean-encoding-or-target-encoding>2. Mean Encoding (or Target Encoding)</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> Replace a category with the average value of the target variable for that category (e.g., replace &lsquo;San Francisco&rsquo; with the average click-through rate for all users from SF). It&rsquo;s a very powerful technique for tree-based models (like XGBoost) because it packs a lot of signal into a single numerical feature. The book&rsquo;s warning about <strong>label leakage</strong> is its most critical point.</li><li><strong>The 2024+ Perspective:</strong> Still a highly relevant and powerful technique, especially in tabular data competitions (Kaggle) and for boosting models where latency isn&rsquo;t the primary concern. Modern feature stores sometimes automate the robust calculation of these encodings (e.g., using out-of-fold calculations to prevent leakage).</li><li><strong>Interview Focus & Phrasing:</strong> Mention this as a strong option for tree-based models, but <em>immediately</em> bring up the risks. &ldquo;We could consider target encoding for features like <code>city</code>, as it can provide a strong signal to a tree-based model. However, we must be extremely careful about data leakage. To mitigate this, we&rsquo;d compute the encoding on a separate dataset or use an out-of-fold strategy during training to ensure the encoding for a given row doesn&rsquo;t use its own label.&rdquo;</li></ul><h3 id=3-feature-hashing-the-hashing-trick>3. Feature Hashing (&ldquo;The Hashing Trick&rdquo;)</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> A technique to handle extremely high-cardinality features by hashing them into a fixed-size vector. It&rsquo;s memory-efficient because you pre-define the vector size. The main drawback is hash collisions, where different features get mapped to the same index, which can hurt model performance.</li><li><strong>The 2024+ Perspective (Outdated):</strong> Feature hashing for model features is now largely a legacy technique. While the idea is clever, the performance loss from collisions is often not worth the memory savings, especially when compared to embeddings. Modern hardware and frameworks can handle large embedding tables more gracefully.</li><li><strong>Interview Focus & Phrasing:</strong> Acknowledge it historically. &ldquo;In the past, for systems with millions of features like in AdTech, feature hashing was a common way to manage memory. It&rsquo;s a trade-off between memory and performance due to collisions. Today, we&rsquo;d almost always prefer to learn a dedicated embedding layer, as it captures semantic relationships and avoids collisions, leading to better model performance.&rdquo;</li></ul><h3 id=4-cross-features>4. Cross Features</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> Explicitly combining two or more categorical features to capture their interaction (e.g., <code>feature_A</code>=&lsquo;USA&rsquo; + <code>feature_B</code>=&lsquo;iPhone&rsquo; -> <code>crossed_feature</code>=&lsquo;USA_iPhone&rsquo;). This helps linear models and shallow networks learn relationships they otherwise couldn&rsquo;t. The book&rsquo;s reference to Wide & Deep is the classic example.</li><li><strong>The 2024+ Perspective:</strong> The <em>concept</em> of capturing feature interactions is more important than ever. The <em>method</em> has evolved. While Wide & Deep is still a valid pattern, modern architectures like Transformers (with their self-attention mechanism) are exceptionally good at learning these interactions <em>implicitly</em> from the raw feature embeddings. You don&rsquo;t always need to manually define the crosses.</li><li><strong>Interview Focus & Phrasing:</strong> Focus on the &ldquo;why.&rdquo; &ldquo;A key challenge in this problem is capturing the interaction between features, for example, a user&rsquo;s country and their device type. A classic approach is the Wide & Deep model, where we&rsquo;d manually create cross-product transformations for the &lsquo;wide&rsquo; part. A more modern approach would be to feed the embeddings for all features into a deep network, perhaps with a cross-network layer like DCN or an attention mechanism, which can learn these interactions automatically.&rdquo;</li></ul><h3 id=5-embedding>5. Embedding</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> This is the <strong>most important</strong> feature engineering technique in the book and in modern ML. An embedding is a dense, low-dimensional, learned vector representation of a categorical feature. The book correctly explains the two main ways to get them:<ol><li><strong>Co-trained:</strong> Learning the embedding as part of your main model (e.g., an embedding layer in Keras/PyTorch).</li><li><strong>Pre-trained:</strong> Learning the embedding separately on a different task (e.g., Word2Vec on text, or node2vec on a graph) and then using it as a static feature.</li></ol></li><li><strong>The 2024+ Perspective:</strong> This is the heart of modern ML.<ul><li><strong>The Rise of Foundation Models:</strong> The &ldquo;pre-trained&rdquo; paradigm now dominates. You don&rsquo;t just use Word2Vec; you use embeddings from massive, powerful foundation models (e.g., OpenAI&rsquo;s <code>text-embedding-3-large</code>, Sentence-BERT, or image embeddings from CLIP). Your job is less about designing the pre-training task and more about choosing the right foundation model.</li><li><strong>The Two-Tower Model:</strong> The book&rsquo;s description of the two-tower model for retrieval (e.g., at YouTube) is still <strong>State-of-the-Art</strong> for recommendation and search. This is a critical pattern to know inside and out.</li></ul></li><li><strong>Interview Focus & Phrasing:</strong> This should be your default answer for high-cardinality features.<ul><li>&ldquo;For user IDs and item IDs, we will learn dense embedding vectors. This allows the model to capture semantic similarities—for instance, users who buy similar products will have similar vectors.&rdquo;</li><li>For a retrieval system, you <em>must</em> bring up the two-tower model. &ldquo;We&rsquo;ll design a two-tower retrieval model. The query tower will ingest user features and produce a query embedding. The candidate tower will ingest item features and produce an item embedding. At training time, we&rsquo;ll optimize these towers using a contrastive loss so that the dot product between a user and a relevant item is high. For serving, we can pre-compute all item embeddings and put them into a vector index for efficient retrieval.&rdquo;</li></ul></li></ul><p>Here is the diagram for the Two-Tower Model, which is central to this discussion.</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;Query Tower&#34;
        U_Features[User Features&lt;br/&gt;context, history] --&gt; U_L1(Dense Layer)
        U_L1 --&gt; U_L2(Dense Layer)
        U_L2 --&gt; QueryEmbedding[Query Embedding]
    end

    subgraph &#34;Candidate Tower&#34;
        I_Features[Item Features&lt;br/&gt;metadata, text] --&gt; I_L1(Dense Layer)
        I_L1 --&gt; I_L2(Dense Layer)
        I_L2 --&gt; ItemEmbedding[Item Embedding]
    end

    subgraph &#34;Training Objective&#34;
        QueryEmbedding -- Dot Product --&gt; Score
        ItemEmbedding -- Similarity --&gt; Score
        Score --&gt; Loss(Contrastive Loss&lt;br/&gt;e.g., Softmax, Hinge)
    end
    
    subgraph &#34;Serving / Inference&#34;
        direction LR
        ItemEmbedding --&gt; AllItemEmbeddings[Pre-computed&lt;br/&gt;Item Embeddings]
        AllItemEmbeddings --&gt; ANN_Index[(Vector Database / FAISS&lt;br/&gt;Approximate Nearest Neighbor)]
        QueryEmbedding --&gt; ANN_Index
        ANN_Index --&gt; Top_K_Candidates[Top K Candidates]
    end

    style QueryEmbedding fill:#cde4ff
    style ItemEmbedding fill:#dff0d8
</code></pre><p>Excellent. Let&rsquo;s dive into the next section of Chapter 1. We&rsquo;ll cover <strong>Training Pipelines</strong>, <strong>Handling Imbalance</strong>, and <strong>Data Generation/Splitting</strong>. These topics are less about the model itself and more about the crucial data infrastructure that makes modeling possible at scale.</p><hr><h2 id=topic-training-pipeline--data-partitioning-page-33>Topic: Training Pipeline & Data Partitioning (Page 33)</h2><ul><li><p><strong>Book&rsquo;s Core Idea (Timeless):</strong> To train models on massive datasets (terabytes or petabytes), you can&rsquo;t just load a CSV file. The book correctly identifies two key strategies:</p><ol><li><strong>Columnar Storage (Parquet, ORC):</strong> Instead of storing data row-by-row like in a CSV, columnar formats store it column-by-column. <strong>Why this matters:</strong> ML training jobs often only need a subset of columns. Columnar formats let you read <em>only the columns you need</em>, dramatically reducing I/O and speeding up data loading. This is a fundamental concept in big data.</li><li><strong>Data Partitioning:</strong> Breaking up the data into a logical directory structure, almost always by date (e.g., <code>/year=2024/month=05/day=21/</code>). <strong>Why this matters:</strong> It allows the query engine to completely skip reading data from partitions that aren&rsquo;t relevant to your query (e.g., &ldquo;give me the last 7 days of data&rdquo;). This is called &ldquo;predicate pushdown&rdquo; and it&rsquo;s a massive performance win.</li></ol></li><li><p><strong>The 2024+ Perspective (Modern Augmentation):</strong> The principles are solid, but the ecosystem built on top has become standard.</p><ul><li><strong>The Rise of Table Formats (Iceberg/Delta Lake):</strong> The biggest change is the widespread adoption of open table formats like <strong>Apache Iceberg</strong> and <strong>Delta Lake</strong>. These are layers that sit <em>on top</em> of your Parquet files in the data lake. They solve critical problems that raw Parquet files don&rsquo;t:<ul><li><strong>ACID Transactions:</strong> They prevent you from reading corrupted or incomplete data if a write job fails midway. This is huge for data reliability.</li><li><strong>Schema Evolution:</strong> They make it safe to add/remove/rename columns without breaking downstream jobs.</li><li><strong>Time Travel:</strong> They allow you to query the state of your data at a specific point in time, which is invaluable for debugging and reproducing experiments.</li></ul></li><li><strong>Orchestration:</strong> Training pipelines aren&rsquo;t run manually. They are scheduled and managed by orchestrators. <strong>Airflow</strong> is the industry veteran, but newer tools like <strong>Dagster</strong> and <strong>Prefect</strong> are gaining traction by offering better data awareness and local development experiences.</li></ul></li><li><p><strong>Interview Focus & Phrasing:</strong></p><ul><li><strong>Good:</strong> &ldquo;We will store our training data in a data lake like S3, using the Parquet format and partitioning by date.&rdquo;</li><li><strong>Senior Level:</strong> &ldquo;We&rsquo;ll design a daily batch training pipeline orchestrated by Airflow. The pipeline&rsquo;s source data will reside in our data lake (S3) and be managed by an open table format like Apache Iceberg. This gives us transactional guarantees and schema safety. The training job itself, running on a Spark cluster, will read the last 90 days of data, leveraging Iceberg&rsquo;s partition pruning for efficiency, compute the necessary features, and then train the model.&rdquo;</li></ul></li></ul><p>Here&rsquo;s a diagram illustrating this modern training pipeline:</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph datasources[&#34;Data Sources&#34;]
        Logs[Log Files]
        DB[(Production DB)]
    end

    subgraph pipeline[&#34;Daily Batch Pipeline&#34;]
        direction LR
        Ingest[Ingestion Job Spark] --&gt; Lake[Data Lake with&lt;br/&gt;Apache Iceberg Tables]
        Lake -- &#34;Reads last N days&#34; --&gt; FeatEng[Feature Engineering Job Spark]
        FeatEng --&gt; TrainJob[Model Training Job&lt;br/&gt;PyTorch/XGBoost on Spark]
        TrainJob --&gt; ModelRegistry[(Model Registry&lt;br/&gt;MLflow, SageMaker)]
    end

    Logs --&gt; Ingest
    DB --&gt; Ingest
    
    style Lake fill:#cde4ff,stroke:#333,stroke-width:2px
</code></pre><h3 id=mapping-generic-concepts-to-aws-services>Mapping Generic Concepts to AWS Services</h3><p>Let&rsquo;s break down the AWS equivalent of that senior-level answer and build a concrete diagram around a common use case: <strong>Training a daily model for a Recommendation System</strong>.</p><table><thead><tr><th style=text-align:left>Generic Concept</th><th style=text-align:left>AWS Primary Service(s)</th><th style=text-align:left>Role / &ldquo;Why&rdquo; you use it</th></tr></thead><tbody><tr><td style=text-align:left><strong>Data Lake Storage</strong></td><td style=text-align:left><strong>Amazon S3</strong> (Simple Storage Service)</td><td style=text-align:left>The foundation. It&rsquo;s the cheap, durable, and scalable object store where you dump all your raw and processed data (like Parquet files).</td></tr><tr><td style=text-align:left><strong>Table Format</strong></td><td style=text-align:left><strong>AWS Glue Data Catalog</strong> (as the metastore) + <strong>Apache Iceberg/Hudi</strong></td><td style=text-align:left><strong>Glue Data Catalog</strong> acts as the central &ldquo;address book&rdquo; for your data lake. It stores the metadata (schema, location, partitions) for your tables. <strong>Iceberg</strong> (or Hudi) manages the actual data files within S3, providing the ACID transactions, time travel, and schema evolution we discussed.</td></tr><tr><td style=text-align:left><strong>Data Querying / Ad-hoc Analysis</strong></td><td style=text-align:left><strong>Amazon Athena</strong></td><td style=text-align:left>A serverless query engine that lets you run standard SQL directly on your data in S3 using the Glue Data Catalog. This is how data scientists explore the data without spinning up a cluster.</td></tr><tr><td style=text-align:left><strong>ETL / Feature Engineering</strong></td><td style=text-align:left><strong>AWS Glue</strong> (for serverless Spark jobs) or <strong>Amazon EMR</strong> (for managed Spark/Hadoop clusters)</td><td style=text-align:left><strong>Glue</strong> is great for simpler, serverless ETL jobs. <strong>EMR</strong> is for heavy-duty, long-running Spark jobs where you need more control over the cluster configuration. Both are used for the heavy lifting of data transformation.</td></tr><tr><td style=text-align:left><strong>Pipeline Orchestration</strong></td><td style=text-align:left><strong>Amazon MWAA</strong> (Managed Workflows for Apache Airflow)</td><td style=text-align:left>This is the AWS-managed version of Airflow. It&rsquo;s the &ldquo;conductor&rdquo; that defines the dependencies between your jobs (e.g., &ldquo;Run the feature engineering job <em>only after</em> the daily data ingestion job succeeds&rdquo;) and schedules them.</td></tr><tr><td style=text-align:left><strong>Model Training</strong></td><td style=text-align:left><strong>Amazon SageMaker Training</strong></td><td style=text-align:left>The service for running training jobs at scale. It handles spinning up the necessary compute instances (with GPUs if needed), running your training script (e.g., PyTorch, TensorFlow), and saving the final model artifact back to S3.</td></tr><tr><td style=text-align:left><strong>Model Registry</strong></td><td style=text-align:left><strong>Amazon SageMaker Model Registry</strong></td><td style=text-align:left>A centralized repository to version, approve, and manage your trained models before they are deployed. It&rsquo;s crucial for governance and reproducibility.</td></tr></tbody></table><p><strong>What about Snowflake?</strong>
Snowflake is a cloud data warehouse, not a data lake tool in this context. While it can connect to S3, it represents a different architectural pattern. The &ldquo;modern data stack&rdquo; we&rsquo;re describing here is centered on an open data lakehouse architecture (S3 + Iceberg + Spark), which is more common for large-scale ML training pipelines because it&rsquo;s more flexible and cost-effective for unstructured and semi-structured data. Mentioning Snowflake is fine, but the S3/Glue/Spark stack is the more direct AWS equivalent.</p><h3 id=use-case-diagram-daily-re-training-of-a-products-you-may-like-model-on-aws>Use Case Diagram: Daily Re-training of a &ldquo;Products You May Like&rdquo; Model on AWS</h3><p>Here is the diagram showing how these services fit together in a production pipeline.</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;Data Sources&#34;
        A[App &amp; Web Logs&lt;br/&gt;via Kinesis Firehose]
        B[S3 Raw Data Bucket]
        C[Production DB&lt;br/&gt;e.g., RDS, DynamoDB]
        D[Daily DB Snapshots]
    end
    
    subgraph &#34;Orchestration Layer: Amazon MWAA Airflow&#34;
        P1(DAG Starts&lt;br/&gt;Daily @ 1 AM)
    end
    
    subgraph &#34;ETL &amp; Feature Engineering Layer&#34;
        S3_Lakehouse[S3 Data Lakehouse&lt;br/&gt;Storage: Parquet Files&lt;br/&gt;Format: Apache Iceberg&lt;br/&gt;Metastore: AWS Glue Data Catalog]
        Glue_Job[AWS Glue ETL Job&lt;br/&gt;Spark]
    end
    
    subgraph &#34;Model Training &amp; Registration Layer&#34;
        SageMaker_Train[SageMaker Training Job&lt;br/&gt;PyTorch]
        SageMaker_Registry[(SageMaker Model Registry)]
    end
    
    subgraph &#34;Analytics &amp; Exploration&#34;
        Athena[Amazon Athena]
    end
    
    A --&gt; B
    C --&gt; D
    D --&gt; B
    B --&gt; Glue_Job
    Glue_Job --&gt; S3_Lakehouse
    S3_Lakehouse --&gt; SageMaker_Train
    SageMaker_Train --&gt; SageMaker_Registry
    Athena --&gt; S3_Lakehouse
    P1 --&gt; Glue_Job
    P1 --&gt; SageMaker_Train
    P1 --&gt; SageMaker_Registry

    style S3_Lakehouse fill:#cde4ff,stroke:#333,stroke-width:2px
    style Glue_Job fill:#fff0b3,stroke:#333,stroke-width:2px
    style SageMaker_Train fill:#dff0d8,stroke:#333,stroke-width:2px
    style Athena fill:#f5c6cb,stroke:#333,stroke-width:2px
</code></pre><h3 id=explaining-the-diagram-in-an-interview>Explaining the Diagram in an Interview</h3><p>&ldquo;Here&rsquo;s how I would architect the daily training pipeline on AWS for our recommendation model.</p><ol><li><p><strong>Orchestration:</strong> The entire process would be managed by a DAG in <strong>Amazon MWAA (Airflow)</strong>, scheduled to run daily.</p></li><li><p><strong>Ingestion:</strong> Raw data, like clickstream logs from Kinesis and daily snapshots from our production RDS database, lands in a dedicated S3 bucket.</p></li><li><p><strong>ETL & Feature Store Creation:</strong> Our first Airflow task kicks off an <strong>AWS Glue ETL job</strong>. This serverless Spark job reads the raw data, cleans and transforms it, computes our features, and writes the output as partitioned <strong>Parquet files</strong> back to our main S3 data lake. We&rsquo;ll use the <strong>AWS Glue Data Catalog</strong> as a metastore with the <strong>Apache Iceberg</strong> format. This gives us ACID compliance, so downstream jobs never read partial data, and allows our data science team to query the data easily with <strong>Athena</strong>.</p></li><li><p><strong>Model Training:</strong> Once the Glue job succeeds, Airflow triggers the next task: a <strong>SageMaker Training Job</strong>. SageMaker pulls our training script, provisions the necessary compute (e.g., GPU instances), reads the clean feature data directly from our S3 data lakehouse, and trains the model.</p></li><li><p><strong>Model Registration:</strong> After training, the final model artifact is versioned and saved to the <strong>SageMaker Model Registry</strong>. This creates a golden copy, tracks its performance metrics, and puts it in a &lsquo;pending approval&rsquo; state before it can be deployed to production. This CI/CD for ML approach is crucial for safe and reproducible deployments.&rdquo;</p></li></ol><hr><h2 id=topic-handle-imbalance-class-distribution-page-34>Topic: Handle Imbalance Class Distribution (Page 34)</h2><ul><li><p><strong>Book&rsquo;s Core Idea (Timeless):</strong> This is a classic problem in domains with rare events (ad clicks, fraud). The book gives the three canonical solutions:</p><ol><li><strong>Class Weighting:</strong> The simplest and often best first step. Penalize errors on the minority class more heavily in your loss function.</li><li><strong>Downsampling (Majority Class):</strong> When you have massive data (e.g., billions of non-clicks), you can throw away a large, random portion to make the training problem more manageable. This is the standard in big tech.</li><li><strong>Oversampling (Minority Class) / SMOTE:</strong> For smaller datasets where every positive example is precious. SMOTE creates synthetic minority class examples. Less common in large-scale systems.
The book&rsquo;s most important warning is to <strong>NEVER resample your validation/test set</strong>. They must reflect reality.</li></ol></li><li><p><strong>The 2024+ Perspective (Modern Augmentation):</strong></p><ul><li><strong>The Calibration Imperative:</strong> This is the critical piece that senior candidates <em>must</em> discuss. When you downsample, you change the baseline probability in your training data (e.g., from 1% clicks to 50% clicks). The model&rsquo;s raw output <code>p</code> will be biased high. You need to correct for this before the prediction can be used for anything quantitative (like bidding in an ad auction). The formula is essential:
<code>calibrated_p = p / (p + (1-p) / w)</code> where <code>w</code> is the downsampling rate (e.g., if you kept 10% of negatives, w=0.1).</li><li><strong>Focal Loss:</strong> As mentioned before, Focal Loss is a more advanced alternative to simple class weighting. It&rsquo;s an excellent talking point to show you know modern techniques.</li></ul></li><li><p><strong>Interview Focus & Phrasing:</strong></p><ul><li><strong>Good:</strong> &ldquo;This is an imbalanced problem, so I&rsquo;ll downsample the negative class in the training set.&rdquo;</li><li><strong>Senior Level:</strong> &ldquo;This is a classic class imbalance problem. We have several strategies. I&rsquo;d start with <strong>class weighting</strong>, as it&rsquo;s non-invasive. If performance is still an issue, we can move to <strong>downsampling the negative class</strong>. This is computationally efficient, but it requires a crucial post-processing step: <strong>we must calibrate the model&rsquo;s outputs</strong> to correct for the artificial sampling rate before they are used downstream. Our validation set will, of course, maintain the original, un-sampled distribution to give us a true measure of performance.&rdquo;</li></ul></li></ul><hr><h4 id=topic-data-generation-strategy--how-to-split-traintest-data-pages-36-39><strong>Topic: Data Generation Strategy & How to Split Train/Test Data (Pages 36, 39)</strong></h4><ul><li><p><strong>Book&rsquo;s Core Idea (Timeless):</strong></p><ul><li><strong>Getting Labels:</strong> How do you get initial labels? The book gives great examples: use chronological feeds, use existing user actions (&ldquo;likes&rdquo; are positive, items they saw but skipped are negative), or use proxy signals (LinkedIn using <code>skills</code> to recommend <code>courses</code>).</li><li><strong>Splitting Data:</strong> The most important rule is that for any time-series problem (forecasting, user behavior models), you <strong>must not split randomly</strong>. A random split would cause data leakage by using future information to train a model that predicts the past. You must use a time-based split (e.g., train on January, validate on February). The book&rsquo;s examples of <strong>Sliding Window</strong> and <strong>Expanding Window</strong> for backtesting are perfect.</li></ul></li><li><p><strong>The 2024+ Perspective (Modern Augmentation):</strong></p><ul><li><strong>Negative Sampling is a Design Choice:</strong> The book treats &ldquo;negative labels&rdquo; as something you just find. In modern recommender systems, particularly with two-tower models, the choice of <em>which</em> negatives to use during training is a key modeling decision.<ul><li><strong>Easy Negatives:</strong> Randomly sampled items.</li><li><strong>Hard Negatives:</strong> Items that are &ldquo;close&rdquo; to the positive item in the embedding space but the user didn&rsquo;t click. Training on these helps the model learn finer distinctions. YouTube&rsquo;s papers talk extensively about the importance of hard negative mining.</li></ul></li><li><strong>User-based Splitting:</strong> For personalization problems, another important splitting strategy is to split by <em>user</em>. You hold out a set of users entirely to test how the model performs on users it has never seen before (the &ldquo;cold start&rdquo; problem).</li></ul></li><li><p><strong>Interview Focus & Phrasing:</strong></p><ul><li><strong>Good:</strong> &ldquo;We&rsquo;ll split our data by time. We&rsquo;ll train on one month and test on the next day.&rdquo;</li><li><strong>Senior Level:</strong> &ldquo;Our splitting strategy is critical to avoid leakage. We will use a strict <strong>temporal split</strong>, for example, training on data up to day T and validating on day T+1. For our two-tower retrieval model, we need to be thoughtful about <strong>negative sampling</strong>. We&rsquo;ll start with random in-batch negatives, but to improve performance, we would implement a strategy for <strong>hard negative mining</strong> to help the model learn the subtle differences between similar items. To evaluate cold-start performance, we&rsquo;ll also maintain a holdout set of users.&rdquo;</li></ul></li></ul><p>Of course. Let&rsquo;s slow down and unpack these concepts properly. This is the core of what separates a junior from a senior MLE. We will go deep into each one, with the goal of you being able to explain it with the same level of clarity and intuition.</p><hr><h2 id=topic--loss-functions---the-why-behind-optimization>Topic : Loss Functions - The &ldquo;Why&rdquo; Behind Optimization</h2><p>A loss function is the mathematical objective your model minimizes during training. It&rsquo;s the most direct way you, the engineer, tell the model what &ldquo;good&rdquo; looks like.</p><h3 id=deep-dive-1-regression-loss-how-wrong-is-our-number>Deep Dive 1: Regression Loss: How wrong is our number?</h3><h4 id=a-mse-vs-mae-the-outlier-dilemma>A. MSE vs. MAE: The Outlier Dilemma</h4><p>Let&rsquo;s imagine we&rsquo;re predicting house prices.</p><ul><li><strong>Target Price:</strong> $300k</li><li><strong>Model A Prediction:</strong> $310k (Error: $10k)</li><li><strong>Model B Prediction:</strong> $350k (Error: $50k)</li></ul><p>Now, let&rsquo;s see how MSE and MAE treat these errors.</p><p><strong>Mean Absolute Error (MAE):</strong> The penalty is linear with the error.</p><ul><li><strong>Equation:</strong> <code>MAE = (1/n) * Σ |y_true - y_pred|</code></li><li><strong>Intuition:</strong> It answers &ldquo;On average, how far off are our predictions in dollars?&rdquo;</li><li><strong>Calculation:</strong><ul><li>Model A&rsquo;s MAE contribution: <code>|$300k - $310k| = $10k</code></li><li>Model B&rsquo;s MAE contribution: <code>|$300k - $350k| = $50k</code></li></ul></li><li><strong>Conclusion:</strong> The error from Model B ($50k) is exactly <strong>5 times</strong> worse than the error from Model A ($10k). This feels intuitive.</li></ul><p><strong>Mean Squared Error (MSE):</strong> The penalty grows quadratically with the error.</p><ul><li><strong>Equation:</strong> <code>MSE = (1/n) * Σ (y_true - y_pred)²</code></li><li><strong>Intuition:</strong> It answers &ldquo;What&rsquo;s the average of the squared errors?&rdquo; It doesn&rsquo;t have an intuitive unit, which is why we often use RMSE (Root Mean Squared Error = <code>√MSE</code>) to bring it back to the original units (dollars).</li><li><strong>Calculation:</strong><ul><li>Model A&rsquo;s MSE contribution: <code>($10k)² = 100,000,000</code></li><li>Model B&rsquo;s MSE contribution: <code>($50k)² = 2,500,000,000</code></li></ul></li><li><strong>Conclusion:</strong> The error from Model B is <strong>25 times</strong> worse than the error from Model A. MSE <em>despises</em> large errors and will aggressively tune the model to avoid them, even at the cost of being slightly worse on other, smaller errors.</li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;MAE (Linear Penalty)&#34;
        A[Error of $10k] --&gt; B(Penalty = 10k)
        C[Error of $50k] --&gt; D(Penalty = 50k)
    end
    
    subgraph &#34;MSE (Quadratic Penalty)&#34;
        E[Error of $10k] --&gt; F(Penalty = 100M)
        G[Error of $50k] --&gt; H(Penalty = 2,500M)
    end

    style D fill:#f5c6cb
    style H fill:#f5c6cb
</code></pre><p><strong>Senior Takeaway:</strong> Choose MSE when outliers are &ldquo;real&rdquo; errors you must avoid (e.g., predicting stress on a bridge beam). Choose MAE when outliers might be measurement noise and you don&rsquo;t want them to dominate the model&rsquo;s training.</p><h4 id=b-quantile-loss-when-over-vs-under-matters><strong>B. Quantile Loss: When Over vs. Under Matters</strong></h4><p>Imagine we&rsquo;re DoorDash, estimating food delivery time.</p><ul><li><strong>Case 1 (Over-prediction):</strong> We predict 30 mins, food arrives in 25 mins. <strong>Result:</strong> Happy, pleasantly surprised customer.</li><li><strong>Case 2 (Under-prediction):</strong> We predict 30 mins, food arrives in 35 mins. <strong>Result:</strong> Unhappy, frustrated customer who might not order again.</li></ul><p>Clearly, under-prediction is more costly. We want our model to &ldquo;know&rdquo; this. This is where Quantile Loss comes in.</p><ul><li><strong>Equation:</strong> <code>Quantile Loss = Σ q * |y_true - y_pred|</code> (if error is positive, i.e., under-prediction) <code>+ Σ (1-q) * |y_true - y_pred|</code> (if error is negative, i.e., over-prediction). Here <code>q</code> is the quantile.</li><li><strong>Intuition:</strong> It&rsquo;s just a weighted MAE. You choose the weights.</li><li><strong>Example (q = 0.8):</strong><ul><li>We want 80% of our predictions to be over-estimates or correct.</li><li>If we predict 30 mins and it arrives in 35 (under-prediction), the error of 5 mins is multiplied by <code>q = 0.8</code>. Loss = <code>5 * 0.8 = 4.0</code>.</li><li>If we predict 30 mins and it arrives in 25 (over-prediction), the error of -5 mins is multiplied by <code>(1-q) = 0.2</code>. Loss = <code>5 * 0.2 = 1.0</code>.</li></ul></li><li>The model now learns it&rsquo;s <strong>4 times more painful</strong> to be late than to be early. It will naturally start &ldquo;padding&rdquo; its estimates to avoid the larger penalty.</li></ul><hr><h3 id=deep-dive-2-classification-loss-how-wrong-is-our-label>Deep Dive 2: Classification Loss: How wrong is our label?</h3><h4 id=a-contrastive-loss-learning-by-comparison><strong>A. Contrastive Loss: Learning by Comparison</strong></h4><p>This is the key to all modern retrieval systems (search, recommendation, RAG). Forget predicting a single score for a moment. The goal is to produce an embedding (a vector) for the user and for every item, such that the vectors of &ldquo;good&rdquo; pairs are close together.</p><ul><li><p><strong>The Setup:</strong> Imagine a mini-batch of training data for a YouTube recommender. It contains one &ldquo;positive&rdquo; pair: <code>(User A, Video_SciFi_1)</code> because User A watched it. It also contains several &ldquo;negative&rdquo; pairs, which are just the other videos in the batch: <code>(User A, Video_Cooking_2)</code>, <code>(User A, Video_Sports_3)</code>.</p></li><li><p><strong>The Goal:</strong> We want to make the similarity score (often dot product) of the positive pair much higher than the similarity scores of the negative pairs.</p><ul><li><code>Sim(User_A_vec, Video_SciFi_1_vec)</code> should be &#187; <code>Sim(User_A_vec, Video_Cooking_2_vec)</code></li><li><code>Sim(User_A_vec, Video_SciFi_1_vec)</code> should be &#187; <code>Sim(User_A_vec, Video_Sports_3_vec)</code></li></ul></li><li><p><strong>How it Works (In-batch Softmax, a common contrastive loss):</strong></p><ol><li>Calculate the similarity score <code>s_i</code> for every user-item pair in the batch.</li><li>Treat it like a classification problem! Apply a Softmax function across all the scores.</li><li>The loss is simply the Cross-Entropy loss of trying to &ldquo;classify&rdquo; the positive pair as the correct one.</li></ol></li><li><p><strong>Equation (Conceptual):</strong> <code>Loss = -log( exp(s_positive) / (exp(s_positive) + Σ exp(s_negative)) )</code></p></li><li><p><strong>Why it&rsquo;s genius:</strong> By minimizing this loss, you are implicitly pushing the positive score up and all the negative scores down, achieving the goal of separating them in the embedding space. This is how the two-tower model is trained.</p></li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph LR
    subgraph &#34;Embedding Space&#34;
        direction BT
        U(User)
        P((Positive&lt;br/&gt;Item))
        N1((Negative&lt;br/&gt;Item))
        N2((Negative&lt;br/&gt;Item))
        
        U -- &#34;Pull Closer&#34; --&gt; P
        U -- &#34;Push Away&#34; --&gt; N1
        U -- &#34;Push Away&#34; --&gt; N2
    end
    
    CL(Contrastive Loss)
    CL --&gt; U
    CL --&gt; P
    CL --&gt; N1
    CL --&gt; N2
    
    style P fill:#dff0d8
    style N1 fill:#f5c6cb
    style N2 fill:#f5c6cb
</code></pre><p><strong>Senior Takeaway:</strong> When designing a system with a retrieval step, you <em>must</em> talk about using a contrastive loss to train the embedding models (the two towers).</p><hr><h3 id=deep-dive-3-huber-loss-the-robust-best-of-both-worlds>Deep Dive 3: Huber Loss: The Robust &ldquo;Best of Both Worlds&rdquo;</h3><ul><li><p><strong>Book&rsquo;s Core Idea (Timeless):</strong> Huber Loss is designed as a composite function to get the best attributes of both MSE and MAE. For small errors, it uses a quadratic function (like MSE), which is strongly convex and leads to stable convergence. For large errors, it switches to a linear function (like MAE), which prevents outliers from dominating the gradient and pulling the model too far off course.</p></li><li><p><strong>Intuition & Equation:</strong> It&rsquo;s defined by a threshold hyperparameter, <code>δ</code> (delta). You, the engineer, decide what constitutes a &ldquo;small&rdquo; vs. &ldquo;large&rdquo; error.</p><p>The loss is defined piecewise:</p><ul><li>If <code>|y_true - y_pred| &lt;= δ</code>: <code>Loss = 1/2 * (y_true - y_pred)²</code> (It&rsquo;s MSE)</li><li>If <code>|y_true - y_pred| > δ</code>: <code>Loss = δ * |y_true - y_pred| - 1/2 * δ²</code> (It&rsquo;s MAE, with a constant adjustment to make the function smooth at the <code>δ</code> boundary)</li></ul></li><li><p><strong>Example:</strong> Let&rsquo;s set <code>δ = 1.0</code>. We&rsquo;re predicting a value of 10.</p><ul><li><strong>Prediction 1 = 10.5 (Error = 0.5):</strong> Since <code>0.5 &lt;= δ</code>, we use the MSE part. Loss = <code>0.5 * (0.5)² = 0.125</code>.</li><li><strong>Prediction 2 = 13.0 (Error = 3.0):</strong> Since <code>3.0 > δ</code>, we use the MAE part. Loss = <code>1.0 * 3.0 - 0.5 * (1.0)² = 2.5</code>.</li><li>Notice that a pure MSE would have given a loss of <code>(3.0)² = 9.0</code>. Huber loss provides a much smaller, more reasonable penalty.</li></ul></li><li><p><strong>The 2024+ Perspective:</strong> Huber Loss remains a fantastic, practical choice for many regression problems. Its main drawback is simply the need to tune the <code>δ</code> hyperparameter. In libraries like XGBoost and LightGBM, it&rsquo;s often available as a built-in objective function. It signals that you are thinking about the robustness of your training process.</p></li><li><p><strong>Interview Phrasing:</strong> &ldquo;For this regression task, I&rsquo;d start with Huber loss instead of a simple MSE. This would make our training process more robust to potential outliers in the data, which are common in real-world datasets. We would need to tune the delta hyperparameter, likely via cross-validation, to define the point at which we treat an error as an outlier.&rdquo;</p></li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;Error Size&#34;
        A[Small Error&lt;br/&gt;error &lt;= δ]
        B[Large Error&lt;br/&gt;error &gt; δ]
    end
    
    subgraph &#34;Huber Loss Behavior&#34;
        C[Quadratic Penalty&lt;br/&gt;Like MSE]
        D[Linear Penalty&lt;br/&gt;Like MAE]
    end
    
    A --&gt; C
    B --&gt; D
    
    C --&gt; E[Result: Stable convergence&lt;br/&gt;for common cases]
    D --&gt; F[Result: Robust to outliers]

    style C fill:#dff0d8
    style D fill:#f5c6cb
</code></pre><hr><h3 id=deep-dive-4-how-facebook-uses-normalized-cross-entropy-nce-the-fair-comparison-metric>Deep Dive 4: How Facebook Uses Normalized Cross Entropy (NCE): The &ldquo;Fair Comparison&rdquo; Metric</h3><ul><li><p><strong>Book&rsquo;s Core Idea (Timeless):</strong> The book presents a crucial real-world problem: how do you compare the performance of two models that operate in different environments? Normal Log Loss is not a fair comparison.</p><ul><li><strong>Problem:</strong> Imagine you have Model A for US ads (base CTR = 2%) and Model B for Indian ads (base CTR = 0.5%). Model A achieves a log loss of 0.10. Model B achieves a log loss of 0.04. Is Model B better? Not necessarily! It&rsquo;s much easier to get a low log loss on a dataset with very low base probability.</li></ul></li><li><p><strong>Intuition & Equation:</strong> NCE re-frames the question from &ldquo;What is the model&rsquo;s log loss?&rdquo; to &ldquo;<strong>How much better is the model than a dumb baseline that always predicts the average?</strong>&rdquo;</p><p><code>NCE = LogLoss(model) / LogLoss(background_rate)</code></p><p>Where <code>LogLoss(background_rate)</code> is the cross-entropy you&rsquo;d get if your model just predicted the average CTR for every single example.</p><ul><li><strong>Interpretation:</strong><ul><li><code>NCE &lt; 1.0</code>: Your model is smarter than the baseline. The lower, the better.</li><li><code>NCE = 1.0</code>: Your model is exactly as good as the baseline.</li><li><code>NCE > 1.0</code>: Your model is actively worse than just predicting the average.</li></ul></li></ul></li><li><p><strong>Example (from the book):</strong></p><ul><li>Model 1 (Fixed Prediction) has a log loss of 0.36 on data with 10% CTR. The baseline log loss is 0.325. <code>NCE = 0.36 / 0.325 = 1.11</code>. This model is <strong>worse</strong> than just guessing the average.</li><li>Model 2 (Fancy Model) has a log loss of 0.65 on data with 50% CTR. The baseline log loss is 0.693. <code>NCE = 0.65 / 0.693 = 0.945</code>. This model is <strong>better</strong> than guessing the average.</li><li><strong>Conclusion:</strong> The fancy model is the &ldquo;smarter&rdquo; model, even though its absolute log loss was higher.</li></ul></li><li><p><strong>The 2024+ Perspective:</strong> This concept is a hallmark of a mature ML practice. It&rsquo;s used for comparing models across different countries, platforms (iOS vs. Android), or any segment with a different underlying target distribution. It&rsquo;s a fantastic point to bring up in a discussion about offline evaluation.</p></li><li><p><strong>Interview Phrasing:</strong> &ldquo;When evaluating our click prediction model across different regions, simply comparing log loss can be misleading due to varying background CTRs. I would implement Normalized Cross-Entropy (NCE) as a key offline metric. This would allow us to measure the &rsquo;lift over baseline&rsquo; for each model in its respective domain, giving us a fair and comparable measure of model intelligence.&rdquo;</p></li></ul><hr><h3 id=deep-dive-4-forecast-metrics-mape-and-smape>Deep Dive 4: Forecast Metrics: MAPE and SMAPE</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> These metrics are used when you care about <strong>relative error</strong> (percentage) rather than absolute error. This is useful for comparing forecast accuracy on items with different scales (e.g., forecasting sales of a $1000 TV vs. a $10 pack of batteries).</li><li><strong>Mean Absolute Percentage Error (MAPE):</strong><ul><li><strong>Equation:</strong> <code>MAPE = (100%/n) * Σ |(Actual - Forecast) / Actual|</code></li><li><strong>The Big Flaw (Asymmetry):</strong> MAPE has a different range for over-forecasting vs. under-forecasting.<ul><li>If <code>Actual = 100</code>, <code>Forecast = 50</code> -> <code>|50/100|</code> = 50% error.</li><li>If <code>Actual = 100</code>, <code>Forecast = 150</code> -> <code>|-50/100|</code> = 50% error.</li><li>If <code>Actual = 100</code>, <code>Forecast = 0</code> (max under-forecast) -> <code>|100/100|</code> = <strong>100% error</strong>.</li><li>If <code>Actual = 100</code>, <code>Forecast = 300</code> -> <code>|-200/100|</code> = <strong>200% error</strong>.</li></ul></li><li>The penalty for over-forecasting is unbounded, while the penalty for under-forecasting is capped at 100%. This means a model trained to minimize MAPE will learn to be biased towards under-forecasting. It also blows up if <code>Actual</code> is zero.</li></ul></li><li><strong>Symmetric Absolute Percentage Error (SMAPE):</strong><ul><li><strong>Equation:</strong> <code>SMAPE = (100%/n) * Σ |Forecast - Actual| / ((|Actual| + |Forecast|)/2)</code></li><li><strong>The Fix:</strong> It attempts to fix the asymmetry by normalizing by the <em>average</em> of the actual and forecast values. This bounds the metric between 0% and 200%.</li><li><strong>The New Problem:</strong> The interpretation is less intuitive, and it can still be unstable if both Actual and Forecast are very close to zero.</li></ul></li><li><strong>The 2024+ Perspective:</strong> While commonly found in business reports due to their intuitive nature, these metrics are often avoided as direct loss functions for training because of their instability. A more common approach in serious forecasting systems is to:<ol><li><strong>Transform the data:</strong> Take the <code>log(1 + target)</code>.</li><li><strong>Train the model:</strong> Use a stable loss like MAE or Huber on the transformed data.</li><li><strong>Inverse transform:</strong> Exponentiate the model&rsquo;s predictions to get back to the original scale.</li><li><strong>Report:</strong> Calculate MAPE/SMAPE on the final predictions for business stakeholders.</li></ol></li><li><strong>Interview Phrasing:</strong> &ldquo;For reporting forecast accuracy to our business partners, MAPE is a good choice because percentage error is easy to understand. However, for the model&rsquo;s objective function, I would avoid MAPE due to its known asymmetry and instability. A more robust approach is to train on the log-transformed sales data using a simple MAE loss, and then report MAPE on the final, inverse-transformed predictions.&rdquo;</li></ul><hr><h3 id=deep-dive-5-focal-loss-focusing-on-the-hard-cases>Deep Dive 5: Focal Loss: Focusing on the Hard Cases</h3><ul><li><p><strong>Book&rsquo;s Core Idea (Timeless):</strong> Focal Loss is an improvement on standard Cross-Entropy for cases with extreme class imbalance, especially when there are many &ldquo;easy&rdquo; examples that dominate the loss. Its canonical use case is object detection, where the vast majority of pixels in an image are &ldquo;easy background.&rdquo;</p></li><li><p><strong>Intuition & Equation:</strong> It adds a modulating factor <code>(1 - p_t)^γ</code> to the cross-entropy loss.</p><p><code>Focal Loss = - (1 - p_t)^γ * log(p_t)</code></p><ul><li><code>p_t</code>: The model&rsquo;s predicted probability for the ground-truth class.</li><li><code>γ</code> (gamma): The focusing parameter (e.g., <code>γ = 2</code>).</li></ul></li><li><p><strong>Example:</strong></p><ul><li><strong>Easy Example:</strong> The model is very confident about a correct prediction, <code>p_t = 0.99</code>. The modulating factor is <code>(1 - 0.99)² = 0.0001</code>. The loss for this example is almost zeroed out.</li><li><strong>Hard Example:</strong> The model is very unsure, <code>p_t = 0.1</code>. The modulating factor is <code>(1 - 0.1)² = 0.81</code>. The loss for this example is only slightly reduced.</li></ul></li><li><p><strong>The Result:</strong> The model is freed from wasting its capacity on perfecting its confidence for easy examples and can focus its updates on learning the hard ones.</p></li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;Standard Cross-Entropy&#34;
        Easy[Easy Example&lt;br/&gt;p=0.99] --&gt; Loss_Easy_CE[Loss = 0.01]
        Hard[Hard Example&lt;br/&gt;p=0.1] --&gt; Loss_Hard_CE[Loss = 2.3]
        Loss_Hard_CE -- &#34;230x larger&#34; --&gt; Loss_Easy_CE
    end
    
    subgraph &#34;Focal Loss (γ=2)&#34;
        Easy2[Easy Example&lt;br/&gt;p=0.99] --&gt; Loss_Easy_FL[Loss ≈ 0.0001]
        Hard2[Hard Example&lt;br/&gt;p=0.1] --&gt; Loss_Hard_FL[Loss ≈ 1.86]
        Loss_Hard_FL -- &#34;~18,600x larger&#34; --&gt; Loss_Easy_FL
    end

    Title1[Total loss is dominated by&lt;br/&gt;a huge number of easy examples]
    Title2[Total loss is dominated by&lt;br/&gt;the few, hard examples]
</code></pre><ul><li><strong>The 2024+ Perspective:</strong> Focal loss remains a very important and widely recognized technique. Mentioning it shows you are familiar with advanced solutions to the common class imbalance problem.</li><li><strong>Interview Phrasing:</strong> &ldquo;Given the extreme imbalance between clicked and non-clicked ads, a simple weighted cross-entropy might not be enough. The loss could be dominated by the millions of easily-classified non-clicks. I would experiment with Focal Loss. By setting a gamma value, we can dynamically down-weight the loss for these easy examples and force the model to focus its capacity on the much smaller and harder-to-predict set of positive clicks.&rdquo;</li></ul><hr><h3 id=deep-dive-6-hinge-loss-learning-with-a-margin>Deep Dive 6: Hinge Loss: Learning with a Margin</h3><ul><li><p><strong>Book&rsquo;s Core Idea (Timeless):</strong> Hinge loss is primarily associated with Support Vector Machines (SVMs). Its goal is not just to get the classification right, but to get it right with a high degree of confidence. It introduces the concept of a &ldquo;margin.&rdquo;</p></li><li><p><strong>Intuition & Equation:</strong> It operates on raw scores (not probabilities) and uses labels of <code>{-1, +1}</code>.</p><p><code>Loss = max(0, 1 - y_true * y_pred)</code></p></li><li><p><strong>Example:</strong> The margin is at <code>+1</code> for positive classes and <code>-1</code> for negative classes.</p><ul><li><strong>Case 1 (Correct, Confident):</strong> <code>y_true = 1</code>, <code>y_pred = 1.5</code>. Loss = <code>max(0, 1 - 1*1.5) = 0</code>. No penalty. The point is outside the margin.</li><li><strong>Case 2 (Correct, Not Confident):</strong> <code>y_true = 1</code>, <code>y_pred = 0.6</code>. Loss = <code>max(0, 1 - 1*0.6) = 0.4</code>. It&rsquo;s penalized because it&rsquo;s correct but falls inside the margin.</li><li><strong>Case 3 (Incorrect):</strong> <code>y_true = 1</code>, <code>y_pred = -0.5</code>. Loss = <code>max(0, 1 - 1*(-0.5)) = 1.5</code>. Heavily penalized.</li></ul></li><li><p><strong>The 2024+ Perspective:</strong> While classic SVMs are less used, Hinge Loss is very relevant today as a form of <strong>contrastive loss</strong>. It&rsquo;s excellent for training retrieval models. You can formulate a <strong>Triplet Loss</strong> using it: <code>Loss = max(0, margin - Sim(anchor, positive) + Sim(anchor, negative))</code>. This loss function directly optimizes to ensure the similarity of a positive pair is greater than the similarity of a negative pair by at least <code>margin</code>.</p></li><li><p><strong>Interview Phrasing:</strong> &ldquo;While Hinge Loss is classically from SVMs, its &lsquo;margin&rsquo; concept is very useful in modern systems. For our two-tower retrieval model, we could use a hinge-based triplet loss. This would train the model to ensure that the dot product of a user with a relevant item is greater than the dot product with an irrelevant item by a specific, pre-defined margin, leading to a more robust separation in the embedding space.&rdquo;</p></li></ul><h2 id=topic-model-evaluation-metrics>Topic: Model Evaluation Metrics</h2><hr><h3 id=area-under-the-curve-auc-the-better-than-chance-metric>Area Under the Curve (AUC): The &ldquo;Better than Chance?&rdquo; Metric</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> AUC, specifically for the Receiver Operating Characteristic (ROC) curve, is one of the most fundamental metrics for binary classification. It answers a single, powerful question: <strong>&ldquo;If I randomly pick one positive example and one negative example, what is the probability that my model assigned a higher score to the positive one?&rdquo;</strong></li><li><strong>Intuition & How it&rsquo;s built:</strong><ol><li>Your model doesn&rsquo;t output 0s and 1s. It outputs a continuous score (e.g., 0.0 to 1.0).</li><li>To make a decision, you need a <strong>threshold</strong>. (e.g., &ldquo;If score > 0.5, classify as 1&rdquo;).</li><li>The ROC curve is generated by plotting the <strong>True Positive Rate (TPR)</strong> against the <strong>False Positive Rate (FPR)</strong> for <em>every possible threshold</em>.<ul><li><strong>TPR (Recall):</strong> <code>True Positives / (True Positives + False Negatives)</code> -> &ldquo;Of all the actual positives, how many did we find?&rdquo;</li><li><strong>FPR:</strong> <code>False Positives / (False Positives + True Negatives)</code> -> &ldquo;Of all the actual negatives, how many did we incorrectly label as positive?&rdquo;</li></ul></li><li><strong>AUC is the literal area under this curve.</strong></li></ol></li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;ROC Curve&#34;
        direction LR
        A((0,0)) -- &#34;Threshold=1.0&lt;br/&gt;Catch nothing&#34; --&gt; B(Model&#39;s Curve)
        B -- &#34;Threshold=0.0&lt;br/&gt;Catch everything&#34; --&gt; C((1,1))
        A -- &#34;Random Guessing&lt;br/&gt;AUC = 0.5&#34; --&gt; C
        D((0,1))
        style D fill:#dff0d8,stroke-width:4px,stroke:green
        X_AXIS --- Y_AXIS
    end
    
    subgraph Legend
        Ideal[&#34;Perfect Model&lt;br/&gt;Point (0,1)&#34;]
        Good[&#34;Good Model&lt;br/&gt;Curve bows to top-left&#34;]
        Random[&#34;Random Model&lt;br/&gt;Diagonal Line&#34;]
    end
    
    Y_AXIS[True Positive Rate]
    X_AXIS[False Positive Rate]
</code></pre><ul><li><strong>Interpretation:</strong><ul><li><code>AUC = 1.0</code>: Perfect classifier.</li><li><code>AUC = 0.5</code>: Useless classifier, equivalent to a random guess.</li><li><code>AUC = 0.0</code>: Perfectly wrong classifier (it&rsquo;s always predicting the opposite).</li></ul></li><li><strong>The 2024+ Perspective:</strong> AUC&rsquo;s main strength is that it&rsquo;s <strong>threshold-independent</strong>. It evaluates the quality of your model&rsquo;s <em>scoring</em> without you having to first pick a decision threshold. However, its weakness is that it can be misleading on highly imbalanced datasets. For a dataset with 99.9% negatives, a model can get a very high AUC by just being good at identifying negatives. This is why <strong>Precision-Recall AUC (PR-AUC)</strong> is often preferred in these imbalanced cases.</li><li><strong>Interview Phrasing:</strong> &ldquo;To get a high-level view of my classifier&rsquo;s discriminative power, I would start by measuring the ROC AUC. This gives us a threshold-independent measure of separability. However, since this is a highly imbalanced ad-click problem, I would also closely monitor the Precision-Recall AUC, as it provides a better picture of performance on the rare positive class, which is our primary business interest.&rdquo;</li></ul><hr><h3 id=mean-average-recall-at-k-mark-the-did-we-find-everything-metric>Mean Average Recall at K (MAR@K): The &ldquo;Did we find everything?&rdquo; Metric</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> As we discussed, this is for the retrieval/candidate generation stage. But what if you have a list of users? MAR@K is simply the average of the Recall@K values across all your users.</li><li><strong>Equation:</strong> <code>MAR@K = (1/|U|) * Σ (for each user u in U) Recall@K(u)</code><ul><li>Where <code>|U|</code> is the total number of users.</li></ul></li><li><strong>Example:</strong><ul><li>User 1: We found 7 of their 10 relevant items. <code>Recall@K = 0.7</code>.</li><li>User 2: We found 4 of their 5 relevant items. <code>Recall@K = 0.8</code>.</li><li>User 3: We found 9 of their 12 relevant items. <code>Recall@K = 0.75</code>.</li><li><code>MAR@K = (0.7 + 0.8 + 0.75) / 3 = 0.75</code></li></ul></li><li><strong>The 2024+ Perspective:</strong> It&rsquo;s a solid metric. The senior-level extension is to ask, &ldquo;What is the <em>distribution</em> of recall?&rdquo; An average of 75% could be great, but it could also hide that the model has 99% recall for US users and only 10% recall for new users from Brazil. You must investigate the variance and look at different percentiles (e.g., p50, p90, p99) of the recall scores, not just the mean.</li><li><strong>Interview Phrasing:</strong> &ldquo;Our primary metric for the candidate generator will be Mean Average Recall at 100. But I won&rsquo;t stop at the mean. I will also analyze the full distribution of the per-user recall scores to ensure the model is performing equitably across different user segments and not just optimizing for the average.&rdquo;</li></ul><hr><h3 id=mean-average-precision-map--mean-reciprocal-rank-mrr-is-the-first-right-answer-high-up>Mean Average Precision (MAP) & Mean Reciprocal Rank (MRR): &ldquo;Is the first right answer high up?&rdquo;</h3><ul><li><p><strong>Book&rsquo;s Core Idea (Timeless):</strong> These are ranking metrics. They care about the <em>order</em> of the results.</p></li><li><p><strong>Mean Reciprocal Rank (MRR):</strong> The simpler of the two. It&rsquo;s only useful when you care about a <strong>single right answer</strong>.</p><ul><li><strong>Question it answers:</strong> &ldquo;On average, how far down the list do I have to go to find the first relevant item?&rdquo;</li><li><strong>Equation:</strong> <code>MRR = (1/|Q|) * Σ (for each query q in Q) (1 / rank_of_first_relevant_item)</code></li><li><strong>Example:</strong><ul><li>Query 1: First correct answer is at position 3. <code>Score = 1/3</code>.</li><li>Query 2: First correct answer is at position 1. <code>Score = 1/1</code>.</li><li>Query 3: First correct answer is at position 8. <code>Score = 1/8</code>.</li><li><code>MRR = (1/3 + 1 + 1/8) / 3</code>.</li></ul></li><li><strong>Use Case:</strong> Fact-based question answering (&ldquo;Who is the CEO of Apple?&rdquo;). There&rsquo;s only one right answer.</li></ul></li><li><p><strong>Mean Average Precision (MAP):</strong> The more sophisticated one. It&rsquo;s used when there can be <strong>multiple relevant documents</strong>.</p><ul><li><strong>Question it answers:</strong> It&rsquo;s a bit complex, but intuitively it&rsquo;s the mean of the &ldquo;Precision@K&rdquo; scores, calculated only at the positions where a relevant document was found. It rewards both finding many relevant documents and finding them early.</li><li><strong>Use Case:</strong> A web search for &ldquo;deep learning tutorials.&rdquo; There are many good results.</li></ul></li><li><p><strong>The 2024+ Perspective (Outdated):</strong> While historically important, <strong>MAP and MRR have been largely superseded by NDCG</strong> in modern ML systems. Why? Because NDCG can handle graded relevance (e.g., a tutorial from a top university is more relevant than a random blog post), while MAP/MRR cannot (an item is just relevant or not). In an interview, it&rsquo;s good to know what they are, but you should recommend NDCG.</p></li><li><p><strong>Interview Phrasing:</strong> &ldquo;For this search ranking task, we could use metrics like MAP or MRR. However, since not all relevant results are equally good, I would propose using <strong>NDCG</strong>. It&rsquo;s a more flexible and powerful metric because it allows us to assign different relevance scores to the results—for example, a direct booking link could have a higher relevance score than a link to a review site.&rdquo;</p></li></ul><hr><h3 id=normalized-discounted-cumulative-gain-ndcg--cumulative-gain-cg>Normalized Discounted Cumulative Gain (NDCG) & Cumulative Gain (CG)</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> We already did a deep dive on NDCG, but let&rsquo;s formalize it and its precursor, CG.</li><li><strong>Cumulative Gain (CG):</strong> The dumbest version. It just sums up the relevance scores of the items in your list, ignoring order.<ul><li><strong>Equation:</strong> <code>CG@k = Σ (from i=1 to k) rel_i</code> (where <code>rel_i</code> is the relevance of the item at position i).</li><li><strong>Example:</strong> Rankings <code>[Rel=3, Rel=2, Rel=1]</code> and <code>[Rel=1, Rel=2, Rel=3]</code> both have a <code>CG@3</code> of 6. This is obviously wrong, as the first ranking is much better. <strong>CG is useless in practice.</strong></li></ul></li><li><strong>Discounted Cumulative Gain (DCG):</strong> The smarter version that fixes CG&rsquo;s problem by introducing a positional penalty (the logarithmic discount). We covered this.</li><li><strong>Normalized Discounted Cumulative Gain (NDCG):</strong> The best version that fixes DCG&rsquo;s problem by normalizing by the score of the perfect ranking. We covered this.</li><li><strong>The 2024+ Perspective:</strong> NDCG is the <strong>gold standard for ranking evaluation</strong> in any major tech company. If the problem involves ranking, you must talk about NDCG. Full stop.</li></ul><hr><h3 id=online-metrics-click-through-rate-time-spent>Online Metrics: Click-Through Rate, Time Spent</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> These are the business metrics you measure in an A/B test.<ul><li><strong>Click-Through Rate (CTR):</strong> <code>(Clicks / Impressions)</code>. The classic engagement metric for anything &ldquo;clickable.&rdquo;</li><li><strong>Time Spent:</strong> The total time a user spends on a page, watching a video, etc. This is often a better proxy for user satisfaction than CTR. A user might click on a clickbait title (high CTR) but leave after 2 seconds (low time spent), which is a bad outcome.</li></ul></li><li><strong>The 2024+ Perspective:</strong> The key evolution here is understanding <strong>complex, long-term trade-offs</strong> and designing metrics to capture them.<ul><li><strong>Example: YouTube Shorts vs. Long-form video.</strong><ul><li>Shorts have an incredibly high CTR and &ldquo;session starts&rdquo; per hour.</li><li>Long-form videos have lower CTR but generate much more total watch time and ad revenue per stream.</li></ul></li><li>If you only optimize for CTR, the algorithm will only show Shorts. If you only optimize for total watch time, it might only show 2-hour documentaries.</li><li><strong>The Solution:</strong> You need a <strong>composite or weighted primary metric</strong> for your A/B test. This is a business decision, not just an ML one. It might be something like <code>Primary_Metric = w1 * CTR + w2 * Total_Watch_Time + w3 * Subscribes</code>. Deciding on these weights is a key product/engineering collaboration.</li></ul></li><li><strong>Interview Phrasing:</strong> &ldquo;For our online A/B test, CTR is a good starting metric. However, it could be susceptible to clickbait and might not capture true user satisfaction. I would propose we track &lsquo;Time Spent on Content&rsquo; as a more robust proxy for engagement. Even better, we should work with the product team to define a <strong>composite primary metric</strong> that balances multiple business goals, such as engagement and creator follows, to prevent our model from over-optimizing for a single, narrow objective.&rdquo;</li></ul><hr><h2 id=deep-dive-3-common-sampling-techniques>Deep Dive 3: Common Sampling Techniques</h2><h3 id=random-sampling-the-simplest-baseline>Random Sampling: The Simplest Baseline</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> This is the most straightforward technique: select items from a population where each item has an equal probability of being chosen.</li><li><strong>Intuition:</strong> Think of drawing names out of a hat.</li><li><strong>Use Case & Why it&rsquo;s important:</strong> Its main application in ML design is for <strong>negative sampling</strong> in recommendation systems, especially in the early stages.<ul><li><strong>Example (YouTube):</strong> You have a user&rsquo;s watch history (positive examples). To train a classifier or a two-tower model, you need negative examples. The simplest way is to randomly sample a few hundred videos from the entire corpus of billions of videos.</li></ul></li><li><strong>The 2024+ Perspective:</strong> While simple, &ldquo;uniform random&rdquo; negative sampling is often considered a weak baseline. The model learns very quickly to distinguish a user&rsquo;s niche interest (e.g., &ldquo;vintage fountain pen restoration&rdquo;) from a completely random video (&ldquo;how to bake a cake&rdquo;). To get better performance, you need to show it &ldquo;harder&rdquo; negatives, which leads to other sampling strategies.</li><li><strong>Interview Phrasing:</strong> &ldquo;For the initial version of our two-tower recommender, we will generate training pairs by matching each positive user-item interaction with N <strong>randomly sampled negative items</strong> from the entire catalog. This establishes a strong baseline. In V2, we would explore more sophisticated hard negative sampling strategies to improve the model&rsquo;s ability to make fine-grained distinctions.&rdquo;</li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;Video Corpus&#34;
        V1(Video 1)
        V2(Video 2)
        V3(...)
        VN(Video N)
    end
    
    U(User A Watched V1) -- Positive Example --&gt; M(Model Training)
    
    subgraph &#34;Negative Sampling&#34;
        V2 -- Randomly Picked --&gt; N1(Negative Example)
        VN -- Randomly Picked --&gt; N2(Negative Example)
    end
    
    N1 --&gt; M
    N2 --&gt; M
    
    style U fill:#dff0d8
    style V2 fill:#f5c6cb
    style VN fill:#f5c6cb
</code></pre><hr><h4 id=rejection-sampling-sampling-from-a><strong>Rejection Sampling: Sampling from a &ldquo;Difficult&rdquo; Distribution</strong></h4><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> Use this when you want to sample from a complex target distribution <code>p(z)</code>, but you don&rsquo;t know how to do it directly. However, you <em>do</em> know how to sample from a simpler proposal distribution <code>q(z)</code> that &ldquo;envelopes&rdquo; <code>p(z)</code>.</li><li><strong>Intuition (The Dartboard Analogy):</strong> Imagine you want to sample points uniformly from within a circle. You don&rsquo;t know how to do that directly, but you know how to sample uniformly from the square that contains the circle.<ol><li><strong>Propose:</strong> Throw a dart at a random point <code>(x, y)</code> inside the square (sampling from <code>q(z)</code>).</li><li><strong>Accept/Reject:</strong> Check if the point <code>(x, y)</code> is also inside the circle (evaluating <code>p(z)</code>).</li><li>If it is, you <strong>accept</strong> the sample. If it&rsquo;s outside the circle but inside the square, you <strong>reject</strong> it and try again.</li></ol></li><li><strong>Diagram:</strong> The book&rsquo;s <code>rand7()</code> from <code>rand10()</code> example is good, but a graphical one is more intuitive.</li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;Rejection Sampling Process&#34;
        A[Start] --&gt; B{Sample z from&lt;br/&gt;Proposal Dist. q}
        B --&gt; C{Sample u from&lt;br/&gt;Uniform 0 to M*q}
        C --&gt; D{Is u &lt;= p z?}
        D -- Yes --&gt; E[Accept z as sample]
        D -- No --&gt; B
    end
    
    subgraph &#34;Graphical View&#34;
        direction LR
        Wrapper(Proposal Distribution q)
        Target(Target Distribution p)
        
        subgraph Wrapper
            P1(Sample 1)
            P2(Sample 2)
            P3(Sample 3)
        end
        
        subgraph Target
            P1
        end
        
        style Target fill:#cde4ff,stroke:#333
        style P2 fill:red
        style P3 fill:red
    end
    
    Note1[P1 is under p so ACCEPT] --&gt; P1
    Note2[P2 is outside p so REJECT] --&gt; P2
    Note3[P3 is outside p so REJECT] --&gt; P3
</code></pre><ul><li><strong>The 2024+ Perspective:</strong> While fundamental in statistics, Rejection Sampling is less common in day-to-day large-scale ML design compared to Importance Sampling. Its main drawback is that it can be very inefficient if the proposal distribution <code>q(z)</code> is not a good fit for <code>p(z)</code>, leading to a very high rejection rate.</li><li><strong>Interview Phrasing:</strong> It&rsquo;s a good concept to know as a sign of a well-rounded education. &ldquo;If we needed to sample from a complex, non-standard distribution, one approach could be rejection sampling, where we would sample from a simpler, enveloping distribution like a Gaussian and then use a probabilistic check to accept or reject the sample.&rdquo;</li></ul><hr><h4 id=importance-sampling-correcting-for-the><strong>Importance Sampling: Correcting for the &ldquo;Wrong&rdquo; Distribution</strong></h4><ul><li><p><strong>Book&rsquo;s Core Idea (Timeless):</strong> This is the inverse of Rejection Sampling. Instead of rejecting samples, we <strong>keep all of them</strong> but assign them a <strong>weight</strong> to correct for the fact that we sampled from the &ldquo;wrong&rdquo; distribution. This is used to estimate an expected value.</p></li><li><p><strong>Intuition:</strong> Imagine you want to find the average height of people in a city, but you only have data from a basketball convention. Your samples are heavily biased towards tall people. To get an unbiased estimate, you need to down-weight the tall people in your calculation. The weight is the correction factor.</p></li><li><p><strong>Equation:</strong> The goal is to estimate <code>E_p[f(x)]</code>, the expected value of <code>f(x)</code> under distribution <code>p</code>. We sample from distribution <code>q</code>.</p><p><code>E_p[f(x)] ≈ (1/N) * Σ [ f(x_i) * ( p(x_i) / q(x_i) ) ]</code></p><p>The term <code>w(x_i) = p(x_i) / q(x_i)</code> is the <strong>importance weight</strong>. It corrects for the bias.</p></li><li><p><strong>Use Case (Off-Policy Evaluation):</strong> This is the killer app for Importance Sampling in modern ML.</p><ul><li><strong>Problem:</strong> You have a new recommendation model (Policy B) you want to evaluate, but you don&rsquo;t want to run a risky A/B test. You only have log data from the old model that is currently in production (Policy A). Can you estimate how well Policy B <em>would have</em> performed using Policy A&rsquo;s data?</li><li><strong>Solution:</strong> Yes. You look at the actions taken by Policy A. For each action, you re-weight the outcome (e.g., whether the user clicked) by the ratio of probabilities: <code>p(action|Policy_B) / p(action|Policy_A)</code>. This gives you an unbiased estimate of Policy B&rsquo;s performance without ever deploying it.</li></ul></li><li><p><strong>Diagram:</strong></p></li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;Data from Production Policy A&#34;
        A[User Context] --&gt; PA[Policy A&lt;br/&gt;Production Model]
        PA --&gt; Rec1(Recommendation 1)
        Rec1 --&gt; R1[Reward: 1 Click]
    end

    subgraph &#34;Offline What If Evaluation&#34;
        A --&gt; PB[Policy B&lt;br/&gt;New Candidate Model]
        PB --&gt; Rec1
        
        W[Importance Weight&lt;br/&gt;w = P Rec1 Policy B / P Rec1 Policy A]
        
        R1 --&gt; Est(Weighted Reward)
        W --&gt; Est
        
        Est --&gt; Final[Estimated Performance&lt;br/&gt;of Policy B]
    end

    style W fill:#fff0b3
</code></pre><ul><li><strong>The 2024+ Perspective:</strong> This is a very active area of research and a critical concept for safe, iterative deployment in reinforcement learning and large-scale recommender systems.</li><li><strong>Interview Phrasing:</strong> &ldquo;To evaluate a new ranking model without a risky online experiment, we can use off-policy evaluation. We&rsquo;ll use the logs from our current production model and apply importance sampling to re-weight the observed outcomes (like clicks). This will give us an unbiased estimate of the new model&rsquo;s performance, allowing us to iterate much more quickly and safely.&rdquo;</li></ul><hr><h4 id=stratified-sampling-fair-representation><strong>Stratified Sampling: Fair Representation</strong></h4><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> Use this when your population has distinct subgroups (strata) and you want to ensure that your sample accurately reflects the proportions of these subgroups.</li><li><strong>Intuition:</strong> You&rsquo;re polling for an election. The country is 52% female and 48% male. A simple random sample might accidentally give you 60% males, skewing your results. With stratified sampling, you would first divide the population by gender, and then draw random samples from each group in the correct proportion.</li></ul><figure><img src=/images/ml-design-interview/stratified-sampling-comparison.png alt="Stratified Sampling vs Random Sampling Comparison" width=600></figure><ul><li><strong>Use Case:</strong> Building a robust <strong>test set</strong>.<ul><li><strong>Problem:</strong> You&rsquo;re building a fraud detection model. Your user base is 70% from the US, 20% from Europe, and 10% from Asia. If you create your test set with pure random sampling, you might get very few examples from Asia, and you won&rsquo;t have a reliable measure of how well your model performs for that critical segment.</li><li><strong>Solution:</strong> You use stratified sampling to build a test set that has the exact 70/20/10 split, guaranteeing a fair evaluation across all geographies.</li></ul></li><li><strong>The 2024+ Perspective:</strong> The principle is more important than ever, especially in the context of Responsible AI and fairness. You must be able to evaluate your model not just on an overall metric, but on its performance across sensitive attributes (age, gender, race, geography). Stratified sampling is the mechanism to create the evaluation datasets that allow you to do this.</li><li><strong>Interview Phrasing:</strong> &ldquo;To ensure our final model performance is robust and fair, we won&rsquo;t use a simple random split for our test set. Instead, we&rsquo;ll create it using <strong>stratified sampling</strong> based on user country. This guarantees that our test set maintains the same geographic distribution as our production traffic, allowing us to confidently measure and report performance for each key region.&rdquo;</li></ul><hr><h4 id=reservoir-sampling-sampling-from-a-stream><strong>Reservoir Sampling: Sampling from a Stream</strong></h4><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> How do you get a uniform random sample of <code>k</code> items from a dataset so large you can&rsquo;t fit it in memory and don&rsquo;t even know its total size (<code>N</code>) in advance? This is the classic &ldquo;sampling from a stream&rdquo; problem.</li><li><strong>Intuition & Algorithm (Algorithm R):</strong><ol><li><strong>Fill the reservoir:</strong> Take the first <code>k</code> items from the stream and put them in your reservoir (an array of size <code>k</code>).</li><li><strong>Process the rest:</strong> For each subsequent item <code>i</code> (from <code>k+1</code> to <code>N</code>):<ul><li>Generate a random integer <code>j</code> from <code>1</code> to <code>i</code>.</li><li>If <code>j</code> is between <code>1</code> and <code>k</code>, swap the <code>i</code>-th item from the stream with the item at the <code>j</code>-th position in your reservoir.</li><li>Otherwise, do nothing (let the <code>i</code>-th item flow past).</li></ul></li></ol></li><li><strong>Why it works:</strong> It&rsquo;s a bit of mathematical magic, but it ensures that at the end of the stream, every item that has ever passed through has had an equal <code>k/N</code> probability of being in the final reservoir.</li><li><strong>Diagram:</strong></li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;Stream of Unknown Length N&#34;
        I1 --&gt; I2 --&gt; I3 --&gt; I4 --&gt; I5 --&gt; I6 --&gt; I7 --&gt; Etc
    end
    
    subgraph &#34;Reservoir size k=3&#34;
        R[Reservoir Array]
        R --&gt; RI1
        R --&gt; RI2 
        R --&gt; RI3
    end
    
    subgraph &#34;Step 1: Fill Reservoir&#34;
        I1 --&gt; RI1(Item 1)
        I2 --&gt; RI2(Item 2)
        I3 --&gt; RI3(Item 3)
    end
    
    subgraph &#34;Step 2: Process Item i=4&#34;
        I4 --&gt; C1[j = random 1 to 4]
        C1 --&gt; Swap[Swap I4 with&lt;br/&gt;item at R j]
        C1 --&gt; Skip1(Do Nothing)
    end
    
    subgraph &#34;Step 3: Process Item i=5&#34;
        I5 --&gt; C2[j = random 1 to 5]
        C2 --&gt; Swap2[Swap I5 with&lt;br/&gt;item at R j]
        C2 --&gt; Skip2(Do Nothing)
    end
    
    Etc --&gt; FinalState(Final Reservoir&lt;br/&gt;A random sample of 3 items)
</code></pre><ul><li><strong>The 2024+ Perspective:</strong> This is a classic algorithm that comes up frequently in coding rounds for data-heavy roles. It&rsquo;s also used in large-scale data processing systems (like in Spark or Flink) for approximating analytics (e.g., &ldquo;get me a random sample of 1000 user IDs who performed this action&rdquo;) without having to do a full, expensive shuffle of the data.</li><li><strong>Interview Phrasing:</strong> &ldquo;If we need to generate a random sample of user sessions from our live Kafka event stream for analysis, we can&rsquo;t load the whole stream. This is a perfect use case for <strong>Reservoir Sampling</strong>. We can implement a streaming job that maintains a reservoir of size <code>k</code>, and for each new event, it probabilistically decides whether to swap it into the reservoir. This gives us a statistically valid, uniform random sample at any point in time without knowing the total size of the stream.&rdquo;</li></ul><hr><h2 id=deep-dive-4-common-deep-learning-model-architectures>Deep Dive 4: Common Deep Learning Model Architectures</h2><h3 id=1-wide-and-deep-architecture-the-power-of-memorization-and-generalization>1. Wide and Deep Architecture: The Power of Memorization and Generalization</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> This is a seminal architecture from Google that elegantly addresses a fundamental tension in recommendation systems:<ul><li><strong>Memorization:</strong> The system needs to learn and exploit simple, direct rules from the data. For example: &ldquo;People who searched for &lsquo;iPhone case&rsquo; often buy the &lsquo;Apple official silicone case&rsquo;.&rdquo; This is about recommending highly related items.</li><li><strong>Generalization:</strong> The system needs to explore and discover new, less obvious connections. For example: &ldquo;People who bought an iPhone case might <em>also</em> be interested in a wireless charger, even if they&rsquo;ve never shown interest before.&rdquo; This is about recommending novel items.</li></ul></li><li><strong>Intuition & Architecture:</strong> The Wide & Deep model combines two distinct parts that are joined only at the very end.<ul><li><strong>The &ldquo;Wide&rdquo; Part (Memorization):</strong> This is a simple, linear model (like logistic regression). Its job is to learn the direct, one-to-one feature interactions. It takes in raw features and, most importantly, <strong>cross-product features</strong>. A cross-product feature is a feature created by combining two others (e.g., <code>AND(user_country='USA', item_category='gardening')</code>). This allows the linear model to explicitly memorize the impact of this specific combination.</li><li><strong>The &ldquo;Deep&rdquo; Part (Generalization):</strong> This is a standard feed-forward neural network (MLP). Its job is to take low-dimensional embeddings of the features and discover complex, non-linear, and unseen feature combinations through its hidden layers. It can learn that <code>user_country='USA'</code> and <code>item_category='gardening'</code> are correlated with <code>item_category='bbq_grills'</code> even if very few users have exhibited all three behaviors.</li></ul></li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;Input Features&#34;
        Cat_Features[Categorical Features&lt;br/&gt;user_id, item_id, country]
        Num_Features[Numerical Features&lt;br/&gt;price, age]
    end

    subgraph &#34;Wide Part Memorization&#34;
        direction LR
        Cross[Cross-Product&lt;br/&gt;Transform]
        Wide_Model(Simple Linear Model)
        Raw_Features[Raw Features]
        Raw_Features --&gt; Cross
        Cross --&gt; Wide_Model
    end

    subgraph &#34;Deep Part Generalization&#34;
        direction LR
        Embedding(Embedding Layer)
        Deep_Model(Deep Neural Network)
        Embedding --&gt; Deep_Model
    end
    
    Cat_Features --&gt; Embedding
    Num_Features --&gt; Deep_Model
    
    subgraph &#34;Output&#34;
        Combine(+)
        Final_Output(Final Prediction&lt;br/&gt;e.g., Sigmoid for p click)
        Combine --&gt; Final_Output
    end
    
    Wide_Model --&gt; Combine
    Deep_Model --&gt; Combine

    style Cross fill:#fff0b3
    style Embedding fill:#cde4ff
</code></pre><ul><li><strong>The 2024+ Perspective:</strong> The <em>principle</em> of combining memorization and generalization is more relevant than ever. The Wide & Deep architecture itself is still a very strong baseline. However, the &ldquo;Deep&rdquo; side has become more sophisticated (e.g., using attention layers), and the &ldquo;Wide&rdquo; side is sometimes replaced or augmented by different explicit feature interaction layers. This leads directly to our next topic&mldr;</li><li><strong>Interview Phrasing:</strong> &ldquo;For a search ranking problem with both sparse categorical features and dense numerical features, a Wide & Deep architecture is a great starting point. The &lsquo;wide&rsquo; component would allow us to explicitly learn the memorized interactions between key query and item features using cross-product transformations. The &lsquo;deep&rsquo; component, operating on learned embeddings, would handle generalization and discover less obvious correlations. This hybrid approach ensures our model is both relevant and capable of novel recommendations.&rdquo;</li></ul><hr><h3 id=2-two-tower-architecture-the-scalable-retrieval-champion>2. Two-Tower Architecture: The Scalable Retrieval Champion</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> We&rsquo;ve touched on this before, but let&rsquo;s formalize it. This architecture is the <strong>industry standard for the candidate generation/retrieval stage</strong> of any large-scale recommendation or search system (YouTube, Facebook, Pinterest, etc.).</li><li><strong>The Problem it Solves:</strong> How do you efficiently find a few hundred &ldquo;pretty good&rdquo; candidates from a corpus of <em>billions</em> of items in real-time (under 50ms)? You cannot score every item against the user query.</li><li><strong>Intuition & Architecture:</strong> It decouples the query and candidate models.<ol><li><strong>Query Tower:</strong> Takes in all information about the user and their context (user profile, recent history, time of day) and crunches it down into a single embedding vector (e.g., a 128-dimensional vector).</li><li><strong>Candidate/Item Tower:</strong> Takes in all information about an item (title, description, category) and crunches it down into a single embedding vector of the <em>same dimension</em>.</li><li><strong>The Magic (Decoupling):</strong> During serving, you can pre-compute the embedding for EVERY item in your corpus and store them in an Approximate Nearest Neighbor (ANN) index like FAISS or a Vector Database. When a user request comes in, you only need to run the lightweight Query Tower to get the user&rsquo;s embedding. Then, you use that vector to query the ANN index, which very quickly finds the <code>k</code> item vectors with the highest dot product similarity.</li></ol></li><li><strong>Diagram:</strong> The diagram I provided in our first session is the canonical representation for this and is perfect to replicate.</li><li><strong>The 2024+ Perspective:</strong> This pattern is now everywhere, not just in classic recommendations. It&rsquo;s the foundation of Retrieval-Augmented Generation (RAG). The &ldquo;query tower&rdquo; is the model that embeds the user&rsquo;s search query, and the &ldquo;candidate tower&rdquo; is the model that embeds the documents in your knowledge base. The principles are identical.</li><li><strong>Interview Phrasing:</strong> &ldquo;To solve the candidate generation problem at scale, I&rsquo;d design a two-tower retrieval model. We&rsquo;d have a user/query tower and an item tower, trained jointly with a contrastive loss to map relevant pairs close in the embedding space. The key advantage of this architecture is its serving efficiency. We can pre-compute all item embeddings offline and load them into a vector index. At inference time, we only need to compute the user embedding on-the-fly and query the index to retrieve the top-k candidates in milliseconds.&rdquo;</li></ul><hr><h3 id=3-deep-cross-network-dcn-explicit-and-bounded-feature-crossing>3. Deep Cross Network (DCN): Explicit and Bounded Feature Crossing</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> DCN is an evolution of the Wide & Deep concept. It asks: &ldquo;What if we could get the benefits of the &lsquo;wide&rsquo; side&rsquo;s feature crossing, but do it more automatically and efficiently than manually defining cross-products?&rdquo;</li><li><strong>Intuition & Architecture:</strong> DCN replaces the &ldquo;wide&rdquo; part with a series of special &ldquo;cross layers.&rdquo;<ul><li>Each cross layer takes the output from the previous layer <code>x_i</code> and explicitly calculates its interaction with the original input <code>x_0</code>.</li><li><strong>Equation of a cross layer:</strong> <code>x_{i+1} = x_0 * (w_i^T * x_i) + b_i + x_i</code></li><li><strong>What this means:</strong> It&rsquo;s a very specific mathematical formula designed to create feature crosses of increasing complexity with each layer. Layer 1 creates 2nd-degree interactions, Layer 2 creates 3rd-degree, and so on. The <code>+ x_i</code> is a residual connection that helps with training stability.</li></ul></li><li><strong>Diagram:</strong> The book&rsquo;s diagram showing the stacked cross layers is a good representation.</li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    Input --&gt; Embedding
    
    subgraph &#34;Deep Part (MLP)&#34;
        D1(Dense) --&gt; D2(Dense) --&gt; D_Out
    end
    
    subgraph &#34;Cross Part (DCN)&#34;
        C1(Cross Layer 1) --&gt; C2(Cross Layer 2) --&gt; C_Out
    end
    
    Embedding --&gt; D1
    Embedding --&gt; C1

    D_Out --&gt; Combine --&gt; Output
    C_Out --&gt; Combine --&gt; Output
    
    style C1 fill:#fff0b3
    style C2 fill:#fff0b3
</code></pre><ul><li><strong>The 2024+ Perspective:</strong> DCN is a very well-respected architecture and a great example to bring up. It shows you know about solutions beyond the original Wide & Deep paper. Its main benefit is creating <em>explicit</em> and <em>bounded-degree</em> feature interactions, which can be more efficient and interpretable than letting a giant MLP learn them implicitly. The latest version, DCNv2, improves the cross-layer to be even more expressive.</li><li><strong>Interview Phrasing:</strong> &ldquo;To improve upon a standard Wide & Deep model, we could explore using a Deep Cross Network (DCN). Instead of manually crafting cross-features for the wide part, DCN uses specialized cross-layers to learn explicit feature interactions of increasing degree automatically. This can be more effective and require less manual feature engineering than the traditional approach.&rdquo;</li></ul><hr><h3 id=4-multitask-learning-learning-more-with-less>4. Multitask Learning: Learning More with Less</h3><ul><li><strong>Book&rsquo;s Core Idea (Timeless):</strong> Instead of training separate models to predict different things, can we train a single model to predict all of them at once?</li><li><strong>Intuition & Architecture:</strong> The most common pattern is &ldquo;Shared Bottom&rdquo; architecture.<ul><li>The model has a large, shared set of bottom layers (e.g., the user and item embedding layers and a few shared dense layers). This is where the model learns a general representation of the input.</li><li>The model then splits into multiple small, separate &ldquo;towers&rdquo; or &ldquo;heads,&rdquo; one for each task. Each tower learns the specific nuances for its own prediction.</li></ul></li><li><strong>Use Case (YouTube - from the book):</strong> A video recommendation model needs to predict multiple outcomes:<ul><li>Will the user <strong>click</strong> on the video? (predict <code>p(click)</code>)</li><li>Will the user <strong>like</strong> the video? (predict <code>p(like)</code>)</li><li>Will the user <strong>subscribe</strong> to the channel? (predict <code>p(subscribe)</code>)</li><li>How much <strong>watch time</strong> will the user generate? (predict <code>expected_watch_time</code>)</li></ul></li><li><strong>Diagram:</strong></li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    Input[Shared Input Features] --&gt; Shared_Bottom(Shared Layers&lt;br/&gt;e.g., Embeddings, MLP)

    subgraph &#34;Task-Specific Heads&#34;
        Tower_Click(Click Tower)
        Tower_Like(Like Tower) 
        Tower_Watch(Watch Time Tower)
        P_Click[Predict p click]
        P_Like[Predict p like]
        P_Time[Predict Watch Time]
        
        Shared_Bottom --&gt; Tower_Click
        Shared_Bottom --&gt; Tower_Like
        Shared_Bottom --&gt; Tower_Watch
        Tower_Click --&gt; P_Click
        Tower_Like --&gt; P_Like
        Tower_Watch --&gt; P_Time
    end
    
    subgraph &#34;Total Loss Sum of individual losses&#34;
        L_Click(LogLoss)
        L_Like(LogLoss)
        L_Time(MSE Loss)
        Total_Loss(Combined Loss)
        
        P_Click --&gt; L_Click
        P_Like --&gt; L_Like
        P_Time --&gt; L_Time
        L_Click --&gt; Total_Loss
        L_Like --&gt; Total_Loss
        L_Time --&gt; Total_Loss
    end
</code></pre><ul><li><strong>Why it&rsquo;s powerful:</strong><ol><li><strong>Implicit Regularization:</strong> Forcing the model to learn a shared representation that is good for <em>multiple</em> tasks prevents it from overfitting to the idiosyncrasies of any single task. This often leads to better generalization.</li><li><strong>Data Sparsity:</strong> It allows tasks with very little data (e.g., &ldquo;subscribes&rdquo;) to benefit from the large amount of data available for other tasks (e.g., &ldquo;clicks&rdquo;). The shared layers learn from all the data.</li></ol></li><li><strong>The 2024+ Perspective:</strong> This is a core technique in any large-scale system. Modern variants like <strong>MMoE (Multi-gate Mixture-of-Experts)</strong> improve on the shared-bottom by learning to route information through different &ldquo;expert&rdquo; sub-networks based on the input, which is even more powerful. Mentioning MMoE is a strong senior signal.</li><li><strong>Interview Phrasing:</strong> &ldquo;Instead of training separate models to predict clicks and likes, I would build a multi-task learning model. It would have a shared bottom to learn a common user/item representation from all interactions, with separate task-specific heads for predicting <code>p(click)</code> and <code>p(like)</code>. This approach acts as a form of regularization and allows the <code>like</code> prediction task, which has sparser data, to benefit from the rich data of the <code>click</code> task, likely improving overall model performance.&rdquo;</li></ul></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>