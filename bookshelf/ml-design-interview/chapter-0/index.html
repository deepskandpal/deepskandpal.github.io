<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><a href=#key-concepts>Key Concepts</a></li><li><a href=#main-topics-covered>Main Topics Covered</a></li><li><a href=#interview-tips>Interview Tips</a><ul><li><a href=#part-1-what-to-add-the-2024-lens>Part 1: What to ADD (The 2024+ Lens)</a><ul><li><a href=#1-retrieval-augmented-generation-rag>1. Retrieval-Augmented Generation (RAG)</a></li><li><a href=#2-vector-databases>2. Vector Databases</a></li><li><a href=#3-llmops--foundation-model-serving>3. LLMOps & Foundation Model Serving</a></li><li><a href=#4-real-time-feature-stores>4. Real-time Feature Stores</a></li></ul></li><li><a href=#part-2-what-to-deprecate-or-de-emphasize>Part 2: What to DEPRECATE (or De-emphasize)</a></li><li><a href=#your-5-day-high-intensity-action-plan>Your 5-Day High-Intensity Action Plan</a></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 0: ML Design Primer</h1><span class=reading-time><em>8 min read</em></span><div class=book-details><div class=book-content><p>This chapter introduces the fundamentals of designing machine learning systems at scale. It covers the key differences between traditional software systems and ML systems, and outlines the unique challenges faced when building production ML systems.</p><h2 id=key-concepts>Key Concepts</h2><ul><li><strong>ML System vs Traditional Systems</strong>: Understanding the differences in architecture, requirements, and challenges</li><li><strong>Scale Considerations</strong>: How scale affects ML system design decisions</li><li><strong>Production ML Pipeline</strong>: Components of a typical ML production pipeline</li></ul><h2 id=main-topics-covered>Main Topics Covered</h2><ol><li>What makes ML systems different</li><li>Key components of ML systems</li><li>Common challenges in ML system design</li><li>Framework for approaching ML system design interviews</li></ol><h2 id=interview-tips>Interview Tips</h2><ul><li>Always start with clarifying requirements</li><li>Think about data flow and system components</li><li>Consider scalability and performance trade-offs</li><li>Don&rsquo;t forget about monitoring and maintenance</li></ul><p>(Your detailed notes for Chapter 1 go here&mldr;)</p><p>Excellent question. This is exactly the mindset a senior candidate needs to have – not just knowing the material, but understanding how it fits into the current landscape. You&rsquo;ve got 5 days, so we need to be efficient and focus on high-impact areas.</p><p>First, let&rsquo;s get straight to your main question:</p><p><strong>Yes, you can and absolutely <em>should</em> use this book.</strong> The 2022 &ldquo;Machine Learning Design Interview&rdquo; by Khang Pham is a fantastic resource. The fundamental principles it covers – the &ldquo;why&rdquo; behind system design choices – are timeless. Things like the two-stage (candidate/ranking) architecture, feature engineering for sparse data (hashing, embeddings), and handling data at scale are the bedrock of ML systems.</p><p>However, you&rsquo;re right that the field moves incredibly fast. What a senior staff engineer expects from a senior MLE candidate in 2024 has evolved. We&rsquo;re not just looking for knowledge of these patterns, but also for an understanding of the <em>new class of problems and tools</em> that have emerged, primarily around Large Language Models (LLMs) and Generative AI.</p><p>Think of this book as your solid foundation. We&rsquo;re going to build a modern extension on top of it.</p><h3 id=part-1-what-to-add-the-2024-lens>Part 1: What to ADD (The 2024+ Lens)</h3><p>Here are the key concepts that have become critical since 2022. You need to be able to discuss these intelligently.</p><h4 id=1-retrieval-augmented-generation-rag>1. Retrieval-Augmented Generation (RAG)</h4><p>This is the single most important ML system design pattern to emerge in the last two years. It&rsquo;s how you make LLMs factual, current, and context-aware. The book talks about retrieval for recommendations (two-tower models), but RAG applies it to conversational AI and generation tasks.</p><p><strong>Simple Intuition:</strong> An LLM knows a lot, but it doesn&rsquo;t know about your company&rsquo;s private data, yesterday&rsquo;s news, or a specific user&rsquo;s profile. RAG is the process of:</p><ol><li>Receiving a user&rsquo;s query.</li><li>Using that query to <strong>retrieve</strong> relevant documents from a knowledge base.</li><li>Stuffing those documents into the LLM&rsquo;s prompt along with the original query.</li><li>Asking the LLM to <strong>generate</strong> an answer based <em>only</em> on the provided context.</li></ol><p>Here is a simple system diagram for RAG:</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>flowchart TD
    subgraph offline[&#34;Offline: Data Indexing&#34;]
        A[Unstructured Data&lt;br/&gt;Docs, PDFs, etc.] --&gt; B[Chunking Service]
        B --&gt; C[Embedding Model&lt;br/&gt;text-embedding-ada-002]
        C --&gt; D[Vector Database&lt;br/&gt;Pinecone, Qdrant]
    end
    
    subgraph online[&#34;Online: Inference&#34;]
        U[User Query] --&gt; Q[Query Embedding]
        Q --&gt; S{Vector Search}
        S --&gt; R[Retrieved Context]
        U --&gt; P[Prompt Engineering]
        R --&gt; P
        P --&gt; LLM[Large Language Model&lt;br/&gt;GPT-4, Llama 3]
        LLM --&gt; F[Final Answer]
    end
    
    D -.-&gt; S
    
    style D fill:#cde4ff,stroke:#333,stroke-width:2px
</code></pre><h4 id=2-vector-databases>2. Vector Databases</h4><p>The book mentions FAISS, which is a library. The modern discussion is about managed <strong>Vector Databases</strong> as a core infrastructure component. They are purpose-built to do fast Approximate Nearest Neighbor (ANN) search on billions of embeddings.</p><p><strong>Key discussion points for a senior role:</strong></p><ul><li><strong>Trade-offs:</strong> Latency vs. recall vs. cost. How do you choose an index type (e.g., HNSW vs. IVF)?</li><li><strong>Metadata Filtering:</strong> How do you retrieve vectors that not only are semantically similar but also match specific criteria (e.g., <code>user_id = '123'</code> AND <code>is_public = true</code>)? This is a crucial production feature.</li><li><strong>Scaling:</strong> How do these databases scale for reads and writes?</li></ul><h4 id=3-llmops--foundation-model-serving>3. LLMOps & Foundation Model Serving</h4><p>Deploying a 5GB BERT model is different from deploying a 70B parameter Llama model.</p><ul><li><strong>Quantization:</strong> Techniques like GPTQ or GGML/GGUF to shrink model size to run on cheaper hardware.</li><li><strong>Specialized Serving Frameworks:</strong> vLLM, TensorRT-LLM. These use techniques like PagedAttention to dramatically increase throughput for LLM inference.</li><li><strong>Fine-tuning vs. Prompt Engineering vs. RAG:</strong> When do you use each? (Hint: Start with Prompting/RAG. Fine-tuning is expensive and only for changing a model&rsquo;s <em>style</em> or teaching it a <em>new skill</em>, not for adding knowledge).</li></ul><h4 id=4-real-time-feature-stores>4. Real-time Feature Stores</h4><p>The book mentions the concept, but their importance has solidified. A feature store is the source of truth that solves the <strong>training-serving skew</strong> problem.</p><ul><li><strong>Modern Take:</strong> Feature stores now often have streaming capabilities (e.g., using Flink or Spark Streaming) to compute real-time features (e.g., &ldquo;user&rsquo;s clicks in the last 5 minutes&rdquo;) and make them available for inference with low latency.</li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>flowchart TD
  subgraph &#34;Real-time Feature Engineering&#34;
    direction TB
    A[&#34;Event Stream\n(Kafka, Kinesis)&#34;] --&gt; B{&#34;Stream Processor\n(Flink, Spark Streaming)&#34;}
    B --&gt; C[&#34;Real-time Feature Store\n(Redis, DynamoDB)&#34;]
  end

  subgraph &#34;Serving&#34;
    direction TB
    D[&#34;Prediction Service&#34;] --&gt; C
  end

  subgraph &#34;Training&#34;
    direction TB
    B --&gt; E[&#34;Batch Storage\n(S3, BigQuery)&#34;]
    F[&#34;Model Training&#34;] --&gt; E
  end
</code></pre><h3 id=part-2-what-to-deprecate-or-de-emphasize>Part 2: What to DEPRECATE (or De-emphasize)</h3><p>Technology moves on. While the principles are good, don&rsquo;t spend your limited time on the implementation details of these.</p><ol><li><p><strong>Pure Hadoop/MapReduce:</strong> The book mentions MapReduce jobs. Today, the de-facto standard for large-scale batch processing is <strong>Apache Spark</strong>, and for streaming, it&rsquo;s <strong>Spark Streaming</strong> or <strong>Apache Flink</strong>. If you say &ldquo;MapReduce job,&rdquo; it will sound dated. Frame everything in terms of Spark or Flink.</p></li><li><p><strong>Elaborate, from-scratch feature engineering for text/images:</strong> For many problems, you no longer build TF-IDF vectors or train a Word2Vec model from scratch on day one.</p><ul><li><strong>The 2024 Way:</strong> You start with a powerful pre-trained foundation model (e.g., a BERT variant for embeddings, or even an LLM). Your &ldquo;feature engineering&rdquo; is now about choosing the right model and deciding on an embedding strategy. The core idea of &ldquo;semantic representation&rdquo; is the same, but the tool has become much more powerful.</li></ul></li><li><p><strong>Over-indexing on specific architectures (like DCNv2):</strong> It&rsquo;s great to know <em>why</em> a model like Deep & Cross Network (DCN) exists – to explicitly learn feature interactions. But a senior staff engineer is more interested in you recognizing the <em>problem</em> (the need for both memorization and generalization) than memorizing the exact architecture of DCNv2. You could say: <em>&ldquo;For a system with many categorical features like this, we need to handle both memorization of specific feature pairs and generalization. Historically, models like Wide & Deep or DCN addressed this. Today, we might handle this with a powerful embedding model that captures these interactions implicitly, or by using an LLM to reason over the features.&rdquo;</em></p></li></ol><h3 id=your-5-day-high-intensity-action-plan>Your 5-Day High-Intensity Action Plan</h3><p><strong>Days 1-2: Master the Book&rsquo;s Fundamentals</strong></p><ul><li>Read the book cover-to-cover. Don&rsquo;t skip.</li><li>For every component (e.g., One-Hot Encoding, Two-Tower Model), force yourself to answer: <strong>&ldquo;What business or system problem does this solve?&rdquo;</strong> (e.g., Two-tower model solves the problem of efficiently retrieving a small set of candidates from a massive corpus).</li><li>Focus on Chapters 1 (Primer), 2 (RecSys Components), and then skim the end-to-end examples.</li></ul><p><strong>Day 3: Layer on the 2024 Concepts</strong></p><ul><li>Deep-dive into <strong>RAG</strong>. Watch 2-3 YouTube videos explaining it. Understand the diagram I drew above.</li><li>Read about <strong>Vector Databases</strong> (e.g., read the intro docs for Pinecone or Weaviate).</li><li>Understand the difference between <strong>real-time, near-real-time, and batch</strong> features.</li></ul><p><strong>Day 4: Practice Designing</strong></p><ul><li>Take a classic problem from the book, like <strong>&ldquo;YouTube Video Recommendations.&rdquo;</strong></li><li><strong>First, design it the &ldquo;classic&rdquo; way</strong> as described in the book (two-tower candidate gen, sophisticated ranker).</li><li><strong>Then, ask yourself: &ldquo;How would I redesign this in 2024?&rdquo;</strong><ul><li>Could I add a conversational search feature using RAG?</li><li>How would my video embeddings be stored and retrieved? (Vector DB!)</li><li>Could an LLM re-rank the final candidates by providing natural language &ldquo;reasons&rdquo; for why a user might like a video?</li><li>Draw the new architecture. This is your key senior-level exercise.</li></ul></li></ul><p><strong>Day 5: Review & Communication</strong></p><ul><li>Practice verbalizing your designs. Use a framework:<ol><li><strong>Clarify:</strong> Ask about scope, scale, latency, and business goals.</li><li><strong>Metrics:</strong> Define offline (e.g., NDCG, Precision) and online (e.g., CTR, user session time) metrics.</li><li><strong>High-Level Design:</strong> Draw the big boxes. Start simple.</li><li><strong>Deep Dive:</strong> Go into the components (data, model, serving). This is where you&rsquo;ll use the book&rsquo;s content and the 2024 additions.</li><li><strong>Scale & Reliability:</strong> Discuss bottlenecks, caching, and failure modes.</li></ol></li></ul><p>You have a solid resource. Your goal in the next 5 days is not to become an expert in LLMOps, but to be able to confidently place the timeless principles from the book into the context of modern AI systems.</p><p>Good luck. You&rsquo;ve got this.</p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>