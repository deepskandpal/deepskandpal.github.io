<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, 2nd Edition on 404EngineerNotFound</title>
    <link>http://localhost:1313/bookshelf/hands-on-ml-copy/</link>
    <description>Recent content in Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, 2nd Edition on 404EngineerNotFound</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Feb 2024 10:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/bookshelf/hands-on-ml-copy/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Chapter 1: The Machine Learning Landscape</title>
      <link>http://localhost:1313/bookshelf/hands-on-ml-copy/chapter-1/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bookshelf/hands-on-ml-copy/chapter-1/</guid>
      <description>&lt;h1 id=&#34;notes-for-chapter-1&#34;&gt;Notes for Chapter 1&lt;/h1&gt;&#xA;&lt;p&gt;When you hear &amp;ldquo;Machine Learning,&amp;rdquo; what pops into your head? Robots? Terminators? Maybe a friendly butler? The book nails it – it&amp;rsquo;s not just sci-fi; it&amp;rsquo;s already here. Think about the &lt;strong&gt;spam filter&lt;/strong&gt;. That was one of the first really big ML applications that touched millions. It learned, from examples of spam and non-spam (or &amp;ldquo;ham,&amp;rdquo; as we call it), to tell the difference. And it got so good that we barely notice it anymore. That&amp;rsquo;s the hallmark of good ML – it just works.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 2: End-to-End Machine Learning Project</title>
      <link>http://localhost:1313/bookshelf/hands-on-ml-copy/chapter-2/</link>
      <pubDate>Thu, 22 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bookshelf/hands-on-ml-copy/chapter-2/</guid>
      <description>&lt;h1 id=&#34;notes-for-chapter-2&#34;&gt;Notes for Chapter 2&lt;/h1&gt;&#xA;&lt;p&gt;Alright class, buckle up! Last time, we got a bird&amp;rsquo;s-eye view of the Machine Learning landscape – the different continents, climates, and major landmarks. Today, with Chapter 2, &amp;ldquo;End-to-End Machine Learning Project,&amp;rdquo; we&amp;rsquo;re grabbing our hiking boots and actually trekking through one of these landscapes. This is where the rubber meets the road!&lt;/p&gt;&#xA;&lt;p&gt;The book says we&amp;rsquo;re pretending to be a recently hired data scientist at a real estate company. Our mission? To predict median housing prices in California. This chapter is &lt;em&gt;fantastic&lt;/em&gt; because it walks you through the practical steps. It’s less about the theory of one specific algorithm and more about the &lt;em&gt;process&lt;/em&gt; of doing ML in the real world.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 3: Classification</title>
      <link>http://localhost:1313/bookshelf/hands-on-ml-copy/chapter-3/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bookshelf/hands-on-ml-copy/chapter-3/</guid>
      <description>&lt;p&gt;This chapter is all about systems that predict a &lt;em&gt;category&lt;/em&gt; or a &lt;em&gt;class&lt;/em&gt; – Is this email spam or not? Is this image a cat or a dog? Is this handwritten digit a &amp;lsquo;5&amp;rsquo; or a &amp;lsquo;3&amp;rsquo;?&lt;/p&gt;&#xA;&lt;p&gt;And speaking of handwritten digits, we&amp;rsquo;re going to be working with a very famous dataset: &lt;strong&gt;MNIST&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It&amp;rsquo;s a set of 70,000 small images of digits (0-9) handwritten by high school students and US Census Bureau employees.&lt;/li&gt;&#xA;&lt;li&gt;Each image is labeled with the digit it represents.&lt;/li&gt;&#xA;&lt;li&gt;The book calls it the &lt;strong&gt;&amp;ldquo;hello world&amp;rdquo; of Machine Learning&lt;/strong&gt; because it&amp;rsquo;s a go-to dataset for testing new classification algorithms. Everyone who learns ML eventually plays with MNIST. It’s like a rite of passage!&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Scikit-Learn makes it super easy to fetch popular datasets like MNIST. The code shows:&#xA;&lt;code&gt;from sklearn.datasets import fetch_openml&lt;/code&gt;&#xA;&lt;code&gt;mnist = fetch_openml(&#39;mnist_784&#39;, version=1)&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 4: Training Models</title>
      <link>http://localhost:1313/bookshelf/hands-on-ml-copy/chapter-4/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:1313/bookshelf/hands-on-ml-copy/chapter-4/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Chapter 4: Training Models&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(Page 111: Introduction - Beyond Black Boxes)&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Up until now, as the book says, we&amp;rsquo;ve treated ML models and their training algorithms mostly like black boxes. We fed them data, they gave us results, and we learned to evaluate those results. You&amp;rsquo;ve optimized regression, improved classifiers, even built a spam filter, often without peeking under the hood. And that&amp;rsquo;s okay for many practical purposes!&lt;/p&gt;&#xA;&lt;p&gt;However, understanding &lt;em&gt;how&lt;/em&gt; these things work internally is incredibly powerful. It helps you:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
