<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#chapter-6-retrieval-augmented-generation><strong>Chapter 6: Retrieval-Augmented Generation</strong></a></li></ul></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 2: Gmail Smart Compose</h1><span class=reading-time><em>15 min read</em></span><div class=book-details><div class=book-content><p>Excellent. Let&rsquo;s dive into Chapter 6. This is one of the most important, practical patterns in the GenAI space right now: <strong>Retrieval-Augmented Generation (RAG)</strong>.</p><p>If the last chapter was about the fundamentals of GenAI models, this chapter is about making them <em>useful</em> in the real world. A base LLM is like a brilliant student who has read the entire internet up to 2023 but has never seen your company&rsquo;s private documents and doesn&rsquo;t know what happened in the news this morning. RAG is the system you build to give that student an &ldquo;open-book exam,&rdquo; allowing them to access specific, up-to-date information to answer questions.</p><p>Let&rsquo;s break this down, piece by piece.</p><hr><h3 id=chapter-6-retrieval-augmented-generation><strong>Chapter 6: Retrieval-Augmented Generation</strong></h3><h4 id=introduction><strong>Introduction</strong></h4><p><strong>What are we trying to achieve?</strong> We&rsquo;re solving the LLM&rsquo;s biggest weaknesses:</p><ol><li><strong>Knowledge Cutoff:</strong> It doesn&rsquo;t know about recent events.</li><li><strong>Lack of Private Data:</strong> It hasn&rsquo;t been trained on your internal company wiki, your customer support database, or a PDF you just uploaded.</li><li><strong>Hallucination:</strong> It can make things up.</li></ol><p>The book uses the example of building a <strong>ChatPDF</strong> system for internal company use. An employee should be able to ask, &ldquo;What is our policy on international travel reimbursement?&rdquo; and get an answer based on the latest HR documents, not on some generic policy the LLM learned from the public internet.</p><h4 id=step-1-clarifying-requirements><strong>Step 1: Clarifying Requirements</strong></h4><p>This is where the interview starts. The book gives a fantastic example of a candidate leading the conversation. Let&rsquo;s analyze it from an interviewer&rsquo;s perspective.</p><ul><li><strong>Candidate:</strong> &ldquo;What does the external knowledge base consist of? Does it change over time?&rdquo;<ul><li><strong>Interviewer&rsquo;s thought:</strong> <em>Good. They&rsquo;re starting with the data. They understand that the nature of the data source is the most important factor.</em></li></ul></li><li><strong>Candidate:</strong> &ldquo;Do the Wiki pages and forums contain text, images, and other modalities?&rdquo;<ul><li><strong>Interviewer&rsquo;s thought:</strong> <em>Excellent. They&rsquo;re thinking about multimodality. This will affect our choice of embedding models.</em></li></ul></li><li><strong>Candidate:</strong> &ldquo;How many pages are there in total?&rdquo; (5 million pages) &ldquo;What is the expected growth?&rdquo; (20% annually)<ul><li><strong>Interviewer&rsquo;s thought:</strong> <em>Great, they&rsquo;re quantifying the scale. This is critical for discussing scalability, cost, and choosing the right database/indexing strategy.</em></li></ul></li><li><strong>Candidate:</strong> &ldquo;Should the system respond in real time?&rdquo; (Slight delay is okay)<ul><li><strong>Interviewer&rsquo;s thought:</strong> <em>They&rsquo;re scoping the latency requirements. This tells me I don&rsquo;t need a sub-50ms system and can make trade-offs for better quality.</em></li></ul></li><li><strong>Candidate:</strong> &ldquo;Is it necessary for the system to include document references?&rdquo; (Yes)<ul><li><strong>Interviewer&rsquo;s thought:</strong> <em>Crucial question. This requirement immediately makes one of the potential solutions (finetuning) much less attractive. They are already thinking ahead.</em></li></ul></li></ul><p>By the end of this, you&rsquo;ve established the core problem: Build a Q&amp;A system over a large (5M pages), slowly growing (+20%/year) internal knowledge base of mixed-format PDFs, which must provide verifiable answers with source references.</p><h4 id=step-2-frame-the-problem-as-an-ml-task><strong>Step 2: Frame the Problem as an ML Task</strong></h4><p><strong>Specifying Input and Output</strong></p><p>This is straightforward but important to state clearly.</p><ul><li><strong>Input:</strong> A user&rsquo;s text query (e.g., &ldquo;How do I submit an expense report?&rdquo;).</li><li><strong>Underlying Data:</strong> A database of 5 million company documents.</li><li><strong>Output:</strong> A text-based answer, grounded in the documents, with references.</li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph LR
    subgraph User
        A[User Query&lt;br/&gt;&#34;How do I submit an&lt;br/&gt;expense report?&#34;]
    end
    subgraph System
        B(ChatPDF System)
    end
    subgraph Data
        C[Document databases]
    end
    subgraph Output
        D[Response&lt;br/&gt;&#34;To submit an expense report, log into...&#34;]
    end
    
    A --&gt; B
    C --&gt; B
    B --&gt; D
</code></pre><p><em>(Based on Figure 6.2)</em></p><p><strong>Choosing a Suitable ML Approach</strong></p><p>This is the first major design decision, and it&rsquo;s a classic interview trade-off question. For a problem like this, the book lays out three main approaches.</p><ol><li><p><strong>Finetuning:</strong></p><ul><li><strong>What it is:</strong> Take a pre-trained LLM and continue training it on your internal documents. The model&rsquo;s weights are updated to &ldquo;absorb&rdquo; the new knowledge.</li><li><strong>Pros:</strong> Can deeply learn the style and terminology of your company.</li><li><strong>Cons (Dealbreakers for our problem):</strong><ul><li><strong>Computationally Expensive:</strong> Continuously retraining an LLM is a massive cost.</li><li><strong>Stale Data:</strong> As soon as a new document is added, the model is out of date until the next expensive finetuning cycle.</li><li><strong>No References:</strong> The book correctly states: <em>&ldquo;Finetuned models usually can&rsquo;t provide references for their answers, making it hard to verify or trace information back to its source.&rdquo;</em> The knowledge is baked into the weights; you can&rsquo;t easily point to the source document. <strong>This violates our requirement.</strong></li></ul></li></ul></li><li><p><strong>Prompt Engineering (In-Context Learning):</strong></p><ul><li><strong>What it is:</strong> Stuff the relevant documents directly into the prompt along with the user&rsquo;s question.</li><li><strong>Pros:</strong> Simple, cheap, no training required.</li><li><strong>Cons (Dealbreakers for our problem):</strong><ul><li><strong>Limited Context Window:</strong> You can&rsquo;t fit 5 million documents into a prompt. You can&rsquo;t even fit one long document. This approach is simply <strong>not scalable.</strong></li></ul></li></ul></li><li><p><strong>Retrieval-Augmented Generation (RAG):</strong></p><ul><li><strong>What it is:</strong> A two-step process. First, <strong>retrieve</strong> a few relevant document snippets from the large database. Then, <strong>generate</strong> an answer using an LLM, with the user&rsquo;s query and the retrieved snippets provided as context in the prompt.</li><li><strong>Pros:</strong><ul><li><strong>Access to Current Info:</strong> The document database can be updated easily. The LLM gets the latest info at query time.</li><li><strong>Verifiable & Factual:</strong> Since you have the retrieved snippets, you can easily add references. It reduces hallucination by forcing the LLM to base its answer on the provided text.</li><li><strong>Scalable & Cost-Effective:</strong> You&rsquo;re not retraining the LLM. The main work is in the retrieval step.</li></ul></li><li><strong>Cons:</strong><ul><li><strong>Implementation Complexity:</strong> It&rsquo;s a multi-component system (retriever + generator) that needs to work well together.</li><li><strong>Dependence on Retrieval Quality:</strong> If you retrieve irrelevant documents, the LLM will give a garbage answer. The retriever is critical.</li></ul></li></ul></li></ol><p><strong>The Decision:</strong> The book concludes, <em>&ldquo;RAG offers a balanced solution in terms of ease of setup, cost, and scalability&mldr; Therefore, we choose RAG to build our ChatPDF system.&rdquo;</em> This is the correct, well-justified choice.</p><h4 id=step-3-data-preparation-the><strong>Step 3: Data Preparation (The &ldquo;R&rdquo; in RAG)</strong></h4><p>This is the entire process of making your knowledge base searchable. The book outlines a three-step pipeline.</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    A[Document Databases (PDFs)] --&gt; B(Document Parsing);
    B --&gt; C(Document Chunking);
    C --&gt; D(Indexing);
    D --&gt; E[Indexed Embeddings&lt;br/&gt;(in Vector DB)];
</code></pre><p><em>(Simplified from Figure 6.8)</em></p><p><strong>1. Document Parsing: Getting Content out of PDFs</strong></p><p>PDFs are a nightmare. They can have columns, tables, images, and weird layouts.</p><ul><li><strong>Rule-based:</strong> You write code that assumes a certain layout. Brittle and fails on complex or varied documents.</li><li><strong>AI-based (The Winner):</strong> Use a model to &ldquo;read&rdquo; the PDF like a human. As the book explains, a tool like <strong>Layout-Parser</strong> does this:<ol><li><strong>Layout Detection:</strong> An object detection model draws boxes around paragraphs, tables, images, etc.</li><li><strong>Text Extraction:</strong> OCR is run inside each text box.</li><li><strong>Structured Output:</strong> You get a list of content blocks with their type (text, image), coordinates, and content. This is much more useful than just a wall of raw text.</li></ol></li></ul><p><strong>2. Document Chunking: Breaking It Down</strong></p><p>You can&rsquo;t create an embedding for an entire 50-page document.</p><ul><li><strong>Why?</strong><ol><li>An embedding of a whole document averages out the meaning and loses specific details. A query for a specific sentence will get lost.</li><li>The retrieved document chunk needs to fit into the LLM&rsquo;s context window.</li></ol></li><li><strong>Strategies:</strong><ul><li><strong>Length-based:</strong> Simple but dumb. Can cut sentences in half. <code>RecursiveCharacterTextSplitter</code> from libraries like LangChain is a smarter version that tries to split on paragraphs, then sentences, then words to keep semantic units together.</li><li><strong>Content-aware:</strong> Better. Split based on document structure, like Markdown headers (<code>#</code>), HTML tags, etc.</li></ul></li></ul><p><strong>3. Indexing: Making Chunks Findable</strong></p><p>Now you have thousands or millions of chunks. How do you find the right ones for a given query, fast?</p><ul><li><strong>Keyword/Full-text Search (e.g., Elasticsearch):</strong> Fast and good for matching exact words or phrases. But it struggles with synonyms and semantic meaning. A query for &ldquo;employee compensation&rdquo; might miss a document that only uses the word &ldquo;staff salary.&rdquo;</li><li><strong>Vector-based Search (The Winner):</strong><ul><li><strong>Why?</strong> It searches based on <strong>semantic meaning</strong>, not keywords.</li><li><strong>How?</strong> You use an <strong>embedding model</strong> (like a Transformer encoder) to convert every chunk of text/image into a high-dimensional vector (an array of numbers). Chunks with similar meanings will have vectors that are &ldquo;close&rdquo; to each other in this vector space.</li><li><strong>The book does a back-of-the-envelope calculation:</strong> 5M pages, chunked, results in ~40M chunks. At this scale, vector search is the only viable option for semantic retrieval.</li></ul></li></ul><p>So, the data prep pipeline is: <strong>Parse</strong> PDFs into structured blocks -> <strong>Chunk</strong> blocks into small, meaningful pieces -> <strong>Embed</strong> each piece into a vector -> <strong>Store</strong> the vectors in a specialized <strong>Vector Database</strong>.</p><h4 id=step-4-model-development><strong>Step 4: Model Development</strong></h4><p>This covers the architecture of the ML models and the processes involved.</p><p><strong>Architecture: The Key ML Models (Figure 6.9)</strong></p><p>A RAG system has three key ML model components:</p><ol><li><strong>Indexing:</strong> An <strong>Encoder Model</strong> to create the vector embeddings.</li><li><strong>Retrieval:</strong> The same <strong>Encoder Model</strong> to turn the user query into a vector. (Plus an ANN search algorithm).</li><li><strong>Generation:</strong> An <strong>LLM</strong> to generate the final answer.</li></ol><p><strong>The Indexing/Retrieval Model: Text-Image Alignment</strong></p><p>Our documents have text and images. Our query is text. How do you find an image relevant to a text query? The embeddings need to live in the same &ldquo;space.&rdquo; The book outlines two approaches (Figure 6.10):</p><ol><li><strong>Shared Embedding Space (Best Approach):</strong> Use a multimodal model like <strong>CLIP</strong>. CLIP is pre-trained to map related images and their text descriptions to nearby points in vector space. You can use its text encoder for text chunks and its image encoder for images. This is elegant and powerful.</li><li><strong>Image Captioning (Workaround):</strong> Use an image captioning model to generate a text description for each image. Then, use a standard text-only encoder to embed that caption. This works but is less direct and might lose information.</li></ol><p><strong>The Retrieval Process: Finding the Needles in the Haystack</strong></p><p>Once the user query is embedded into a vector <code>Eq</code>, we need to find the <code>k</code> closest chunk vectors in our database of 40 million.</p><ul><li><strong>Exact Nearest Neighbor (Linear Search):</strong> Compare <code>Eq</code> to all 40 million vectors. Guarantees a perfect result but is way too slow. O(N*D) complexity. Unacceptable.</li><li><strong>Approximate Nearest Neighbor (ANN):</strong> The only practical solution. It trades a tiny bit of accuracy for a massive speedup. The book mentions four families of ANN algorithms:<ul><li><strong>Tree-based (e.g., Annoy):</strong> Recursively partition the data space. Fast, but can struggle in very high dimensions.</li><li><strong>Hashing-based (LSH):</strong> Uses clever hash functions where similar vectors are likely to get the same hash key. You only search within the query&rsquo;s hash bucket.</li><li><strong>Clustering-based (The book&rsquo;s choice):</strong> Pre-cluster the 40M vectors into, say, 100,000 clusters. The search becomes a two-step process:<ol><li>Find the few clusters whose center is closest to the query vector.</li><li>Do an exact search <em>only</em> within those few clusters. This massively reduces the search space.</li></ol></li><li><strong>Graph-based (e.g., HNSW):</strong> The state-of-the-art for many use cases. It builds a graph where nodes are data points and edges connect close neighbors. Searching is like navigating this graph to find the closest point.</li></ul></li></ul><p>Modern vector databases like Pinecone, Weaviate, or libraries like <strong>FAISS</strong> (from Meta) and <strong>ScaNN</strong> (from Google) implement these advanced ANN algorithms for you.</p><p>Here is the overall retrieval process from Figure 6.16:</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph Data Preparation
        direction TB
        A[Document databases] --&gt; B(Data preparation&lt;br&gt;Parsing/Chunking);
        B --&gt; C[Index (images)];
        B --&gt; D[Index (text)];
        C &amp; D --&gt; E(Clustering);
    end

    subgraph Retrieval
        direction LR
        F[User query&lt;br&gt;&#34;How many cats live&lt;br&gt;in the company?&#34;] --&gt; G(Text Encoder);
        G --&gt; H(Inter-Cluster&lt;br&gt;Search);
        E -- Selected Clusters --&gt; H;
        H --&gt; I(Intra-Cluster&lt;br&gt;Search);
        I --&gt; J[Retrieved&lt;br&gt;data chunks];
    end
</code></pre><p><strong>The Generation Process: Crafting the Final Answer</strong></p><p>This is where the LLM comes in. The process is:</p><ol><li>Take the original user query.</li><li>Take the top <code>k</code> retrieved data chunks from the retrieval step.</li><li>Combine them into a single, well-structured prompt.</li><li>Feed this prompt to the LLM.</li><li>The LLM generates the answer, using a sampling strategy like <strong>top-p sampling</strong> for a good balance of correctness and fluency.</li></ol><p><strong>A Deeper Look at Prompt Engineering for Generation</strong></p><p>How do you structure that final prompt for the best results? The book dives into several powerful techniques.</p><ul><li><strong>Chain-of-Thought (CoT) Prompting:</strong> Instruct the model to &ldquo;think step-by-step.&rdquo; This forces it to lay out its reasoning process before giving the final answer, which often improves accuracy on complex questions.</li><li><strong>Few-shot Prompting:</strong> Give the model 2-3 examples of a Q&amp;A pair in the desired format before giving it the real query. This helps it understand the expected tone and structure.</li><li><strong>Role-specific Prompting:</strong> Tell the model who it is. <em>&ldquo;You are an expert contract lawyer&mldr; explain this clause in simple terms.&rdquo;</em> This grounds the model and helps it adopt the correct persona and level of detail.</li><li><strong>User-context Prompting:</strong> Include metadata about the user (language, location, etc.) to get more personalized results.</li></ul><p>Here is the final prompt structure from Figure 6.22, showing how all these pieces come together.</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;Final Prompt Sent to LLM&#34;
        A[&#34;User&#39;s initial query: [...]&#34;];
        B[&#34;--- RETRIEVED CONTEXT ---&lt;br/&gt;[Retrieved document chunk 1]&lt;br/&gt;[Retrieved document chunk 2]&lt;br/&gt;...&#34;];
        C[&#34;--- INSTRUCTIONS ---&lt;br/&gt;You are a helpful assistant for Company XYZ...&#34;];
        D[&#34;--- EXAMPLES ---&lt;br/&gt;Example 1: Q: ... A: ...&lt;br/&gt;Example 2: Q: ... A: ...&#34;];
        E[&#34;--- REASONING ---&lt;br/&gt;Based on the context, think step-by-step to answer the user&#39;s query.&#34;];
        F[&#34;--- USER INFO ---&lt;br/&gt;User language: English&#34;];
    end

    subgraph Labels
        direction LR
        L1[Role-Specific&lt;br&gt;Prompting]
        L2[Few-Shot&lt;br&gt;Prompting]
        L3[Chain-of-Thought&lt;br&gt;(CoT)]
        L4[User-Context&lt;br&gt;Prompting]
        L5[Retrieved&lt;br&gt;Context]
    end

    B -- Is --&gt; L5
    C -- Is --&gt; L1
    D -- Is --&gt; L2
    E -- Is --&gt; L3
    F -- Is --&gt; L4
</code></pre><p><strong>Training (Advanced Topic: RAFT)</strong>
Most of the time, you start with pre-trained models. But what if your retrieval is noisy and the LLM struggles to distinguish good context from bad? <strong>RAFT (Retrieval-Augmented Fine-Tuning)</strong> is a technique to solve this.</p><ul><li><strong>The Idea:</strong> During finetuning, you create training examples that include the question, a &ldquo;golden&rdquo; (correct) document, AND several &ldquo;distractor&rdquo; (irrelevant) documents that were also retrieved.</li><li><strong>The Goal:</strong> You train the LLM to specifically pay attention to the golden document and <em>ignore</em> the distractors when generating the answer. This makes the LLM more robust to imperfect retrieval.</li></ul><h4 id=step-5-evaluation-critical-for-rag><strong>Step 5: Evaluation (CRITICAL for RAG)</strong></h4><p>Evaluating a RAG system is more complex than a standard LLM. You need to evaluate both the retriever and the generator. The book introduces an excellent &ldquo;Triad of RAG evaluation&rdquo; (Figure 6.23).</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    Query --&gt;|Context Relevance| Context
    Context --&gt;|Faithfulness| Results
    Query --&gt;|Answer Relevance&lt;br&gt;Answer Correctness| Results
</code></pre><p>This diagram shows that the final <code>Results</code> (the generated answer) depend on the <code>Query</code>, the retrieved <code>Context</code>, and the relationships between them. This leads to four key evaluation aspects:</p><ol><li><p><strong>Context Relevance:</strong> <strong>Is the retriever working?</strong> Did we retrieve documents that are relevant to the query?</p><ul><li><strong>Metrics:</strong> Standard information retrieval metrics like <code>Precision@k</code>, <code>nDCG</code>, <code>Hit Rate</code>. You need a labeled dataset of (query, relevant_doc) pairs for this.</li></ul></li><li><p><strong>Faithfulness (or Groundedness):</strong> <strong>Is the generator hallucinating?</strong> Is the generated answer factually consistent with the retrieved context? You check if every statement in the answer can be backed up by the provided snippets.</p><ul><li><strong>Methods:</strong> This is hard to automate. Often requires human evaluation or using another powerful LLM as a judge. Figure 6.24 shows a great example: if the context says Marie Curie won two Nobel prizes, an answer saying she won one has low faithfulness.</li></ul></li><li><p><strong>Answer Relevance:</strong> <strong>Did the generator answer the user&rsquo;s actual question?</strong> The retrieved context might be relevant, but the LLM could get sidetracked and generate an answer that doesn&rsquo;t directly address the user&rsquo;s intent.</p><ul><li><strong>Methods:</strong> Again, often requires a human or an LLM judge to compare the user&rsquo;s query and the final answer.</li></ul></li><li><p><strong>Answer Correctness:</strong> Is the answer factually correct according to a ground truth reference? This is the classic accuracy measure.</p></li></ol><p>In an interview, discussing this four-part evaluation framework shows a deep, practical understanding of the challenges of building reliable RAG systems.</p><h4 id=step-6-overall-ml-system-design><strong>Step 6: Overall ML System Design</strong></h4><p>This is the final blueprint. Figure 6.27 shows the end-to-end flow. Let&rsquo;s recreate and walk through it.</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph Offline Process
        direction TB
        A[Document Databases] --&gt; B(Document Parsing &amp; Chunking)
        B --&gt; C{Text Encoder};
        B --&gt; C_img{Image Encoder};
        C --&gt; D[Index (text)];
        C_img --&gt; D_img[Index (images)];
    end
    
    subgraph Online / Inference Process
        direction LR
        E[User Query] --&gt; F(Safety Filtering);
        F --&gt; G(Query Expansion);
        G --&gt; H{Text Encoder};
        H --&gt; I(Nearest Neighbor Search);
        D &amp; D_img --&gt; I;
        I --&gt; J(Prompt Engineering);
        E --&gt; J;
        J --&gt; K[LLM];
        K --&gt; L(Safety Filtering);
        L --&gt; M[Response];
    end
    
    %% Grouping for clarity
    subgraph Indexing Process
      A;B;C;C_img;D;D_img
    end
    subgraph Retrieval
      G;H;I;
    end
    subgraph Generation
      J;K;L
    end
</code></pre><p><strong>A user query&rsquo;s journey:</strong></p><ol><li><strong>Offline:</strong> A pipeline runs periodically to <strong>Parse, Chunk, and Index</strong> all 5 million documents into a vector database. This is the <strong>Indexing Process</strong>.</li><li><strong>Online:</strong> A user sends a query.</li><li><strong>Safety Filtering:</strong> The query is checked for harmful content.</li><li><strong>Query Expansion (Optional but good):</strong> The query is expanded with synonyms or rephrased to improve retrieval. &ldquo;How much do I get for trips?&rdquo; -> &ldquo;travel reimbursement policy allowance&rdquo;.</li><li><strong>Retrieval:</strong> The query is encoded into a vector, and an <strong>ANN search</strong> is performed on the vector DB to get the top-k chunks.</li><li><strong>Generation:</strong> The user&rsquo;s query and the retrieved chunks are assembled into a prompt using techniques like CoT and role-prompting. This is fed to the <strong>LLM</strong>.</li><li><strong>Safety Filtering:</strong> The LLM&rsquo;s response is checked for safety, PII, etc.</li><li>The final, safe, and grounded response is sent to the user.</li></ol><p>This diagram is your high-level design for the interview. Being able to draw this and explain each component&rsquo;s purpose and the trade-offs involved is an A+ answer.</p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>