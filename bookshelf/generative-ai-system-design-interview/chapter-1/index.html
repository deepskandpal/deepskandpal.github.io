<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><a href=#1-genai-overview-the-big-picture>1. GenAI Overview: The Big Picture</a></li><li><a href=#2-why-is-genai-so-powerful-the-three-pillars>2. Why is GenAI So Powerful? The Three Pillars</a></li><li><a href=#3-scaling-laws-the-recipe-for-success>3. Scaling Laws: The Recipe for Success</a></li><li><a href=#4-the-framework-for-ml-system-design-interviews>4. The Framework for ML System Design Interviews</a><ul><li><a href=#step-1-clarifying-requirements>Step 1: Clarifying Requirements</a></li><li><a href=#step-2-framing-the-problem-as-an-ml-task>Step 2: Framing the Problem as an ML Task</a></li><li><a href=#step-3-data-preparation>Step 3: Data Preparation</a></li><li><a href=#step-4-model-development>Step 4: Model Development</a></li></ul></li><li><a href=#5-deep-dive-into-model-training-the-heavy-lifting>5. Deep Dive into Model Training: The Heavy Lifting</a><ul><li><a href=#step-5-evaluation>Step 5: Evaluation</a></li><li><a href=#step-6-overall-ml-system-design>Step 6: Overall ML System Design</a></li><li><a href=#step-7-deployment-and-monitoring>Step 7: Deployment and Monitoring</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 1: Introduction and Overview</h1><span class=reading-time><em>25 min read</em></span><div class=book-details><div class=book-content><h2 id=1-genai-overview-the-big-picture>1. GenAI Overview: The Big Picture</h2><p>Before we build anything, we need to know what we&rsquo;re working with.</p><ul><li><strong>What are we trying to achieve?</strong> We&rsquo;re trying to categorize our problem. Are we trying to <em>classify</em> something that already exists, or are we trying to <em>create</em> something new? This is the most fundamental decision you&rsquo;ll make.</li></ul><p>The book correctly breaks ML models into two camps: <strong>Discriminative</strong> and <strong>Generative</strong>.</p><p>Let&rsquo;s break down these two categories of ML models. This is a classic interview question, and you need to nail it.</p><p><strong>1. Discriminative Models: The Art Critic</strong></p><ul><li><strong>What they do:</strong> They learn the <em>boundary</em> or the <em>difference</em> between different classes of data.</li><li><strong>The formal definition:</strong> They learn the conditional probability <code>P(Y | X)</code>.</li><li><strong>Intuitive Explanation:</strong> Think of an art critic. You show them a painting (<code>X</code>), and they tell you the probability that it&rsquo;s a Picasso (<code>Y=Picasso</code>) or a Monet (<code>Y=Monet</code>). The critic doesn&rsquo;t know how to paint a Picasso; they only know how to <em>distinguish</em> a Picasso from other paintings based on features like brush strokes, color palette, and subject matter.</li><li><strong>Core Task:</strong> Classification (Is this email spam or not?) and Regression (What is the price of this house?).</li><li><strong>Examples:</strong> Logistic Regression, SVMs, the &ldquo;classifier&rdquo; part of a GAN.</li></ul><p><strong>2. Generative Models: The Artist</strong></p><ul><li><strong>What they do:</strong> They learn the <em>underlying distribution</em> of the data itself. They learn what makes a Picasso a Picasso.</li><li><strong>The formal definition:</strong> They model the distribution <code>P(X)</code> or the joint distribution <code>P(X, Y)</code>.</li><li><strong>Intuitive Explanation:</strong> This is the artist. You can ask them, &ldquo;Create a new painting in the style of Picasso.&rdquo; Because they have learned the <em>essence</em> of what a Picasso painting looks like (<code>P(X)</code> where <code>X</code> is the distribution of Picasso paintings), they can generate a brand new, never-before-seen example. If you ask for a &ldquo;Picasso-style painting of a cat&rdquo; (<code>Y="cat"</code>), they are modeling the joint distribution <code>P(X, Y)</code>.</li><li><strong>Core Task:</strong> Creating new data (text, images, audio, etc.).</li><li><strong>Examples:</strong> VAEs, GANs, Diffusion Models, Autoregressive models (like GPT).</li></ul><p>The key takeaway is: <em>&ldquo;While discriminative algorithms can predict a target variable from input features, most of them lack the capability to learn the underlying data distribution needed to generate new data instances. For that, we turn to generative models.&rdquo;</em></p><p>Let&rsquo;s visualize this relationship.</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>%%{init: {&#39;flowchart&#39;: {&#39;nodeSpacing&#39;: 50, &#39;rankSpacing&#39;: 80}}}%%
graph TD
    A[🤖 Artificial Intelligence] --&gt; B[🧠 Machine Learning]
    B --&gt; C{🔀 Model Type}
    C --&gt; D[⚖️ Discriminative&lt;br/&gt;The Judge]
    C --&gt; E[🎨 Generative&lt;br/&gt;The Artist]

    D --&gt; D1[📊 Classification:&lt;br/&gt;Is this a cat?]
    D --&gt; D2[💡 Recommendation:&lt;br/&gt;Will you like this movie?]

    E --&gt; E1[🖼️ Image Generation:&lt;br/&gt;Create a picture of a cat]
    E --&gt; E2[📝 Text Generation:&lt;br/&gt;Write a story about a cat]

    subgraph Core[&#34;🎯 Core Task&#34;]
        F[🚪 Learns the boundary&lt;br/&gt;between data]
        G[📈 Learns the distribution&lt;br/&gt;of data]
    end

    D -.-&gt; F
    E -.-&gt; G

    style A fill:#dae8fc,stroke:#6c8ebf,stroke-width:3px
    style B fill:#d5e8d4,stroke:#82b366,stroke-width:3px
    style C fill:#f9f9f9,stroke:#333,stroke-width:2px
    style D fill:#f8cecc,stroke:#b85450,stroke-width:2px
    style E fill:#e1d5e7,stroke:#9673a6,stroke-width:2px
    style F fill:#fff2cc,stroke:#d6b656,stroke-width:2px
    style G fill:#fff2cc,stroke:#d6b656,stroke-width:2px
</code></pre><p>Here are popular tasks for each type, which helps clarify the distinction:</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>%%{init: {&#39;flowchart&#39;: {&#39;nodeSpacing&#39;: 40, &#39;rankSpacing&#39;: 70}}}%%
graph TD
    A[🤖 ML-powered Tasks] --&gt; B[⚖️ Discriminative]
    A --&gt; C[🎨 Generative]

    B --&gt; B1[🖼️ Image Segmentation]
    B --&gt; B2[👁️ Object Detection]
    B --&gt; B3[😊 Sentiment Analysis]
    B --&gt; B4[🏷️ Named Entity Recognition]
    B --&gt; B5[💡 Recommendation Systems]
    B --&gt; B6[🔍 Visual Search]

    C --&gt; C1[💬 Chatbots]
    C --&gt; C2[📄 Summarization]
    C --&gt; C3[🖼️📝 Image Captioning]
    C --&gt; C4[📝➡️🖼️ Text-to-Image]
    C --&gt; C5[👤 Face Generation]
    C --&gt; C6[🎵 Audio Synthesis]

    style A fill:#f0f0f0,stroke:#666,stroke-width:3px
    style B fill:#f8cecc,stroke:#b85450,stroke-width:2px
    style C fill:#e1d5e7,stroke:#9673a6,stroke-width:2px
</code></pre><h2 id=2-why-is-genai-so-powerful-the-three-pillars>2. Why is GenAI So Powerful? The Three Pillars</h2><p>The book nails the three key drivers. In an interview, you can frame this as the &ldquo;perfect storm&rdquo; that made GenAI possible.</p><ul><li><strong>What are we trying to achieve?</strong> We&rsquo;re explaining the fundamental enablers. This shows you understand the context of the current AI boom.</li></ul><ol><li><p><strong>Data (The Library of Alexandria):</strong> Traditional ML needed meticulously labeled data (e.g., &ldquo;this image is a cat,&rdquo; &ldquo;this one is a dog&rdquo;). This is slow and expensive. The breakthrough for GenAI was <strong>self-supervised learning</strong>. We can now feed models the <em>entire internet</em>—unlabeled text from Wikipedia, books, code from GitHub. The model creates its own learning tasks (e.g., &ldquo;predict the next word&rdquo;). The sheer volume and diversity of this data allow models to learn nuanced patterns about language, reasoning, and the world.</p></li><li><p><strong>Model Capacity (The Brain Size):</strong></p><ul><li><strong>Parameters:</strong> Think of these as the knobs or synapses in the model&rsquo;s &ldquo;brain.&rdquo; The more parameters (e.g., GPT-3 with 175B, PaLM with 540B), the more information and complex patterns it can learn and store.</li><li><strong>FLOPs (Floating Point Operations):</strong> This isn&rsquo;t about size, but about <em>computational cost</em>. How much &ldquo;thinking&rdquo; does it take to get an answer? A model can have fewer parameters but a more complex architecture (like dense connections) that requires more FLOPs. Understanding the difference between model size (parameters) and computational complexity (FLOPs) is a sign of a senior engineer.</li></ul></li><li><p><strong>Compute (The Engine):</strong> You can have a giant brain and a massive library, but if you can only read one word per second, it&rsquo;s useless. Specialized hardware like <strong>GPUs (NVIDIA&rsquo;s A100, H100)</strong> and <strong>TPUs (Google&rsquo;s custom chips)</strong> are the powerful engines that can perform the trillions of calculations needed to train these massive models in a feasible amount of time (weeks instead of centuries).</p></li></ol><h2 id=3-scaling-laws-the-recipe-for-success>3. Scaling Laws: The Recipe for Success</h2><p>This is a critical, FAANG-level concept. In an interview, discussing scaling laws shows you&rsquo;re thinking about the science and economics of training, not just throwing compute at a problem.</p><ul><li><strong>What are we trying to achieve?</strong> We want to train the best possible model without wasting money. Given a fixed budget for computation (the total FLOPs we can afford), what&rsquo;s the best way to spend it? Should we make the model bigger (more parameters, <code>N</code>) or train it on more data (more tokens, <code>D</code>)?</li></ul><p>Before scaling laws, it was a bit of a guessing game: &ldquo;For my next model, should I double the data or double the model size?&rdquo;</p><ul><li><p><strong>The OpenAI (2020) Finding:</strong> They found that performance scales predictably as a <strong>power-law</strong> with model size, dataset size, and compute. Crucially, <em>&ldquo;the impact of scaling on model performance is significantly more pronounced than the influence of architectural variations.&rdquo;</em> This means that for a while, making models bigger was more important than making them architecturally cleverer.</p></li><li><p><strong>The DeepMind (Chinchilla, 2022) Refinement:</strong> DeepMind found that many existing LLMs were <em>undertrained</em>. We were building huge models but not feeding them enough data. They proposed that for optimal performance, model size and training dataset size should be scaled <em>linearly</em> together. This led to models like &ldquo;Chinchilla,&rdquo; which was smaller than Gopher but trained on much more data, and outperformed it.</p></li><li><p><strong>Why this matters for an interview:</strong> It shows you understand the economics. If a VP gives you a $50 million budget to train a new model, you can use scaling laws to propose a plan: &ldquo;Based on Chinchilla&rsquo;s scaling laws, to get the optimal performance for this budget, we should aim for a model with X parameters and train it on Y trillion tokens.&rdquo; This is a data-driven engineering decision.</p></li></ul><p><strong>Scaling laws gave us a predictable recipe.</strong> They demonstrated that for a given compute budget, there&rsquo;s an optimal ratio between model size (number of parameters) and the amount of training data (number of tokens). This turned model training from a &ldquo;dark art&rdquo; into a more predictable engineering discipline.</p><h2 id=4-the-framework-for-ml-system-design-interviews>4. The Framework for ML System Design Interviews</h2><p>This is the absolute core of the chapter and your roadmap for any design interview. <strong>This is what the interviewer is evaluating you on.</strong></p><ul><li><strong>What are we trying to achieve?</strong> We&rsquo;re trying to demonstrate a structured, logical, and comprehensive approach to solving a complex, open-ended problem. We need to show we think about the entire system, not just the model.</li></ul><p>Let&rsquo;s recreate their flowchart. This is your mental checklist.</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>%%{init: {&#39;flowchart&#39;: {&#39;nodeSpacing&#39;: 60, &#39;rankSpacing&#39;: 90}}}%%
graph TD
    A[📋 1. Clarify Requirements] --&gt; B[🎯 2. Frame as ML Task]
    B --&gt; C[📊 3. Data Preparation]
    C --&gt; D[🛠️ 4. Model Development]
    D --&gt; E[📈 5. Evaluation]
    E --&gt; F[🏗️ 6. Overall System Design]
    F --&gt; G[🚀 7. Deployment &amp; Monitoring]

    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:3px
    style B fill:#fff2e6,stroke:#cc6600,stroke-width:3px
    style C fill:#e6ffe6,stroke:#00cc00,stroke-width:3px
    style D fill:#ffe6f3,stroke:#cc0066,stroke-width:3px
    style E fill:#f3e6ff,stroke:#6600cc,stroke-width:3px
    style F fill:#ffe6e6,stroke:#cc0000,stroke-width:3px
    style G fill:#f0f0f0,stroke:#666666,stroke-width:3px
</code></pre><p>Let&rsquo;s walk through the first few steps from this part of the book.</p><h3 id=step-1-clarifying-requirements>Step 1: Clarifying Requirements</h3><p>Before you write a single line of code or draw a single box, you must understand the problem. This is where you distinguish yourself as a senior engineer. You&rsquo;re not just a code monkey; you&rsquo;re a problem solver.</p><ul><li><p><strong>Functional Requirements:</strong> What should the system <em>do</em>?</p><ul><li><em>Example:</em> &ldquo;Generate an image and customize its style based on the user&rsquo;s prompt.&rdquo;</li><li><em>My advice:</em> Be specific. For a customer service chatbot, is it just for answering FAQs, or should it be able to process returns? Does it need to remember conversation history?</li></ul></li><li><p><strong>Non-Functional Requirements:</strong> How should the system <em>perform</em>? This is where the real engineering challenges lie. Ask about:</p><ul><li><strong>Business Objective:</strong> Why are we building this? To reduce customer support costs? To increase user engagement? This North Star dictates your trade-offs.</li><li><strong>System Features:</strong> Does the user rate the outputs? Can they edit the generated image? These are features that create feedback loops.</li><li><strong>Data:</strong> What data can we use? Is it sensitive (PII, medical)? Is it labeled?</li><li><strong>Constraints:</strong> Will it run on-device (on a phone) or in the cloud? This has massive implications for model size and latency.</li><li><strong>Scale:</strong> Are we serving 100 users or 100 million users? This impacts everything from database choice to inference architecture.</li><li><strong>Performance:</strong> What is the acceptable latency? Does a user expect an image in 2 seconds or 20 seconds? Is quality or speed more important?</li></ul></li></ul><p>In an interview, spend a solid 5 minutes here. Ask these questions. The interviewer often has a more detailed scenario in mind, and you need to extract it from them.</p><h3 id=step-2-framing-the-problem-as-an-ml-task>Step 2: Framing the Problem as an ML Task</h3><p>Now you translate the requirements into a machine learning problem.</p><ol><li>Specify Input and Output: This seems simple, but it&rsquo;s crucial.</li></ol><ul><li><em>Example for Text-to-Image:</em> Input = text prompt (string), maybe some style parameters. Output = Image (e.g., a 1024x1024 pixel grid).</li><li><em>Example for Chatbot:</em> Input = user&rsquo;s text query. Output = system&rsquo;s text response.</li><li><em>Example for Video generation:</em> Input = text prompt, maybe a starting image. Output = sequence of frames (video).</li></ul><ol start=2><li>Choose a Suitable ML Approach: This is where you make your first major architectural decision.</li></ol><p>Here&rsquo;s the logic, simplified:</p><ol><li><p><strong>Discriminative vs. Generative?</strong> Look at the output. Are you predicting a label from a fixed set (discriminative) or creating new content (generative)? For GenAI problems, the answer is almost always <strong>generative</strong>.</p></li><li><p><strong>Identify the Task Type:</strong> What kind of content are you generating? Text? Image? Audio? Video? This narrows down your model choices significantly.</p></li><li><p><strong>Choose a Suitable Algorithm:</strong> Now you get into the specific families of models. For image generation, the main contenders are GANs, VAEs, Autoregressive models, and Diffusion models. You should be able to briefly discuss the trade-offs:</p><ul><li><strong>GANs:</strong> Fast to generate, but can be unstable to train and may have &ldquo;mode collapse&rdquo; (lack diversity).</li><li><strong>VAEs:</strong> Stable to train, produce diverse outputs, but can sometimes be blurrier than GANs.</li><li><strong>Diffusion Models:</strong> Produce state-of-the-art, high-quality images, but are computationally expensive and slow to sample from (though this is improving).</li><li><strong>Autoregressive Models:</strong> Generate pixel by pixel. Can be very high quality but are extremely slow.</li></ul></li></ol><p>In an interview, you&rsquo;d say: <em>&ldquo;This is a text-to-image generation task. The key requirements are high-quality output and the ability to follow complex prompts. Given this, a Diffusion model is a strong candidate. While they are historically slow for inference, recent advancements like Latent Diffusion and improved sampling schedules have made them practical. I&rsquo;d choose this over a GAN because training stability and output quality are paramount for this product.&rdquo;</em></p><h3 id=step-3-data-preparation>Step 3: Data Preparation</h3><p>Great models are built on great data. Garbage in, garbage out. The book makes a key distinction between data prep for traditional ML vs. GenAI.</p><ul><li><strong>Traditional ML (Structured Data):</strong> The focus is on <strong>Feature Engineering</strong>. You have tables of data (e.g., customer purchase history) and you spend your time creating clever features (e.g., &ldquo;days since last purchase,&rdquo; &ldquo;average transaction value&rdquo;).</li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>%%{init: {&#39;flowchart&#39;: {&#39;nodeSpacing&#39;: 50, &#39;rankSpacing&#39;: 60}}}%%
graph LR
    A[📊 Data Sources] --&gt; B(🔧 Data Engineering ETL)
    B --&gt; C(⚙️ Feature Engineering)
    C --&gt; D[✨ Prepared Features]
    
    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px
    style B fill:#fff2e6,stroke:#cc6600,stroke-width:2px
    style C fill:#e6ffe6,stroke:#00cc00,stroke-width:2px
    style D fill:#f3e6ff,stroke:#6600cc,stroke-width:2px
</code></pre><ul><li><strong>GenAI (Unstructured Data):</strong> The focus shifts. Since we&rsquo;re using massive internet-scale datasets, feature engineering is less of a thing. The model learns the features itself. The challenges are different:</li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>%%{init: {&#39;flowchart&#39;: {&#39;nodeSpacing&#39;: 50, &#39;rankSpacing&#39;: 60}}}%%
graph TD
    A[🌐 Various Data Sources&lt;br/&gt;Internet, Books, Code] --&gt; B(🕷️ Data Collection&lt;br/&gt;Web Scraping)
    B --&gt; C[📄 Collected Raw Data]
    C --&gt; D(🧽 Data Cleaning)
    D --&gt; E[✨ Clean Data]
    
    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px
    style B fill:#fff2e6,stroke:#cc6600,stroke-width:2px
    style C fill:#f0f0f0,stroke:#666,stroke-width:2px
    style D fill:#e6ffe6,stroke:#00cc00,stroke-width:2px
    style E fill:#f3e6ff,stroke:#6600cc,stroke-width:2px
</code></pre><p>Key Steps in GenAI Data Prep:</p><ol><li><p><strong>Data Collection:</strong> How do you get 15 trillion tokens of text? You build scrapers for web pages (like the Common Crawl dataset), digitize books, and pull from sources like GitHub.</p></li><li><p><strong>Data Cleaning:</strong> This is arguably the most important, underrated step. The internet is a messy place. You need to:</p><ul><li><strong>Remove Harmful/Toxic/NSFW content.</strong></li><li><strong>Deduplicate:</strong> You don&rsquo;t want the model to see the exact same text 10,000 times. It can bias the model and is an inefficient use of compute.</li><li><strong>Filter Low-Quality Content:</strong> Remove boilerplate text, machine-translated garbage, etc. You might even use another model to assign a quality score.</li><li><strong>Balance Data:</strong> Ensure you have a diverse mix of data types (prose, dialogue, code, different languages) to create a well-rounded model.</li></ul></li><li><p><strong>Data Efficiency (Storage and Retrieval):</strong> When you have 50 Terabytes of data, you can&rsquo;t just load it into memory. You need:</p><ul><li><strong>Efficient Storage:</strong> Use distributed file systems (HDFS, S3) and efficient formats (Parquet, ORC).</li><li><strong>Efficient Retrieval:</strong> Use techniques like <strong>sharding</strong> (splitting data across machines) and <strong>indexing</strong> to quickly access the data needed for training batches.</li></ul></li></ol><p>A Hot Topic: Synthetic Data</p><ul><li><strong>Pros:</strong> Can improve diversity and scale up your dataset, especially for niche topics where real data is scarce.</li><li><strong>Cons:</strong> Quality depends on the original model. You risk creating a model that just copies the biases and errors of its predecessor, or misses the complexity of the real world. This is a very active area of research.</li></ul><h3 id=step-4-model-development>Step 4: Model Development</h3><p>This is the largest and most technical step. It covers three phases: Architecture, Training, and Sampling.</p><p>A. Model Architecture</p><p>Here you zoom in on your chosen model family. You need to talk about the specific components.</p><ul><li><strong>Example: U-Net for Diffusion Models</strong>
If you chose a Diffusion model for image generation, the backbone is often a U-Net. You should be able to describe it.</li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>%%{init: {&#39;flowchart&#39;: {&#39;nodeSpacing&#39;: 40, &#39;rankSpacing&#39;: 60}}}%%
graph TD
    Input[🖼️ Image] --&gt; D1(📉 Downsampling Block 1)
    D1 --&gt; D2(📉 Downsampling Block 2)
    D2 --&gt; D3(📉 Downsampling Block 3)
    
    D3 --&gt; U1(📈 Upsampling Block 1)
    U1 --&gt; U2(📈 Upsampling Block 2)
    U2 --&gt; U3(📈 Upsampling Block 3)
    U3 --&gt; Output[🔮 Predicted Noise]
    
    D3 -.-&gt;|Skip Connection| U1
    D2 -.-&gt;|Skip Connection| U2
    D1 -.-&gt;|Skip Connection| U3

    style Input fill:#e6f3ff,stroke:#0066cc,stroke-width:2px
    style Output fill:#f3e6ff,stroke:#6600cc,stroke-width:2px
    style D1 fill:#fff2e6,stroke:#cc6600,stroke-width:2px
    style D2 fill:#fff2e6,stroke:#cc6600,stroke-width:2px
    style D3 fill:#fff2e6,stroke:#cc6600,stroke-width:2px
    style U1 fill:#e6ffe6,stroke:#00cc00,stroke-width:2px
    style U2 fill:#e6ffe6,stroke:#00cc00,stroke-width:2px
    style U3 fill:#e6ffe6,stroke:#00cc00,stroke-width:2px
</code></pre><p><strong>Your explanation:</strong> <em>&ldquo;The U-Net architecture consists of an encoder (downsampling path) that captures contextual information, and a decoder (upsampling path) that reconstructs the image. The critical feature is the <strong>skip connections</strong>, which connect layers from the encoder directly to corresponding layers in the decoder. This allows the model to reuse low-level feature information (like fine textures and edges) during reconstruction, which is essential for generating sharp, detailed images.&rdquo;</em></p><p>Deep Dive: Transformer&rsquo;s Self-Attention (The heart of LLMs)
This is the most important architectural concept in modern GenAI. You MUST understand it intuitively.</p><ol><li><p><strong>The Goal:</strong> For any given word in a sentence, we want to understand how it relates to all other words in that sentence to get its true contextual meaning. The word &ldquo;bank&rdquo; means something different in &ldquo;river bank&rdquo; vs. &ldquo;money bank&rdquo;.</p></li><li><p>The Q, K, V Analogy:</p><ul><li><strong>Query (Q):</strong> From the perspective of the current word, this is a question: &ldquo;What am I, and what context do I need?&rdquo;</li><li><strong>Key (K):</strong> From the perspective of every <em>other</em> word, this is a label: &ldquo;Here&rsquo;s the kind of information I have.&rdquo;</li><li><strong>Value (V):</strong> From the perspective of every <em>other</em> word, this is the actual content: &ldquo;Here is my information.&rdquo;</li></ul></li><li><p>The Mechanism (Scaled Dot-Product Attention):</p><ul><li>You take the Query of your current word and compute the dot-product with the Key of every other word. This gives you a <strong>compatibility score</strong>.</li><li>You <strong>scale</strong> these scores (divide by the square root of the dimension) to keep gradients stable during training.</li><li>You run these scores through a <strong>Softmax</strong> function. This turns the scores into weights that sum to 1. It&rsquo;s like distributing 100% of your &ldquo;attention&rdquo; across all other words.</li><li>You multiply these attention weights by the <strong>Value</strong> of each word and sum them up. The result is a new representation for your current word, blended with contextual information from all other words it paid attention to.</li></ul></li></ol><p>Here&rsquo;s the Scaled Dot-Product Attention:</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>%%{init: {&#39;flowchart&#39;: {&#39;nodeSpacing&#39;: 40, &#39;rankSpacing&#39;: 50}}}%%
graph TD
    Q[🤔 Query] --&gt; MatMul1[× Matrix Multiply]
    K[🔑 Key] --&gt; MatMul1
    MatMul1 --&gt; Scale[📏 Scale]
    Scale --&gt; Mask[🎭 Mask Optional]
    Mask --&gt; SoftMax[🧮 SoftMax]
    SoftMax --&gt; MatMul2[× Matrix Multiply]
    V[💎 Value] --&gt; MatMul2
    MatMul2 --&gt; Output[✨ Attention Output]
    
    style Q fill:#e6f3ff,stroke:#0066cc,stroke-width:2px
    style K fill:#fff2e6,stroke:#cc6600,stroke-width:2px
    style V fill:#e6ffe6,stroke:#00cc00,stroke-width:2px
    style Output fill:#f3e6ff,stroke:#6600cc,stroke-width:2px
</code></pre><ol start=4><li>Multi-Head Attention:<ul><li><strong>The Problem:</strong> Just one set of Q, K, V matrices might only learn one type of relationship (e.g., just grammatical relationships).</li><li><strong>The Solution:</strong> Do the whole attention process multiple times in parallel, each with different, learned Q, K, V weight matrices. Each &ldquo;head&rdquo; can specialize in learning a different type of relationship (e.g., one head for semantic meaning, one for syntax, one for long-distance dependencies).</li><li>You then concatenate the outputs from all heads and pass them through a final linear layer to combine the knowledge.</li></ul></li></ol><p>B. Model Training</p><p>Once you have the architecture, you need to train it.</p><ul><li><p><strong>Training Methodology:</strong></p><ul><li>This is model-specific. For Diffusion, it&rsquo;s a process of adding noise and then training the U-Net to predict and remove that noise. For GANs, it&rsquo;s an adversarial process between a generator and a discriminator.</li><li>For LLMs, it&rsquo;s often a <strong>multi-stage process</strong>:<ol><li><strong>Pre-training:</strong> On massive, general data (the internet). The goal is to learn language, facts, and reasoning. The objective is often next-token prediction.</li><li><strong>Supervised Fine-Tuning (SFT):</strong> On a smaller, high-quality dataset of instruction-response pairs to teach the model how to follow commands.</li><li><strong>Alignment (e.g., RLHF):</strong> Using reinforcement learning from human feedback to make the model more helpful, harmless, and honest.</li></ol></li></ul></li><li><p><strong>ML Objective and Loss Function:</strong></p><ul><li>The <strong>objective</strong> is what you want the model to do (e.g., &ldquo;predict the next token&rdquo;).</li><li>The <strong>loss function</strong> is the mathematical formula that measures how far the model&rsquo;s prediction is from the truth. The entire training process is about minimizing this loss.</li></ul></li></ul><p>This covers the essence of the first half of the chapter. We&rsquo;ve established the landscape and have a clear, structured plan to tackle the design.</p><hr><h2 id=5-deep-dive-into-model-training-the-heavy-lifting>5. Deep Dive into Model Training: The Heavy Lifting</h2><p>The book talks about techniques for training <em>large-scale</em> models. These are your bread and butter as a senior engineer at FAANG.</p><ul><li><strong>What are we trying to achieve?</strong> We&rsquo;re trying to train a model that is too big to fit in one GPU&rsquo;s memory and on a dataset that is too large to process quickly on one machine. We need to be efficient with memory, time, and money.</li></ul><p>Here are the key optimization techniques:</p><ol><li><p><strong>Gradient Checkpointing:</strong></p><ul><li><strong>Intuition:</strong> During training, you need to store intermediate values (activations) to calculate the gradients for backpropagation. This consumes a ton of memory. Gradient checkpointing is a clever trade-off: <strong>it throws away most of these intermediate values to save memory, and then re-computes them on the fly during the backward pass.</strong></li><li><strong>Trade-off:</strong> You use less memory, but it increases your training time (more compute). It&rsquo;s perfect for when you want to train a massive model on GPUs that don&rsquo;t have enough VRAM.</li></ul></li><li><p><strong>Mixed Precision Training:</strong></p><ul><li><strong>Intuition:</strong> By default, calculations are done in 32-bit floating point (FP32). But do we need that much precision for <em>everything</em>? Mixed precision uses faster, less memory-intensive 16-bit floats (FP16) for most of the calculations, while keeping critical parts (like weight updates) in FP32 to maintain stability.</li><li><strong>Benefit:</strong> On modern GPUs with Tensor Cores, this can speed up training by 2-3x and cut memory usage in half. Frameworks like PyTorch (with <code>torch.amp</code>) make this almost automatic.</li></ul></li><li><p><strong>Distributed Training (The Teamwork):</strong> This is how you train on hundreds or thousands of GPUs at once. The book correctly identifies the main types of parallelism.</p></li></ol><pre tabindex=0><code class=language-mermaid data-lang=mermaid>%%{init: {&#39;flowchart&#39;: {&#39;nodeSpacing&#39;: 50, &#39;rankSpacing&#39;: 80}}}%%
graph LR
    P[Parallelism] --&gt; DP[Data Parallelism]
    P --&gt; MP[Model Parallelism]
    
    DP --&gt; DP_Desc[Split the DATA&lt;br/&gt;Each GPU gets a full&lt;br/&gt;copy of the MODEL]
    
    MP --&gt; PP[Pipeline Parallelism&lt;br/&gt;Inter-layer]
    MP --&gt; TP[Tensor Parallelism&lt;br/&gt;Intra-layer]
    
    PP --&gt; PP_Desc[Split the MODEL LAYERS&lt;br/&gt;GPU_0: Layers 1-8&lt;br/&gt;GPU_1: Layers 9-16]
    TP --&gt; TP_Desc[Split SINGLE LAYER ops&lt;br/&gt;GPU_0: half matrix&lt;br/&gt;GPU_1: other half]

    style P fill:#f9f9f9,stroke:#333,stroke-width:3px
    style DP fill:#e8f4fd,stroke:#1f77b4,stroke-width:2px
    style MP fill:#fff2e8,stroke:#ff7f0e,stroke-width:2px
    style DP_Desc fill:#d5e8d4,stroke:#82b366,stroke-width:2px
    style PP_Desc fill:#e1d5e7,stroke:#9673a6,stroke-width:2px
    style TP_Desc fill:#e1d5e7,stroke:#9673a6,stroke-width:2px
</code></pre><ul><li><strong>Data Parallelism:</strong> The simplest and most common. You have a giant dataset. You give a small chunk (a mini-batch) to each GPU. Each GPU calculates the gradients for its batch, and then they all sync up their results via a Parameter Server or an all-reduce operation.</li><li><strong>Model Parallelism:</strong> You use this when the model itself is too big for a single GPU.<ul><li><strong>Pipeline Parallelism:</strong> You put different <em>layers</em> of the model on different GPUs. GPU 0 computes layers 1-8 and passes its output to GPU 1, which computes layers 9-16, etc. It&rsquo;s like an assembly line. The main challenge is keeping all GPUs busy (the &ldquo;pipeline bubble&rdquo;).</li><li><strong>Tensor Parallelism:</strong> You use this when a <em>single layer</em> is too big. You split the actual matrix multiplications of that layer across multiple GPUs. This is more complex but essential for the massive layers in models like GPT.</li></ul></li></ul><p>A state-of-the-art training setup (like what&rsquo;s used for Llama 3) uses a <strong>hybrid approach</strong>, combining all of these techniques (Data, Pipeline, and Tensor parallelism) to work efficiently at massive scale. This is what frameworks like FSDP (Fully Sharded Data Parallel) help manage.</p><p>C. Model Sampling (Inference)</p><p>After the model is trained, how do you generate output? This is sampling.</p><ul><li><strong>Greedy Search:</strong> At each step, pick the single most likely next word/pixel. Fast, but boring, repetitive, and deterministic.</li><li><strong>Beam Search:</strong> Keep track of the <code>k</code> most likely sequences at each step. Better than greedy, but can still suppress creativity.</li><li><strong>Stochastic Sampling (Top-k, Top-p):</strong> This is what&rsquo;s used in modern chatbots.<ul><li><strong>Top-k Sampling:</strong> Consider only the <code>k</code> most likely next words, and then sample from that smaller set.</li><li><strong>Top-p (Nucleus) Sampling:</strong> Consider the smallest set of words whose cumulative probability is greater than <code>p</code>. This is adaptive; if the model is very certain, the set is small. If it&rsquo;s uncertain, the set is larger. This often gives the best balance of coherence and creativity.</li></ul></li></ul><h3 id=step-5-evaluation>Step 5: Evaluation</h3><p>How do you know if your model is any good?</p><ul><li><p><strong>Offline Evaluation (Using a test dataset):</strong></p><ul><li><strong>Discriminative Metrics:</strong> Easy. Accuracy, Precision, Recall, F1 score. You have a ground truth.</li><li><strong>Generative Metrics (Harder!):</strong> There&rsquo;s no single &ldquo;right&rdquo; answer.<ul><li><strong>Text:</strong> BLEU, ROUGE (compare generated text to references), Perplexity (how surprised is the model by the text).</li><li><strong>Images:</strong> FID, KID (measure how similar the distribution of generated images is to real images), CLIPScore (measures how well an image matches a text prompt).</li></ul></li><li><strong>The Interview Point:</strong> You need to show you understand that for generative models, <em>no single metric is enough</em>. You need a suite of metrics to measure different aspects: quality, diversity, alignment to the prompt, etc. And ultimately&mldr;</li></ul></li><li><p><strong>Online Evaluation (In production, with real users):</strong></p><ul><li>This is about business impact.</li><li><strong>Metrics:</strong> Click-Through Rate (CTR), Conversion Rate, User Retention, Latency, User Satisfaction surveys.</li><li>You&rsquo;ll often use <strong>A/B testing</strong> to compare a new model against the old one on these business metrics.</li></ul></li><li><p><strong>Human Evaluation:</strong> For creative tasks, you can&rsquo;t escape it. You need human raters to score outputs on dimensions like &ldquo;creativity,&rdquo; &ldquo;coherence,&rdquo; &ldquo;factuality,&rdquo; and &ldquo;harmlessness.&rdquo; This is a core part of the RLHF process.</p></li></ul><p>The Golden Rule: Online and offline metrics don&rsquo;t always align. A model might get a great ROUGE score but generate text that users find unhelpful. The ultimate test is how it performs with real users and impacts your business goals.</p><h3 id=step-6-overall-ml-system-design>Step 6: Overall ML System Design</h3><p>Now, zoom out from just the model and draw the whole system. This is where you integrate everything. You need to think about more than just the <code>model.predict()</code> call.</p><ul><li><strong>System Components:</strong> A request comes in. What happens?<ol><li><strong>Input Preprocessing:</strong> The raw prompt is cleaned.</li><li><strong>Safety & Moderation (Pre-filter):</strong> Check the input prompt for harmful content.</li><li><strong>Core Model Inference:</strong> Call your generative model.</li><li><strong>Post-processing:</strong> Convert model output to a user-friendly format.</li><li><strong>Safety & Moderation (Post-filter):</strong> Check the <em>generated</em> output for harmful content, bias, or PII leakage.</li><li><strong>User Feedback Loop:</strong> Log the output, the prompt, and any user feedback (thumbs up/down) for future retraining.</li></ol></li><li><strong>Scalability:</strong> How do you serve 100 million users? You&rsquo;ll have a load balancer distributing requests to a fleet of inference servers. You might use model parallelism for inference if the model is huge.</li><li><strong>Security & Bias:</strong> How do you prevent misuse? How do you detect and filter biased outputs? This is a huge area and showing awareness is critical.</li></ul><p>Here is a simple, intuitive diagram of what a production GenAI system looks like.</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>%%{init: {&#39;flowchart&#39;: {&#39;nodeSpacing&#39;: 60, &#39;rankSpacing&#39;: 100, &#39;curve&#39;: &#39;basis&#39;}}}%%
graph TD
    User[👤 User] --&gt;|1. Request| API[🌐 API Gateway /&lt;br/&gt;Load Balancer]
    API --&gt;|2. Forward| Pre[🛡️ Preprocessing &amp;&lt;br/&gt;Safety]
    Pre --&gt;|3. Cleaned Prompt| Inference[🧠 Model Inference&lt;br/&gt;Cluster GPUs]
    Inference --&gt;|4. Raw Output| Post[🛡️ Postprocessing &amp;&lt;br/&gt;Safety]
    Post --&gt;|5. Final Response| API
    API --&gt;|6. Generated Image| User

    Pre -.-&gt;|Check violations| P_Filter[🚫 Policy Filter]
    Post -.-&gt;|Scan output| O_Filter[🚫 Output Filter]
    
    User -.-&gt;|👍👎 Feedback| Feedback[🔄 Feedback Loop&lt;br/&gt;RLHF]
    Feedback -.-&gt; Retraining[🔧 Model Retraining&lt;br/&gt;Pipeline]
    Retraining -.-&gt; Inference

    style User fill:#e6f3ff,stroke:#0066cc,stroke-width:3px
    style API fill:#fff2e6,stroke:#cc6600,stroke-width:2px
    style Inference fill:#d5e8d4,stroke:#82b366,stroke-width:3px
    style Pre fill:#f8cecc,stroke:#b85450,stroke-width:2px
    style Post fill:#f8cecc,stroke:#b85450,stroke-width:2px
    style P_Filter fill:#fff2cc,stroke:#d6b656,stroke-width:2px
    style O_Filter fill:#fff2cc,stroke:#d6b656,stroke-width:2px
    style Feedback fill:#e1d5e7,stroke:#9673a6,stroke-width:2px
    style Retraining fill:#e1d5e7,stroke:#9673a6,stroke-width:2px
</code></pre><p>Key talking points for this diagram:</p><ul><li><strong>System Components:</strong> It&rsquo;s not just the model. There are safety filters <em>before</em> and <em>after</em> the model, load balancers, caching layers, and monitoring services.</li><li><strong>Safety Mechanisms:</strong> You absolutely must talk about this. How do you prevent users from generating harmful content? You need input filters (prompt filtering) and output filters (content moderation classifiers).</li><li><strong>User Feedback & Continuous Learning (RLHF):</strong> The thumbs up/down buttons aren&rsquo;t just for show. That data is collected and used to continuously fine-tune the model to better align with user preferences. This is a critical feedback loop.</li><li><strong>Scalability:</strong> How do you serve millions of users? You use load balancers to distribute traffic to a cluster of GPU machines for inference. You use techniques like model parallelism within that cluster if the model is huge.</li><li><strong>Monitoring:</strong> The final step. You log everything. Latency, error rates, GPU utilization, metric scores from your safety classifiers. If something breaks, you need to know immediately.</li></ul><h3 id=step-7-deployment-and-monitoring>Step 7: Deployment and Monitoring</h3><ul><li><strong>Deployment:</strong> How do you get the model into production? You might have a CI/CD pipeline for models.</li><li><strong>Monitoring:</strong> What are you tracking?<ul><li><strong>System Metrics:</strong> Latency, error rates, GPU utilization.</li><li><strong>Model Metrics:</strong> Monitor the distribution of inputs and outputs. Is there a &ldquo;drift&rdquo; over time? Is the model&rsquo;s performance degrading? This is crucial for knowing when you need to retrain.</li></ul></li></ul><h2 id=summary>Summary</h2><p>This framework is your bible for a GenAI system design interview. Start with the user and the business problem, frame it as an ML task, go deep on the data and model, evaluate your results, design the end-to-end production system, and think about what happens after it&rsquo;s live.</p><p>By walking through this chapter, you&rsquo;ve built a powerful mental model for GenAI system design:</p><ol><li><strong>Frame the Problem:</strong> Start broad (AI vs. ML), then narrow down (Discriminative vs. Generative). Understand the &ldquo;why&rdquo; (Data, Model, Compute).</li><li><strong>Follow the Roadmap:</strong> Use the 7-step framework as your guide. It prevents you from getting lost and ensures you cover all your bases.</li><li><strong>Think Like an Engineer, Not Just a Scientist:</strong> Don&rsquo;t just talk about the model. Talk about the data pipelines, the training optimizations (parallelism!), the evaluation metrics (offline AND online), and the full production system with its safety and feedback loops.</li><li><strong>Know Your Architectures:</strong> Be able to explain U-Net for diffusion models and self-attention for transformers at an intuitive level.</li><li><strong>Understand the Economics:</strong> Scaling laws help you make data-driven decisions about compute budgets.</li><li><strong>Safety and Ethics First:</strong> Always discuss content moderation, bias detection, and user feedback loops.</li></ol><p>If you can walk an interviewer through these steps, providing the &ldquo;why&rdquo; behind your choices and discussing the trade-offs at each stage, you&rsquo;re not just answering the question—you&rsquo;re demonstrating the strategic thinking of a senior staff engineer at a top company.</p><p>This approach will make you appear structured, thorough, and deeply knowledgeable—exactly what a FAANG interviewer is looking for.</p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>