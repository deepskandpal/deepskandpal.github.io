<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><a href=#page-153-linear-svm-classification---the-big-idea>(Page 153: Linear SVM Classification - The Big Idea)</a></li><li><a href=#page-154-155-soft-margin-classification>(Page 154-155: Soft Margin Classification)</a></li><li><a href=#page-157-161-nonlinear-svm-classification>(Page 157-161: Nonlinear SVM Classification)</a></li><li><a href=#page-162-computational-complexity>(Page 162: Computational Complexity)</a></li><li><a href=#page-162-163-svm-regression>(Page 162-163: SVM Regression)</a></li></ul></li><li><a href=#under-the-hood>Under the hood</a></li><li><a href=#glossary>Glossary</a><ul><li><a href=#page-164-173-under-the-hood---how-svms-work>(Page 164-173: Under the Hood - How SVMs Work)</a></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 5: Support Vector Machines</h1><span class=reading-time><em>27 min read</em></span><div class=book-details><div class=book-content><p>Alright class, let&rsquo;s gear up for <strong>Chapter 5: Support Vector Machines (SVMs)</strong>. This is a big one! SVMs are incredibly powerful and versatile models. You&rsquo;ll find them used for linear or nonlinear classification, regression, and even outlier detection. They are a cornerstone of classical machine learning, and definitely a tool everyone interested in ML should have in their arsenal.</p><p>The book mentions they&rsquo;re particularly good for <strong>complex small- or medium-sized datasets.</strong> This chapter will walk us through their core concepts, how to use them, and, importantly for our &ldquo;what it&rsquo;s ultimately trying to achieve&rdquo; philosophy, how they actually work.</p><h2 id=page-153-linear-svm-classification---the-big-idea>(Page 153: Linear SVM Classification - The Big Idea)</h2><p>The fundamental idea behind SVMs is best explained with pictures, and Figure 5-1 (page 154) does a great job.<figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-1.png alt="Figure 5-1. Large margin classification" width=700></figure>Imagine you have two classes of data points that are <strong>linearly separable</strong> – meaning you can draw a straight line to separate them perfectly.</p><ul><li><p>The plot on the left of Figure 5-1 shows three possible straight lines (decision boundaries) that could separate the Iris data.</p><ul><li>One (dashed) is terrible; it doesn&rsquo;t even separate the classes.</li><li>The other two separate the classes perfectly on the <em>training data</em>, but their decision boundaries are very close to some of the training instances. This means they might not generalize well to new, unseen instances. A slight variation in a new point could cause it to be misclassified.</li></ul></li><li><p>The plot on the right of Figure 5-1 shows the decision boundary of an <strong>SVM classifier</strong>.</p><ul><li><strong>What is it ultimately trying to achieve?</strong> An SVM doesn&rsquo;t just find <em>any</em> line that separates the classes. It tries to find the line that has the <strong>largest possible margin</strong> between itself and the closest instances from <em>each</em> class.</li><li>Think of it as fitting the <strong>widest possible street</strong> between the two classes. The decision boundary is the median line of this street, and the edges of the street are defined by the closest points.</li><li>This is called <strong>large margin classification</strong>. The intuition is that a wider margin leads to better generalization because the decision boundary is less sensitive to the exact position of individual training instances and has more &ldquo;room for error&rdquo; with new data.</li></ul></li><li><p><strong>Support Vectors (Page 154):</strong></p><ul><li>Notice that the position of this &ldquo;widest street&rdquo; is entirely determined by the instances located right on the edge of the street. These instances are called the <strong>support vectors</strong> (they are circled in Figure 5-1).</li><li><em>What they are ultimately trying to achieve:</em> They are the critical data points that &ldquo;support&rdquo; or define the decision boundary and the margin. If you move a support vector, the decision boundary will likely change. If you add more training instances that are &ldquo;off the street&rdquo; (far away from the margin), they won&rsquo;t affect the decision boundary at all! The SVM is only sensitive to the points closest to the boundary.</li></ul></li><li><p><strong>Sensitivity to Feature Scales (Figure 5-2, page 154):</strong><figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-2.png alt="Figure 5-2. Sensitivity to feature scales" width=700></figure></p><ul><li>The scorpion icon highlights a crucial point: <strong>SVMs are sensitive to feature scales.</strong></li><li>If one feature has a much larger range of values than another (e.g., vertical scale much larger than horizontal in the left plot of Figure 5-2), the &ldquo;widest street&rdquo; will tend to be oriented to accommodate the larger scale. The street might look wide in the scaled units, but it might be very narrow along the axis with smaller-scaled features.</li><li><strong>Solution:</strong> Always <strong>scale your features</strong> (e.g., using Scikit-Learn&rsquo;s <code>StandardScaler</code>) before training an SVM. The right plot in Figure 5-2 shows a much better, more balanced decision boundary after scaling.</li><li><em>What scaling is ultimately trying to achieve:</em> It ensures that all features contribute more equally to the distance calculations involved in finding the largest margin, preventing features with larger numerical values from dominating.</li></ul></li></ul><h2 id=page-154-155-soft-margin-classification>(Page 154-155: Soft Margin Classification)</h2><p>The &ldquo;widest street&rdquo; idea so far assumed <strong>hard margin classification</strong>:</p><ol><li>All instances must be strictly off the street and on the correct side.</li><li>This only works if the data is perfectly linearly separable.</li><li>It&rsquo;s very sensitive to outliers (Figure 5-3, page 155). A single outlier can make it impossible to find a hard margin, or it can drastically change the decision boundary, leading to poor generalization.<figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-3.png alt="Figure 5-3. Hard margin sensitivity to outliers" width=700></figure></li></ol><p>To avoid these issues, we use a more flexible approach: <strong>Soft Margin Classification</strong>.</p><ul><li><p><strong>What it&rsquo;s ultimately trying to achieve:</strong> Find a good balance between:</p><ol><li>Keeping the &ldquo;street&rdquo; (margin) as wide as possible.</li><li>Limiting <strong>margin violations</strong> – instances that end up <em>inside</em> the street or even on the wrong side of the decision boundary.</li></ol></li><li><p>This allows the model to handle data that isn&rsquo;t perfectly linearly separable and makes it less sensitive to outliers.</p></li><li><p><strong>The <code>C</code> Hyperparameter (Figure 5-4, page 155):</strong><figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-4.png alt="Figure 5-4. Large margin (left) versus fewer margin violations (right)" width=700></figure></p><ul><li>When using SVMs (e.g., in Scikit-Learn), the hyperparameter <code>C</code> controls this trade-off.</li><li><strong>Low <code>C</code> value (e.g., <code>C=1</code> in the left plot of Figure 5-4):</strong><ul><li>Wider street (larger margin).</li><li>More margin violations are tolerated.</li><li>This generally leads to a model that is more regularized and might generalize better, even if it makes more mistakes on the training data margin.</li></ul></li><li><strong>High <code>C</code> value (e.g., <code>C=100</code> in the right plot of Figure 5-4):</strong><ul><li>Narrower street (smaller margin).</li><li>Fewer margin violations are tolerated. The model tries harder to classify all training instances correctly.</li><li>This can lead to overfitting if <code>C</code> is too high, as the model might be too sensitive to individual data points, including noise or outliers.</li></ul></li><li><em>What <code>C</code> is ultimately trying to achieve:</em> It&rsquo;s a regularization parameter. A smaller <code>C</code> means more regularization (larger margin, more tolerance for violations). A larger <code>C</code> means less regularization (smaller margin, less tolerance for violations, tries to fit training data more perfectly).</li><li>If your SVM model is overfitting, try reducing <code>C</code>. If it&rsquo;s underfitting, try increasing <code>C</code> (but be mindful of also allowing the margin to shrink too much).</li></ul></li><li><p><strong>Scikit-Learn Implementation (Page 155-156):</strong>
The code shows using <code>LinearSVC</code> (Linear Support Vector Classifier) for the Iris dataset:
<code>from sklearn.svm import LinearSVC</code>
<code>from sklearn.preprocessing import StandardScaler</code>
<code>from sklearn.pipeline import Pipeline</code>
<code>svm_clf = Pipeline([</code>
<code>("scaler", StandardScaler()),</code>
<code>("linear_svc", LinearSVC(C=1, loss="hinge"))</code>
<code>])</code>
<code>svm_clf.fit(X, y)</code></p><ul><li>Note the use of <code>StandardScaler</code> in a pipeline – good practice!</li><li><code>loss="hinge"</code>: The hinge loss is the cost function typically associated with linear SVMs (more on this in the &ldquo;Under the Hood&rdquo; section).</li><li>The bird icon (page 156) mentions that unlike Logistic Regression, SVM classifiers generally <strong>do not output probabilities</strong> directly. They output a decision score, and the sign of the score determines the class.</li><li><strong>Alternatives for Linear SVMs:</strong><ul><li><code>SVC(kernel="linear", C=1)</code>: Uses the <code>SVC</code> class with a linear kernel. It&rsquo;s generally slower than <code>LinearSVC</code> but can be useful if you later want to try other kernels.</li><li><code>SGDClassifier(loss="hinge", alpha=1/(m*C))</code>: Uses Stochastic Gradient Descent to train a linear SVM. Slower to converge than <code>LinearSVC</code> but good for online learning or very large datasets that don&rsquo;t fit in memory (out-of-core).</li></ul></li><li>The scorpion icon (page 156) gives important tips for <code>LinearSVC</code>:<ul><li>It regularizes the bias term, so centering data (done by <code>StandardScaler</code>) is important.</li><li>Ensure <code>loss="hinge"</code> is set (not always the default).</li><li>For better performance, set <code>dual=False</code> unless features > training instances (duality is an advanced optimization concept we&rsquo;ll touch on).</li></ul></li></ul></li></ul><h2 id=page-157-161-nonlinear-svm-classification>(Page 157-161: Nonlinear SVM Classification)</h2><p>Linear SVMs are great, but many datasets aren&rsquo;t linearly separable. What then?</p><ol><li><p><strong>Adding Polynomial Features (Page 157):</strong></p><ul><li>As we saw in Chapter 4, you can add polynomial features to your data. This can transform a nonlinearly separable dataset into a linearly separable one in a higher-dimensional space.</li><li>Figure 5-5 illustrates this: a 1D dataset that&rsquo;s not linearly separable becomes linearly separable in 2D if you add <code>x₂ = x₁²</code> as a new feature.<figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-5.png alt="Figure 5-5. Adding features to make a dataset linearly separable" width=700></figure></li><li><strong>Implementation (Moons dataset):</strong>
<code>from sklearn.datasets import make_moons</code>
<code>X, y = make_moons(n_samples=100, noise=0.15)</code>
<code>polynomial_svm_clf = Pipeline([</code>
<code>("poly_features", PolynomialFeatures(degree=3)),</code>
<code>("scaler", StandardScaler()),</code>
<code>("svm_clf", LinearSVC(C=10, loss="hinge"))</code>
<code>])</code>
<code>polynomial_svm_clf.fit(X, y)</code>
Figure 5-6 (page 158) shows the resulting decision boundary on the moons dataset. It&rsquo;s curved and does a decent job!<figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-6.png alt="Figure 5-6. Linear SVM classifier using polynomial features" width=700></figure></li></ul></li><li><p><strong>The Kernel Trick (Polynomial Kernel - Page 158):</strong></p><ul><li>Adding polynomial features works, but at high degrees, it creates a <em>huge</em> number of features (combinatorial explosion!), making the model very slow.</li><li><strong>Enter the Kernel Trick – one of the &ldquo;magic&rdquo; ideas in SVMs!</strong></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> It allows you to get the <em>same result</em> as if you had added many polynomial features (even very high-degree ones) <em>without actually creating or adding those features explicitly</em>.</li><li>This avoids the computational cost and memory overhead of explicitly transforming the data into a very high-dimensional space.</li><li><strong>How it works (conceptually):</strong> Instead of transforming the data points and then taking dot products in the high-dimensional space, the kernel is a function that can compute what the dot product <em>would have been</em> in that high-dimensional space, using only the original low-dimensional data points. (We&rsquo;ll see more math in &ldquo;Under the Hood&rdquo;).</li><li><strong>Using Polynomial Kernel in Scikit-Learn:</strong>
<code>from sklearn.svm import SVC</code>
<code>poly_kernel_svm_clf = Pipeline([</code>
<code>("scaler", StandardScaler()),</code>
<code>("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=5))</code>
<code>])</code>
<code>poly_kernel_svm_clf.fit(X, y)</code><ul><li><code>kernel="poly"</code>: Tells SVC to use the polynomial kernel trick.</li><li><code>degree=3</code>: Simulates a 3rd-degree polynomial expansion.</li><li><code>coef0=1</code>: (Gamma_zero) Controls how much the model is influenced by high-degree vs. low-degree polynomials in the kernel.</li><li><code>C=5</code>: Regularization parameter.</li><li>Figure 5-7 (page 159) shows the results for <code>degree=3</code> (left) and <code>degree=10</code> (right). Higher degree can lead to more complex boundaries (and potential overfitting).<figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-7.png alt="Figure 5-7. SVM classifiers with a polynomial kernel" width=700></figure></li></ul></li><li>The scorpion icon suggests using grid search to find good hyperparameter values.</li></ul></li><li><p><strong>Similarity Features (Gaussian RBF Kernel - Page 159):</strong></p><ul><li>Another way to handle nonlinear problems is to add features based on <strong>similarity</strong> to certain <strong>landmarks</strong>.</li><li>Imagine you pick a few landmark points in your feature space. For each data instance, you can calculate how similar it is to each landmark. These similarity scores become new features.</li><li><strong>Gaussian Radial Basis Function (RBF) as a similarity function (Equation 5-1):</strong>
<code>φ_γ(x, ℓ) = exp(-γ ||x - ℓ||²)</code><ul><li><code>x</code>: An instance.</li><li><code>ℓ</code>: A landmark.</li><li><code>||x - ℓ||²</code>: Squared Euclidean distance between the instance and the landmark.</li><li><code>γ</code> (gamma): A hyperparameter.</li><li><em>What this function is ultimately trying to achieve:</em> It outputs a value between 0 (if <code>x</code> is very far from landmark <code>ℓ</code>) and 1 (if <code>x</code> is at the landmark <code>ℓ</code>). It&rsquo;s a bell-shaped curve. A higher <code>γ</code> makes the bell narrower (influence of the landmark drops off more quickly).</li></ul></li><li>Figure 5-8 shows a 1D dataset. Two landmarks are added. Each original instance <code>x₁</code> is transformed into two new features: its RBF similarity to landmark 1, and its RBF similarity to landmark 2. The transformed 2D dataset (right plot) becomes linearly separable!<figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-8.png alt="Figure 5-8. Similarity features using the Gaussian RBF" width=700></figure></li><li><strong>Choosing landmarks:</strong> A simple approach is to create a landmark at the location of <em>every single instance</em> in the dataset. This transforms an <code>m x n</code> dataset into an <code>m x m</code> dataset (if original features are dropped). This can make a dataset linearly separable, but it&rsquo;s computationally expensive if <code>m</code> is large.</li></ul></li><li><p><strong>Gaussian RBF Kernel (Page 160):</strong></p><ul><li>Again, the <strong>kernel trick</strong> comes to the rescue! The Gaussian RBF kernel allows SVMs to get the same effect as adding many RBF similarity features, without actually computing them.</li><li><strong>Using Gaussian RBF Kernel in Scikit-Learn:</strong>
<code>rbf_kernel_svm_clf = Pipeline([</code>
<code>("scaler", StandardScaler()),</code>
<code>("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001))</code>
<code>])</code>
<code>rbf_kernel_svm_clf.fit(X, y)</code><ul><li><code>kernel="rbf"</code>: Use the Gaussian RBF kernel.</li><li><code>gamma</code> (γ): Controls the width of the RBF &ldquo;bell.&rdquo;<ul><li><strong>Increasing <code>gamma</code>:</strong> Makes the bell narrower. Each instance&rsquo;s range of influence is smaller. Decision boundary becomes more irregular, wiggling around individual instances (can lead to overfitting). (See Figure 5-9, page 161, comparing <code>gamma=0.1</code> to <code>gamma=5</code>).</li><li><strong>Decreasing <code>gamma</code>:</strong> Makes the bell wider. Instances have a larger range of influence. Decision boundary becomes smoother (can lead to underfitting if too smooth).</li><li>So, <code>gamma</code> acts like a regularization hyperparameter: if overfitting, reduce <code>gamma</code>; if underfitting, increase <code>gamma</code>.</li></ul></li><li><code>C</code>: Regularization parameter, as before. Lower <code>C</code> = more regularization.</li><li>Figure 5-9 shows how different <code>gamma</code> and <code>C</code> values affect the decision boundary for the RBF kernel.<figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-9.png alt="Figure 5-9. SVM classifiers using an RBF kernel" width=700></figure></li></ul></li><li><strong>Which kernel to use? (Scorpion icon, page 161):</strong><ul><li><strong>Always try linear kernel first</strong> (<code>LinearSVC</code> is much faster than <code>SVC(kernel="linear")</code>). Especially if training set is large or has many features.</li><li>If training set isn&rsquo;t too large, <strong>try Gaussian RBF kernel next</strong>. It works well in most cases.</li><li>If you have time/compute power, experiment with other kernels (e.g., polynomial, or specialized kernels like string kernels for text) using cross-validation and grid search.</li></ul></li></ul></li></ol><h2 id=page-162-computational-complexity>(Page 162: Computational Complexity)</h2><ul><li><strong><code>LinearSVC</code>:</strong> Based on <code>liblinear</code>. Does <em>not</em> support the kernel trick. Scales almost linearly with number of instances (<code>m</code>) and features (<code>n</code>) – roughly O(m × n). Good for large datasets if a linear model is sufficient.</li><li><strong><code>SVC</code>:</strong> Based on <code>libsvm</code>. <em>Does</em> support the kernel trick.<ul><li>Training time complexity: Between O(m² × n) and O(m³ × n).</li><li>Gets <strong>dreadfully slow</strong> when <code>m</code> (number of instances) is large (e.g., hundreds of thousands).</li><li>Perfect for complex small or medium-sized datasets.</li><li>Scales well with number of features, especially sparse features.</li></ul></li><li>Table 5-1 summarizes this.<figure><img src=/bookshelf/hands-on-ml/ch-5-tbl-5-1.png alt="Table 5-1. Comparison of Scikit-Learn classes for SVM classification" width=700></figure></li></ul><h2 id=page-162-163-svm-regression>(Page 162-163: SVM Regression)</h2><p>SVMs can also do regression!</p><ul><li><strong>The Goal (Reversed Objective):</strong><ul><li>For classification: Fit the widest street <em>between</em> classes, limiting margin violations.</li><li>For SVM Regression (SVR): Fit as many instances as possible <em>on</em> the street, while limiting margin violations (instances <em>off</em> the street).</li></ul></li><li><strong><code>ϵ</code> (epsilon) hyperparameter:</strong> Controls the width of the street. (Figure 5-10, page 163).<figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-10.png alt="Figure 5-10. SVM Regression" width=700></figure><ul><li>Larger <code>ϵ</code>: Wider street. More instances can fit inside the margin without penalty.</li><li>Smaller <code>ϵ</code>: Narrower street.</li></ul></li><li><strong><code>ϵ</code>-insensitive:</strong> Adding more training instances <em>within</em> the margin (inside the street) does not affect the model&rsquo;s predictions.</li><li><strong>Scikit-Learn Classes:</strong><ul><li><strong><code>LinearSVR</code>:</strong> For linear SVM regression.
<code>from sklearn.svm import LinearSVR</code>
<code>svm_reg = LinearSVR(epsilon=1.5)</code> (Data should be scaled and centered first).</li><li><strong><code>SVR</code>:</strong> For nonlinear SVM regression using kernels.
<code>from sklearn.svm import SVR</code>
<code>svm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1)</code>
Figure 5-11 (page 164) shows SVR with a polynomial kernel.<figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-11.png alt="Figure 5-11. SVM Regression using a second-degree polynomial kernel" width=700></figure><ul><li>Large <code>C</code>: Little regularization. Model tries to fit data points closely.</li><li>Small <code>C</code>: More regularization. Smoother fit.</li></ul></li></ul></li><li><code>LinearSVR</code> scales linearly with training set size. <code>SVR</code> gets slow as training set size grows (like <code>SVC</code>).</li><li>SVMs can also be used for outlier detection.</li></ul><h1 id=under-the-hood>Under the hood</h1><p>Let&rsquo;s start with the big picture goals for a <strong>Linear SVM</strong>:</p><p><strong>Goal 1: Separate the Classes.</strong> (Obvious)
We want a line (or plane/hyperplane) that divides the data.</p><p><strong>Goal 2: Make this Separation as &ldquo;Safe&rdquo; or &ldquo;Robust&rdquo; as Possible (Large Margin).</strong>
This is the core SVM idea. We don&rsquo;t just want <em>any</em> separating line; we want the one that stays farthest away from the closest points of both classes. This &ldquo;widest street&rdquo; (the margin) makes the classifier less sensitive to small variations in data and hopefully better at classifying new, unseen points.</p><p><strong>How do we achieve Goal 2 mathematically? (This is where <code>w</code> and <code>b</code> come in)</strong></p><ol><li><p><strong>The Decision Boundary:</strong> Our line is defined by <code>wᵀx + b = 0</code>.</p><ul><li><code>w</code> (weights vector): Determines the <em>orientation</em> or <em>slope</em> of the line.</li><li><code>b</code> (bias): Shifts the line up or down without changing its orientation.</li></ul></li><li><p><strong>The &ldquo;Street&rdquo;:</strong> The edges of our street (the margins) are defined by <code>wᵀx + b = 1</code> and <code>wᵀx + b = -1</code>.</p><ul><li>All positive class points should ideally be on or &ldquo;above&rdquo; <code>wᵀx + b = 1</code>.</li><li>All negative class points should ideally be on or &ldquo;below&rdquo; <code>wᵀx + b = -1</code>.</li></ul></li><li><p><strong>Width of the Street:</strong> It turns out mathematically that the width of this street is <code>2 / ||w||</code> (where <code>||w||</code> is the magnitude or length of the vector <code>w</code>).</p><ul><li><strong>Key Insight:</strong> To make the street (margin) WIDE, we need <code>||w||</code> to be SMALL.</li><li><em>What are we trying to achieve by minimizing <code>||w||</code>?</em> We are trying to maximize the margin.</li><li>For mathematical convenience (easier to take derivatives), instead of minimizing <code>||w||</code>, we minimize <code>(1/2)||w||²</code> (which is <code>(1/2)wᵀw</code>). Minimizing one also minimizes the other.</li></ul></li></ol><p><strong>So, the Training Objective (Hard Margin - Equation 5-3) becomes:</strong></p><ul><li><strong>Objective:</strong> Minimize <code>(1/2)wᵀw</code> (make the margin large)</li><li><strong>Subject to (Constraints):</strong> <code>t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1</code> for all training instances <code>i</code>.<ul><li><code>t⁽ⁱ⁾</code> is +1 for positive class, -1 for negative.</li><li>This constraint says: &ldquo;Every training point must be on the correct side of its respective margin boundary, or right on it.&rdquo;</li></ul></li></ul><p><em>What this constrained optimization is ultimately trying to achieve:</em> Find the line orientation (<code>w</code>) and position (<code>b</code>) that gives the widest possible street while ensuring all training points are correctly classified and stay out of (or on the edge of) the street.</p><p><strong>What if the data isn&rsquo;t perfectly separable (Soft Margin - Equation 5-4)?</strong></p><p>Real data is messy. Outliers exist. We might not be able to find a street where <em>all</em> points are perfectly on the correct side.</p><ul><li><strong>Slack Variables <code>ζ⁽ⁱ⁾</code> (zeta):</strong> We introduce a &ldquo;fudge factor&rdquo; or &ldquo;slack&rdquo; <code>ζ⁽ⁱ⁾ ≥ 0</code> for each point <code>i</code>.<ul><li>If <code>ζ⁽ⁱ⁾ = 0</code>, point <code>i</code> respects the margin.</li><li>If <code>ζ⁽ⁱ⁾ > 0</code>, point <code>i</code> violates the margin (it&rsquo;s inside the street or on the wrong side). The value of <code>ζ⁽ⁱ⁾</code> tells us <em>how much</em> it violates.</li></ul></li><li><strong>New Objective (Soft Margin):</strong><ul><li>Minimize <code>(1/2)wᵀw + C Σᵢ ζ⁽ⁱ⁾</code></li><li>Subject to: <code>t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 - ζ⁽ⁱ⁾</code> and <code>ζ⁽ⁱ⁾ ≥ 0</code>.</li></ul></li><li><em>What this soft margin optimization is ultimately trying to achieve:</em><ol><li>Still try to make <code>(1/2)wᵀw</code> small (maximize margin).</li><li>But <em>also</em> try to make <code>Σᵢ ζ⁽ⁱ⁾</code> small (minimize the total amount of margin violations).</li><li>The hyperparameter <code>C</code> is the <strong>trade-off</strong>.<ul><li>Large <code>C</code>: Penalizes violations heavily. Model will try very hard to get points right, even if it means a narrower margin. (Closer to hard margin).</li><li>Small <code>C</code>: Tolerates more violations in favor of a wider margin. (More regularization).</li></ul></li></ol></li></ul><p>The hard margin and soft margin problems are both convex quadratic optimization
problems with linear constraints. This problem (finding <code>w</code>, <code>b</code>, and <code>ζ</code>s) is a <strong>Quadratic Programming (QP)</strong> problem. We can give it to a standard QP solver.</p><p><strong>Why Introduce the &ldquo;Dual Problem&rdquo; (Equation 5-6)? This is often the confusing part.</strong></p><p>Solving the QP problem directly (called the &ldquo;primal problem&rdquo;) is fine for linear SVMs. But it has limitations:</p><ol><li>It can be slow if the number of features <code>n</code> is very large.</li><li>More importantly, <strong>it doesn&rsquo;t allow for the &ldquo;kernel trick&rdquo;</strong> which is essential for efficient non-linear SVMs.</li></ol><p>The <strong>Dual Problem</strong> is a different but related mathematical formulation of the same optimization task.</p><ul><li>Instead of finding <code>w</code> and <code>b</code> directly, it focuses on finding a set of new variables, <code>αᵢ</code> (alpha-i), one for each training instance.</li><li><strong>Key Property of these <code>αᵢ</code>s:</strong> It turns out that <code>αᵢ</code> will be greater than zero <em>only for the support vectors</em>. For all other data points, <code>αᵢ</code> will be zero!</li><li><em>What solving the dual problem is ultimately trying to achieve:</em> It&rsquo;s finding the importance (<code>αᵢ</code>) of each training instance in defining the optimal margin. Only the support vectors end up having non-zero importance.</li></ul><p><strong>Why is the Dual useful?</strong></p><ol><li><strong>Computational Efficiency in some cases:</strong> If the number of training instances <code>m</code> is smaller than the number of features <code>n</code>, solving the dual can be faster.</li><li><strong>THE KERNEL TRICK (THIS IS THE BIG ONE!):</strong>
Look at the dual objective function (Equation 5-6):
<code>minimize (1/2) Σᵢ Σⱼ αᵢαⱼt⁽ⁱ⁾t⁽ʲ⁾(x⁽ⁱ⁾ᵀx⁽ʲ⁾) - Σᵢ αᵢ</code>
Notice the term <code>(x⁽ⁱ⁾ᵀx⁽ʲ⁾)</code>. This is a <strong>dot product</strong> between pairs of training instances.
The solution <code>w</code> from the dual (Equation 5-7) also involves sums of <code>αᵢt⁽ⁱ⁾x⁽ⁱ⁾</code>.
And crucially, when you make predictions using the dual formulation (Equation 5-11), the decision function becomes:
<code>h(x_new) = Σᵢ αᵢt⁽ⁱ⁾(x⁽ⁱ⁾ᵀx_new) + b</code> (sum over support vectors <code>i</code>)
Again, it only involves <strong>dot products</strong> between the support vectors and the new instance <code>x_new</code>.</li></ol><p><strong>The Kernel Trick - The &ldquo;Aha!&rdquo; Moment for Nonlinear SVMs:</strong></p><p>Now, imagine we want to do nonlinear classification. One way (as discussed for Polynomial Regression) is to map our data <code>x</code> to a much higher-dimensional space using a transformation <code>φ(x)</code>, where the data becomes linearly separable.</p><ul><li><p>If we did this explicitly, we would then have to compute dot products like <code>φ(x⁽ⁱ⁾)ᵀφ(x⁽ʲ⁾)</code> in this very high (maybe even infinite) dimensional space. This would be computationally impossible or extremely inefficient.</p></li><li><p><strong>The Kernel Trick says:</strong> What if there&rsquo;s a special function <code>K(a,b)</code> called a <strong>kernel</strong> that can compute the dot product <code>φ(a)ᵀφ(b)</code> for us, <em>using only the original vectors <code>a</code> and <code>b</code></em>, without us ever having to compute <code>φ(a)</code> or <code>φ(b)</code> explicitly?</p><ul><li>For example, the polynomial kernel <code>K(a,b) = (γaᵀb + r)ᵈ</code> calculates what the dot product would be if you mapped <code>a</code> and <code>b</code> to a d-th degree polynomial feature space.</li><li>The Gaussian RBF kernel <code>K(a,b) = exp(-γ ||a - b||²)</code> implicitly maps to an <em>infinite-dimensional</em> space!</li></ul></li><li><p><em>What the kernel trick is ultimately trying to achieve:</em> It allows us to get the benefit of working in a very high-dimensional feature space (where complex separations might become linear) without the prohibitive computational cost of actually creating and working with those high-dimensional vectors. We just replace all dot products <code>x⁽ⁱ⁾ᵀx⁽ʲ⁾</code> in the dual formulation and in the prediction equation with <code>K(x⁽ⁱ⁾, x⁽ʲ⁾)</code>.</p></li></ul><p><strong>So, the &ldquo;Under the Hood&rdquo; Flow for Kernelized SVMs:</strong></p><ol><li><strong>Start with a nonlinear problem.</strong></li><li><strong>Choose a kernel function</strong> <code>K(a,b)</code> (e.g., Polynomial, RBF). This <em>implicitly</em> defines a transformation <code>φ</code> to a higher-dimensional space where the data is hopefully linearly separable.</li><li><strong>Solve the DUAL optimization problem</strong> (Equation 5-6, but with <code>x⁽ⁱ⁾ᵀx⁽ʲ⁾</code> replaced by <code>K(x⁽ⁱ⁾, x⁽ʲ⁾)</code>). This finds the <code>αᵢ</code> values (which will be non-zero only for support vectors).<ul><li><em>What this is trying to achieve:</em> Find the &ldquo;importance&rdquo; of each training point in defining the optimal <em>linear</em> margin in that <em>implicit high-dimensional space</em>.</li></ul></li><li><strong>Make predictions for a new instance <code>x_new</code></strong> (Equation 5-11, with <code>K</code>):
<code>h(x_new) = Σᵢ αᵢt⁽ⁱ⁾K(x⁽ⁱ⁾, x_new) + b</code><ul><li><em>What this is trying to achieve:</em> Classify the new point based on its kernel &ldquo;similarity&rdquo; (as defined by <code>K</code>) to the support vectors, effectively performing a linear separation in the implicit high-dimensional space.</li></ul></li></ol><p><strong>Think of it like this:</strong></p><ul><li><strong>Primal problem (for linear SVM):</strong> Directly find the best &ldquo;street&rdquo; (<code>w</code>, <code>b</code>) in the original feature space.</li><li><strong>Dual problem (for linear SVM):</strong> Find out which data points (<code>αᵢ > 0</code>) are the crucial &ldquo;support posts&rdquo; for that street. This formulation happens to only use dot products.</li><li><strong>Kernelized SVM (using the dual):</strong> We want a curvy street in our original space. The kernel trick lets us <em>pretend</em> we&rsquo;ve straightened out the data into a super high-dimensional space where a straight street works. We use the dual formulation because it only relies on dot products, and we can replace those dot products with our kernel function <code>K</code>. The kernel function cleverly calculates what those dot products <em>would have been</em> in the high-dimensional space, without us ever going there.</li></ul><p><strong>So, the progression of &ldquo;what are we trying to achieve&rdquo;:</strong></p><ol><li><strong>Linear SVM (Primal):</strong> Maximize margin directly by finding <code>w</code> and <code>b</code>.</li><li><strong>Challenges:</strong> Can be slow for many features, doesn&rsquo;t easily extend to non-linear.</li><li><strong>Linear SVM (Dual):</strong> Reformulate to find <code>α</code>s (support vector indicators). Only uses dot products.</li><li><strong>Kernel Trick Motivation:</strong> We want non-linear separation. Idea: map to a higher space where it&rsquo;s linear. Problem: mapping is expensive.</li><li><strong>Kernelized SVM (Dual + Kernel):</strong> Realize the dual only needs dot products. If we can find a kernel <code>K(a,b)</code> that equals <code>φ(a)ᵀφ(b)</code> without computing <code>φ</code>, we can do complex non-linear classification efficiently by solving the dual problem with <code>K</code> and making predictions with <code>K</code>.</li></ol><p>The &ldquo;headache&rdquo; the book mentions with Equation 5-12 for computing <code>b</code> with kernels is because even <code>b</code> needs to be calculated using these kernelized dot products, as we no longer have an explicit <code>w</code> in the original space.</p><p>It&rsquo;s a chain of clever mathematical steps, each designed to overcome a limitation of the previous one, ultimately allowing us to achieve large margin classification for complex, non-linear data in an efficient way. The dual formulation is the bridge that makes the kernel trick possible.</p><p>Does this step-by-step focus on the &ldquo;why&rdquo; for each part (especially the dual and kernel) help make the overall strategy a bit more intuitive? It&rsquo;s okay if it still feels complex; SVM theory is one of the more mathematically rich areas in classical ML.</p><hr><h1 id=glossary>Glossary</h1><h2 id=page-164-173-under-the-hood---how-svms-work>(Page 164-173: Under the Hood - How SVMs Work)</h2><p>This is where the math gets a bit deeper, but remember our goal: &ldquo;what is it ultimately trying to achieve?&rdquo;
The book changes notation here for SVMs, which is common:</p><ul><li><p>Bias term: <code>b</code> (instead of <code>θ₀</code>)</p></li><li><p>Feature weights vector: <code>w</code> (instead of <code>θ₁</code> to <code>θₙ</code>)</p></li><li><p>No <code>x₀=1</code> bias input feature is added.</p></li><li><p><strong>Decision Function and Predictions (Linear SVM - page 165):</strong></p><ul><li>Decision function: <code>wᵀx + b</code></li><li><strong>Equation 5-2: Linear SVM classifier prediction</strong><figure><img src=/bookshelf/hands-on-ml/ch-5-eq-5-2.png alt="Equation 5-2. Linear SVM classifier prediction" width=700></figure><code>ŷ = 1 if wᵀx + b ≥ 0</code>
<code>ŷ = 0 if wᵀx + b &lt; 0</code> (or class -1 if using -1/1 labels)</li><li>Figure 5-12 shows this in 3D for 2 features: <code>wᵀx + b</code> is a plane. The decision boundary is where <code>wᵀx + b = 0</code> (a line). The &ldquo;street&rdquo; is defined by <code>wᵀx + b = 1</code> and <code>wᵀx + b = -1</code>. These are parallel lines/planes forming the margin.<figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-12.png alt="Figure 5-12. Linear SVM classifier decision boundary" width=700></figure></li></ul></li><li><p><strong>Training Objective (Page 166):</strong></p><ul><li><p><strong>Goal:</strong> Find <code>w</code> and <code>b</code> that make the margin (the &ldquo;street&rdquo;) as wide as possible, while controlling margin violations.</p></li><li><p>The slope of the decision function is <code>||w||</code> (norm of the weight vector).</p></li><li><p><strong>Key insight (Figure 5-13):</strong> The <em>smaller</em> the norm <code>||w||</code>, the <em>larger</em> the margin. (If you divide <code>w</code> and <code>b</code> by 2, the slope <code>||w||/2</code> is halved, effectively doubling the distance to the <code>±1</code> lines, thus doubling the margin width).<figure><img src=/bookshelf/hands-on-ml/ch-5-fig-5-13.png alt="Figure 5-13. A smaller weight vector results in a larger margin" width=700></figure></p></li><li><p>So, we want to <strong>minimize <code>||w||</code></strong> (or equivalently, minimize <code>(1/2)wᵀw</code> which is <code>(1/2)||w||²</code> – easier to differentiate).</p></li><li><p><strong>Hard Margin Objective (Equation 5-3):</strong><figure><img src=/bookshelf/hands-on-ml/ch-5-eq-5-3.png alt="Equation 5-3. Hard margin linear SVM classifier objective" width=700></figure><code>minimize (1/2)wᵀw</code>
<code>subject to t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1</code> for all instances <code>i</code>.</p><ul><li><code>t⁽ⁱ⁾</code> is 1 for positive class, -1 for negative class.</li><li>The constraint <code>t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1</code> means:<ul><li>For positive instances (<code>t⁽ⁱ⁾=1</code>): <code>wᵀx⁽ⁱ⁾ + b ≥ 1</code> (on or outside the positive margin boundary).</li><li>For negative instances (<code>t⁽ⁱ⁾=-1</code>): <code>wᵀx⁽ⁱ⁾ + b ≤ -1</code> (on or outside the negative margin boundary).</li></ul></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> Find the <code>w</code> with the smallest norm (largest margin) such that all points are correctly classified and outside or on the margin boundaries.</li></ul></li><li><p><strong>Soft Margin Objective (Equation 5-4, page 167):</strong><figure><img src=/bookshelf/hands-on-ml/ch-5-eq-5-4.png alt="Equation 5-4. Soft margin linear SVM classifier objective" width=700></figure>To allow for margin violations, introduce <strong>slack variables</strong> <code>ζ⁽ⁱ⁾ ≥ 0</code> (zeta-i) for each instance <code>i</code>.
<code>ζ⁽ⁱ⁾</code> measures how much instance <code>i</code> is allowed to violate the margin.
<code>minimize (1/2)wᵀw + C Σᵢ ζ⁽ⁱ⁾</code>
<code>subject to t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 - ζ⁽ⁱ⁾</code> and <code>ζ⁽ⁱ⁾ ≥ 0</code>.</p><ul><li><em>What it&rsquo;s ultimately trying to achieve:</em><ul><li>Still minimize <code>(1/2)wᵀw</code> (maximize margin).</li><li>But also minimize <code>Σᵢ ζ⁽ⁱ⁾</code> (sum of slack/violations).</li><li><code>C</code> is the hyperparameter that trades off between these two conflicting objectives: large margin vs. few violations.<ul><li>Small <code>C</code>: Margin width is prioritized (more slack allowed).</li><li>Large <code>C</code>: Few violations prioritized (margin might be smaller).</li></ul></li></ul></li></ul></li></ul></li><li><p><strong>Quadratic Programming (QP) (Page 167):</strong></p><ul><li>Both hard and soft margin SVM objectives are <strong>convex quadratic optimization problems with linear constraints</strong>. These are known as QP problems.</li><li>Specialized QP solvers exist to find the optimal <code>w</code> and <code>b</code>. Equation 5-5 gives the general QP formulation. The book explains how to map SVM parameters to this general form.</li></ul></li><li><p><strong>The Dual Problem (Page 168-169):</strong></p><ul><li>For constrained optimization problems (like SVMs), there&rsquo;s often a &ldquo;primal problem&rdquo; and a related &ldquo;dual problem.&rdquo;</li><li>Solving the dual can sometimes be easier or offer advantages. For SVMs, the dual problem:<ul><li>Is faster to solve than the primal when number of training instances (<code>m</code>) &lt; number of features (<code>n</code>).</li><li>Crucially, <strong>makes the kernel trick possible!</strong> (The primal does not).</li></ul></li><li><strong>Equation 5-6: Dual form of the linear SVM objective:</strong><figure><img src=/bookshelf/hands-on-ml/ch-5-eq-5-6.png alt="Equation 5-6. Dual form of the linear SVM objective" width=700></figure>This involves minimizing a function with respect to new variables <code>αᵢ</code> (alpha-i), one for each training instance.
<code>minimize (1/2) Σᵢ Σⱼ αᵢαⱼt⁽ⁱ⁾t⁽ʲ⁾(x⁽ⁱ⁾ᵀx⁽ʲ⁾) - Σᵢ αᵢ</code>
<code>subject to αᵢ ≥ 0</code>.</li><li>Once you find the optimal <code>α̂</code> vector (using a QP solver), you can compute <code>ŵ</code> and <code>b̂</code> for the primal problem using Equation 5-7.</li><li>An important property is that <code>αᵢ</code> will be non-zero <em>only</em> for the support vectors! Most <code>αᵢ</code> will be zero.</li></ul></li><li><p><strong>Kernelized SVMs (The Kernel Trick Explained - Page 169-171):</strong>
This is where the magic happens for nonlinear SVMs.</p><ul><li>Suppose we have a mapping function <code>φ(x)</code> that transforms our input <code>x</code> into a higher-dimensional space where it might become linearly separable (Equation 5-8 for a 2nd-degree polynomial mapping).</li><li>If we apply this transformation <code>φ</code> to all training instances, the dual problem (Equation 5-6) would contain dot products of these transformed vectors: <code>φ(x⁽ⁱ⁾)ᵀφ(x⁽ʲ⁾)</code>.</li><li><strong>The Key Insight (Equation 5-9 for 2nd-degree polynomial):</strong>
It turns out that for some transformations <code>φ</code>, the dot product <code>φ(a)ᵀφ(b)</code> in the high-dimensional space can be computed by a simpler function <code>K(a,b)</code> using only the <em>original</em> vectors <code>a</code> and <code>b</code>.
For the 2nd-degree polynomial mapping, <code>φ(a)ᵀφ(b) = (aᵀb)²</code>.</li><li><strong>The Kernel Function <code>K(a,b)</code>:</strong>
A kernel <code>K(a,b)</code> is a function that computes <code>φ(a)ᵀφ(b)</code> based only on <code>a</code> and <code>b</code>, without needing to know or compute <code>φ</code> itself!<ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> It allows us to operate in a very high (even infinite) dimensional feature space implicitly, without ever actually creating those features. This avoids the computational nightmare of explicit transformation.</li></ul></li><li>So, in the dual problem (Equation 5-6), we just replace <code>x⁽ⁱ⁾ᵀx⁽ʲ⁾</code> with <code>K(x⁽ⁱ⁾, x⁽ʲ⁾)</code>.</li><li><strong>Common Kernels (Equation 5-10, page 171):</strong><figure><img src=/bookshelf/hands-on-ml/ch-5-eq-5-10.png alt="Equation 5-10. Common kernels" width=700></figure><ul><li>Linear: <code>K(a,b) = aᵀb</code> (no transformation)</li><li>Polynomial: <code>K(a,b) = (γaᵀb + r)ᵈ</code></li><li>Gaussian RBF: <code>K(a,b) = exp(-γ ||a - b||²)</code></li><li>Sigmoid: <code>K(a,b) = tanh(γaᵀb + r)</code></li></ul></li><li><strong>Mercer&rsquo;s Theorem (sidebar):</strong> Provides mathematical conditions for a function <code>K</code> to be a valid kernel (i.e., for a corresponding <code>φ</code> to exist). Gaussian RBF kernel actually maps to an infinite-dimensional space! Good thing we don&rsquo;t have to compute <code>φ</code>.<figure><img src=/bookshelf/hands-on-ml/ch-5-mercers-theorm.png alt="Mercer's Theorem" width=700></figure></li></ul></li><li><p><strong>Making Predictions with Kernels (Equation 5-11, page 172):</strong><figure><img src=/bookshelf/hands-on-ml/ch-5-eq-5-11.png alt="Equation 5-11. Making predictions with a kernelized SVM" width=700></figure>We need <code>w</code> to make predictions, but <code>w</code> lives in the (potentially huge) <code>φ</code> space. How do we predict without computing <code>w</code>?
Equation 5-7 for <code>w</code> involves <code>φ(x⁽ⁱ⁾)</code>. If we plug this into the decision function <code>wᵀφ(x⁽ⁿ⁾) + b</code>, we get an equation that only involves dot products of <code>φ</code> terms, which can be replaced by kernels:
<code>h(x⁽ⁿ⁾) = Σᵢ αᵢt⁽ⁱ⁾K(x⁽ⁱ⁾, x⁽ⁿ⁾) + b</code> (sum over support vectors <code>i</code> where <code>αᵢ > 0</code>).</p><ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> Predictions for a new instance <code>x⁽ⁿ⁾</code> are made by computing its kernel similarity to only the <em>support vectors</em>.</li><li>The bias term <code>b</code> can also be computed using kernels (Equation 5-12).<figure><img src=/bookshelf/hands-on-ml/ch-5-eq-5-12.png alt="Equation 5-12. Using the kernel trick to compute the bias term" width=700></figure></li></ul></li><li><p><strong>Online SVMs (Page 172-173):</strong>
For online learning (incremental learning).</p><ul><li>One method for linear SVMs: Use Gradient Descent on a cost function derived from the primal problem (Equation 5-13).</li><li><strong>Equation 5-13: Linear SVM classifier cost function (Hinge Loss)</strong>
<code>J(w,b) = (1/2)wᵀw + C Σᵢ max(0, 1 - t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b))</code><ul><li><code>(1/2)wᵀw</code>: Aims for a large margin (small <code>w</code>).</li><li><code>max(0, 1 - t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b))</code>: This is the <strong>hinge loss</strong>.<ul><li><em>What hinge loss is ultimately trying to achieve:</em> It penalizes instances that violate the margin. If an instance <code>i</code> is correctly classified and outside or on the margin (<code>t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1</code>), then <code>1 - t⁽ⁱ⁾(...) ≤ 0</code>, so <code>max(0, ...) = 0</code> (zero loss for this instance).</li><li>If it violates the margin, the loss is proportional to how far it is from its correct margin boundary.</li></ul></li><li>This cost function is what <code>SGDClassifier(loss="hinge")</code> minimizes.</li></ul></li><li>Online kernelized SVMs are also possible but more complex.</li></ul></li></ul><p>And that&rsquo;s the grand tour of Support Vector Machines! The core idea of <strong>large margin classification</strong> is simple and elegant. The <strong>kernel trick</strong> is the &ldquo;magic&rdquo; that allows SVMs to handle complex nonlinear data efficiently by implicitly operating in very high-dimensional feature spaces.</p><p>The &ldquo;Under the Hood&rdquo; section is definitely more mathematical, but hopefully, by focusing on &ldquo;what is it ultimately trying to achieve&rdquo; for each equation (like minimizing <code>||w||</code> for a large margin, or using kernels to avoid explicit high-dimensional transformations), the core concepts become clearer.</p><p>Any part of that, especially the kernel trick or the dual problem, that still feels a bit fuzzy?</p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script></body></html>