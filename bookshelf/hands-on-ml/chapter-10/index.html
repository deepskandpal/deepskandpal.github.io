<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul></nav></header><main><article class=book-single><h1>Chapter 10: Introduction to Artificial Neural Networks</h1><span class=reading-time><em>16 min read</em></span><div class=book-details><div class=book-content><p><strong>(Page 279: Introduction - Inspiration from Nature)</strong></p><p>The chapter beautifully starts by reminding us how nature has often inspired human inventions: birds inspired planes, burdock plants inspired Velcro. So, it&rsquo;s logical to look at the brain&rsquo;s architecture for inspiration on building intelligent machines. This is the core idea that sparked ANNs.</p><ul><li><p><strong>ANNs vs. Biological Neurons:</strong> While ANNs were inspired by the networks of biological neurons in our brains, they have evolved to be quite different. Just like planes don&rsquo;t flap their wings, ANNs don&rsquo;t need to be biologically perfectly realistic to be effective. The footnote mentions a good philosophy: be open to biological inspiration but don&rsquo;t be afraid to create biologically <em>unrealistic</em> models if they work well. Some researchers even prefer calling the components &ldquo;units&rdquo; rather than &ldquo;neurons&rdquo; to avoid this restrictive analogy.</p></li><li><p><strong>The Power of ANNs:</strong></p><ul><li>They are at the heart of <strong>Deep Learning</strong>.</li><li><strong>Versatile, powerful, and scalable.</strong></li><li>Ideal for large, complex tasks like:<ul><li>Image classification (Google Images)</li><li>Speech recognition (Apple&rsquo;s Siri)</li><li>Recommendation systems (YouTube)</li><li>Game playing (DeepMind&rsquo;s AlphaGo)</li></ul></li></ul></li><li><p><strong>Chapter Structure:</strong></p><ul><li><strong>Part 1:</strong> Introduces ANNs, starting from early architectures and leading up to <strong>Multilayer Perceptrons (MLPs)</strong>, which are heavily used today.</li><li><strong>Part 2:</strong> Focuses on implementing neural networks using the <strong>Keras API</strong>. Keras is described as a &ldquo;beautifully designed and simple high-level API&rdquo; for building, training, evaluating, and running neural networks.</li></ul></li></ul><p><strong>(Page 280: From Biological to Artificial Neurons - A Brief History)</strong></p><ul><li><p><strong>Early Beginnings (1943):</strong> ANNs are surprisingly old! They were first introduced by neurophysiologist Warren McCulloch and mathematician Walter Pitts. Their landmark paper proposed a simplified computational model of how biological neurons might perform complex computations using propositional logic. This was the first ANN architecture.</p></li><li><p><strong>The First &ldquo;AI Winter&rdquo; (1960s-1970s):</strong> Early successes led to widespread belief in imminent truly intelligent machines. When this didn&rsquo;t materialize quickly, funding dried up, and ANNs entered a long &ldquo;winter.&rdquo;</p></li><li><p><strong>Revival (1980s - Connectionism):</strong> New architectures and better training techniques sparked renewed interest. However, progress was slow.</p></li><li><p><strong>The Second &ldquo;AI Winter&rdquo; (1990s):</strong> Other ML techniques like Support Vector Machines (Chapter 5) emerged, seeming to offer better results and stronger theoretical foundations, pushing ANNs to the background again.</p></li><li><p><strong>The Current Wave (Now!):</strong> We&rsquo;re in another, much stronger wave of interest in ANNs. Why is this time different?</p><ol><li><strong>Huge Quantity of Data:</strong> We now have vast amounts of data to train large neural networks (e.g., ImageNet). ANNs often outperform other ML techniques on very large and complex problems.</li><li><strong>Tremendous Increase in Computing Power:</strong><ul><li>Moore&rsquo;s Law (components in circuits doubling roughly every 2 years).</li><li>Powerful <strong>GPUs</strong> (Graphics Processing Units), initially driven by the gaming industry, are exceptionally good at the kind of parallel computations needed for ANNs.</li><li>Cloud platforms make this power accessible to everyone.</li></ul></li><li><strong>Improved Training Algorithms:</strong> While often only slight tweaks from 1990s algorithms, these have had a huge positive impact (e.g., better optimization algorithms, initialization techniques, regularization).</li><li><strong>Theoretical Limitations Turning Benign:</strong> Fears that ANNs would always get stuck in poor local optima have largely proven less of an issue in practice. When they do get stuck, the local optima are often fairly close to the global optimum.</li><li><strong>Virtuous Circle of Funding and Progress:</strong> Amazing products based on ANNs make headlines (AlphaGo, GPT-3/4, etc.), attracting more attention, funding, and talent, leading to further progress.</li></ol></li></ul><p><strong>(Page 281-283: Biological Neurons and a Simple Artificial Neuron Model)</strong></p><ul><li><p><strong>Biological Neurons (Figure 10-1, page 282):</strong></p><ul><li>A quick look at the structure: cell body (soma), dendrites (receive signals), axon (transmits signals), synaptic terminals (connect to other neurons).</li><li>Neurons produce electrical impulses (action potentials). When a neuron receives enough neurotransmitter signals at its synapses within a short period, it &ldquo;fires&rdquo; its own impulse. Some neurotransmitters are excitatory (encourage firing), some are inhibitory.</li><li>Individual neurons are relatively simple, but billions of them, each connected to thousands of others, form a vast network capable of highly complex computations. The brain&rsquo;s architecture, especially the layered structure of the cerebral cortex (Figure 10-2), provides inspiration.</li></ul></li><li><p><strong>Logical Computations with Artificial Neurons (McCulloch & Pitts Model - Page 283):</strong></p><ul><li>Their early model was very simple:<ul><li>Binary (on/off) inputs.</li><li>One binary (on/off) output.</li><li>The neuron activates its output if a certain <em>number</em> of its inputs are active.</li></ul></li><li><strong>Figure 10-3</strong> shows how such simple neurons can perform basic logical computations (assuming activation if at least two inputs are active):<ul><li><strong>Identity (C=A):</strong> Neuron A sends two signals to C. If A is on, C gets two active inputs and turns on.</li><li><strong>Logical AND (C = A ∧ B):</strong> C activates only if both A and B are active (one active input isn&rsquo;t enough).</li><li><strong>Logical OR (C = A ∨ B):</strong> C activates if A is active, or B is active, or both (any one provides two inputs to a common intermediate neuron, which then activates C, or if A and B both directly input to C and one active input is enough, though the diagram is a bit more complex). The diagram shows intermediate neurons. The idea is that if A is active, it can trigger enough input for C to fire, same for B.</li><li><strong>Complex Logic (e.g., A AND NOT B):</strong> If we assume an input can <em>inhibit</em> activity, this is also possible. If A is active and B is off, C activates. If B is on, it inhibits C.</li></ul></li><li><em>What this was ultimately trying to achieve:</em> To show that even a very simplified model of a neuron, when networked, could perform fundamental logical computations, suggesting a path towards building computational intelligence.</li></ul></li></ul><p><strong>(Page 284-288: The Perceptron)</strong></p><p>Invented by Frank Rosenblatt in 1957. One of the simplest ANN architectures.</p><ul><li><p><strong>Based on a Threshold Logic Unit (TLU) or Linear Threshold Unit (LTU) (Figure 10-4, page 284):</strong></p><ul><li><strong>Inputs & Output:</strong> Numbers (not just binary on/off).</li><li><strong>Weights:</strong> Each input connection <code>i</code> has an associated weight <code>wᵢ</code>.</li><li><strong>Weighted Sum:</strong> The TLU computes a weighted sum of its inputs: <code>z = w₁x₁ + w₂x₂ + ... + wₙxₙ = wᵀx</code>.</li><li><strong>Step Function:</strong> It then applies a <strong>step function</strong> to this sum <code>z</code> to produce the output: <code>h_w(x) = step(z)</code>.</li><li><em>What the TLU is ultimately trying to achieve:</em> It makes a decision based on whether a weighted combination of evidence (<code>z</code>) exceeds some threshold.</li></ul></li><li><p><strong>Common Step Functions (Equation 10-1, page 285):</strong></p><ul><li><strong>Heaviside step function:</strong> Outputs 0 if <code>z &lt; 0</code>, outputs 1 if <code>z ≥ 0</code> (assuming threshold is 0).</li><li><strong>Sign function:</strong> Outputs -1 if <code>z &lt; 0</code>, 0 if <code>z = 0</code>, +1 if <code>z > 0</code>.</li></ul></li><li><p><strong>Single TLU for Classification:</strong></p><ul><li>A single TLU can perform simple linear binary classification. It&rsquo;s very similar to a Logistic Regression or linear SVM classifier, but with a hard threshold output instead of a probability or a margin.</li><li>Example: Classify Iris flowers based on petal length and width. You&rsquo;d add a bias feature <code>x₀=1</code>. Training means finding weights <code>w₀, w₁, w₂</code>.</li></ul></li><li><p><strong>Perceptron Architecture (Figure 10-5, page 286):</strong></p><ul><li>A Perceptron is typically a single layer of TLUs.</li><li>Each TLU in this layer is connected to all inputs. This is a <strong>fully connected layer</strong> (or <strong>dense layer</strong>).</li><li>Inputs are fed through special &ldquo;passthrough&rdquo; <strong>input neurons</strong>.</li><li>A <strong>bias neuron</strong> (always outputting 1) is usually added and connected to each TLU, providing the bias term <code>w₀</code> (or <code>b</code> in the new notation used later).</li><li>The Perceptron in Figure 10-5 has 2 inputs, 1 bias neuron, and 3 output TLUs. This can classify instances into three <em>different binary classes simultaneously</em> (making it a multioutput classifier). For example, output 1 could be &ldquo;is it a cat?&rdquo;, output 2 &ldquo;is it a dog?&rdquo;, output 3 &ldquo;is it a bird?&rdquo;. An input could be classified as a cat AND a bird if both TLUs fire (though that specific example isn&rsquo;t ideal for mutually exclusive animal classes).</li></ul></li><li><p><strong>Computing Outputs for a Layer (Equation 10-2, page 286):</strong>
For a whole layer of artificial neurons, for several instances at once:
<code>h_W,b(X) = φ(XW + b)</code></p><ul><li><code>X</code>: Matrix of input features (instances x features).</li><li><code>W</code>: Weight matrix (input neurons x artificial neurons in the layer). Contains connection weights <em>excluding</em> bias.</li><li><code>b</code>: Bias vector (one bias term per artificial neuron in the layer).</li><li><code>XW + b</code>: Computes the weighted sum <code>z</code> for every neuron and every instance.</li><li><code>φ</code> (phi): The <strong>activation function</strong>. For TLUs, this is a step function.</li><li><em>What this equation is ultimately trying to achieve:</em> Efficiently calculate the output of every neuron in a layer for every instance in a batch of data, using matrix multiplication.</li></ul></li><li><p><strong>Perceptron Training (Hebbian Learning & Perceptron Learning Rule - Page 286):</strong></p><ul><li>Inspired by <strong>Hebb&rsquo;s Rule</strong> (&ldquo;Cells that fire together, wire together&rdquo;): When neuron A often triggers neuron B, the connection between them strengthens.</li><li>Perceptrons use a variant: The <strong>Perceptron learning rule</strong> reinforces connections that help <em>reduce the error</em>.</li><li><strong>Process:</strong><ol><li>Feed one training instance at a time.</li><li>For each instance, make predictions.</li><li>For <em>every output neuron that produced a wrong prediction</em>, reinforce the connection weights from the inputs that <em>would have contributed to the correct prediction</em>.</li></ol></li><li><strong>Equation 10-3 (Weight Update Rule):</strong>
<code>wᵢⱼ⁽ⁿᵉˣᵗ ˢᵗᵉᵖ⁾ = wᵢⱼ + η(yⱼ - ŷⱼ)xᵢ</code><ul><li><code>wᵢⱼ</code>: Weight between i-th input and j-th output neuron.</li><li><code>η</code> (eta): Learning rate.</li><li><code>yⱼ</code>: Target output for j-th neuron.</li><li><code>ŷⱼ</code>: Predicted output for j-th neuron.</li><li><code>xᵢ</code>: Value of i-th input for the current instance.</li><li><em>What this rule is ultimately trying to achieve:</em><ul><li>If <code>ŷⱼ</code> is correct (<code>yⱼ - ŷⱼ = 0</code>), weights don&rsquo;t change.</li><li>If <code>ŷⱼ</code> is wrong:<ul><li>If <code>yⱼ=1</code> and <code>ŷⱼ=0</code> (neuron should have fired but didn&rsquo;t): <code>yⱼ - ŷⱼ = 1</code>. Weights <code>wᵢⱼ</code> are increased if <code>xᵢ</code> was positive (strengthening connections that should have contributed to firing).</li><li>If <code>yⱼ=0</code> and <code>ŷⱼ=1</code> (neuron fired but shouldn&rsquo;t have): <code>yⱼ - ŷⱼ = -1</code>. Weights <code>wᵢⱼ</code> are decreased if <code>xᵢ</code> was positive (weakening connections that wrongly contributed to firing).</li></ul></li></ul></li></ul></li></ul></li><li><p><strong>Perceptron Convergence Theorem (Page 287):</strong>
If training instances are linearly separable, Rosenblatt showed this algorithm <em>would converge</em> to a solution (a set of weights that separates the classes).</p></li><li><p><strong>Scikit-Learn <code>Perceptron</code> class:</strong> Implements a single-TLU network.
<code>from sklearn.linear_model import Perceptron</code>
<code>per_clf = Perceptron()</code>
<code>per_clf.fit(X, y)</code></p><ul><li>The book notes this is equivalent to <code>SGDClassifier(loss="perceptron", learning_rate="constant", eta0=1, penalty=None)</code>.</li><li>Unlike Logistic Regression, Perceptrons output hard predictions (0 or 1), not probabilities. This is one reason to prefer Logistic Regression.</li></ul></li><li><p><strong>Limitations of Perceptrons (Minsky & Papert, 1969 - Page 288):</strong></p><ul><li>Highlighted serious weaknesses, famously that Perceptrons (being linear classifiers) cannot solve some trivial problems like the <strong>Exclusive OR (XOR)</strong> problem (Figure 10-6, left). XOR is not linearly separable.</li><li>This disappointment led to another decline in ANN research (part of the first AI winter).</li></ul></li></ul><p><strong>(Page 288-293: The Multilayer Perceptron (MLP) and Backpropagation)</strong></p><ul><li><p><strong>Overcoming Perceptron Limitations: Stacking Perceptrons (Page 288):</strong></p><ul><li>Limitations can be overcome by stacking multiple layers of Perceptrons. The resulting ANN is a <strong>Multilayer Perceptron (MLP)</strong>.</li><li><strong>Figure 10-6 (right)</strong> shows an MLP that <em>can</em> solve the XOR problem. It uses an intermediate &ldquo;hidden&rdquo; layer of neurons.</li><li><em>What the MLP is ultimately trying to achieve:</em> By having hidden layers, MLPs can learn more complex, non-linear decision boundaries. The hidden layers can transform the input features into a new representation where the problem becomes linearly separable for the output layer.</li></ul></li><li><p><strong>MLP Architecture (Figure 10-7, page 289):</strong></p><ul><li>One (passthrough) <strong>input layer</strong>.</li><li>One or more layers of TLUs, called <strong>hidden layers</strong>.</li><li>One final layer of TLUs called the <strong>output layer</strong>.</li><li>Layers near input are &ldquo;lower layers&rdquo;; layers near output are &ldquo;upper layers.&rdquo;</li><li>Every layer (except output) usually includes a bias neuron and is fully connected to the next layer.</li><li>Signal flows one way (input -> output): <strong>Feedforward Neural Network (FNN)</strong>.</li><li><strong>Deep Neural Network (DNN):</strong> An ANN with a &ldquo;deep&rdquo; stack of hidden layers (definition of &ldquo;deep&rdquo; is fuzzy but generally means more than one or two these days). Deep Learning studies DNNs.</li></ul></li><li><p><strong>Training MLPs: The Backpropagation Algorithm (Page 289-290):</strong>
For many years, training MLPs was a major challenge. In 1986, Rumelhart, Hinton, and Williams published the <strong>backpropagation</strong> training algorithm, still fundamental today.</p><ul><li><strong>What it is:</strong> Essentially, it&rsquo;s <strong>Gradient Descent</strong> (Chapter 4) applied to an MLP, using an efficient technique to compute all the necessary gradients.</li><li><strong>How it computes gradients (Autodiff - sidebar, page 290):</strong><ul><li>It uses <strong>reverse-mode automatic differentiation (autodiff)</strong>.</li><li>In just <em>two passes</em> through the network (one forward, one backward), it can compute the gradient of the network&rsquo;s error with respect to <em>every single model parameter</em> (all weights and biases in all layers).</li><li>This tells us how each weight/bias should be tweaked to reduce the error.</li></ul></li><li>Once gradients are computed, it performs a regular Gradient Descent step. Repeat until convergence.</li></ul></li><li><p><strong>Backpropagation Algorithm in More Detail (Page 290):</strong></p><ol><li><strong>Mini-batch Processing:</strong> Handles one mini-batch of training instances at a time. Goes through the full training set multiple times; each full pass is an <strong>epoch</strong>.</li><li><strong>Forward Pass:</strong><ul><li>Pass the mini-batch to the input layer.</li><li>Compute outputs of neurons in the first hidden layer.</li><li>Pass these outputs to the next layer, compute its outputs, and so on, until the output layer.</li><li>This is like making predictions, but all intermediate results (activations of all neurons) are <em>preserved</em> because they are needed for the backward pass.</li></ul></li><li><strong>Measure Error:</strong> Use a loss function (e.g., MSE for regression, Cross-Entropy for classification) to compare the network&rsquo;s output with the desired output (true labels).</li><li><strong>Backward Pass (Propagating Error Gradients):</strong><ul><li>Compute how much each output connection contributed to the error (using the chain rule of calculus).</li><li>Propagate these error contributions backward:<ul><li>Measure how much connections in the layer below contributed to the output layer&rsquo;s error contributions (again, using chain rule).</li><li>Continue backward until the input layer is reached.</li></ul></li><li>This reverse pass efficiently measures the error gradient across all connection weights.</li></ul></li><li><strong>Gradient Descent Step:</strong> Tweak all connection weights in the network using the computed error gradients to reduce the overall error.</li></ol></li><li><p><strong>Key Change for Backpropagation: Activation Functions (Page 291):</strong></p><ul><li>The original Perceptron used a step function. Step functions have flat segments (zero gradient), so Gradient Descent gets stuck.</li><li><strong>Crucial innovation for MLPs:</strong> Replace the step function with a differentiable activation function, like the <strong>logistic (sigmoid) function</strong> <code>σ(z) = 1 / (1 + exp(-z))</code>.<ul><li>It has a well-defined, non-zero derivative everywhere, allowing GD to make progress.</li></ul></li><li><strong>Other Popular Activation Functions (Figure 10-8, page 292):</strong><ul><li><strong>Hyperbolic Tangent (tanh):</strong> <code>tanh(z) = 2σ(2z) - 1</code>.<ul><li>S-shaped, continuous, differentiable.</li><li>Output range: -1 to 1 (vs. 0 to 1 for sigmoid).</li><li>Centering output around 0 often helps speed up convergence at start of training.</li></ul></li><li><strong>Rectified Linear Unit (ReLU):</strong> <code>ReLU(z) = max(0, z)</code>.<ul><li>Outputs 0 if <code>z &lt; 0</code>, outputs <code>z</code> if <code>z ≥ 0</code>.</li><li>Continuous, but not differentiable at <code>z=0</code> (slope changes abruptly). In practice, GD still works (can use a subgradient or just assume gradient is 0 or 1 at <code>z=0</code>).</li><li>Derivative is 0 for <code>z &lt; 0</code>.</li><li><strong>Advantages:</strong> Fast to compute. Has become the default in many cases. No maximum output value (helps with some GD issues like vanishing gradients, Ch 11).</li><li>The footnote on page 292 notes that ReLU, despite being less biologically plausible than sigmoids, often works better in ANNs – a case where the biological analogy can be misleading.</li></ul></li></ul></li></ul></li><li><p><strong>Why Activation Functions? (Page 292):</strong></p><ul><li>If you chain several <em>linear</em> transformations, all you get is another <em>linear</em> transformation.
<code>f(x) = 2x + 3</code>, <code>g(x) = 5x - 1</code> => <code>f(g(x)) = 2(5x - 1) + 3 = 10x + 1</code> (still linear).</li><li>If you have no <em>nonlinearity</em> between layers, even a deep stack of layers is equivalent to a single linear layer. It can&rsquo;t solve complex non-linear problems.</li><li><strong>Nonlinear activation functions are essential for giving ANNs the power to approximate complex, non-linear functions.</strong> A large enough DNN with nonlinear activations can theoretically approximate any continuous function.</li></ul></li></ul><p><strong>(Page 292-294: Regression and Classification MLPs)</strong></p><p>Now that we know the architecture and training algorithm (backpropagation), what can we do?</p><ul><li><p><strong>Regression MLPs (Page 292-293):</strong></p><ul><li><strong>Single value prediction (e.g., house price):</strong> Need a single output neuron.</li><li><strong>Multivariate regression (e.g., 2D coordinates for object center):</strong> One output neuron per output dimension. (4 output neurons if predicting bounding box: x, y, width, height).</li><li><strong>Output Layer Activation:</strong><ul><li>Usually <strong>no activation function</strong> for output neurons in regression (so they can output any range of values).</li><li>If output must be positive: Use ReLU or <strong>softplus</strong> (<code>log(1 + exp(z))</code>, a smooth ReLU variant).</li><li>If output must be in a specific range: Use logistic (for 0-1) or tanh (for -1 to 1) and scale labels accordingly.</li></ul></li><li><strong>Loss Function:</strong><ul><li>Typically <strong>MSE</strong>.</li><li>If many outliers: Prefer <strong>MAE</strong> or <strong>Huber loss</strong> (quadratic for small errors, linear for large errors – less sensitive to outliers than MSE but converges faster than MAE).</li></ul></li><li><strong>Table 10-1 (Typical Regression MLP Architecture):</strong> Summarizes typical choices for number of neurons, layers, activations, and loss.</li></ul></li><li><p><strong>Classification MLPs (Page 294):</strong></p><ul><li><strong>Binary classification:</strong> Single output neuron, <strong>logistic (sigmoid) activation function</strong>. Output is probability of positive class.</li><li><strong>Multilabel binary classification (e.g., email is spam/ham AND urgent/non-urgent):</strong><ul><li>One output neuron <em>per positive class label</em> (e.g., one for &ldquo;is spam,&rdquo; one for &ldquo;is urgent&rdquo;).</li><li>Each uses <strong>logistic activation</strong>.</li><li>Output probabilities don&rsquo;t necessarily sum to 1 (an email can be &ldquo;not spam&rdquo; and &ldquo;urgent&rdquo;).</li></ul></li><li><strong>Multiclass classification (mutually exclusive classes, e.g., digits 0-9):</strong><ul><li>One output neuron <em>per class</em>.</li><li>Use <strong>softmax activation function</strong> for the <em>whole output layer</em> (as in Chapter 4). This ensures probabilities are between 0-1 and sum to 1.</li><li><strong>Figure 10-9</strong> shows a modern MLP for classification (ReLU in hidden layers, softmax in output).</li></ul></li><li><strong>Loss Function:</strong><ul><li><strong>Cross-entropy loss</strong> (log loss, as in Chapter 4) is generally a good choice when predicting probability distributions.</li></ul></li><li><strong>Table 10-2 (Typical Classification MLP Architecture):</strong> Summarizes typical choices.</li></ul></li></ul><p>Phew! That&rsquo;s a dense introduction to the historical context, the biological inspiration (and divergence from it), the basic Perceptron, the jump to Multilayer Perceptrons, the crucial backpropagation algorithm, and how MLPs are structured for regression and classification.</p><p>The key takeaway is that MLPs are layered networks of simple processing units (neurons), where hidden layers learn increasingly complex representations of the input, enabled by non-linear activation functions and trained by backpropagation (Gradient Descent with efficient gradient calculation).</p><p>This sets us up perfectly for the second part of the chapter: actually implementing these with Keras!</p><p>Any questions about this foundational theory before we see how easy Keras makes it to build these? The distinction between a single Perceptron and an MLP, and the role of backpropagation and activation functions, are really central.</p></div></div></article></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer></body></html>