<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul></nav></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents></nav></nav></aside><article class=book-single><h1>Chapter 11: Training Deep Neural Networks</h1><span class=reading-time><em>7 min read</em></span><div class=book-details><div class=book-content><p>Okay, great! We&rsquo;ve built a solid conceptual understanding of Artificial Neural Networks and how to approach their design and hyperparameter tuning, without getting bogged down in the specific Keras/TensorFlow code from the later parts of Chapter 10.</p><p>Now, we&rsquo;re ready to tackle <strong>Chapter 11: Training Deep Neural Networks</strong>. This chapter addresses the practical challenges and advanced techniques that arise when you start working with <em>deep</em> networks – those with many layers.</p><p><strong>(Page 331: Introduction - Challenges of Deep DNNs)</strong></p><p>Chapter 10 introduced ANNs, and we even notionally &ldquo;trained&rdquo; some (though they were relatively shallow, with just a few hidden layers). But what happens when you need to tackle really complex problems, like detecting hundreds of object types in high-resolution images?</p><ul><li>You might need a much <strong>deeper DNN</strong> (10+ layers, hundreds of neurons per layer, hundreds of thousands or millions of connections).</li><li>Training such a deep DNN is <strong>not a walk in the park</strong>.</li></ul><p>The chapter immediately highlights several common problems you could run into:</p><ol><li><p><strong>Vanishing or Exploding Gradients:</strong></p><ul><li>This is a critical issue. As gradients flow backward through the network during backpropagation (from the output layer to the input layer), they can either:<ul><li><strong>Vanish:</strong> Get smaller and smaller, until the gradients for the lower layers (those closer to the input) are tiny. This means the weights of these lower layers barely get updated, and the network effectively stops learning in those early layers.</li><li><strong>Explode:</strong> Get larger and larger, until the weight updates for lower layers become insanely huge, causing the training algorithm to diverge.</li></ul></li><li>Both make lower layers very hard to train effectively.</li></ul></li><li><p><strong>Not Enough Training Data:</strong></p><ul><li>Large, deep networks have many parameters (weights and biases).</li><li>They often require vast amounts of training data to learn these parameters well and generalize without overfitting.</li><li>Getting enough <em>labeled</em> data can be very costly and time-consuming.</li></ul></li><li><p><strong>Extremely Slow Training:</strong></p><ul><li>More layers, more neurons, more connections, more data = much more computation per epoch. Training can take days, weeks, or even longer.</li></ul></li><li><p><strong>Risk of Overfitting:</strong></p><ul><li>A model with millions of parameters can easily memorize the training set (especially if the dataset isn&rsquo;t proportionally huge or is noisy), leading to poor performance on new, unseen data.</li></ul></li></ol><p><strong>Chapter Goals:</strong>
This chapter will go through each of these problems and present techniques to solve them:</p><ul><li><strong>Vanishing/Exploding Gradients:</strong> We&rsquo;ll look at their causes and popular solutions (like better weight initialization, non-saturating activation functions, Batch Normalization, Gradient Clipping).</li><li><strong>Lack of Labeled Data / Complex Tasks:</strong> We&rsquo;ll explore <strong>transfer learning</strong> and <strong>unsupervised pretraining</strong>.</li><li><strong>Slow Training:</strong> We&rsquo;ll discuss various <strong>faster optimizers</strong>.</li><li><strong>Overfitting:</strong> We&rsquo;ll cover popular <strong>regularization techniques</strong> for large neural networks.</li></ul><p>The ultimate aim: &ldquo;With these tools, you will be able to train very deep nets. Welcome to Deep Learning!&rdquo;</p><p><strong>(Page 332-338: The Vanishing/Exploding Gradients Problems)</strong></p><p>This is a fundamental challenge in training deep networks.</p><ul><li><p><strong>The Core Issue:</strong></p><ul><li>Backpropagation involves propagating the error gradient backward from the output layer to the input layer.</li><li>As these gradients are passed from layer to layer, they are multiplied by the weights of those layers (and the derivatives of activation functions).</li><li>If weights (or activation function derivatives) are consistently small, the gradients shrink exponentially as they go backward -> <strong>Vanishing Gradients</strong>. The lower layers learn very slowly or not at all.</li><li>If weights (or derivatives) are consistently large, the gradients grow exponentially -> <strong>Exploding Gradients</strong>. Training becomes unstable and diverges.</li><li>More generally, deep networks suffer from <strong>unstable gradients</strong>, where different layers may learn at vastly different speeds.</li></ul></li><li><p><strong>Historical Context:</strong></p><ul><li>This unstable gradient behavior was observed long ago and was a key reason deep NNs were largely abandoned in the early 2000s.</li><li>The cause wasn&rsquo;t entirely clear until a landmark 2010 paper by <strong>Xavier Glorot and Yoshua Bengio</strong>.</li></ul></li><li><p><strong>Glorot and Bengio&rsquo;s Findings (2010 Paper - Footnote 1):</strong>
They identified key culprits:</p><ol><li><strong>Logistic Sigmoid Activation Function:</strong><ul><li><strong>Figure 11-1 (page 333)</strong> shows the logistic sigmoid. When its inputs become large (positive or negative), the function <strong>saturates</strong> at 0 or 1.</li><li>In these saturated regions, the <strong>derivative of the sigmoid function is extremely close to 0.</strong></li><li>During backpropagation, when gradients are passed through a saturated sigmoid neuron, they get multiplied by this near-zero derivative, effectively getting squashed.</li><li>If many layers have saturated sigmoids, the gradient can get diluted to almost nothing by the time it reaches the lower layers.</li></ul></li><li><strong>Traditional Weight Initialization:</strong> At the time, weights were often initialized from a normal distribution with mean 0 and standard deviation 1.</li></ol><ul><li><strong>Variance Imbalance:</strong> Glorot and Bengio showed that with this combination (sigmoid activation + standard normal initialization), the <strong>variance of the outputs of each layer is much greater than the variance of its inputs.</strong></li><li>As the signal flows forward, the variance keeps increasing layer by layer. This pushes the inputs to the activation functions of the upper layers into their saturated regions (where derivatives are ~0).</li><li>The problem is worsened because the logistic function has a mean of 0.5, not 0 (tanh, with mean 0, behaves slightly better).</li></ul></li><li><p><strong>The Goal for Proper Signal Flow (Glorot & Bengio&rsquo;s Insight - Page 333):</strong>
For a signal (activations forward, gradients backward) to flow properly without dying out or exploding:</p><ol><li>The <strong>variance of the outputs</strong> of each layer should be (roughly) equal to the <strong>variance of its inputs</strong>.</li><li>The <strong>gradients</strong> should have (roughly) equal variance <em>before</em> and <em>after</em> flowing through a layer in the reverse direction.</li></ol><ul><li>The analogy (footnote 2) is excellent: a chain of microphone amplifiers. Each needs to be set correctly so your voice comes out clearly at the end, with consistent amplitude through the chain.</li></ul><p>It&rsquo;s hard to guarantee both conditions 1 & 2 simultaneously unless a layer has an equal number of inputs (<strong>fan-in</strong>) and outputs/neurons (<strong>fan-out</strong>).</p></li><li><p><strong>Xavier/Glorot Initialization (Equation 11-1, Page 334):</strong>
Glorot and Bengio proposed a practical compromise for weight initialization that works well:</p><ul><li>Initialize connection weights randomly.</li><li>The distribution should have a mean of 0.</li><li>The variance <code>σ²</code> should depend on <code>fan_in</code> and <code>fan_out</code>:
<code>σ² = 1 / fan_avg</code> where <code>fan_avg = (fan_in + fan_out) / 2</code>.
(For a uniform distribution between <code>-r</code> and <code>r</code>, <code>r = sqrt(3 / fan_avg)</code>).</li><li><em>What this initialization is ultimately trying to achieve:</em> It aims to keep the variance of activations and backpropagated gradients roughly constant across layers, preventing them from vanishing or exploding. It helps the signal propagate properly.</li><li>This significantly speeds up training and was a key factor in the success of Deep Learning.</li></ul></li><li><p><strong>LeCun Initialization (Page 334):</strong></p><ul><li>An earlier strategy by Yann LeCun (1990s). If you replace <code>fan_avg</code> with just <code>fan_in</code> in Equation 11-1, you get LeCun initialization.</li><li>Equivalent to Glorot when <code>fan_in = fan_out</code>.</li></ul></li><li><p><strong>He Initialization (Kaiming He et al., 2015 - Footnote 3, Page 334):</strong></p><ul><li>Glorot initialization works well for sigmoid, tanh, and softmax.</li><li>For <strong>ReLU</strong> and its variants (which became very popular), a different initialization is needed because ReLU behaves differently (it kills half the activations, which changes variance).</li><li><strong>He Initialization</strong> uses:
<code>σ² = 2 / fan_in</code>
(For a uniform distribution, <code>r = sqrt(6 / fan_in)</code>).</li></ul></li><li><p><strong>Table 11-1 (Page 334):</strong> Summarizes initialization strategies for different activation functions:</p><ul><li><strong>Glorot:</strong> For None (linear output), tanh, logistic, softmax. Variance <code>1 / fan_avg</code>.</li><li><strong>He:</strong> For ReLU and variants. Variance <code>2 / fan_in</code>.</li><li><strong>LeCun:</strong> For SELU (we&rsquo;ll see this soon). Variance <code>1 / fan_in</code>.</li></ul></li><li><p><strong>Keras Implementation (Page 334):</strong></p><ul><li>Keras uses Glorot initialization with a uniform distribution by default for its <code>Dense</code> layers.</li><li>You can change it using <code>kernel_initializer</code>:
<code>keras.layers.Dense(10, activation="relu", kernel_initializer="he_normal")</code>
<code>keras.layers.Dense(10, activation="relu", kernel_initializer="he_uniform")</code></li><li>For He initialization with uniform distribution based on <code>fan_avg</code> (instead of <code>fan_in</code>):
<code>he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')</code>
<code>keras.layers.Dense(..., kernel_initializer=he_avg_init)</code></li></ul></li></ul><p>These initialization strategies are crucial first steps to combat unstable gradients. <strong>They are all trying to set the initial weights to a &ldquo;sensible&rdquo; scale so that the signal (activations and gradients) can propagate through many layers without becoming too small or too large too quickly.</strong></p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer></body></html>