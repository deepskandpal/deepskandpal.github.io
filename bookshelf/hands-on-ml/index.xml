<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, 2nd Edition on 404EngineerNotFound</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/</link><description>Recent content in Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, 2nd Edition on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 22 Feb 2024 10:00:00 +0000</lastBuildDate><atom:link href="https://deepskandpal.github.io/bookshelf/hands-on-ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 1: The Machine Learning Landscape</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-1/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-1/</guid><description>&lt;h1 id="notes-for-chapter-1">Notes for Chapter 1&lt;/h1>
&lt;p>When you hear &amp;ldquo;Machine Learning,&amp;rdquo; what pops into your head? Robots? Terminators? Maybe a friendly butler? The book nails it – it&amp;rsquo;s not just sci-fi; it&amp;rsquo;s already here. Think about the &lt;strong>spam filter&lt;/strong>. That was one of the first really big ML applications that touched millions. It learned, from examples of spam and non-spam (or &amp;ldquo;ham,&amp;rdquo; as we call it), to tell the difference. And it got so good that we barely notice it anymore. That&amp;rsquo;s the hallmark of good ML – it just works.&lt;/p></description></item><item><title>Chapter 2: End-to-End Machine Learning Project</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-2/</link><pubDate>Thu, 22 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-2/</guid><description>&lt;h1 id="notes-for-chapter-2">Notes for Chapter 2&lt;/h1>
&lt;p>Alright class, buckle up! Last time, we got a bird&amp;rsquo;s-eye view of the Machine Learning landscape – the different continents, climates, and major landmarks. Today, with Chapter 2, &amp;ldquo;End-to-End Machine Learning Project,&amp;rdquo; we&amp;rsquo;re grabbing our hiking boots and actually trekking through one of these landscapes. This is where the rubber meets the road!&lt;/p>
&lt;p>The book says we&amp;rsquo;re pretending to be a recently hired data scientist at a real estate company. Our mission? To predict median housing prices in California. This chapter is &lt;em>fantastic&lt;/em> because it walks you through the practical steps. It’s less about the theory of one specific algorithm and more about the &lt;em>process&lt;/em> of doing ML in the real world.&lt;/p></description></item><item><title>Chapter 3: Classification</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-3/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-3/</guid><description>&lt;p>This chapter is all about systems that predict a &lt;em>category&lt;/em> or a &lt;em>class&lt;/em> – Is this email spam or not? Is this image a cat or a dog? Is this handwritten digit a &amp;lsquo;5&amp;rsquo; or a &amp;lsquo;3&amp;rsquo;?&lt;/p>
&lt;p>And speaking of handwritten digits, we&amp;rsquo;re going to be working with a very famous dataset: &lt;strong>MNIST&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>It&amp;rsquo;s a set of 70,000 small images of digits (0-9) handwritten by high school students and US Census Bureau employees.&lt;/li>
&lt;li>Each image is labeled with the digit it represents.&lt;/li>
&lt;li>The book calls it the &lt;strong>&amp;ldquo;hello world&amp;rdquo; of Machine Learning&lt;/strong> because it&amp;rsquo;s a go-to dataset for testing new classification algorithms. Everyone who learns ML eventually plays with MNIST. It’s like a rite of passage!&lt;/li>
&lt;/ul>
&lt;p>Scikit-Learn makes it super easy to fetch popular datasets like MNIST. The code shows:
&lt;code>from sklearn.datasets import fetch_openml&lt;/code>
&lt;code>mnist = fetch_openml('mnist_784', version=1)&lt;/code>&lt;/p></description></item><item><title>Chapter 4: Training Models</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-4/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-4/</guid><description>&lt;p>Chapter 4: Training Models.&lt;/p>
&lt;h2 id="introduction---beyond-black-boxes">Introduction - Beyond Black Boxes&lt;/h2>
&lt;p>Up until now, as the book says, we&amp;rsquo;ve treated ML models and their training algorithms mostly like black boxes. We fed them data, they gave us results, and we learned to evaluate those results. You&amp;rsquo;ve optimized regression, improved classifiers, even built a spam filter, often without peeking under the hood. And that&amp;rsquo;s okay for many practical purposes!&lt;/p>
&lt;p>However, understanding &lt;em>how&lt;/em> these things work internally is incredibly powerful. It helps you:&lt;/p></description></item><item><title>Chapter 5: Support Vector Machines</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-5/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-5/</guid><description>&lt;p>Alright class, let&amp;rsquo;s gear up for &lt;strong>Chapter 5: Support Vector Machines (SVMs)&lt;/strong>. This is a big one! SVMs are incredibly powerful and versatile models. You&amp;rsquo;ll find them used for linear or nonlinear classification, regression, and even outlier detection. They are a cornerstone of classical machine learning, and definitely a tool everyone interested in ML should have in their arsenal.&lt;/p>
&lt;p>The book mentions they&amp;rsquo;re particularly good for &lt;strong>complex small- or medium-sized datasets.&lt;/strong> This chapter will walk us through their core concepts, how to use them, and, importantly for our &amp;ldquo;what it&amp;rsquo;s ultimately trying to achieve&amp;rdquo; philosophy, how they actually work.&lt;/p></description></item><item><title>Chapter 6: Decision Trees</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-6/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-6/</guid><description>&lt;h1 id="introduction-to-decision-trees">Introduction to Decision Trees&lt;/h1>
&lt;p>Alright class, settle in! After exploring the world of Support Vector Machines, we&amp;rsquo;re now turning our attention to another incredibly versatile and powerful family of algorithms: &lt;strong>Decision Trees&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Versatility:&lt;/strong> Like SVMs, Decision Trees can perform both &lt;strong>classification&lt;/strong> and &lt;strong>regression&lt;/strong> tasks. They can even handle &lt;strong>multioutput tasks&lt;/strong> (where each instance can have multiple output labels or values).&lt;/li>
&lt;li>&lt;strong>Power:&lt;/strong> They are capable of fitting complex datasets. You might recall from Chapter 2, when we looked at the California housing data, a &lt;code>DecisionTreeRegressor&lt;/code> was able to fit the training data perfectly (though, as we noted, it was actually overfitting).&lt;/li>
&lt;li>&lt;strong>Fundamental Building Blocks:&lt;/strong> Decision Trees are also the core components of &lt;strong>Random Forests&lt;/strong> (which we&amp;rsquo;ll see in Chapter 7). Random Forests are among the most powerful and widely used ML algorithms today. So, understanding Decision Trees is crucial for understanding Random Forests.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>What this chapter will cover:&lt;/strong>&lt;/p></description></item><item><title>Chapter 10: Introduction to Artificial Neural Networks</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-10/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-10/</guid><description>&lt;h1 id="introduction---inspiration-from-nature">Introduction - Inspiration from Nature&lt;/h1>
&lt;p>The chapter beautifully starts by reminding us how nature has often inspired human inventions: birds inspired planes, burdock plants inspired Velcro. So, it&amp;rsquo;s logical to look at the brain&amp;rsquo;s architecture for inspiration on building intelligent machines. This is the core idea that sparked ANNs.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>ANNs vs. Biological Neurons:&lt;/strong> While ANNs were inspired by the networks of biological neurons in our brains, they have evolved to be quite different. Just like planes don&amp;rsquo;t flap their wings, ANNs don&amp;rsquo;t need to be biologically perfectly realistic to be effective. The footnote mentions a good philosophy: be open to biological inspiration but don&amp;rsquo;t be afraid to create biologically &lt;em>unrealistic&lt;/em> models if they work well. Some researchers even prefer calling the components &amp;ldquo;units&amp;rdquo; rather than &amp;ldquo;neurons&amp;rdquo; to avoid this restrictive analogy.&lt;/p></description></item><item><title>Chapter 11: Training Deep Neural Networks</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-11/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-11/</guid><description>&lt;p>Link to the conversation with &lt;a href="https://aistudio.google.com/prompts/1bO4Mc-_wnw9swIgVzKyYO2F1z9iNqKJN">LLM&lt;/a>&lt;/p>
&lt;p>Chapter 10 introduced ANNs, and we even notionally &amp;ldquo;trained&amp;rdquo; some (though they were relatively shallow, with just a few hidden layers). But what happens when you need to tackle really complex problems, like detecting hundreds of object types in high-resolution images?&lt;/p>
&lt;ul>
&lt;li>You might need a much &lt;strong>deeper DNN&lt;/strong> (10+ layers, hundreds of neurons per layer, hundreds of thousands or millions of connections).&lt;/li>
&lt;li>Training such a deep DNN is &lt;strong>not a walk in the park&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;p>The chapter immediately highlights several common problems you could run into:&lt;/p></description></item><item><title>Chapter 15: Processing Sequences Using RNNs and CNNs</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-15/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-15/</guid><description>&lt;h2 id="the-world-of-sequences">The World of Sequences&lt;/h2>
&lt;p>The chapter opens with a vivid example: an outfielder catching a ball. This involves:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Anticipating the future:&lt;/strong> Predicting the ball&amp;rsquo;s trajectory.&lt;/li>
&lt;li>&lt;strong>Adapting to new information:&lt;/strong> Tracking the ball and adjusting movements.&lt;/li>
&lt;/ul>
&lt;p>This ability to process information over time and predict what comes next is something humans do constantly (finishing a friend&amp;rsquo;s sentence, smelling coffee brewing). This chapter introduces &lt;strong>Recurrent Neural Networks (RNNs)&lt;/strong> as a class of neural networks designed for precisely this kind of task.&lt;/p></description></item><item><title>Chapter 18: Reinforcement Learning</title><link>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-18/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-18/</guid><description>&lt;p>&lt;strong>(Page 609: Introduction - The Excitement of RL)&lt;/strong>&lt;/p>
&lt;p>The chapter starts by highlighting that &lt;strong>Reinforcement Learning (RL)&lt;/strong> is one of the most exciting fields in Machine Learning today, yet it&amp;rsquo;s also one of the oldest, with roots in the 1950s.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Historical Applications:&lt;/strong> It has seen interesting applications over the years, particularly in games (like TD-Gammon, a Backgammon program) and machine control, but it didn&amp;rsquo;t often make headline news.&lt;/li>
&lt;li>&lt;strong>The DeepMind Revolution (2013 onwards):&lt;/strong> A major turning point occurred in 2013 when researchers from DeepMind (a British startup, later bought by Google) demonstrated a system that could learn to play almost any Atari game from scratch, using only raw pixel inputs and no prior knowledge of the game rules. It eventually outperformed humans in most of them.
&lt;ul>
&lt;li>This was followed by a series of amazing feats, most famously &lt;strong>AlphaGo&lt;/strong> defeating legendary Go player Lee Sedol in 2016 and world champion Ke Jie in 2017. This was a monumental achievement, as Go is an incredibly complex game.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>The &amp;ldquo;Simple&amp;rdquo; Idea Behind DeepMind&amp;rsquo;s Success:&lt;/strong> With hindsight, the core idea was to apply the power of &lt;strong>Deep Learning&lt;/strong> (which we&amp;rsquo;ve been studying) to the field of Reinforcement Learning. The combination proved to be incredibly effective.&lt;/li>
&lt;li>&lt;strong>Current State:&lt;/strong> The field of RL is now &amp;ldquo;boiling with new ideas&amp;rdquo; and has a wide range of potential applications.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Chapter Goals:&lt;/strong>&lt;/p></description></item></channel></rss>