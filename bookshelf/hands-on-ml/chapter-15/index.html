<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><a href=#the-world-of-sequences>The World of Sequences</a></li><li><a href=#recurrent-neurons-and-layers>Recurrent Neurons and Layers</a></li><li><a href=#training-rnns---backpropagation-through-time-bptt>Training RNNs - Backpropagation Through Time (BPTT)</a></li><li><a href=#forecasting-a-time-series---the-task>Forecasting a Time Series - The Task</a></li><li><a href=#generating-time-series-data>Generating Time Series Data</a></li><li><a href=#baseline-metrics>Baseline Metrics</a></li><li><a href=#implementing-a-simple-rnn>Implementing a Simple RNN</a></li><li><a href=#deep-rnns>Deep RNNs</a></li><li><a href=#forecasting-several-time-steps-ahead>Forecasting Several Time Steps Ahead</a></li><li><a href=#handling-long-sequences---the-problems>Handling Long Sequences - The Problems</a></li><li><a href=#fighting-the-unstable-gradients-problem-in-rnns>Fighting the Unstable Gradients Problem in RNNs</a></li><li><a href=#tackling-the-short-term-memory-problem---introduction>Tackling the Short-Term Memory Problem - Introduction</a></li><li><a href=#lstm-cells---long-short-term-memory>LSTM Cells - Long Short-Term Memory</a></li><li><a href=#gru-cells---gated-recurrent-unit>GRU Cells - Gated Recurrent Unit</a></li><li><a href=#using-1d-convolutional-layers-to-process-sequences>Using 1D Convolutional Layers to Process Sequences</a></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 15: Processing Sequences Using RNNs and CNNs</h1><span class=reading-time><em>48 min read</em></span><div class=book-details><div class=book-content><h2 id=the-world-of-sequences>The World of Sequences</h2><p>The chapter opens with a vivid example: an outfielder catching a ball. This involves:</p><ul><li><strong>Anticipating the future:</strong> Predicting the ball&rsquo;s trajectory.</li><li><strong>Adapting to new information:</strong> Tracking the ball and adjusting movements.</li></ul><p>This ability to process information over time and predict what comes next is something humans do constantly (finishing a friend&rsquo;s sentence, smelling coffee brewing). This chapter introduces <strong>Recurrent Neural Networks (RNNs)</strong> as a class of neural networks designed for precisely this kind of task.</p><ul><li><p><strong>What RNNs are good for:</strong></p><ul><li>Analyzing time series data (e.g., stock prices, weather patterns).</li><li>Anticipating trajectories (e.g., in autonomous driving).</li><li>Working on sequences of <strong>arbitrary lengths</strong> (unlike the fixed-sized inputs we&rsquo;ve seen so far with MLPs and standard CNNs). This is a key differentiator.</li><li>Processing sentences, documents, or audio samples.</li><li>Hugely important for <strong>Natural Language Processing (NLP)</strong> tasks like automatic translation or speech-to-text.</li></ul></li><li><p><strong>Chapter Focus (for Chapter 15):</strong></p><ol><li>Fundamental concepts of RNNs.</li><li>How to train them using <strong>backpropagation through time (BPTT)</strong>.</li><li>Using RNNs to forecast a time series.</li><li>Exploring two main difficulties RNNs face:<ul><li><strong>Unstable gradients</strong> (vanishing/exploding, as discussed in Chapter 11), and techniques to alleviate them (recurrent dropout, recurrent layer normalization).</li><li><strong>Limited short-term memory</strong>, and how to extend it using <strong>LSTM (Long Short-Term Memory)</strong> and <strong>GRU (Gated Recurrent Unit)</strong> cells.</li></ul></li></ol></li><li><p><strong>Alternatives to RNNs for Sequential Data (Page 497, bottom):</strong></p><ul><li>The chapter acknowledges that RNNs aren&rsquo;t the <em>only</em> way to handle sequential data.</li><li>For <strong>small sequences</strong>, a regular dense network (MLP) might suffice.</li><li>For <strong>very long sequences</strong> (audio, long text), <strong>Convolutional Neural Networks (CNNs)</strong> can also work surprisingly well. The chapter will discuss this and implement <strong>WaveNet</strong>, a CNN architecture for long sequences.</li><li>(Chapter 16 will continue with RNNs for NLP and then move to attention mechanisms).</li></ul></li></ul><p><em>What this chapter is ultimately trying to achieve:</em> To introduce a new type of neural network architecture capable of understanding and making predictions based on ordered sequences of data, where the order and context over time are crucial.*</p><h2 id=recurrent-neurons-and-layers>Recurrent Neurons and Layers</h2><p>Up to now, we&rsquo;ve focused on <strong>feedforward neural networks</strong>, where activations flow in one direction: input -> hidden layers -> output.</p><ul><li><p><strong>Recurrent Neural Network (RNN) Difference:</strong> An RNN looks like a feedforward network, but it also has <strong>connections pointing backward</strong>. This creates a loop, allowing information to persist.</p></li><li><p><strong>Simplest RNN: A Single Recurrent Neuron (Figure 15-1, page 498):</strong></p><ul><li><strong>Left diagram (rolled):</strong> A single neuron receives an input <code>x</code>, produces an output <code>y</code>, and importantly, sends its <em>own output back to itself</em> as an input for the next step.</li><li><strong>At each time step <code>t</code> (also called a frame):</strong><ul><li>The recurrent neuron receives the current external input <code>x(t)</code>.</li><li>It also receives its <em>own output from the previous time step</em>, <code>y(t-1)</code>. This <code>y(t-1)</code> is the &ldquo;memory&rdquo; or &ldquo;state&rdquo; from the past.</li><li>At the very first time step (<code>t=0</code>), there&rsquo;s no previous output, so <code>y(-1)</code> is generally initialized to 0.</li></ul></li><li><strong>Right diagram (unrolled through time):</strong> This is a crucial visualization. It shows the <em>same</em> recurrent neuron being represented at different points in time (<code>t-3, t-2, t-1, t</code>).<ul><li>At time <code>t</code>, the neuron takes <code>x(t)</code> and the output from itself at <code>t-1</code> (which is <code>y(t-1)</code>) to produce <code>y(t)</code>.</li><li>This &ldquo;unrolling&rdquo; helps visualize how information flows and how gradients will be calculated.</li></ul></li><li><em>What this recurrent connection is ultimately trying to achieve:</em> It allows the neuron to maintain a &ldquo;state&rdquo; or &ldquo;memory&rdquo; of past inputs, influencing its current output based not just on the current input but also on what it has seen before.</li></ul></li><li><p><strong>Layer of Recurrent Neurons (Figure 15-2, page 499):</strong></p><ul><li><p>You can have a whole layer of these recurrent neurons.</p></li><li><p>At each time step <code>t</code>:</p><ul><li>Every neuron in the layer receives the entire input vector <code>x(t)</code>.</li><li>Every neuron also receives the entire output vector <code>y(t-1)</code> from <em>all neurons in this same layer</em> at the previous time step.</li></ul></li><li><p><strong>Weights:</strong> Each recurrent neuron (or the layer as a whole) now has <em>two</em> sets of weights:</p><ul><li><code>Wₓ</code>: Weights for the current inputs <code>x(t)</code>.</li><li><code>Wᵧ</code>: Weights for the outputs from the previous time step <code>y(t-1)</code>.</li></ul></li><li><p><strong>Equation 15-1: Output of a recurrent layer for a single instance at time <code>t</code>:</strong>
<code>y(t) = φ( Wₓᵀx(t) + Wᵧᵀy(t-1) + b )</code></p><ul><li><code>φ</code> is the activation function (e.g., tanh is common, ReLU can be used but needs care as mentioned in Ch 11).</li><li><code>b</code> is the bias vector.</li><li><em>What this equation is ultimately trying to achieve:</em> It formalizes how the current output <code>y(t)</code> is a function of both the current input <code>x(t)</code> and the collective state/output of the layer from the previous time step <code>y(t-1)</code>.</li></ul></li><li><p><strong>Equation 15-2: Output for a mini-batch:</strong>
<code>Y(t) = φ( X(t)Wₓ + Y(t-1)Wᵧ + b )</code>
This is the vectorized form for a whole mini-batch <code>X(t)</code>.</p><ul><li><code>Y(t)</code>: Matrix of outputs for all instances in the batch at time step <code>t</code>.</li><li><code>X(t)</code>: Matrix of inputs for all instances at time step <code>t</code>.</li><li><code>Y(t-1)</code>: Matrix of outputs from the previous time step.
The equation also shows a common convention: <code>Wₓ</code> and <code>Wᵧ</code> are often concatenated into a single weight matrix <code>W</code>, and inputs <code>[X(t) Y(t-1)]</code> are concatenated before multiplication.</li></ul></li></ul></li><li><p><strong>Memory and State (Page 500):</strong></p><ul><li>Since the output <code>Y(t)</code> depends on all inputs from <code>X(0)</code> up to <code>X(t)</code>, the RNN has a form of memory.</li><li>A part of a neural network that preserves some state across time steps is called a <strong>memory cell</strong> (or simply a &ldquo;cell&rdquo;).</li><li>A single recurrent neuron or a layer of them is a very basic cell, typically capable of learning only short patterns (e.g., ~10 steps). More complex cells (LSTM, GRU) can learn longer patterns.</li><li><strong>Cell State <code>h(t)</code>:</strong> In general, a cell&rsquo;s state at time <code>t</code> is denoted <code>h(t)</code> (&ldquo;h&rdquo; for hidden state). It&rsquo;s a function of the previous state <code>h(t-1)</code> and the current input <code>x(t)</code>: <code>h(t) = f(h(t-1), x(t))</code>.</li><li><strong>Output <code>y(t)</code>:</strong> The cell&rsquo;s output <code>y(t)</code> is also a function of the previous state and current inputs.</li><li><strong>Figure 15-3:</strong> For basic recurrent cells like <code>SimpleRNN</code> in Keras, the output <code>y(t)</code> is simply <em>equal</em> to the hidden state <code>h(t)</code>. However, for more complex cells like LSTMs (which we&rsquo;ll see later), the hidden state <code>h(t)</code> and the output <code>y(t)</code> can be different. The cell might maintain an internal state that is more complex than what it chooses to output at each time step.</li></ul></li><li><p><strong>Input and Output Sequences (Figure 15-4, page 501):</strong>
RNNs can handle various types of input/output sequence configurations:</p><ol><li><strong>Sequence-to-Sequence (Top-Left):</strong><ul><li>Input: A sequence of vectors <code>X(0), X(1), ..., X(T)</code>.</li><li>Output: A sequence of vectors <code>Y(0), Y(1), ..., Y(T)</code>.</li><li>Example: Predicting stock prices (input last N days&rsquo; prices, output prices shifted one day into the future).</li><li><em>What it&rsquo;s ultimately trying to achieve:</em> Transform an input sequence into an output sequence, often for prediction or transduction.</li></ul></li><li><strong>Sequence-to-Vector (Top-Right):</strong><ul><li>Input: A sequence of vectors.</li><li>Output: A single vector (usually from the last time step&rsquo;s output). All other outputs are ignored.</li><li>Example: Sentiment analysis of a movie review (input sequence of words, output a single sentiment score like -1 to +1).</li><li><em>What it&rsquo;s ultimately trying to achieve:</em> Summarize or classify an entire input sequence into a single vector representation or decision.</li></ul></li><li><strong>Vector-to-Sequence (Bottom-Left):</strong><ul><li>Input: A single fixed vector, fed repeatedly at each time step (or just at the first time step with subsequent inputs being zero).</li><li>Output: A sequence of vectors.</li><li>Example: Image captioning (input an image or its CNN feature vector, output a sequence of words forming a caption).</li><li><em>What it&rsquo;s ultimately trying to achieve:</em> Generate a sequence based on a static input.</li></ul></li><li><strong>Encoder-Decoder (Bottom-Right):</strong><ul><li>A sequence-to-vector network (the <strong>encoder</strong>) is followed by a vector-to-sequence network (the <strong>decoder</strong>).</li><li>Example: Neural Machine Translation (NMT).<ul><li>Encoder reads an input sentence (e.g., English) and compresses it into a single vector representation (often called the &ldquo;context vector&rdquo; or &ldquo;thought vector&rdquo;).</li><li>Decoder takes this context vector and generates an output sentence (e.g., French), word by word.</li></ul></li><li>This two-step model generally works much better for tasks like translation than a single sequence-to-sequence RNN because the meaning of the whole input sentence needs to be captured before starting to generate the translation (the end of an English sentence can affect the beginning of its French translation).</li><li>(More complex Encoder-Decoder details in Chapter 16).</li></ul></li></ol></li></ul><p>The core idea so far is that recurrent neurons have a &ldquo;loop&rdquo; that allows them to pass their previous output (their state) as an input to their next computation, enabling them to process sequences and maintain a memory of past information.</p><p>The next logical step in the chapter is <strong>Training RNNs</strong> (Page 502).</p><h2 id=training-rnns---backpropagation-through-time-bptt>Training RNNs - Backpropagation Through Time (BPTT)</h2><p>So, we have this recurrent structure. How do we train it? How do we adjust <code>Wₓ</code>, <code>Wᵧ</code>, and <code>b</code>?</p><ul><li><p><strong>The Trick: Unroll and Use Regular Backpropagation:</strong></p><ul><li>The core idea is surprisingly simple:<ol><li><strong>Unroll the RNN through time</strong> for the length of your input sequences (as we saw in Figure 15-1 and 15-2). When you unroll it, it looks just like a very deep feedforward network, where each &ldquo;time slice&rdquo; of the RNN becomes a layer in this unrolled network.</li><li>Crucially, the weights (<code>Wₓ</code>, <code>Wᵧ</code>, <code>b</code>) are <strong>shared</strong> across all these time-step &ldquo;layers&rdquo; in the unrolled network. The <code>Wₓ</code> used at time <code>t=1</code> is the same <code>Wₓ</code> used at <code>t=2</code>, <code>t=3</code>, etc.</li><li>Then, you simply apply <strong>regular backpropagation</strong> (as discussed in Chapter 10) to this unrolled network.</li></ol></li><li>This strategy is called <strong>Backpropagation Through Time (BPTT)</strong>.</li></ul></li><li><p><strong>The BPTT Process (Figure 15-5, page 503):</strong></p><ol><li><strong>Forward Pass:</strong><ul><li>Feed the input sequence through the unrolled network, time step by time step (dashed arrows in the figure).</li><li>At each time step, calculate the outputs <code>Y(0), Y(1), ..., Y(T)</code> (where <code>T</code> is the maximum time step or sequence length).</li></ul></li><li><strong>Evaluate Loss:</strong><ul><li>The output sequence <code>Y(0)...Y(T)</code> is evaluated using a <strong>cost function <code>C(Y(0), ..., Y(T))</code></strong>.</li><li><em>Important Note:</em> The cost function might ignore some outputs. For example:<ul><li>In a <strong>sequence-to-vector</strong> RNN (like sentiment analysis), the cost function might only care about the very last output <code>Y(T)</code>.</li><li>In Figure 15-5, the cost function <code>C(Y(2), Y(3), Y(4))</code> uses only the last three outputs.</li></ul></li></ul></li><li><strong>Backward Pass:</strong><ul><li>The gradients of the cost function are then propagated backward through the unrolled network (solid arrows in the figure).</li><li>Gradients flow backward through <em>all the outputs that were used by the cost function</em>. In Figure 15-5, gradients flow from <code>Y(4)</code>, <code>Y(3)</code>, and <code>Y(2)</code>. They do <em>not</em> flow from <code>Y(1)</code> or <code>Y(0)</code> because those were not part of the cost calculation in this specific example.</li></ul></li><li><strong>Update Parameters:</strong><ul><li>Since the same parameters <code>Wₓ</code>, <code>Wᵧ</code>, and <code>b</code> are used at <em>each time step</em> in the unrolled network, the gradients calculated for these parameters at each time step are <strong>summed up (or averaged) across all time steps.</strong></li><li>These aggregated gradients are then used to update <code>Wₓ</code>, <code>Wᵧ</code>, and <code>b</code> using a standard gradient descent step.</li><li><em>What this summing of gradients across time achieves:</em> It ensures that the parameter updates reflect how those parameters contributed to the error across the <em>entire sequence processing</em>, not just at one particular time step.</li></ul></li></ol></li><li><p><strong>Keras Handles the Complexity:</strong></p><ul><li>The good news is that <code>tf.keras</code> takes care of all this BPTT complexity for you when you use its recurrent layers! You define the recurrent layer, and during <code>fit()</code>, Keras handles the unrolling, BPTT, and parameter updates.</li></ul></li></ul><p><strong>Key Takeaway for BPTT:</strong>
BPTT is the standard way to train RNNs. It cleverly transforms the problem of training a network with loops into training a deep feedforward network (the unrolled version) where weights are shared across &ldquo;layers&rdquo; (time steps). Regular backpropagation can then be applied, with the understanding that gradients for shared weights are aggregated across all time steps.</p><p>This is a foundational concept for training any kind of recurrent network. Next, the chapter moves into a practical example: forecasting a time series.</p><p>How does the idea of &ldquo;unrolling in time&rdquo; and then applying standard backpropagation feel as a strategy for training RNNs?</p><p>Excellent! It&rsquo;s good that BPTT makes conceptual sense. It&rsquo;s a clever way to adapt a known algorithm (backpropagation) to a new type of network architecture (RNNs).</p><p>Now, let&rsquo;s dive into the first practical application in <strong>Chapter 15</strong>: <strong>Forecasting a Time Series</strong> (Pages 503-511). This section will also involve Keras implementations, but we&rsquo;ll continue to focus on the &ldquo;what are we trying to achieve&rdquo; for the RNN-specific concepts and the data handling.</p><h2 id=forecasting-a-time-series---the-task>Forecasting a Time Series - The Task</h2><ul><li><strong>What is a Time Series?</strong><ul><li>A sequence of one or more values per time step.</li><li><strong>Univariate time series:</strong> Single value per time step (e.g., daily temperature in your city, active users per hour on a website).</li><li><strong>Multivariate time series:</strong> Multiple values per time step (e.g., a company&rsquo;s quarterly revenue, debt, profit, etc.).</li></ul></li><li><strong>Common Tasks:</strong><ul><li><strong>Forecasting:</strong> Predicting future values.</li><li><strong>Imputation:</strong> Predicting (or &ldquo;postdicting&rdquo;) missing values from the past.</li></ul></li><li><strong>Example for this section (Figure 15-6, page 504):</strong><ul><li>Three univariate time series, each 50 time steps long.</li><li>Goal: Forecast the value at the <em>next</em> time step (the &lsquo;X&rsquo; in the figure) for each of them.</li></ul></li></ul><h2 id=generating-time-series-data>Generating Time Series Data</h2><ul><li><p>For simplicity, the book uses a <code>generate_time_series()</code> function to create synthetic data.</p><ul><li>It creates <code>batch_size</code> time series, each of length <code>n_steps</code>.</li><li>Each series is univariate (one value per time step).</li><li>Each series is a sum of two sine waves with random frequencies and phases, plus some noise.</li><li>The function returns a NumPy array of shape <code>[batch_size, time_steps, 1]</code> (the last dimension is for the number of features per time step, which is 1 for univariate).</li></ul></li><li><p><strong>Input Data Shape for RNNs (Bird Icon, page 504):</strong></p><ul><li>When dealing with sequences, input features are generally represented as 3D arrays:
<code>[batch_size, time_steps, dimensionality]</code><ul><li><code>dimensionality</code>: 1 for univariate time series, more for multivariate.</li></ul></li></ul></li><li><p><strong>Creating Train/Validation/Test Sets:</strong></p><ul><li><code>n_steps = 50</code></li><li>Generate 10,000 series, each <code>n_steps + 1</code> long (the extra step is for the target).
<code>series = generate_time_series(10000, n_steps + 1)</code></li><li>Split the data:<ul><li><code>X_train</code>: First 7000 series, first <code>n_steps</code> time steps each <code>[:7000, :n_steps]</code></li><li><code>y_train</code>: First 7000 series, the <em>last</em> time step each <code>[:7000, -1]</code> (this is the value we want to predict after seeing <code>n_steps</code>).</li><li>Similarly for <code>X_valid</code> (next 2000) and <code>X_test</code> (last 1000).</li></ul></li><li>Shapes: <code>X_train</code> is <code>. `y_train` is </code>.</li></ul></li></ul><h2 id=baseline-metrics>Baseline Metrics</h2><p>Before building complex RNNs, it&rsquo;s crucial to establish some simple baselines.</p><ul><li><em>What baselines are ultimately trying to achieve:</em> They give you a simple reference point. If your fancy RNN can&rsquo;t beat a very basic model, something is wrong, or the task might be too simple for an RNN, or the basic model is surprisingly effective.</li></ul><ol><li><p><strong>Naive Forecasting:</strong></p><ul><li>Predict the last observed value in each series as the next value.</li><li><code>y_pred_naive = X_valid[:, -1, :]</code> (Take the last time step from each validation sequence).</li><li>Calculate Mean Squared Error (MSE): <code>np.mean(keras.losses.mean_squared_error(y_valid, y_pred_naive))</code></li><li>Result: ~0.020 MSE.</li></ul></li><li><p><strong>Simple Linear Regression Model:</strong></p><ul><li>Use a fully connected network (a Dense layer) with a linear activation.</li><li>Since Dense layers expect a flat list of features, add a <code>Flatten</code> layer first.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_linear <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Flatten(input_shape<span style=color:#f92672>=</span>[<span style=color:#ae81ff>50</span>, <span style=color:#ae81ff>1</span>]), <span style=color:#75715e># Input: 50 time steps, 1 feature each</span>
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>) <span style=color:#75715e># Output: 1 predicted value</span>
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><ul><li>Compile with MSE loss and Adam optimizer. Train for 20 epochs.</li><li>Result: ~0.004 MSE. Much better than naive forecasting!</li></ul></li></ol><h2 id=implementing-a-simple-rnn>Implementing a Simple RNN</h2><p>Now let&rsquo;s try a very basic RNN. Can it beat the linear model?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_simple_rnn <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>SimpleRNN(<span style=color:#ae81ff>1</span>, input_shape<span style=color:#f92672>=</span>[<span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>1</span>]) <span style=color:#75715e># Single layer, single neuron</span>
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><ul><li><p><code>keras.layers.SimpleRNN(1, ...)</code>: Creates a single recurrent layer with just <strong>one recurrent neuron</strong>.</p></li><li><p><code>input_shape=[None, 1]</code>:</p><ul><li><code>None</code>: We don&rsquo;t need to specify the length of the input sequences (<code>n_steps</code>). RNNs can process sequences of arbitrary length. This is a major advantage.</li><li><code>1</code>: Dimensionality of the input at each time step (univariate).</li></ul></li><li><p><strong>Default Behavior of <code>SimpleRNN</code>:</strong></p><ul><li>Uses <code>tanh</code> (hyperbolic tangent) as the activation function.</li><li><strong>Returns only the final output:</strong> By default, recurrent layers in Keras only return the output of the last time step. If our input sequence has 50 steps (<code>y(0)</code> to <code>y(49)</code>), this layer will output <code>y(49)</code>. (Bird icon, page 506).</li><li>This is exactly what we want for this forecasting task (predicting the value at the <em>next</em> step after seeing <code>n_steps</code>).</li></ul></li><li><p><strong>How it works (conceptually, as in Figure 15-1):</strong></p><ul><li>Initial hidden state <code>h_init</code> is set to 0.</li><li>For <code>t=0</code>: Neuron takes <code>x(0)</code> and <code>h_init</code>, computes <code>h(0)</code> (which is also <code>y(0)</code> for <code>SimpleRNN</code>).</li><li>For <code>t=1</code>: Neuron takes <code>x(1)</code> and <code>h(0)</code>, computes <code>h(1)</code> (which is also <code>y(1)</code>).</li><li>&mldr;</li><li>For <code>t=49</code>: Neuron takes <code>x(49)</code> and <code>h(48)</code>, computes <code>h(49)</code> (which is also <code>y(49)</code>).</li><li>The layer outputs this final <code>y(49)</code>.</li></ul></li><li><p><strong>Training:</strong> Compile with MSE, Adam optimizer, train for 20 epochs.</p></li><li><p><strong>Result:</strong> MSE of ~0.014.</p><ul><li>Better than naive forecasting (0.020).</li><li>But <em>worse</em> than the simple linear model (0.004)!</li></ul></li><li><p><strong>Why is it worse? Parameters:</strong></p><ul><li>The linear model had <code>50 inputs * 1 weight_per_input + 1 bias = 51</code> parameters.</li><li>A simple RNN neuron has:<ul><li>Weights for current input <code>x(t)</code> (vector <code>Wₓ</code>, if <code>x(t)</code> is a vector). Here, <code>x(t)</code> is scalar, so 1 weight.</li><li>Weights for previous hidden state <code>h(t-1)</code> (vector <code>Wᵧ</code>, if <code>h(t-1)</code> is a vector). Here, <code>h(t-1)</code> is scalar (output of one neuron), so 1 weight.</li><li>One bias term.</li><li>Total: 3 parameters for this single <code>SimpleRNN(1)</code> neuron.</li></ul></li><li>With only 3 parameters, it&rsquo;s much less powerful than the 51-parameter linear model for this task.</li></ul></li><li><p><strong>Trend and Seasonality (Sidebar, page 506):</strong></p><ul><li>Traditional time series models (like ARIMA) often require you to manually remove trends (e.g., 10% monthly growth) and seasonality (e.g., sunscreen sales peak in summer) from the data before training. After prediction, you add them back.</li><li>With RNNs, this is generally <em>not necessary</em>. The RNN can learn trends and seasonality if they exist in the data. However, preprocessing to remove them <em>might</em> sometimes improve performance, as the model then has a simpler pattern to learn.</li></ul></li></ul><h2 id=deep-rnns>Deep RNNs</h2><p>The simple RNN with one neuron was too simple. Let&rsquo;s try stacking multiple layers of cells – a <strong>deep RNN</strong> (Figure 15-7, page 507).</p><ul><li><p><strong>Keras Implementation:</strong> Just stack recurrent layers.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_deep_rnn <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>SimpleRNN(<span style=color:#ae81ff>20</span>, return_sequences<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, input_shape<span style=color:#f92672>=</span>[<span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>1</span>]),
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>SimpleRNN(<span style=color:#ae81ff>20</span>, return_sequences<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>), <span style=color:#75715e># Second layer</span>
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>SimpleRNN(<span style=color:#ae81ff>1</span>)                          <span style=color:#75715e># Output layer</span>
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><ul><li><strong>First <code>SimpleRNN(20, return_sequences=True, ...)</code>:</strong><ul><li>20 recurrent neurons (units).</li><li><code>return_sequences=True</code>: This is crucial! By default, a recurrent layer only outputs the hidden state of the <em>last</em> time step. For stacking, the <em>next</em> recurrent layer needs to receive a full sequence of outputs from the current layer (one output vector per time step). Setting <code>return_sequences=True</code> makes it output the hidden state at <em>every</em> time step. So, if input is 50 steps, output is 50 steps (each step being a 20-dim vector).</li></ul></li><li><strong>Second <code>SimpleRNN(20, return_sequences=True)</code>:</strong><ul><li>Also 20 units. Also returns sequences to feed into what would be the next layer.</li><li>(The scorpion icon on page 507 emphasizes setting <code>return_sequences=True</code> for all recurrent layers except possibly the last one if you only care about its final output).</li></ul></li><li><strong>Third <code>SimpleRNN(1)</code>:</strong><ul><li>The final layer has 1 unit (to predict a single value).</li><li>It does <em>not</em> have <code>return_sequences=True</code> (or it&rsquo;s <code>False</code> by default), so it will only output the value from the very last time step.</li></ul></li></ul></li><li><p><strong>Training:</strong> Compile with MSE, Adam optimizer, train.</p></li><li><p><strong>Result:</strong> MSE of ~0.003! We finally beat the linear model (0.004).</p></li><li><p><strong>Improving the Output Layer (Page 507):</strong></p><ul><li>The last <code>SimpleRNN(1)</code> layer is not ideal:<ol><li>Its hidden state is just a single number, which isn&rsquo;t much memory. The model likely relies more on the hidden states of the previous 20-unit layers.</li><li><code>SimpleRNN</code> uses <code>tanh</code> by default, so predictions are bounded between -1 and 1. What if our time series values go outside this range?</li></ol></li><li><strong>Better approach:</strong> Replace the final recurrent layer with a <code>Dense</code> layer.<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_deep_rnn_dense_output <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>SimpleRNN(<span style=color:#ae81ff>20</span>, return_sequences<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, input_shape<span style=color:#f92672>=</span>[<span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>1</span>]),
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>SimpleRNN(<span style=color:#ae81ff>20</span>), <span style=color:#75715e># Now this is the last recurrent layer, so no return_sequences=True</span>
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><ul><li>The second <code>SimpleRNN</code> now only outputs its final state (a 20-dim vector).</li><li>The <code>Dense(1)</code> layer takes this 20-dim vector and outputs a single value.</li><li><strong>Benefits:</strong><ul><li><code>Dense</code> layer is often faster.</li><li>No <code>tanh</code> constraint on the output value (unless you add an activation to the <code>Dense</code> layer).</li><li>This model converges faster and performs just as well or better.</li></ul></li></ul></li></ul></li></ul><h2 id=forecasting-several-time-steps-ahead>Forecasting Several Time Steps Ahead</h2><p>So far, we&rsquo;ve only predicted the <em>very next</em> value (<code>t+1</code>). What if we want to predict multiple steps ahead (e.g., <code>t+1</code> to <code>t+10</code>)?</p><p><strong>Option 1: Iterative Predictions (Predict one step, use it as input for next, etc. - Page 508)</strong></p><ul><li>Use one of the models trained to predict just one step ahead.</li><li>To predict 10 steps:<ol><li>Feed the last <code>n_steps</code> of known data to the model, get prediction for step <code>n_steps+1</code>.</li><li>Take this prediction, append it to your known sequence (as if it were a true value).</li><li>Feed the <em>new</em> last <code>n_steps</code> (which now includes the first prediction) to the model to get prediction for <code>n_steps+2</code>.</li><li>Repeat 10 times.</li></ol></li><li><strong>Code Example:</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># series is [1, n_steps + 10, 1]</span>
</span></span><span style=display:flex><span><span style=color:#75715e># X_new is initial sequence [1, n_steps, 1]</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Y_new is target future sequence [1, 10, 1]</span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> X_new 
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> step_ahead <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    y_pred_one <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(X[:, step_ahead:]) <span style=color:#75715e># Predict one step</span>
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate([X, y_pred_one[:, np<span style=color:#f92672>.</span>newaxis, :]], axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>) <span style=color:#75715e># Append prediction</span>
</span></span><span style=display:flex><span>Y_pred_iterative <span style=color:#f92672>=</span> X[:, n_steps:] <span style=color:#75715e># The 10 appended predictions</span>
</span></span></code></pre></div></li><li><strong>Problem:</strong> Errors can accumulate. If the prediction for step 1 is slightly off, it makes the input for predicting step 2 slightly off, and so on. (Figure 15-8, page 509, shows this).</li><li><strong>Result:</strong> MSE on validation set is ~0.029. Much higher than previous models, but it&rsquo;s a harder task.<ul><li>Naive forecast (predict last value for all 10 future steps) gives MSE ~0.223 (terrible).</li><li>A simple linear model (trained to predict 10 steps ahead) gives MSE ~0.0188 – much better than this iterative RNN approach and faster to train.</li></ul></li><li><strong>When it might work:</strong> For a few steps ahead, or for complex tasks where linear models fail, this iterative approach might be okay.</li></ul><p><strong>Option 2: Train an RNN to Predict All Future Values at Once (Sequence-to-Vector - Page 509)</strong></p><ul><li>Change the targets: Instead of <code>y_train</code> being the value at <code>n_steps+1</code>, make <code>Y_train</code> a vector of the 10 values from <code>n_steps+1</code> to <code>n_steps+10</code>.
<code>Y_train = series[:7000, -10:, 0]</code> (if <code>series</code> has <code>n_steps + 10</code> length). The last dimension <code>0</code> is to flatten it if the series had multiple features per step. If it&rsquo;s already <code>[batch, 10]</code>, no need. The book actually means the targets are <code>series[:7000, n_steps:n_steps+10, 0]</code>. Let&rsquo;s assume the targets are reshaped to <code>[batch_size, 10]</code>.</li><li>Modify the model: Output layer now needs 10 units (instead of 1).<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_seq_to_vec_10 <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>SimpleRNN(<span style=color:#ae81ff>20</span>, return_sequences<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, input_shape<span style=color:#f92672>=</span>[<span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>1</span>]),
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>SimpleRNN(<span style=color:#ae81ff>20</span>), <span style=color:#75715e># Last RNN outputs its final state (20-dim vector)</span>
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>10</span>)      <span style=color:#75715e># Dense layer predicts 10 values</span>
</span></span><span style=display:flex><span>])
</span></span></code></pre></div></li><li><strong>Result:</strong> MSE for next 10 steps is ~0.008. Much better than the linear model (0.0188)!</li></ul><p><strong>Option 3: Train an RNN to Predict Future Values at <em>Each</em> Time Step (Sequence-to-Sequence - Page 509-510)</strong>
This is even better. Instead of only forecasting the next 10 values at the <em>very last time step</em>, train the model to forecast the next 10 values <em>at each and every time step</em> of the input.</p><ul><li><p><strong>Targets <code>Y</code> (Page 510):</strong></p><ul><li>For each input sequence <code>X(0)...X(n_steps-1)</code>, the target <code>Y</code> needs to be a sequence of the same length.</li><li>At time step <code>t</code> of the input, the target <code>Y(t)</code> is a vector of the <em>actual future values</em> <code>[actual(t+1), actual(t+2), ..., actual(t+10)]</code>.</li><li>The code snippet shows how to construct this <code>Y</code> array of shape <code>[batch_size, n_steps, 10]</code>.</li></ul></li><li><p><strong>Causal Model (Bird Icon, page 510):</strong></p><ul><li>The targets <code>Y_train</code> will contain values that also appear in <code>X_train</code> (e.g., <code>X_train[0, t+1]</code> is part of the target for <code>X_train[0,t]</code>). Is this cheating?</li><li>No, because at each time step <code>t</code>, the model <em>only knows about past time steps</em> up to <code>t</code>. It cannot look ahead in the input <code>X</code> to see <code>X(t+1)</code> when predicting the target for <code>Y(t)</code>. It&rsquo;s a <strong>causal model</strong>.</li></ul></li><li><p><strong>Model Architecture (Sequence-to-Sequence - Page 510):</strong></p><ol><li>All recurrent layers must have <code>return_sequences=True</code> (even the last recurrent one).</li><li>The output <code>Dense(10)</code> layer must be applied <em>at every time step</em>. Keras offers a <strong><code>TimeDistributed</code></strong> layer wrapper for this.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_seq_to_seq <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>SimpleRNN(<span style=color:#ae81ff>20</span>, return_sequences<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, input_shape<span style=color:#f92672>=</span>[<span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>1</span>]),
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>SimpleRNN(<span style=color:#ae81ff>20</span>, return_sequences<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>), <span style=color:#75715e># Both RNNs return sequences</span>
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>TimeDistributed(keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>10</span>)) <span style=color:#75715e># Apply Dense(10) at each time step</span>
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><ul><li><code>TimeDistributed(Dense(10))</code>: Wraps the <code>Dense(10)</code> layer. It takes the input sequence from the previous <code>SimpleRNN</code> (shape <code>[batch_size, time_steps, 20]</code>). It reshapes it to <code>[batch_size * time_steps, 20]</code>, applies the <code>Dense(10)</code> layer (getting <code>[batch_size * time_steps, 10]</code>), and then reshapes it back to <code>[batch_size, time_steps, 10]</code>.</li><li>The footnote 2 on page 510 mentions that a <code>Dense</code> layer itself can often handle sequence inputs correctly (applying to the last dimension independently across time steps), making <code>TimeDistributed(Dense(...))</code> sometimes redundant. However, <code>TimeDistributed</code> makes the intent very clear.</li></ul></li><li><p><strong>Loss and Evaluation (Page 511):</strong></p><ul><li>During training, all outputs are needed, so MSE over all outputs is fine.</li><li>For prediction and final evaluation, often only the output at the <em>last time step</em> is useful (i.e., given the full input sequence <code>X(0)...X(n_steps-1)</code>, what&rsquo;s the forecast for <code>X(n_steps)...X(n_steps+9)</code>?).</li><li>A custom metric <code>last_time_step_mse</code> is defined to compute MSE only on the output at the last time step.
<code>model.compile(loss="mse", optimizer="adam", metrics=[last_time_step_mse])</code></li></ul></li><li><p><strong>Result:</strong> Validation MSE (for the last time step&rsquo;s 10-step forecast) is ~0.006. This is 25% better than the sequence-to-vector model (0.008)!</p></li><li><p><strong>Why it&rsquo;s better:</strong></p><ul><li>The loss includes terms for the RNN&rsquo;s output at <em>every</em> time step.</li><li>This means many more error gradients flow through the model during BPTT.</li><li>Gradients don&rsquo;t just flow through time (horizontally in unrolled view); they also flow from the output of each time step (vertically).</li><li>This stabilizes and speeds up training.</li></ul></li><li><p><strong>Generating Arbitrarily Long Sequences (Page 511):</strong>
You can combine this sequence-to-sequence approach (predicting next 10 values) with the iterative approach (feed predictions back as input) to generate very long sequences (e.g., for music or text generation, Chapter 16). May not be accurate for long-term forecasting but fine for generation.</p></li><li><p><strong>Error Bars with MC Dropout (Bird Icon, page 511):</strong>
For forecasting, it&rsquo;s useful to have error bars. MC Dropout (from Chapter 11) can be used: add MC Dropout layers within recurrent cells, then at inference, run the model many times with dropout active and compute mean/std dev of predictions.</p></li></ul><h2 id=handling-long-sequences---the-problems>Handling Long Sequences - The Problems</h2><p>The chapter states: &ldquo;Simple RNNs can be quite good at forecasting time series or handling other kinds of sequences, but they do not perform as well on long time series or sequences.&rdquo;</p><ul><li><strong>Why?</strong> To train an RNN on long sequences, we must run it over many time steps. This means the unrolled RNN becomes a very deep network.</li><li><strong>Problems with Deep Unrolled RNNs:</strong><ol><li><strong>Unstable Gradients Problem (Vanishing/Exploding):</strong> Just like any deep neural network (as discussed in Chapter 11), it may suffer from gradients becoming too small or too large as they propagate back through many time steps. This makes training very slow or unstable.</li><li><strong>Forgetting Early Inputs (Limited Short-Term Memory):</strong> As an RNN processes a long sequence, the information from the earlier time steps has to be carried through many transformations in the hidden state. Due to these repeated transformations (and often the &ldquo;squashing&rdquo; nature of activation functions like tanh), the information from the initial inputs tends to get diluted or lost. The RNN&rsquo;s state effectively &ldquo;forgets&rdquo; what happened much earlier in the sequence.</li></ol></li></ul><p>This section will address both these problems.</p><h2 id=fighting-the-unstable-gradients-problem-in-rnns>Fighting the Unstable Gradients Problem in RNNs</h2><p>Many tricks from Chapter 11 for deep feedforward nets can also be used for RNNs:</p><ul><li>Good parameter initialization (Glorot, He).</li><li>Faster optimizers (Adam, Nadam).</li><li>Dropout (with caveats).</li></ul><p>However, some techniques behave differently or have limitations with RNNs:</p><ol><li><p><strong>Nonsaturating Activation Functions (e.g., ReLU):</strong></p><ul><li>May <em>not</em> help as much with unstable gradients in RNNs and can even make them <em>more unstable</em>.</li><li><strong>Why?</strong> The <em>same weights</em> (<code>Wₓ</code>, <code>Wᵧ</code>) are applied at <em>every time step</em>. If GD updates these weights in a way that slightly increases the outputs at time step 1, this effect can compound at time step 2, then time step 3, and so on, potentially leading to outputs exploding. A nonsaturating activation function (like ReLU) doesn&rsquo;t prevent this explosion for positive values.</li><li><strong>Solution/Preference:</strong> A <strong>saturating activation function like <code>tanh</code></strong> is often the default and preferred choice for simple RNN cells because its bounded output (-1 to 1) helps control the explosion of activations. The vanishing gradient issue (due to tanh saturation) is then tackled by other means (like LSTMs/GRUs or a smaller learning rate).</li><li>If training is unstable (gradients exploding), monitor gradient sizes (e.g., with TensorBoard) and perhaps use <strong>Gradient Clipping</strong> (as discussed in Chapter 11).</li></ul></li><li><p><strong>Batch Normalization (BN) with RNNs:</strong></p><ul><li>Cannot be used as efficiently with RNNs as with deep feedforward nets.</li><li>You <strong>cannot use standard BN <em>between time steps</em></strong> (i.e., to normalize the hidden state <code>h(t)</code> before it&rsquo;s fed into the next time step <code>t+1</code> along with <code>x(t+1)</code>). This is because BN&rsquo;s moving averages for mean/std dev are calculated per feature across a batch, and the statistics of the hidden state can vary significantly from time step to time step. Applying the same BN parameters (learned across all time steps) to each step doesn&rsquo;t work well.</li><li><strong>Where it <em>can</em> be used:</strong><ul><li>It <em>is</em> technically possible to add a BN layer <em>inside</em> a memory cell so it&rsquo;s applied at each time step (to both <code>x(t)</code> and <code>h(t-1)</code> inputs to the cell). However, the <em>same</em> BN parameters (<code>γ</code>, <code>β</code>, and the moving averages <code>μ</code>, <code>σ</code>) would be used at each time step, regardless of the actual scale/offset of inputs at that step. A 2015 paper by Laurent et al. (footnote 3) found this didn&rsquo;t yield good results when applied to hidden states, only slightly beneficial when applied to the inputs <code>x(t)</code>.</li><li><strong>What works (somewhat):</strong> Apply BN <em>between recurrent layers</em> (i.e., &ldquo;vertically&rdquo; in the unrolled view of Figure 15-7, not &ldquo;horizontally&rdquo; across time steps). You can do this in Keras by adding a <code>BatchNormalization</code> layer before each recurrent layer. Don&rsquo;t expect miracles, but it might help a bit.</li></ul></li></ul></li><li><p><strong>Layer Normalization (LN) with RNNs (Page 512):</strong></p><ul><li><p>Introduced by Ba et al. in a 2016 paper (footnote 4). Often works <em>better</em> than BN with RNNs.</p></li><li><p><strong>Key Difference from BN:</strong></p><ul><li>BN normalizes across the <em>batch dimension</em> (for each feature, calculate mean/std over all instances in the batch).</li><li>LN normalizes across the <strong>features dimension</strong> (for each instance, calculate mean/std over all features/units at the current time step).</li></ul></li><li><p><strong>Advantages for RNNs:</strong></p><ul><li>LN can compute its required statistics (mean/std for normalization) <strong>on the fly at each time step, independently for each instance.</strong> It doesn&rsquo;t need to average over a batch or estimate population stats with moving averages.</li><li>This means it behaves the same way during training and testing.</li></ul></li><li><p>Like BN, LN learns scale (<code>γ</code>) and offset (<code>β</code>) parameters per layer.</p></li><li><p>In an RNN, it&rsquo;s typically used right after the linear combination of inputs <code>x(t)</code> and hidden states <code>h(t-1)</code>, <em>before</em> the activation function.</p></li><li><p><strong>Implementing Layer Normalization in a Custom Keras Cell (Page 513):</strong>
The book shows how to create a custom RNN cell (<code>LNSimpleRNNCell</code>) that incorporates Layer Normalization.</p><ul><li>It inherits from <code>keras.layers.Layer</code>.</li><li>The constructor (<code>__init__</code>) sets up:<ul><li><code>self.state_size</code> and <code>self.output_size</code> (both equal to <code>units</code> for a simple RNN cell).</li><li>An internal <code>keras.layers.SimpleRNNCell(units, activation=None)</code> (note: <code>activation=None</code> because LN will happen <em>before</em> the final activation).</li><li>A <code>keras.layers.LayerNormalization()</code> layer.</li><li>The desired final activation function (e.g., <code>keras.activations.get("tanh")</code>).</li></ul></li><li>The <code>call(self, inputs, states)</code> method (which defines the cell&rsquo;s logic for one time step):<ol><li><code>outputs, new_states = self.simple_rnn_cell(inputs, states)</code>: Get the linear combination from the internal simple RNN cell. (<code>outputs</code> and <code>new_states</code> are the same here).</li><li><code>norm_outputs = self.layer_norm(outputs)</code>: Apply Layer Normalization to these pre-activation values.</li><li><code>activated_norm_outputs = self.activation(norm_outputs)</code>: Apply the final activation.</li><li>Return <code>activated_norm_outputs, [activated_norm_outputs]</code> (output and new state are the same).</li></ol></li><li><strong>Using the Custom Cell (Page 514):</strong>
Wrap this custom cell in a <code>keras.layers.RNN</code> layer:
<code>model.add(keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, ...))</code>
The <code>RNN</code> layer handles the unrolling through time, feeding states, etc., using the logic defined in your custom cell.</li></ul></li></ul></li><li><p><strong>Dropout in RNNs (Page 514):</strong></p><ul><li>You can apply dropout to the inputs fed <em>to</em> the recurrent layer (at each time step).</li><li>You can also apply dropout to the hidden states as they are passed <em>between time steps</em>. This is often called <strong>recurrent dropout</strong>.</li><li>Most Keras recurrent layers (like <code>SimpleRNN</code>, <code>LSTM</code>, <code>GRU</code>, but <em>not</em> the generic <code>keras.layers.RNN</code> wrapper unless the cell implements it) have two dropout hyperparameters:<ul><li><code>dropout</code>: Dropout rate for the inputs to the cell.</li><li><code>recurrent_dropout</code>: Dropout rate for the hidden states.</li></ul></li><li>No need for a custom cell just for dropout if using built-in LSTM/GRU layers.</li></ul></li></ol><p>With these techniques (proper initialization, careful choice of activation, Layer Normalization, recurrent dropout, Gradient Clipping), you can significantly alleviate the unstable gradients problem and train RNNs much more effectively, even fairly deep ones.</p><p><strong>Key Takeaway for Fighting Unstable Gradients in RNNs:</strong>
The same weights being applied repeatedly across time steps makes RNNs particularly susceptible to unstable gradients.</p><ul><li>Standard solutions like good initialization and optimizers help.</li><li>Nonsaturating activations like ReLU need care; saturating ones like <code>tanh</code> are often safer defaults for basic RNN cells.</li><li>Batch Normalization is tricky to apply effectively <em>within</em> the recurrence (horizontally); Layer Normalization is generally better.</li><li>Recurrent dropout and gradient clipping are also valuable tools.</li><li><em>What these techniques are ultimately trying to achieve:</em> Create a stable learning environment where gradients can flow effectively through many time steps without vanishing or exploding, allowing the RNN to learn dependencies across time.</li></ul><h2 id=tackling-the-short-term-memory-problem---introduction>Tackling the Short-Term Memory Problem - Introduction</h2><ul><li><p><strong>The Problem:</strong></p><ul><li>When an RNN processes a sequence, the data goes through many transformations as the hidden state is updated at each time step.</li><li>Due to these repeated transformations (and often the squashing effect of activation functions like <code>tanh</code>), information from the <em>early parts</em> of the sequence tends to get diluted or completely lost by the time the RNN reaches later time steps.</li><li>The RNN&rsquo;s state <code>h(t)</code> might contain virtually no trace of the first inputs <code>x(0), x(1), ...</code> if <code>t</code> is large.</li><li>This is a <strong>showstopper</strong> for tasks requiring understanding long-range dependencies (e.g., understanding the beginning of a long paragraph to make sense of the end, or translating a long sentence where the first word in the source affects the last word in the target).</li><li>The book uses the analogy of Dory the fish from &ldquo;Finding Nemo&rdquo; trying to translate a long sentence – by the time she finishes reading it, she has no clue how it started.</li></ul></li><li><p><strong>The Solution: Cells with Long-Term Memory:</strong></p><ul><li>To tackle this, various types of recurrent cells with mechanisms for <strong>long-term memory</strong> have been introduced.</li><li>These have proven so successful that basic <code>SimpleRNN</code> cells are not used much anymore for complex sequence tasks.</li><li>We&rsquo;ll look at the most popular ones: <strong>LSTM</strong> and <strong>GRU</strong> cells.</li></ul></li></ul><h2 id=lstm-cells---long-short-term-memory>LSTM Cells - Long Short-Term Memory</h2><ul><li><p><strong>History:</strong> Proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber, and gradually improved by others (Alex Graves, Haşim Sak, Wojciech Zaremba).</p></li><li><p><strong>As a Black Box (Page 515):</strong></p><ul><li>From the outside, an LSTM cell can be used much like a basic <code>SimpleRNN</code> cell. You can just replace <code>keras.layers.SimpleRNN</code> with <code>keras.layers.LSTM</code>.</li><li><strong>Benefits:</strong><ul><li>Performs much better.</li><li>Training converges faster.</li><li>Can detect long-term dependencies in the data.</li></ul></li><li><strong>Keras Implementation:</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>LSTM(<span style=color:#ae81ff>20</span>, return_sequences<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, input_shape<span style=color:#f92672>=</span>[<span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>1</span>]),
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>LSTM(<span style=color:#ae81ff>20</span>, return_sequences<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>TimeDistributed(keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span>])
</span></span></code></pre></div>Or, using the generic <code>RNN</code> layer with an <code>LSTMCell</code> (though the specialized <code>LSTM</code> layer is usually preferred as it can use optimized GPU implementations, see Chapter 19):<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>RNN(keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>LSTMCell(<span style=color:#ae81ff>20</span>), return_sequences<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, input_shape<span style=color:#f92672>=</span>[<span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>1</span>]),
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...</span>
</span></span><span style=display:flex><span>])
</span></span></code></pre></div></li></ul></li><li><p><strong>How Does an LSTM Cell Work? (The &ldquo;Internals&rdquo; - Figure 15-9, page 516):</strong>
This is where the magic happens.</p><ul><li><strong>Split State:</strong> An LSTM cell&rsquo;s state is split into two vectors:<ul><li><strong><code>h(t)</code> (hidden state):</strong> The short-term state. This is also the output <code>y(t)</code> of the cell at time step <code>t</code>.</li><li><strong><code>c(t)</code> (cell state or memory cell):</strong> The long-term state.</li></ul></li><li><strong>Core Idea:</strong> The network can learn <em>what to store</em> in the long-term state <code>c(t)</code>, <em>what to throw away</em> from <code>c(t-1)</code>, and <em>what to read</em> from <code>c(t)</code> to influence the current output <code>h(t)</code>.</li><li><strong>Data Flow (Left to Right for <code>c(t)</code>):</strong><ol><li>The previous long-term state <code>c(t-1)</code> first goes through a <strong>forget gate</strong>.</li><li>Some new memories (selected by an <strong>input gate</strong>) are added to it via an addition operation.</li><li>The result is the new long-term state <code>c(t)</code>, which is sent straight out (without further transformation) to be the long-term state for the next time step.</li></ol></li><li><strong>Output Path (for <code>h(t)</code>):</strong><ol><li>After the addition operation (that produced <code>c(t)</code>), the long-term state <code>c(t)</code> is copied.</li><li>This copy is passed through a <code>tanh</code> function.</li><li>The result is then filtered by an <strong>output gate</strong>.</li><li>This produces the short-term state <code>h(t)</code> (which is also the cell&rsquo;s output <code>y(t)</code>).</li></ol></li></ul></li><li><p><strong>The Gates (The Key Components - Page 516):</strong>
The &ldquo;forget,&rdquo; &ldquo;input,&rdquo; and &ldquo;output&rdquo; gates are the controllers.</p><ul><li>The current input vector <code>x(t)</code> and the previous short-term state <code>h(t-1)</code> are fed into <strong>four different fully connected layers</strong> (these are internal to the LSTM cell, each with its own weights and biases).</li><li><strong>1. Main Layer (outputs <code>g(t)</code>):</strong><ul><li>This layer analyzes <code>x(t)</code> and <code>h(t-1)</code>. Its output <code>g(t)</code> (often passed through <code>tanh</code>) represents candidate values/memories to be potentially added to the long-term state.</li><li>In a basic RNN, this <code>g(t)</code> would directly become <code>h(t)</code> and <code>y(t)</code>. In an LSTM, its most important parts are stored in <code>c(t)</code>.</li></ul></li><li><strong>2, 3, 4. Gate Controller Layers (output <code>f(t)</code>, <code>i(t)</code>, <code>o(t)</code>):</strong><ul><li>These three layers use the <strong>logistic (sigmoid) activation function</strong>, so their outputs range from 0 to 1.</li><li>These outputs are then used in <strong>element-wise multiplication</strong> operations, acting like gates:<ul><li>Output near 0: &ldquo;Closes&rdquo; the gate (blocks information flow).</li><li>Output near 1: &ldquo;Opens&rdquo; the gate (lets information through).</li></ul></li><li><strong>Forget Gate (controlled by <code>f(t)</code>):</strong>
<code>c(t) ← f(t) ⊗ c(t-1) + ...</code>
It decides which parts of the <em>previous long-term state <code>c(t-1)</code></em> should be erased or kept. If an element of <code>f(t)</code> is 0, the corresponding part of <code>c(t-1)</code> is forgotten.</li><li><strong>Input Gate (controlled by <code>i(t)</code>):</strong>
<code>c(t) ← ... + i(t) ⊗ g(t)</code>
It decides which parts of the <em>candidate memories <code>g(t)</code></em> (from the main layer) should be added to the long-term state.</li><li><strong>Output Gate (controlled by <code>o(t)</code>):</strong>
<code>h(t) = y(t) = o(t) ⊗ tanh(c(t))</code>
It decides which parts of the <em>current long-term state <code>c(t)</code></em> (after passing through <code>tanh</code>) should be read out and output as the short-term state <code>h(t)</code> and the cell output <code>y(t)</code>.</li></ul></li></ul></li><li><p><strong>Equation 15-3: LSTM Computations (Page 517):</strong>
Summarizes the math for a single instance:</p><ul><li><code>i(t) = σ(Wₓᵢᵀx(t) + Wₕᵢᵀh(t-1) + bᵢ)</code> (Input gate)</li><li><code>f(t) = σ(Wₓfᵀx(t) + Wₕfᵀh(t-1) + b_f)</code> (Forget gate - TensorFlow initializes <code>b_f</code> to 1s to prevent forgetting everything at start)</li><li><code>o(t) = σ(Wₓₒᵀx(t) + Wₕₒᵀh(t-1) + bₒ)</code> (Output gate)</li><li><code>g(t) = tanh(Wₓgᵀx(t) + Wₕgᵀh(t-1) + b_g)</code> (Main layer transforming inputs/prev_state)</li><li><code>c(t) = f(t) ⊗ c(t-1) + i(t) ⊗ g(t)</code> (New long-term state: forget old, add new)</li><li><code>y(t) = h(t) = o(t) ⊗ tanh(c(t))</code> (Output/new short-term state: filter long-term state)</li><li>Each gate and the main layer have their own weight matrices for <code>x(t)</code> (e.g., <code>Wₓᵢ</code>) and <code>h(t-1)</code> (e.g., <code>Wₕᵢ</code>), and their own bias terms (e.g., <code>bᵢ</code>). These are all learned.</li></ul></li><li><p><em>What the LSTM cell is ultimately trying to achieve:</em> By having these gate mechanisms, the LSTM can learn:</p><ul><li>When to let new important information <strong>in</strong> (<code>input gate</code>).</li><li>What old information to <strong>forget</strong> (<code>forget gate</code>).</li><li>What information to <strong>preserve</strong> in its long-term memory cell <code>c(t)</code> over many time steps.</li><li>What part of its current memory to <strong>output</strong> or use for its short-term state <code>h(t)</code> (<code>output gate</code>).
This allows it to capture long-term patterns and dependencies much more effectively than simple RNNs.</li></ul></li><li><p><strong>Peephole Connections (Felix Gers & Jürgen Schmidhuber, 2000 - Page 518):</strong></p><ul><li>An LSTM variant where the gate controllers can also &ldquo;peek&rdquo; at the long-term state <code>c(t-1)</code> (for input/forget gates) or <code>c(t)</code> (for output gate) in addition to <code>x(t)</code> and <code>h(t-1)</code>.</li><li>This can sometimes improve performance, but not always.</li><li>Standard Keras <code>LSTMCell</code> doesn&rsquo;t support them, but <code>tf.keras.experimental.PeepholeLSTMCell</code> does.</li></ul></li></ul><p><strong>1. Why Sigmoid (Logistic) for the Gates (<code>f(t)</code>, <code>i(t)</code>, <code>o(t)</code>)?</strong></p><ul><li><p><strong>What a Gate Does:</strong> A gate&rsquo;s purpose is to <em>control the flow of information</em>. It needs to decide &ldquo;how much&rdquo; of some signal should be allowed to pass through. This is like a valve that can be fully open, fully closed, or partially open.</p></li><li><p><strong>Sigmoid Output Range (0 to 1):</strong> The sigmoid function <code>σ(z) = 1 / (1 + exp(-z))</code> outputs values strictly between 0 and 1.</p><ul><li>Output near 0: &ldquo;Close the gate&rdquo; – block most/all of the information.</li><li>Output near 1: &ldquo;Open the gate&rdquo; – let most/all of the information pass through.</li><li>Output around 0.5: &ldquo;Partially open the gate&rdquo; – let some proportion of the information pass.</li></ul></li><li><p><strong>Use in Element-wise Multiplication:</strong> The outputs of these sigmoid gates are then used in element-wise multiplications (the <code>⊗</code> symbol in Equation 15-3):</p><ul><li><code>f(t) ⊗ c(t-1)</code>: How much of the old long-term state to <em>keep</em> (if <code>f(t)</code> is near 1) or <em>forget</em> (if <code>f(t)</code> is near 0).</li><li><code>i(t) ⊗ g(t)</code>: How much of the new candidate memory <code>g(t)</code> to <em>let in</em>.</li><li><code>o(t) ⊗ tanh(c(t))</code>: How much of the (processed) long-term state to <em>output</em>.</li></ul></li><li><p><strong>Differentiability:</strong> Sigmoid is differentiable, which is essential for backpropagation to learn the weights of the gate controller layers.</p></li><li><p><strong>What if we used Tanh for Gates?</strong></p><ul><li>Tanh outputs values between -1 and 1.</li><li>If a gate outputted -1, and you multiplied it by a value, it would <em>invert the sign</em> of that value and also scale it. This isn&rsquo;t really what &ldquo;gating&rdquo; or &ldquo;controlling flow&rdquo; means in an intuitive sense. We want to scale information between &ldquo;none of it&rdquo; (0) and &ldquo;all of it&rdquo; (1).</li><li>It might be possible to design a system where tanh is used and its output is then rescaled (e.g., <code>(tanh(z) + 1) / 2</code> to map it to 0-1), but sigmoid naturally provides the 0-1 range needed for this multiplicative gating effect.</li></ul></li><li><p><strong>What if we used ReLU for Gates?</strong></p><ul><li>ReLU outputs 0 for negative inputs and <code>z</code> for positive inputs (unbounded positive output).</li><li>An output of 0 would effectively &ldquo;close the gate.&rdquo;</li><li>An output > 1 would not just &ldquo;open the gate&rdquo; but also <em>amplify</em> the signal passing through. This could lead to exploding values and instability, which is precisely what LSTMs are trying to manage better than simple RNNs.</li><li>The gating mechanism generally needs a bounded &ldquo;how much&rdquo; signal, and sigmoid&rsquo;s 0-1 range is perfect for this.</li></ul></li></ul><p><strong>2. Why Tanh for the Main Layer (<code>g(t)</code>) and for Processing <code>c(t)</code> before the Output Gate?</strong></p><ul><li><p><strong>Main Layer <code>g(t)</code>:</strong> This layer <code>g(t) = tanh(W_xgᵀx(t) + W_hgᵀh(t-1) + b_g)</code> computes the &ldquo;candidate values&rdquo; to be potentially added to the long-term cell state <code>c(t)</code>.</p></li><li><p><strong>Processing <code>c(t)</code> for Output:</strong> The output <code>y(t) = o(t) ⊗ tanh(c(t))</code>. The long-term state <code>c(t)</code> is passed through <code>tanh</code> before being gated by <code>o(t)</code>.</p></li><li><p><strong>Tanh Output Range (-1 to 1):</strong></p><ul><li>The <code>tanh</code> function squashes its input to a range between -1 and 1.</li><li><em>What this achieves for <code>g(t)</code> (candidate values):</em> It keeps the potential updates to the cell state bounded. This helps prevent the cell state <code>c(t)</code> from growing uncontrollably large (exploding).</li><li><em>What this achieves for <code>tanh(c(t))</code> before output:</em> It ensures that the values being considered for output are also in a bounded range before the output gate decides how much of it to pass through.</li></ul></li><li><p><strong>Zero-Centered Output:</strong> The output of <code>tanh</code> is roughly centered around 0. This can be beneficial for learning in subsequent layers or time steps, as discussed in Chapter 10 (it&rsquo;s a property that sometimes helps with convergence speed compared to sigmoid which is centered at 0.5).</p></li><li><p><strong>Differentiability:</strong> Tanh is also differentiable.</p></li><li><p><strong>What if we used Sigmoid for <code>g(t)</code> or <code>tanh(c(t))</code>?</strong></p><ul><li>Sigmoid outputs 0 to 1. If you continually add positive values (from sigmoid) to the cell state, the cell state could still grow very large without bound (unless the forget gate is very aggressive).</li><li>Tanh allowing for both positive and negative values (and being zero-centered) gives the network more flexibility to <em>increment or decrement</em> values in the cell state <code>c(t)</code> and to output values that are centered around zero. This can lead to more stable dynamics.</li></ul></li><li><p><strong>What if we used ReLU for <code>g(t)</code> or <code>tanh(c(t))</code>?</strong></p><ul><li>For <code>g(t)</code>: If ReLU were used, <code>g(t)</code> could be unbounded positive. If the input gate <code>i(t)</code> is open, this could lead to the cell state <code>c(t)</code> growing indefinitely, causing exploding values. LSTMs are designed to <em>prevent</em> this kind of instability that plagues simple RNNs with ReLU.</li><li>For <code>tanh(c(t))</code> before output: Using ReLU here might also be problematic if <code>c(t)</code> can take on large values, as the output <code>h(t)</code> could then become unbounded before being used by subsequent layers or calculations. The <code>tanh</code> here provides a final &ldquo;squashing&rdquo; to keep things in a controlled range.</li></ul></li></ul><p><strong>In Summary:</strong></p><ul><li><strong>Sigmoid for Gates (0 to 1 range):</strong><ul><li><strong>Ultimately trying to achieve:</strong> A probabilistic-like control over information flow – &ldquo;block&rdquo; (0), &ldquo;pass through&rdquo; (1), or &ldquo;partially pass through&rdquo; (between 0 and 1). This is ideal for multiplicative gating.</li></ul></li><li><strong>Tanh for Candidate Values and Pre-Output Processing (-1 to 1 range):</strong><ul><li><strong>Ultimately trying to achieve:</strong> Keep the values that modify the cell state and the values considered for output bounded and roughly zero-centered. This contributes to the stability of the LSTM cell and helps prevent the cell state from exploding, which is a key part of solving the long-term memory problem.</li></ul></li></ul><p>The choice of these specific activation functions is a result of careful design and empirical evidence showing that this combination allows LSTMs to effectively learn, store, and retrieve information over long sequences while maintaining training stability. They are not arbitrary choices but rather integral to the LSTM&rsquo;s mechanism for managing information flow and memory.</p><h2 id=gru-cells---gated-recurrent-unit>GRU Cells - Gated Recurrent Unit</h2><ul><li><p><strong>Proposed by Kyunghyun Cho et al. in 2014</strong> (in the same paper that introduced the Encoder-Decoder architecture we briefly touched upon).</p></li><li><p><strong>What it is:</strong> A simplified version of the LSTM cell.</p></li><li><p><strong>Performance:</strong> It seems to perform just as well as LSTMs on many tasks (footnote 12 refers to a 2015 paper by Greff et al. that found LSTM variants perform roughly the same). This, along with its simplicity, explains its growing popularity.</p></li><li><p><strong>Main Simplifications Compared to LSTM (Figure 15-10, page 519):</strong></p><ol><li><strong>Merged State Vectors:</strong> Both state vectors (<code>c(t)</code> for long-term and <code>h(t)</code> for short-term in LSTM) are merged into a <strong>single state vector <code>h(t)</code></strong> in GRU.</li><li><strong>Single &ldquo;Update&rdquo; Gate <code>z(t)</code> controls both Forget and Input:</strong><ul><li>LSTM has a separate forget gate <code>f(t)</code> and an input gate <code>i(t)</code>.</li><li>GRU has a single <strong>update gate <code>z(t)</code></strong>.</li><li>If <code>z(t)</code> is close to 1 (gate &ldquo;open&rdquo; for update), it means &ldquo;forget the previous state and update with the new candidate state.&rdquo; Specifically, <code>h(t) = z(t) ⊗ h(t-1) + (1-z(t)) ⊗ g(t)</code>.<ul><li>If <code>z(t) ≈ 1</code>: <code>h(t) ≈ 1 ⊗ h(t-1) + 0 ⊗ g(t) ≈ h(t-1)</code> (keep old state, ignore candidate). <em>Correction: The equation in the book and standard GRU is <code>h(t) = (1-z(t)) ⊗ h(t-1) + z(t) ⊗ g(t)</code>. So if <code>z(t) ≈ 1</code> (update gate is &ldquo;active for new candidate&rdquo;), then <code>h(t) ≈ 0 ⊗ h(t-1) + 1 ⊗ g(t) ≈ g(t)</code> (take the new candidate). If <code>z(t) ≈ 0</code>, then <code>h(t) ≈ 1 ⊗ h(t-1) + 0 ⊗ g(t) ≈ h(t-1)</code> (keep old state).</em></li><li><em>What the update gate <code>z(t)</code> is ultimately trying to achieve:</em> It learns to decide how much of the previous state <code>h(t-1)</code> to keep versus how much of the new candidate state <code>g(t)</code> to incorporate.</li></ul></li></ul></li><li><strong>No Output Gate:</strong> The full state vector <code>h(t)</code> is output at every time step. (In LSTM, the output gate <code>o(t)</code> controlled which parts of <code>tanh(c(t))</code> were output as <code>h(t)</code>).</li><li><strong>New &ldquo;Reset&rdquo; Gate <code>r(t)</code>:</strong><ul><li>There&rsquo;s a new gate controller <code>r(t)</code> (also using sigmoid, outputting 0 to 1).</li><li>This gate controls which part of the <em>previous state <code>h(t-1)</code></em> will be shown to the main layer <code>g(t)</code> when calculating the candidate state.</li><li>The candidate state is <code>g(t) = tanh(W_xgᵀx(t) + W_hgᵀ(r(t) ⊗ h(t-1)) + b_g)</code>.</li><li>If <code>r(t)</code> is close to 0 for some components, those components of <code>h(t-1)</code> are effectively ignored when computing the new candidate <code>g(t)</code>.</li><li><em>What the reset gate <code>r(t)</code> is ultimately trying to achieve:</em> It allows the cell to &ldquo;forget&rdquo; or ignore parts of its previous state that are deemed irrelevant for computing the <em>next candidate state</em>, before deciding how to update the state with that candidate.</li></ul></li></ol></li><li><p><strong>Equation 15-4: GRU Computations (Page 520):</strong>
Summarizes the math for a single instance.</p><ul><li><code>z(t) = σ(W_xzᵀx(t) + W_hzᵀh(t-1) + b_z)</code> (Update gate)</li><li><code>r(t) = σ(W_xrᵀx(t) + W_hrᵀh(t-1) + b_r)</code> (Reset gate)</li><li><code>g(t) = tanh(W_xgᵀx(t) + W_hgᵀ(r(t) ⊗ h(t-1)) + b_g)</code> (Candidate hidden state)</li><li><code>h(t) = (1-z(t)) ⊗ h(t-1) + z(t) ⊗ g(t)</code> (Final hidden state for the time step)</li><li><strong>Parameters to Learn:</strong> <code>W_xz, W_hz, b_z</code>, <code>W_xr, W_hr, b_r</code>, and <code>W_xg, W_hg, b_g</code>. Notice fewer weight matrices than LSTM because there are fewer gates and only one state vector.</li></ul></li><li><p><strong>Keras Implementation:</strong></p><ul><li>Keras provides a <code>keras.layers.GRU</code> layer (based on <code>keras.layers.GRUCell</code>).</li><li>Using it is just a matter of replacing <code>SimpleRNN</code> or <code>LSTM</code> with <code>GRU</code> in your <code>Sequential</code> model.</li></ul></li><li><p><strong>Why LSTMs and GRUs are Successful:</strong></p><ul><li>&ldquo;LSTM and GRU cells are one of the main reasons behind the success of RNNs.&rdquo;</li><li>They can tackle much longer sequences than simple RNNs.</li><li>However, they still have fairly limited short-term memory and can have a hard time learning long-term patterns in sequences of 100 time steps or more (e.g., very long audio samples, long time series, long sentences).</li></ul></li></ul><p><strong>One way to help LSTMs/GRUs handle even longer sequences is to shorten the input sequences they directly see, for example, by using 1D convolutional layers first.</strong></p><p><strong>Key Takeaway for GRUs:</strong>
GRUs are a streamlined version of LSTMs that often perform just as well but are computationally simpler (fewer parameters, one state vector).</p><ul><li>They use an <strong>update gate</strong> (to decide how much of the past state to keep vs. new candidate state) and a <strong>reset gate</strong> (to decide how much of the past state influences the current candidate state).</li><li><em>What GRUs are ultimately trying to achieve:</em> Similar to LSTMs, they aim to capture dependencies over longer sequences by using gating mechanisms to control information flow and memory, but with a more compact architecture.</li></ul><p>This covers the main advanced recurrent cell types (LSTM and GRU) designed to combat the short-term memory problem of simple RNNs. The next step in the chapter is to see how to combine these with other types of layers, like 1D convolutions, to process sequences even more effectively.</p><h2 id=using-1d-convolutional-layers-to-process-sequences>Using 1D Convolutional Layers to Process Sequences</h2><p>While LSTMs and GRUs are much better than simple RNNs at handling longer sequences, they can still struggle with very long dependencies (e.g., hundreds or thousands of time steps, like in raw audio or very long text documents).</p><ul><li><p><strong>The Idea:</strong> One way to solve this is to <strong>shorten the input sequences</strong> before feeding them to the recurrent layers. How? By using <strong>1D convolutional layers</strong>.</p></li><li><p><strong>Recall 2D Convolutional Layers (from Chapter 14, though not yet covered by us in detail):</strong></p><ul><li>They work by sliding small kernels (filters) across an image (a 2D grid of pixels).</li><li>They produce multiple 2D feature maps (one per kernel).</li><li>Each kernel learns to detect a specific local pattern (e.g., an edge, a texture).</li></ul></li><li><p><strong>1D Convolutional Layers for Sequences:</strong></p><ul><li>Similarly, a <strong>1D convolutional layer</strong> slides several kernels (filters) across an input sequence (a 1D array of features over time).</li><li>It produces a 1D feature map per kernel.</li><li><em>What each 1D kernel is ultimately trying to achieve:</em> It learns to detect a single, very short sequential pattern (no longer than the kernel size). For example, if processing text character by character, a kernel of size 3 might learn to detect common trigrams like &ldquo;the&rdquo; or &ldquo;ing&rdquo;.</li><li>If you use, say, 10 kernels, the layer&rsquo;s output will be composed of 10 1-dimensional sequences (feature maps). You can view this output as a single sequence where each time step now has 10 features (instead of, say, 1 feature if it was a univariate time series).</li></ul></li><li><p><strong>Building Hybrid Networks:</strong></p><ul><li>This means you can build neural networks composed of a mix of:<ul><li>Recurrent layers (LSTMs, GRUs).</li><li>1D convolutional layers.</li><li>Even 1D pooling layers (which downsample the sequence, similar to 2D pooling for images).</li></ul></li></ul></li><li><p><strong>Impact on Sequence Length:</strong></p><ul><li>If a 1D convolutional layer uses a <code>stride</code> of 1 and <code>"same"</code> padding, the output sequence will have the same length as the input sequence.</li><li>If it uses <code>"valid"</code> padding or a <code>stride</code> greater than 1, then the output sequence will be <strong>shorter</strong> than the input sequence.<ul><li>This <strong>downsampling</strong> of the sequence by the convolutional layer is the key benefit here.</li><li>The convolutional layer can learn to preserve useful local information while dropping unimportant details, effectively compressing the sequence.</li><li>The subsequent recurrent layers then have a shorter sequence to process, making it easier for them to detect longer-range patterns within that compressed representation.</li></ul></li></ul></li><li><p><strong>Keras Example (Page 521):</strong>
The book shows modifying the time series forecasting model to include a 1D convolutional layer at the beginning.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv1D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, strides<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;valid&#34;</span>,
</span></span><span style=display:flex><span>                        input_shape<span style=color:#f92672>=</span>[<span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>1</span>]), <span style=color:#75715e># Input is [batch, time_steps, features]</span>
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>GRU(<span style=color:#ae81ff>20</span>, return_sequences<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>GRU(<span style=color:#ae81ff>20</span>, return_sequences<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>TimeDistributed(keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>10</span>)) <span style=color:#75715e># Assuming seq-to-seq for 10-step forecast</span>
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><ul><li><code>keras.layers.Conv1D(...)</code>:<ul><li><code>filters=20</code>: It will learn 20 different short patterns.</li><li><code>kernel_size=4</code>: Each pattern is 4 time steps long.</li><li><code>strides=2</code>: The kernel slides by 2 steps each time, effectively downsampling the input sequence by a factor of 2.</li><li><code>padding="valid"</code>: No padding, so output sequence length will be <code>(input_length - kernel_size) / strides + 1</code>.</li><li><code>input_shape=[None, 1]</code>: Takes sequences of any length, with 1 feature per time step.</li></ul></li><li><strong>Target Adjustment:</strong> Because the <code>Conv1D</code> layer shortens the sequence (and the first output corresponds to input steps 0-3), the target sequences <code>Y_train</code> need to be adjusted (cropped and downsampled) to match the output length and alignment of the <code>Conv1D</code> layer. The example <code>Y_train[:, 3::2]</code> does this:<ul><li><code>3::</code>: Skips the first 3 time steps in the target (because the first Conv1D output depends on inputs 0-3, so it effectively predicts starting from a later point relative to the original targets).</li><li><code>::2</code>: Downsamples the targets by a factor of 2 to match the <code>strides=2</code>.</li></ul></li></ul></li><li><p><strong>Result:</strong> The book states that if you train and evaluate this model, &ldquo;you will find that it is the best model so far. The convolutional layer really helps.&rdquo;</p></li><li><p>It even mentions that it&rsquo;s possible to use <em>only</em> 1D convolutional layers (and drop recurrent layers entirely) for sequence processing, which leads to the WaveNet architecture.</p></li></ul><p><strong>Key Takeaway for 1D Convolutions in Sequence Processing:</strong>
1D convolutional layers can be used as a preprocessing step for RNNs (or even as a replacement for them in some cases like WaveNet).</p><ul><li><em>What they are ultimately trying to achieve when used with RNNs:</em><ul><li><strong>Feature Extraction:</strong> Learn to detect relevant local patterns in the input sequence.</li><li><strong>Downsampling/Shortening Sequences:</strong> By using strides, they can reduce the length of the sequence fed into the RNN layers. This allows the RNNs (like LSTMs/GRUs) to focus their memory capacity on longer-range dependencies within a more compressed, feature-rich representation of the original sequence.</li></ul></li></ul><p><strong>(Page 521-522: WaveNet)</strong></p><p>This is an example of a powerful architecture that uses <em>only</em> 1D convolutional layers (no recurrent layers) to process very long sequences.</p><ul><li><p>Introduced in a 2016 paper by van den Oord et al. (DeepMind) for generative modeling of raw audio.</p></li><li><p><strong>Architecture (Figure 15-11, page 522):</strong></p><ul><li>It stacks 1D convolutional layers.</li><li>Crucially, it <strong>doubles the dilation rate</strong> at every layer.<ul><li><strong>Dilation Rate:</strong> Controls how spread apart the kernel&rsquo;s input taps are.<ul><li>Layer 1: Dilation rate 1 (looks at, say, 2 adjacent time steps).</li><li>Layer 2: Dilation rate 2 (looks at inputs 2 steps apart, e.g., time <code>t</code> and <code>t-2</code>). Its receptive field is wider.</li><li>Layer 3: Dilation rate 4 (looks at inputs 4 steps apart).</li><li>And so on (1, 2, 4, 8, 16, &mldr;).</li></ul></li><li><em>What doubling dilation rates achieves:</em> The receptive field of the network (how far back in time it can &ldquo;see&rdquo;) grows exponentially with depth. This allows the network to capture very long-range dependencies efficiently.<ul><li>Lower layers learn short-term patterns.</li><li>Higher layers learn long-term patterns by combining features from lower layers that already cover wider time spans.</li></ul></li></ul></li><li>The paper stacked blocks of 10 such convolutional layers (dilation rates 1, 2, &mldr;, 512), and repeated these blocks.</li><li>Used &ldquo;causal&rdquo; padding to ensure convolutions don&rsquo;t peek into the future.</li></ul></li><li><p><strong>Keras Implementation of a Simplified WaveNet (Page 522):</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>Sequential()
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>add(keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>InputLayer(input_shape<span style=color:#f92672>=</span>[<span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>1</span>]))
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> rate <span style=color:#f92672>in</span> (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>8</span>) <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>: <span style=color:#75715e># Two blocks of these dilation rates</span>
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>add(keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv1D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;causal&#34;</span>,
</span></span><span style=display:flex><span>                                   activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>, dilation_rate<span style=color:#f92672>=</span>rate))
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>add(keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv1D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)) <span style=color:#75715e># Output layer</span>
</span></span></code></pre></div><ul><li><code>padding="causal"</code>: Ensures that the output at time <code>t</code> only depends on inputs up to time <code>t</code> (no future peeking).</li><li>The output layer is a <code>Conv1D</code> with <code>kernel_size=1</code>. This is like applying a <code>Dense</code> layer independently at each time step to the features extracted by the dilated convolutions.</li></ul></li><li><p><strong>Performance:</strong></p><ul><li>The book states that these last two models (RNN with initial Conv1D, and the simplified WaveNet) offer the best performance so far on the time series forecasting task.</li><li>The original WaveNet achieved state-of-the-art on audio generation (text-to-speech, music), handling tens of thousands of time steps per second of audio. This is something LSTMs/GRUs would struggle immensely with.</li></ul></li></ul><p><strong>Key Takeaway for WaveNet:</strong>
WaveNet demonstrates that stacked 1D convolutional layers with exponentially increasing dilation rates can be extremely effective at capturing long-range dependencies in sequences, offering an alternative to RNNs for very long sequences.</p><ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> Efficiently model long-term patterns by having a hierarchy of convolutions that look at progressively larger time scales, without the computational cost of very large kernels or the memory limitations of very deep unrolled RNNs.</li></ul><p>This concludes the main content of Chapter 15! It&rsquo;s taken us from the basic concept of recurrence to sophisticated cells like LSTMs and GRUs, and finally to using CNNs (either with RNNs or alone like WaveNet) for sequence processing.</p><p>The core theme has been how to enable neural networks to effectively &ldquo;remember&rdquo; and utilize information from previous parts of a sequence to make current predictions or classifications.</p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script></body></html>