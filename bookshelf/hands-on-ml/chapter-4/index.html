<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul></nav></header><main><article class=book-single><h1>Chapter 4: Training Models</h1><span class=reading-time><em>41 min read</em></span><div class=book-details><div class=book-content><p><strong>Chapter 4: Training Models</strong>.</p><p><strong>(Page 111: Introduction - Beyond Black Boxes)</strong></p><p>Up until now, as the book says, we&rsquo;ve treated ML models and their training algorithms mostly like black boxes. We fed them data, they gave us results, and we learned to evaluate those results. You&rsquo;ve optimized regression, improved classifiers, even built a spam filter, often without peeking under the hood. And that&rsquo;s okay for many practical purposes!</p><p>However, understanding <em>how</em> these things work internally is incredibly powerful. It helps you:</p><ul><li><strong>Choose the right model and algorithm:</strong> Knowing the mechanics helps you match the tool to the task.</li><li><strong>Select good hyper parameters:</strong> Hyper parameters often control the learning process itself. Understanding that process helps you tune them effectively.</li><li><strong>Debug issues and perform error analysis:</strong> When things go wrong, or your model makes weird mistakes, knowing the &ldquo;why&rdquo; is crucial.</li><li><strong>Foundation for advanced topics:</strong> Especially for neural networks (Part II of the book), the concepts here are fundamental.</li></ul><p>This chapter focuses on <strong>Linear Regression</strong> as a starting point because it&rsquo;s simple, yet we can train it in very different ways. We&rsquo;ll explore two main approaches:</p><ol><li><strong>A direct &ldquo;closed-form&rdquo; equation (The Normal Equation):</strong> This is like having a magic formula that directly spits out the best model parameters in one go.</li><li><strong>An iterative optimization approach (Gradient Descent):</strong> This is more like a trial-and-error process. We start with a guess for the parameters and gradually tweak them, step by step, to minimize the error, eventually (hopefully!) arriving at the same best parameters. We&rsquo;ll see different &ldquo;flavors&rdquo; of Gradient Descent: Batch, Mini-batch, and Stochastic.</li></ol><p>Then, the chapter will touch on:</p><ul><li><strong>Polynomial Regression:</strong> How to use linear models for non-linear data.</li><li><strong>Learning Curves:</strong> Tools to diagnose over fitting or under fitting.</li><li><strong>Regularization:</strong> Techniques to prevent over fitting.</li><li><strong>Logistic Regression and Softmax Regression:</strong> Models commonly used for classification tasks.</li></ul><p>The scorpion icon on page 112 gives a fair warning: there will be some math (linear algebra, calculus). If you&rsquo;re &ldquo;allergic,&rdquo; the book suggests you can still get the concepts by focusing on the text. My job is to make sure you get those concepts, regardless of how comfortable you are with the equations. We&rsquo;ll always ask: <strong>&ldquo;What is this equation ultimately trying to achieve?&rdquo;</strong></p><p><strong>(Page 112-113: Linear Regression - The Model)</strong></p><p>Remember our life satisfaction model from Chapter 1? <code>life_satisfaction = θ₀ + θ₁ × GDP_per_capita</code>. That was a simple linear regression with one feature.</p><p>More generally, a linear model predicts a value by:</p><ol><li>Taking all the input features (like a house&rsquo;s square footage, number of bedrooms, age, etc.).</li><li>Multiplying each feature by a specific <strong>weight</strong> (a model parameter).</li><li>Summing up these weighted features.</li><li>Adding a constant <strong>bias term</strong> (another model parameter, also called the intercept).</li></ol><p><strong>Equation 4-1 (page 112): Linear Regression model prediction</strong>
<code>ŷ = θ₀ + θ₁x₁ + θ₂x₂ + ⋯ + θₙxₙ</code></p><ul><li><code>ŷ</code> (y-hat): The predicted value.</li><li><code>n</code>: The number of features.</li><li><code>xᵢ</code>: The value of the i-th feature.</li><li><code>θ₀</code>: The bias term (theta-zero). <em>What it&rsquo;s ultimately trying to achieve:</em> It&rsquo;s the baseline prediction if all feature values were zero. It allows the line/plane to shift up or down.</li><li><code>θ₁</code> to <code>θₙ</code>: The feature weights. <code>θⱼ</code> is the weight for the j-th feature. <em>What they&rsquo;re ultimately trying to achieve:</em> They represent how much a one-unit change in that feature <code>xⱼ</code> affects the predicted value <code>ŷ</code>, holding other features constant. A positive weight means the feature positively influences the prediction; a negative weight means it negatively influences it. The magnitude shows the strength of the influence.</li></ul><p><strong>Equation 4-2 (page 113): Vectorized form</strong>
<code>ŷ = h_θ(x) = θ · x</code></p><p>This is just a more compact way to write Equation 4-1 using vector notation.</p><ul><li><code>θ</code> (theta): Is now a <strong>parameter vector</strong> <code>[θ₀, θ₁, ..., θₙ]</code>.</li><li><code>x</code>: Is the <strong>instance&rsquo;s feature vector</strong> <code>[x₀, x₁, ..., xₙ]</code>. Here, we add a &ldquo;dummy&rdquo; feature <code>x₀</code> which is always set to 1. This allows us to include the bias term <code>θ₀</code> neatly into the dot product (because <code>θ₀ * x₀ = θ₀ * 1 = θ₀</code>).</li><li><code>θ · x</code>: This is the <strong>dot product</strong> of the two vectors. It&rsquo;s exactly the sum <code>θ₀x₀ + θ₁x₁ + ... + θₙxₙ</code>.</li><li><code>h_θ(x)</code>: This is our <strong>hypothesis function</strong> (our model), parameterized by <code>θ</code>. Given an input <code>x</code>, it predicts <code>ŷ</code>.</li></ul><p>The bird sidebar (page 113) explains that vectors are often column vectors (2D arrays with one column). So, if <code>θ</code> and <code>x</code> are column vectors, the dot product can be written as a matrix multiplication: <code>ŷ = θᵀx</code> (where <code>θᵀ</code> is the transpose of <code>θ</code>, making it a row vector). Don&rsquo;t let this bog you down; it&rsquo;s a notational convenience. <strong>The goal is the same: calculate a weighted sum of features plus a bias.</strong></p><p><strong>How do we train it?</strong>
Training means finding the values for the parameters <code>θ</code> (the bias <code>θ₀</code> and the weights <code>θ₁</code> to <code>θₙ</code>) that make the model &ldquo;best fit&rdquo; the training data.</p><p>To do this, we need a way to measure how well (or poorly) the model fits. We learned in Chapter 2 that for regression, a common measure is <strong>RMSE (Root Mean Square Error)</strong>.</p><ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> Quantify the typical prediction error.
However, for mathematical convenience during training, it&rsquo;s often easier to minimize the <strong>MSE (Mean Squared Error)</strong> instead. Minimizing MSE will also minimize RMSE (since the square root function is monotonic).
The footnote on page 113 is important: the function we optimize during training (the <em>cost function</em>, here MSE) might be different from the final performance metric we use to evaluate the model (e.g., RMSE). This is often because the cost function has nice mathematical properties (like being easily differentiable) that make optimization easier.</li></ul><p><strong>(Page 114: MSE Cost Function & The Normal Equation)</strong></p><p><strong>Equation 4-3 (page 114): MSE cost function for a Linear Regression model</strong>
<code>MSE(X, h_θ) = (1/m) * Σᵢ (θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾)²</code>
(summing from i=1 to m, where m is the number of instances)</p><ul><li><em>What it&rsquo;s ultimately trying to achieve:</em><ul><li>For each training instance <code>i</code>:<ul><li><code>θᵀx⁽ⁱ⁾</code> is the model&rsquo;s prediction for that instance.</li><li><code>y⁽ⁱ⁾</code> is the actual target value.</li><li><code>(θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾)</code> is the error for that instance.</li><li>We square this error: <code>(error)²</code>. (Why square? It makes all errors positive, and it penalizes larger errors more heavily).</li></ul></li><li>We sum these squared errors over all <code>m</code> training instances: <code>Σᵢ (error)²</code>.</li><li>We divide by <code>m</code> to get the <em>mean</em> of the squared errors.
This function tells us, on average, how &ldquo;bad&rdquo; our model&rsquo;s predictions are for a given set of parameters <code>θ</code>. Our goal in training is to find the <code>θ</code> that makes this MSE as small as possible.</li></ul></li></ul><p><strong>The Normal Equation: A Direct Solution</strong></p><p>For Linear Regression with an MSE cost function, there&rsquo;s a wonderful mathematical shortcut. Instead of iteratively searching for the best <code>θ</code>, there&rsquo;s a direct formula that gives you the <code>θ</code> that minimizes the cost function in one shot! This is called the <strong>Normal Equation</strong>.</p><p><strong>Equation 4-4 (page 114): Normal Equation</strong>
<code>θ̂ = (XᵀX)⁻¹ Xᵀy</code></p><ul><li><code>θ̂</code> (theta-hat): This is the value of <code>θ</code> that minimizes the cost function.</li><li><code>X</code>: The matrix of input features for all training instances (each row is an instance, <code>x₀</code> for each instance is 1).</li><li><code>y</code>: The vector of actual target values for all training instances.</li><li><code>Xᵀ</code>: The transpose of matrix <code>X</code>.</li><li><code>(XᵀX)⁻¹</code>: The inverse of the matrix <code>XᵀX</code>.</li></ul><p><em>What it&rsquo;s ultimately trying to achieve:</em> This equation, derived using calculus (setting the derivative of the cost function to zero and solving for θ), directly calculates the optimal parameter vector <code>θ̂</code> that makes the linear model fit the training data as closely as possible (in the mean squared error sense). It&rsquo;s like a direct recipe: plug in your data <code>X</code> and <code>y</code>, and out pops the best <code>θ</code>.</p><p><strong>(Page 115: Testing the Normal Equation)</strong></p><p>The book generates some linear-looking data (Figure 4-1):
<code>X = 2 * np.random.rand(100, 1)</code> (100 instances, 1 feature)
<code>y = 4 + 3 * X + np.random.randn(100, 1)</code> (True model is <code>y = 4 + 3x₁ + noise</code>)
So, the ideal <code>θ₀</code> is 4, and the ideal <code>θ₁</code> is 3.</p><p>To use the Normal Equation, we need to add <code>x₀ = 1</code> to each instance in <code>X</code>:
<code>X_b = np.c_[np.ones((100, 1)), X]</code> (<code>np.c_</code> concatenates arrays column-wise)</p><p>Now, apply the Normal Equation:
<code>theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</code>
The result is something like <code>[[4.215...], [2.770...]]</code>.
So, <code>θ₀̂ ≈ 4.215</code> and <code>θ₁̂ ≈ 2.770</code>.
It&rsquo;s close to the true values (4 and 3), but not exact because of the random noise we added to <code>y</code>. The noise makes it impossible to recover the exact original parameters.</p><p>We can then use this <code>theta_best</code> to make predictions (Figure 4-2).</p><p><strong>(Page 116-117: Scikit-Learn and Computational Complexity)</strong></p><ul><li><p><strong>Scikit-Learn <code>LinearRegression</code>:</strong>
<code>from sklearn.linear_model import LinearRegression</code>
<code>lin_reg = LinearRegression()</code>
<code>lin_reg.fit(X, y)</code>
Scikit-Learn handles adding the <code>x₀=1</code> feature (or rather, it separates the bias term <code>lin_reg.intercept_</code> from the feature weights <code>lin_reg.coef_</code>). The results are the same as the Normal Equation.</p></li><li><p><strong>Underlying Method (<code>scipy.linalg.lstsq</code>):</strong>
Scikit-Learn&rsquo;s <code>LinearRegression</code> actually uses <code>scipy.linalg.lstsq()</code> (&ldquo;least squares&rdquo;). This function computes <code>θ̂ = X⁺y</code>, where <code>X⁺</code> is the <strong>pseudoinverse</strong> (or Moore-Penrose inverse) of <code>X</code>.
You can compute <code>X⁺</code> using <code>np.linalg.pinv()</code>.
The pseudoinverse is calculated using a technique called <strong>Singular Value Decomposition (SVD)</strong>.</p><ul><li><em>Why SVD/pseudoinverse instead of the direct Normal Equation <code>(XᵀX)⁻¹ Xᵀy</code>?</em><ol><li><strong>More efficient:</strong> SVD is generally more computationally efficient.</li><li><strong>Handles edge cases:</strong> The Normal Equation requires <code>XᵀX</code> to be invertible. If it&rsquo;s not (e.g., if you have more features than instances, <code>m &lt; n</code>, or if some features are redundant/linearly dependent), the Normal Equation breaks down. The pseudoinverse is <em>always</em> defined, making SVD more robust.</li></ol></li></ul></li><li><p><strong>Computational Complexity:</strong></p><ul><li><strong>Normal Equation (inverting <code>XᵀX</code>):</strong> About O(n²·⁴) to O(n³) where <code>n</code> is the number of features. This gets very slow if you have many features (e.g., 100,000 features). Doubling features can increase time by 5-8x.</li><li><strong>SVD (used by Scikit-Learn):</strong> About O(n²). Doubling features increases time by ~4x. Still slow for very large <code>n</code>.</li><li><strong>Both</strong> are O(m) with respect to the number of instances <code>m</code>. So, they handle large numbers of training instances efficiently, <em>as long as the data fits in memory</em>.</li><li><strong>Predictions:</strong> Once trained, making predictions is very fast: O(m) and O(n) – linear with number of instances and features.</li></ul></li></ul><p><strong>The Problem with Normal Equation/SVD:</strong> They get slow with many features and require all data to be in memory. This leads us to the next method&mldr;</p><p><strong>(Page 118-123: Gradient Descent - The Iterative Approach)</strong></p><p>When the Normal Equation is too slow (too many features) or the dataset is too large to fit in memory, we need a different approach. Enter <strong>Gradient Descent</strong>.</p><ul><li><p><strong>The Core Idea:</strong> Gradient Descent is a generic optimization algorithm. It iteratively tweaks model parameters to minimize a cost function.</p><ul><li>Imagine you&rsquo;re lost in a foggy mountain valley. You can only feel the slope of the ground under your feet. To get to the bottom, you&rsquo;d take a step in the direction of the steepest downhill slope. Repeat.</li><li>This is Gradient Descent:<ol><li>It measures the <strong>local gradient</strong> of the cost function (e.g., MSE) with respect to the parameter vector <code>θ</code>. The gradient tells you the direction of steepest <em>ascent</em>.</li><li>It takes a step in the <em>opposite</em> direction (descending gradient) to reduce the cost.</li><li>Repeat until the gradient is zero (or very close), meaning you&rsquo;ve reached a minimum.</li></ol></li></ul></li><li><p><strong>The Process (Figure 4-3, page 118):</strong><figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-3.png alt="Figure 4-3" width=700></figure></p><ol><li><strong>Random Initialization:</strong> Start with random values for <code>θ</code>.</li><li><strong>Iterative Improvement:</strong> In each step:<ul><li>Calculate the gradient of the cost function at the current <code>θ</code>.</li><li>Update <code>θ</code> by taking a small step in the negative gradient direction.</li></ul></li><li><strong>Convergence:</strong> Continue until the algorithm converges to a minimum (cost stops decreasing significantly).</li></ol></li><li><p><strong>Learning Rate (η - eta):</strong></p><ul><li>This is a crucial hyperparameter that determines the <em>size</em> of the steps.</li><li><strong>Too small (Figure 4-4):</strong> Many iterations needed to converge (very slow).<figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-4.png alt="Figure 4-4" width=700></figure></li><li><strong>Too large (Figure 4-5):</strong> Might jump across the valley, diverge, and fail to find a solution.<figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-5.png alt="Figure 4-5" width=700></figure></li></ul></li><li><p><strong>Challenges (Figure 4-6, page 119):</strong></p><ul><li><strong>Local Minima:</strong> If the cost function has multiple minima (not a smooth bowl), GD might converge to a <em>local minimum</em>, which isn&rsquo;t as good as the <em>global minimum</em>.</li><li><strong>Plateaus:</strong> If the cost function has flat areas, GD can take a very long time to cross them.</li><li><strong>Irregular Terrains:</strong> Holes, ridges make convergence difficult.</li></ul></li><li><p><strong>Good News for Linear Regression MSE:</strong></p><ul><li>The MSE cost function for Linear Regression is a <strong>convex function</strong>.<ul><li><em>What this means:</em> It&rsquo;s shaped like a bowl. It has <em>no local minima</em>, only one global minimum.</li><li>It&rsquo;s also continuous with a slope that doesn&rsquo;t change abruptly.</li></ul></li><li><strong>Consequence:</strong> Gradient Descent is <em>guaranteed</em> to approach the global minimum if you wait long enough and the learning rate isn&rsquo;t too high.</li></ul></li><li><p><strong>Feature Scaling Matters! (Figure 4-7, page 120):</strong></p><ul><li>If features have very different scales (e.g., feature 1 ranges 0-1, feature 2 ranges 0-1000), the cost function &ldquo;bowl&rdquo; becomes elongated.</li><li>GD will take a long, zig-zag path to the minimum.</li><li><strong>Solution:</strong> Ensure all features have a similar scale (e.g., using <code>StandardScaler</code>). GD will then converge much faster.</li></ul></li><li><p><strong>Parameter Space:</strong> Training a model is essentially a search in the model&rsquo;s <em>parameter space</em> for the combination of parameters that minimizes the cost. More parameters = higher dimensional space = harder search. For Linear Regression (convex cost), it&rsquo;s like finding the bottom of a D-dimensional bowl.</p></li></ul><p><strong>(Page 121-123: Batch Gradient Descent - BGD)</strong></p><p>To implement GD, we need the gradient of the cost function with respect to <em>each</em> model parameter <code>θⱼ</code>. This is the <strong>partial derivative</strong> <code>∂MSE(θ) / ∂θⱼ</code>.</p><ul><li><p><strong>Equation 4-5 (page 121): Partial derivative of MSE w.r.t. <code>θⱼ</code></strong>
<code>∂MSE(θ)/∂θⱼ = (2/m) * Σᵢ (θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾) * xⱼ⁽ⁱ⁾</code></p><ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> For each parameter <code>θⱼ</code>, it calculates how much the MSE would change if <code>θⱼ</code> changed a tiny bit.<ul><li><code>(θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾)</code> is the prediction error for instance <code>i</code>.</li><li>We multiply this error by the value of the <code>j</code>-th feature of instance <code>i</code>, <code>xⱼ⁽ⁱ⁾</code>. (If <code>xⱼ⁽ⁱ⁾</code> is large, <code>θⱼ</code> has a bigger impact on the prediction and thus on the error).</li><li>We average this product over all instances <code>m</code>.</li></ul></li></ul></li><li><p><strong>Equation 4-6 (page 122): Gradient vector <code>∇_θ MSE(θ)</code></strong>
<code>∇_θ MSE(θ) = (2/m) Xᵀ(Xθ - y)</code></p><ul><li>This is the compact, vectorized way to compute all partial derivatives at once. <code>∇_θ MSE(θ)</code> is a vector containing <code>∂MSE(θ)/∂θ₀</code>, <code>∂MSE(θ)/∂θ₁</code>, &mldr;, <code>∂MSE(θ)/∂θₙ</code>.</li><li><em>What it&rsquo;s ultimately trying to achieve:</em> It gives the direction of steepest <em>increase</em> in the cost function.</li></ul><p><strong>Crucial point for BATCH GD:</strong> This formula uses the <em>entire training set <code>X</code></em> at each step to calculate the gradients. This is why it&rsquo;s called <strong>Batch Gradient Descent</strong>.</p><ul><li>Consequence: Terribly slow on very large training sets.</li><li>Advantage: Scales well with the number of features (unlike Normal Equation).</li></ul></li><li><p><strong>Equation 4-7 (page 122): Gradient Descent step</strong>
<code>θ⁽ⁿᵉˣᵗ ˢᵗᵉᵖ⁾ = θ - η ∇_θ MSE(θ)</code></p><ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> Update the current parameters <code>θ</code> by taking a step of size <code>η</code> (learning rate) in the direction <em>opposite</em> to the gradient (downhill).</li></ul></li><li><p><strong>Implementation (page 122):</strong>
The code shows a loop:
<code>for iteration in range(n_iterations):</code>
<code>gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)</code>
<code>theta = theta - eta * gradients</code>
With <code>eta = 0.1</code> and <code>n_iterations = 1000</code>, the resulting <code>theta</code> is exactly what the Normal Equation found! Perfect.</p></li><li><p><strong>Effect of Learning Rate <code>η</code> (Figure 4-8, page 123):</strong></p><ul><li><code>η = 0.02</code> (left): Too slow, many steps to converge.</li><li><code>η = 0.1</code> (middle): Good, converges quickly.</li><li><code>η = 0.5</code> (right): Too high, diverges, jumps around.</li></ul></li><li><p><strong>Finding a good learning rate:</strong> Grid search (Chapter 2).</p></li><li><p><strong>Setting number of iterations:</strong> If too low, far from optimum. If too high, waste time after convergence.</p><ul><li>Solution: Set many iterations, but stop when the gradient vector becomes tiny (its norm &lt; <code>ϵ</code>, a small tolerance). This means we&rsquo;re (almost) at the minimum.</li></ul></li><li><p><strong>Convergence Rate (sidebar, page 124):</strong>
For convex cost functions like MSE, BGD with fixed <code>η</code> takes O(1/ϵ) iterations to reach within <code>ϵ</code> of the optimum. To get 10x more precision (divide <code>ϵ</code> by 10), you need ~10x more iterations.</p></li></ul><p><strong>(Page 124-127: Stochastic Gradient Descent - SGD)</strong></p><p>Batch GD is slow on large datasets because it uses all training data at each step.</p><ul><li><p><strong>Stochastic Gradient Descent (SGD):</strong></p><ul><li>At each step, picks <em>one random instance</em> from the training set and computes gradients based <em>only on that single instance</em>.</li><li><strong>Advantages:</strong><ul><li>Much faster per step (very little data to process).</li><li>Can train on huge datasets (only one instance in memory at a time – good for out-of-core learning, Ch 1).</li></ul></li><li><strong>Disadvantages (Figure 4-9, page 124):</strong><ul><li><strong>Stochastic nature:</strong> The path to the minimum is much more erratic (&ldquo;bouncy&rdquo;) than BGD. Cost function goes up and down, decreasing only on average.</li><li><strong>Never settles:</strong> Once near the minimum, it keeps bouncing around, never perfectly settling. Final parameters are good, but not optimal.</li></ul></li><li><strong>Advantage of randomness:</strong> If cost function is irregular (non-convex, Figure 4-6), SGD&rsquo;s randomness can help it jump out of local minima and find a better global minimum.</li></ul></li><li><p><strong>Learning Schedule (page 125):</strong></p><ul><li>To help SGD settle at the minimum, we can gradually reduce the learning rate <code>η</code>.</li><li>Start with large <code>η</code> (quick progress, escape local minima).</li><li>Make <code>η</code> smaller over time (settle at global minimum).</li><li>This is like <strong>simulated annealing</strong> in metallurgy.</li><li>The function determining <code>η</code> at each iteration is the <strong>learning schedule</strong>.</li><li>If <code>η</code> reduces too quickly -> stuck in local minimum or frozen half-way.</li><li>If <code>η</code> reduces too slowly -> bounce around minimum for long, or stop too early with suboptimal solution.</li></ul></li><li><p><strong>Implementation of SGD (page 125):</strong></p><ul><li>Outer loop for <code>epochs</code> (an epoch is one pass through the entire training set, by convention).</li><li>Inner loop for <code>m</code> iterations (number of instances). In each inner iteration:<ul><li>Pick a <code>random_index</code>.</li><li>Get <code>xi</code> and <code>yi</code> for that instance.</li><li>Gradients calculated using <em>only</em> <code>xi</code> and <code>yi</code>: <code>gradients = 2 * xi.T.dot(xi.dot(theta) - yi)</code>. (Note: <code>2</code> not <code>2/m</code> because <code>m=1</code> here).</li><li>Update <code>eta</code> using <code>learning_schedule(epoch * m + i)</code>.</li><li>Update <code>theta</code>.</li></ul></li><li>After 50 epochs (much fewer iterations than BGD&rsquo;s 1000), it finds a <code>theta</code> very close to BGD&rsquo;s.</li><li>Figure 4-10 (page 126) shows the irregular first 20 steps.<figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-10.png alt="Figure 4-10" width=700></figure></li></ul></li><li><p><strong>Important note on SGD (sidebar, page 126):</strong></p><ul><li>Training instances <em>must be independent and identically distributed (IID)</em> for SGD to converge to global optimum on average.</li><li><strong>Shuffle instances</strong> during training (pick randomly, or shuffle set at start of each epoch). If data is sorted (e.g., by label), SGD will optimize for one label, then the next, and not find global minimum.</li></ul></li><li><p><strong>SGD with Scikit-Learn (<code>SGDRegressor</code>, page 126):</strong></p><ul><li>Defaults to optimizing MSE.</li><li><code>sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)</code><ul><li><code>max_iter</code>: max epochs.</li><li><code>tol</code>: stop if loss drops by less than this in one epoch.</li><li><code>penalty=None</code>: no regularization (more later).</li><li><code>eta0</code>: initial learning rate. Uses its own default learning schedule.</li></ul></li><li><code>sgd_reg.fit(X, y.ravel())</code> (<code>.ravel()</code> flattens <code>y</code> into 1D array, often needed by Scikit-Learn).</li><li>Resulting intercept and coefficient are very close to Normal Equation&rsquo;s.</li></ul></li></ul><p><strong>(Page 127: Mini-batch Gradient Descent)</strong></p><p>A compromise between Batch GD and Stochastic GD.</p><ul><li>At each step, computes gradients on <strong>small random sets of instances</strong> called <strong>mini-batches</strong>.</li><li><strong>Main advantage over SGD:</strong> Performance boost from hardware optimization of matrix operations (especially on GPUs).</li><li><strong>Behavior (Figure 4-11, page 127):</strong><figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-11.png alt="Figure 4-11" width=700></figure><ul><li>Less erratic path than SGD.</li><li>Ends up closer to the minimum than SGD (with fixed <code>η</code>).</li><li>But may be harder to escape local minima (for non-convex problems) compared to SGD.</li><li>All three (Batch, Stochastic, Mini-batch) end up near the minimum. Batch stops <em>at</em> the minimum. SGD and Mini-batch would also reach minimum with a good learning schedule.</li><li>But Batch GD takes much longer per step.</li></ul></li></ul><p><strong>(Page 128: Comparison Table 4-1)</strong></p><p>This table nicely summarizes Normal Equation, SVD, Batch GD, Stochastic GD, Mini-batch GD for Linear Regression based on:</p><ul><li><code>Large m</code> (instances): Normal Eq/SVD are fast if data fits memory. GD methods are also fast (SGD/Mini-batch can do out-of-core).</li><li><code>Out-of-core support</code>: Only SGD/Mini-batch.</li><li><code>Large n</code> (features): Normal Eq/SVD are slow. GD methods are fast.</li><li><code>Hyperparameters</code>: Normal Eq/SVD have 0. BGD has 2 (η, iterations). SGD/Mini-batch have >=2 (η, iterations, schedule params).</li><li><code>Scaling required</code>: No for Normal Eq/SVD. Yes for GD methods.</li><li><code>Scikit-Learn class</code>: <code>LinearRegression</code> for SVD. <code>SGDRegressor</code> for GDs.<figure><img src=/bookshelf/hands-on-ml/ch-4-tbl-1.png alt="Table 4-1" width=700></figure></li></ul><p>The key is that after training, <strong>all these methods (if converged) produce very similar models and make predictions in the same way.</strong> The difference is in <em>how they get there</em> (the training process).</p><p>Excellent! Glad you&rsquo;re with me. Let&rsquo;s push on to the next sections of Chapter 4. We&rsquo;ve just laid the groundwork for how Linear Regression models are trained. Now, let&rsquo;s see how we can adapt these ideas for more complex scenarios.</p><p><strong>(Page 128-130: Polynomial Regression)</strong></p><p>So far, Linear Regression assumes a straight-line (or flat plane/hyperplane) relationship between features and the target. But what if your data is more complex, like the curved data in Figure 4-12 (page 129)?<figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-12.png alt="Figure 4-12" width=700></figure>The book shows data generated by a simple quadratic equation: <code>y = 0.5 * X**2 + X + 2 + noise</code>. Clearly, a straight line won&rsquo;t fit this well.</p><p>This is where <strong>Polynomial Regression</strong> comes in.</p><ul><li><p><strong>The Core Idea:</strong> You can still use a <em>linear model</em> to fit <em>nonlinear data</em>!</p></li><li><p><strong>How?</strong> By adding powers of each feature as new features, and then training a linear model on this <em>extended</em> set of features.</p><ul><li>For example, if you have one feature <code>X</code>, you can create a new feature <code>X²</code>. Then your linear model becomes <code>ŷ = θ₀ + θ₁X + θ₂X²</code>. Even though the equation is quadratic in <code>X</code>, it&rsquo;s <em>linear</em> in terms of the parameters <code>θ₀, θ₁, θ₂</code> and the features <code>X</code> and <code>X²</code>.</li></ul></li><li><p><strong>Scikit-Learn&rsquo;s <code>PolynomialFeatures</code> (page 129):</strong> This class transforms your data.
<code>from sklearn.preprocessing import PolynomialFeatures</code>
<code>poly_features = PolynomialFeatures(degree=2, include_bias=False)</code></p><ul><li><code>degree=2</code>: We want to add 2nd-degree polynomial features (i.e., squares).</li><li><code>include_bias=False</code>: <code>PolynomialFeatures</code> can add a column of 1s (for the bias term), but <code>LinearRegression</code> handles that, so we set it to <code>False</code> to avoid redundancy.
<code>X_poly = poly_features.fit_transform(X)</code>
If <code>X[0]</code> was <code>[-0.75]</code>, then <code>X_poly[0]</code> becomes <code>[-0.75, 0.566]</code> (original X, and X²).</li></ul></li><li><p><strong>Train a Linear Model on Extended Data:</strong>
Now, you just fit a standard <code>LinearRegression</code> model to this <code>X_poly</code>:
<code>lin_reg = LinearRegression()</code>
<code>lin_reg.fit(X_poly, y)</code>
The model predictions are shown in Figure 4-13 (page 130). It&rsquo;s a nice curve that fits the data much better than a straight line! The estimated equation (e.g., <code>ŷ = 0.56x₁² + 0.93x₁ + 1.78</code>) is close to the original <code>y = 0.5x₁² + 1.0x₁ + 2.0 + noise</code>.<figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-13.png alt="Figure 4-13" width=700></figure></p></li><li><p><strong>Multiple Features and Combinations (page 130):</strong>
If you have multiple features (say, <code>a</code> and <code>b</code>) and use <code>PolynomialFeatures(degree=3)</code>, it will add <code>a²</code>, <code>a³</code>, <code>b²</code>, <code>b³</code>, AND also the <em>combination terms</em> like <code>ab</code>, <code>a²b</code>, <code>ab²</code>. This allows the model to find relationships <em>between</em> features.</p></li><li><p><strong>Warning: Combinatorial Explosion!</strong> (Scorpion icon, page 130)
The number of features can explode if you have many original features and use a high polynomial degree. The formula is <code>(n+d)! / (d!n!)</code> where <code>n</code> is original features, <code>d</code> is degree. Be careful! This can make the model very slow and prone to overfitting.</p></li></ul><p><strong>(Page 130-134: Learning Curves)</strong></p><p>With high-degree Polynomial Regression, you can fit the training data <em>very</em> well, maybe too well. Figure 4-14 (page 131) shows:<figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-14.png alt="Figure 4-14" width=700></figure></p><ul><li>A 300-degree polynomial model: Wiggles wildly to hit every training point. This is severe <strong>overfitting</strong>.</li><li>A plain linear model: Misses the curve. This is <strong>underfitting</strong>.</li><li>A quadratic model (2nd degree): Fits best, which makes sense as the data was generated quadratically.</li></ul><p>But how do you know in general if your model is too complex (overfitting) or too simple (underfitting), especially if you don&rsquo;t know the true underlying function?</p><ol><li><p><strong>Cross-Validation (from Chapter 2):</strong></p><ul><li>If model performs well on training data but poorly on cross-validation, it&rsquo;s overfitting.</li><li>If it performs poorly on both, it&rsquo;s underfitting.</li></ul></li><li><p><strong>Learning Curves (page 131):</strong></p><ul><li><p><strong>What they are:</strong> Plots of the model&rsquo;s performance (e.g., RMSE) on the <strong>training set</strong> and the <strong>validation set</strong>, as a function of the <strong>training set size</strong> (or training iteration).</p></li><li><p><strong>How to generate them:</strong> Train the model multiple times on different-sized subsets of the training set.</p></li><li><p>The book provides a <code>plot_learning_curves</code> function (page 132).</p></li><li><p><strong>Learning Curves for an Underfitting Model (e.g., plain Linear Regression on the quadratic data - Figure 4-15, page 132):</strong><figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-15.png alt="Figure 4-15" width=700></figure></p><ul><li><strong>Training error:</strong> Starts at 0 (perfect fit with 1-2 instances), then rises as more noisy/nonlinear data is added, eventually plateauing.</li><li><strong>Validation error:</strong> Starts very high (model trained on few instances generalizes poorly), then decreases as model learns from more examples, eventually plateauing close to the training error.</li><li><strong>Key characteristics of underfitting curves:</strong> Both curves plateau, they are close together, and the error is fairly high.</li><li><em>What it&rsquo;s ultimately trying to achieve:</em> The plot tells you that the model is too simple. Adding more training examples <em>will not help</em> if the curves have plateaued (as the scorpion icon on page 133 says). You need a more complex model or better features.</li></ul></li><li><p><strong>Learning Curves for an Overfitting Model (e.g., 10th-degree polynomial on the quadratic data - Figure 4-16, page 133):</strong></p><ul><li><strong>Training error:</strong> Much lower than the Linear Regression model&rsquo;s training error. It fits the training data well.</li><li><strong>Validation error:</strong> There&rsquo;s a <strong>gap</strong> between the training error and the validation error. The model performs significantly better on the training data than on the validation data. This is the hallmark of overfitting.</li><li>If you had a much larger training set, these two curves would eventually get closer.</li><li><em>What it&rsquo;s ultimately trying to achieve:</em> The plot tells you the model is too complex for the current amount of data. One way to improve an overfitting model is to feed it more training data (as the scorpion icon on page 134 suggests), until the validation error meets the training error (or use regularization, which we&rsquo;ll see next).</li></ul></li></ul></li></ol><ul><li><p><strong>The Bias/Variance Trade-off (sidebar, page 134):</strong>
This is a fundamental concept in statistics and ML. A model&rsquo;s <strong>generalization error</strong> (how well it performs on unseen data) can be expressed as the sum of three components:</p><ol><li><strong>Bias:</strong><ul><li>Error due to <strong>wrong assumptions</strong> made by the model. E.g., assuming data is linear when it&rsquo;s quadratic.</li><li>A high-bias model is likely to <strong>underfit</strong>.</li><li><em>What it&rsquo;s ultimately trying to achieve (in a bad way):</em> It has a strong preconceived notion of how the data should look, and it sticks to it even if the data says otherwise.</li></ul></li><li><strong>Variance:</strong><ul><li>Error due to the model&rsquo;s <strong>excessive sensitivity to small variations</strong> in the training data. A model with many degrees of freedom (like a high-degree polynomial) can have high variance.</li><li>A high-variance model is likely to <strong>overfit</strong>. It learns the noise in the training data, not just the signal.</li><li><em>What it&rsquo;s ultimately trying to achieve (in a bad way):</em> It tries to fit every little nook and cranny of the training data, making it unstable and perform poorly on new, slightly different data.</li></ul></li><li><strong>Irreducible Error:</strong><ul><li>Error due to the <strong>inherent noisiness of the data itself</strong>.</li><li>This part cannot be reduced by model changes; only by cleaning the data (e.g., fixing broken sensors, removing outliers).</li></ul></li></ol><p><strong>The Trade-off:</strong></p><ul><li>Increasing model complexity typically <em>increases variance</em> (more likely to overfit) and <em>reduces bias</em> (better fit to complex patterns).</li><li>Decreasing model complexity typically <em>increases bias</em> (more likely to underfit) and <em>reduces variance</em>.
The goal is to find a sweet spot, balancing bias and variance.</li></ul></li></ul><p><strong>(Page 134-141: Regularized Linear Models)</strong></p><p>We saw that overfitting is a problem with complex models. <strong>Regularization</strong> is a way to reduce overfitting by <em>constraining</em> the model.</p><ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> For linear models, this usually means constraining the model&rsquo;s <strong>weights</strong>. The idea is to prevent the weights from becoming too large, which can happen when the model tries too hard to fit the noise in the training data. Smaller weights generally lead to simpler, smoother models that generalize better.</li></ul><p>The book discusses three types of regularized linear models: Ridge, Lasso, and Elastic Net.</p><ul><li><p><strong>Ridge Regression (also Tikhonov regularization) (page 135):</strong></p><ul><li>It adds a <strong>regularization term</strong> to the MSE cost function.</li><li><strong>Equation 4-8: Ridge Regression cost function</strong>
<code>J(θ) = MSE(θ) + α * (1/2) * Σᵢ (θᵢ)²</code> (sum from i=1 to n, so bias term <code>θ₀</code> is NOT regularized)<ul><li>The regularization term is <code>α * (1/2) * Σᵢ (θᵢ)²</code>. This is <code>α/2</code> times the <strong>sum of the squares of the feature weights</strong>. This is related to the <strong>ℓ₂ norm</strong> of the weight vector <code>w = [θ₁, ..., θₙ]</code>, specifically <code>(1/2) * ||w||₂²</code>.</li><li><code>α</code> (alpha) is a hyperparameter that controls how much you want to regularize.<ul><li>If <code>α = 0</code>, it&rsquo;s just Linear Regression.</li><li>If <code>α</code> is very large, all weights <code>θᵢ</code> (for i>0) end up very close to zero, and the model becomes a flat line through the data&rsquo;s mean.</li></ul></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> The learning algorithm is now forced to not only <em>fit the data</em> (minimize MSE) but also keep the model weights <em>as small as possible</em>.</li></ul></li><li><strong>Important:</strong> Scale the data (e.g., <code>StandardScaler</code>) <em>before</em> performing Ridge Regression, as it&rsquo;s sensitive to the scale of input features (scorpion icon, page 136). This is true for most regularized models.</li><li>Figure 4-17 (page 136) shows the effect of <code>α</code>:<figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-17.png alt="Figure 4-17" width=700></figure><ul><li>Left: Plain Ridge Regression on linear data. Higher <code>α</code> makes predictions flatter.</li><li>Right: Polynomial features (degree=10) + scaling + Ridge. Increasing <code>α</code> leads to flatter (less extreme, more reasonable) predictions, reducing variance but increasing bias.</li></ul></li><li><strong>Training Ridge Regression:</strong><ul><li><strong>Closed-form solution (Equation 4-9, page 137):</strong>
<code>θ̂ = (XᵀX + αA)⁻¹ Xᵀy</code> (where <code>A</code> is an identity matrix with a 0 at the top-left for the bias term).
Scikit-Learn&rsquo;s <code>Ridge(alpha=..., solver="cholesky")</code> uses this.</li><li><strong>Gradient Descent (page 137):</strong>
Add <code>αw</code> to the MSE gradient vector (where <code>w</code> is the vector of weights, excluding bias).
Scikit-Learn&rsquo;s <code>SGDRegressor(penalty="l2")</code> does this. The <code>"l2"</code> means add a regularization term equal to half the square of the ℓ₂ norm of the weights.</li></ul></li></ul></li><li><p><strong>Lasso Regression (Least Absolute Shrinkage and Selection Operator) (page 137):</strong></p><ul><li>Also adds a regularization term to the MSE cost function, but uses the <strong>ℓ₁ norm</strong> of the weight vector.</li><li><strong>Equation 4-10: Lasso Regression cost function</strong>
<code>J(θ) = MSE(θ) + α * Σᵢ |θᵢ|</code> (sum from i=1 to n)<ul><li>The regularization term is <code>α</code> times the sum of the <em>absolute values</em> of the weights.</li></ul></li><li>Figure 4-18 (page 138) shows Lasso models, similar to Figure 4-17 for Ridge, but with smaller <code>α</code> values.<figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-18.png alt="Figure 4-18" width=700></figure></li><li><strong>Key characteristic of Lasso:</strong> It tends to completely eliminate the weights of the least important features (i.e., set them to zero). It automatically performs <strong>feature selection</strong> and outputs a <strong>sparse model</strong> (a model with few non-zero feature weights).</li><li><strong>Why does Lasso do this? (Figure 4-19, page 139):</strong><figure><img src=/bookshelf/hands-on-ml/ch-4-fig-19.png alt="Figure 4-19" width=700></figure><ul><li>This is a bit more advanced, but the intuition comes from looking at the &ldquo;shape&rdquo; of the ℓ₁ penalty vs. the ℓ₂ penalty.</li><li>The ℓ₁ penalty <code>|θ₁| + |θ₂|</code> has &ldquo;corners&rdquo; along the axes in the parameter space. When minimizing the combined MSE + ℓ₁ penalty, the optimization path often hits these corners, forcing one of the parameters to zero.</li><li>The ℓ₂ penalty <code>θ₁² + θ₂²</code> is circular. The optimization path approaches the origin smoothly, shrinking weights but not usually making them exactly zero.</li><li><em>What it&rsquo;s ultimately trying to achieve:</em> The ℓ₁ penalty encourages sparsity by pushing less important feature weights all the way to zero.</li></ul></li><li><strong>Training Lasso Regression:</strong><ul><li>The Lasso cost function is not differentiable at <code>θᵢ = 0</code>. However, Gradient Descent can still work if you use a <strong>subgradient vector</strong> (Equation 4-11, page 140). This is a technical detail; the main idea is that an iterative approach can still find the minimum.</li><li>Scikit-Learn: <code>Lasso(alpha=...)</code> or <code>SGDRegressor(penalty="l1")</code>.</li><li>When using Lasso with GD, you often need to gradually reduce the learning rate to help it converge without bouncing around the optimum (due to the &ldquo;sharp corners&rdquo; of the ℓ₁ penalty).</li></ul></li></ul></li><li><p><strong>Elastic Net (page 140):</strong></p><ul><li>A middle ground between Ridge and Lasso. Its regularization term is a simple mix of both Ridge (ℓ₂) and Lasso (ℓ₁) terms.</li><li><strong>Equation 4-12: Elastic Net cost function</strong>
<code>J(θ) = MSE(θ) + rα Σᵢ|θᵢ| + ((1-r)/2)α Σᵢθᵢ²</code><ul><li><code>r</code> is the mix ratio.<ul><li>If <code>r = 0</code>, Elastic Net is equivalent to Ridge.</li><li>If <code>r = 1</code>, Elastic Net is equivalent to Lasso.</li></ul></li></ul></li><li>Scikit-Learn: <code>ElasticNet(alpha=..., l1_ratio=...)</code> (where <code>l1_ratio</code> is <code>r</code>).</li></ul></li><li><p><strong>When to use which? (page 140):</strong></p><ul><li>Plain Linear Regression (no regularization) should generally be avoided. Some regularization is almost always better.</li><li><strong>Ridge</strong> is a good default.</li><li>If you suspect only a few features are useful, prefer <strong>Lasso</strong> or <strong>Elastic Net</strong> because they perform feature selection by shrinking useless feature weights to zero.</li><li><strong>Elastic Net</strong> is generally preferred over Lasso because Lasso can behave erratically when the number of features is greater than the number of training instances, or when several features are strongly correlated. Elastic Net is more stable in these cases.</li></ul></li><li><p><strong>Early Stopping (page 141):</strong></p><ul><li>A very different way to regularize iterative learning algorithms like Gradient Descent.</li><li><strong>The Idea:</strong> Stop training as soon as the validation error reaches a minimum.</li><li>Figure 4-20 (page 141) shows a complex model (high-degree polynomial) being trained with Batch GD:<figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-20.png alt="Figure 4-20" width=700></figure><ul><li>As epochs go by, training error (RMSE) goes down.</li><li>Validation error also goes down initially, but then starts to go back <em>up</em>. This indicates the model has started to overfit.</li><li>With early stopping, you just stop training when the validation error is at its minimum.</li></ul></li><li>It&rsquo;s a simple and efficient regularization technique. Geoffrey Hinton called it a &ldquo;beautiful free lunch.&rdquo;</li><li><strong>Implementation (page 142):</strong><ul><li>Loop for epochs.</li><li>In each epoch, train the model (e.g., <code>SGDRegressor</code> with <code>warm_start=True</code> so it continues training, <code>max_iter=1</code> so it does one epoch).</li><li>Evaluate on validation set.</li><li>If validation error is less than current <code>minimum_val_error</code>, save the model (clone it) and update <code>minimum_val_error</code> and <code>best_epoch</code>.</li><li>After the loop, the <code>best_model</code> is your regularized model.</li></ul></li><li>With SGD/Mini-batch GD, validation curves are noisy. You might stop only after validation error has been above minimum for a while, then roll back to the best model.</li></ul></li></ul><p>Fantastic! It&rsquo;s great that the intuition behind L1 and L2 regularization clicked. Let&rsquo;s continue our journey through Chapter 4, moving on to models designed for classification.</p><p><strong>(Page 142-147: Logistic Regression)</strong></p><p>We&rsquo;ve seen Linear Regression for predicting continuous values. Now, what if we want to predict a class? For example, is an email spam or not spam? This is a binary classification problem.</p><p>The book points out (as we saw in Chapter 1) that some regression algorithms can be adapted for classification. <strong>Logistic Regression</strong> (also called Logit Regression) is a prime example.</p><ul><li><p><strong>What it&rsquo;s ultimately trying to achieve:</strong> Logistic Regression estimates the <em>probability</em> that an instance belongs to a particular class (typically the &ldquo;positive&rdquo; class, labeled &lsquo;1&rsquo;). If this probability is greater than a certain threshold (usually 50%), the model predicts &lsquo;1&rsquo;; otherwise, it predicts &lsquo;0&rsquo;.</p></li><li><p><strong>Estimating Probabilities (Page 143):</strong></p><ol><li><strong>Linear Combination:</strong> Just like Linear Regression, it first computes a weighted sum of the input features plus a bias term: <code>t = xᵀθ</code>. (This <code>t</code> is often called the <strong>logit</strong>).</li><li><strong>Logistic Function (Sigmoid):</strong> Instead of outputting <code>t</code> directly, it passes <code>t</code> through the <strong>logistic function</strong> (also called the <strong>sigmoid function</strong>), denoted as <code>σ(t)</code>.<ul><li><strong>Equation 4-13: Logistic Regression model estimated probability (vectorized form)</strong>
<code>p̂ = h_θ(x) = σ(xᵀθ)</code>
where <code>p̂</code> (p-hat) is the estimated probability that the instance <code>x</code> belongs to the positive class.</li><li><strong>Equation 4-14: Logistic function</strong>
<code>σ(t) = 1 / (1 + exp(-t))</code></li><li><strong>Figure 4-21 (page 143)</strong> shows the characteristic S-shape of the sigmoid function.<figure><img src=/bookshelf/hands-on-ml/ch-4-fig-4-21.png alt="Figure 4-21" width=700></figure><ul><li><em>What the sigmoid is ultimately trying to achieve:</em> It squashes any input value <code>t</code> (which can range from -∞ to +∞) into an output value between 0 and 1. This output can then be interpreted as a probability.<ul><li>If <code>t</code> is large and positive, <code>exp(-t)</code> is close to 0, so <code>σ(t)</code> is close to 1.</li><li>If <code>t</code> is large and negative, <code>exp(-t)</code> is very large, so <code>σ(t)</code> is close to 0.</li><li>If <code>t = 0</code>, <code>exp(-t) = 1</code>, so <code>σ(t) = 1/2 = 0.5</code>.</li></ul></li></ul></li></ul></li></ol></li><li><p><strong>Making Predictions (Page 143):</strong></p><ul><li><strong>Equation 4-15: Logistic Regression model prediction</strong>
<code>ŷ = 0 if p̂ &lt; 0.5</code>
<code>ŷ = 1 if p̂ ≥ 0.5</code></li><li>Since <code>σ(t) ≥ 0.5</code> when <code>t ≥ 0</code>, the model predicts 1 if <code>xᵀθ</code> (the logit) is positive, and 0 if it&rsquo;s negative.</li></ul></li><li><p><strong>Training and Cost Function (Page 144):</strong></p><ul><li><strong>Objective:</strong> We want to set the parameter vector <code>θ</code> so that the model estimates a <em>high probability</em> for positive instances (actual y=1) and a <em>low probability</em> for negative instances (actual y=0).</li><li><strong>Cost Function for a Single Instance (Equation 4-16):</strong>
<code>c(θ) = -log(p̂) if y = 1</code>
<code>c(θ) = -log(1 - p̂) if y = 0</code><ul><li><em>What this cost function is ultimately trying to achieve:</em><ul><li>If <code>y=1</code> (actual is positive):<ul><li>If model predicts <code>p̂</code> close to 1 (correct), <code>-log(p̂)</code> is close to 0 (low cost).</li><li>If model predicts <code>p̂</code> close to 0 (incorrect), <code>-log(p̂)</code> is very large (high cost).</li></ul></li><li>If <code>y=0</code> (actual is negative):<ul><li>If model predicts <code>p̂</code> close to 0 (so <code>1-p̂</code> is close to 1, correct), <code>-log(1-p̂)</code> is close to 0 (low cost).</li><li>If model predicts <code>p̂</code> close to 1 (so <code>1-p̂</code> is close to 0, incorrect), <code>-log(1-p̂)</code> is very large (high cost).
This cost function penalizes the model heavily when it&rsquo;s confident and wrong.</li></ul></li></ul></li></ul></li><li><strong>Cost Function for the Whole Training Set (Log Loss - Equation 4-17):</strong>
<code>J(θ) = - (1/m) * Σᵢ [ y⁽ⁱ⁾log(p̂⁽ⁱ⁾) + (1 - y⁽ⁱ⁾)log(1 - p̂⁽ⁱ⁾) ]</code>
This is just the average cost over all training instances. It&rsquo;s a single, clever expression that combines the two cases from Equation 4-16.</li><li><strong>Good news:</strong> This log loss cost function is <strong>convex</strong>. So, Gradient Descent (or other optimization algorithms) can find the global minimum.</li><li><strong>Bad news:</strong> There&rsquo;s <strong>no closed-form solution</strong> (like the Normal Equation for Linear Regression) to find the <code>θ</code> that minimizes this cost function. We <em>must</em> use an iterative optimization algorithm like Gradient Descent.</li><li><strong>Partial Derivatives (Equation 4-18, page 145):</strong>
<code>∂J(θ)/∂θⱼ = (1/m) * Σᵢ (σ(θᵀx⁽ⁱ⁾) - y⁽ⁱ⁾) * xⱼ⁽ⁱ⁾</code><ul><li>This looks very similar to the partial derivative for Linear Regression&rsquo;s MSE (Equation 4-5)!</li><li><code>σ(θᵀx⁽ⁱ⁾)</code> is the predicted probability <code>p̂⁽ⁱ⁾</code>.</li><li><code>(p̂⁽ⁱ⁾ - y⁽ⁱ⁾)</code> is the prediction error.</li><li>This error is multiplied by the feature value <code>xⱼ⁽ⁱ⁾</code> and averaged.
Once you have these gradients, you can use Batch GD, Stochastic GD, or Mini-batch GD to find the optimal <code>θ</code>.</li></ul></li></ul></li><li><p><strong>Decision Boundaries (Page 145-147):</strong>
Let&rsquo;s use the Iris dataset to illustrate. We&rsquo;ll try to classify <em>Iris virginica</em> based only on petal width.</p><ul><li>Load data: <code>X = iris["data"][:, 3:]</code> (petal width), <code>y = (iris["target"] == 2).astype(int)</code> (1 if virginica, else 0).</li><li>Train <code>LogisticRegression()</code>:
<code>log_reg = LogisticRegression()</code>
<code>log_reg.fit(X, y)</code></li><li><strong>Figure 4-23 (page 146):</strong> Shows estimated probabilities vs. petal width.<ul><li>The S-shape is clear.</li><li>For petal widths > ~2cm, probability of being <em>Iris virginica</em> is high.</li><li>For petal widths &lt; ~1cm, probability is low.</li><li>The <strong>decision boundary</strong> (where <code>p̂ = 0.5</code>) is around 1.6 cm. If petal width > 1.6cm, it predicts virginica; otherwise, not virginica.</li></ul></li><li><strong>Figure 4-24 (page 147):</strong> Shows decision boundary using two features (petal width and petal length).<ul><li>The dashed line is where the model estimates 50% probability – this is the linear decision boundary.</li><li>Other parallel lines show other probability contours (e.g., 15%, 90%).</li></ul></li><li><strong>Regularization:</strong> Logistic Regression models in Scikit-Learn use ℓ₂ regularization by default. The hyperparameter is <code>C</code> (inverse of <code>α</code>): higher <code>C</code> means <em>less</em> regularization.</li></ul></li></ul><p><strong>(Page 148-150: Softmax Regression)</strong></p><p>Logistic Regression is for binary classification. What if we have more than two classes, and we want a model that handles them directly (not with OvR/OvO strategies)? Enter <strong>Softmax Regression</strong> (or Multinomial Logistic Regression).</p><ul><li><p><strong>The Idea:</strong></p><ol><li>For a given instance <code>x</code>, compute a <strong>score <code>sₖ(x)</code> for each class <code>k</code></strong>. This is done just like Linear Regression: <code>sₖ(x) = xᵀθ⁽ᵏ⁾</code> (Equation 4-19). Each class <code>k</code> has its own dedicated parameter vector <code>θ⁽ᵏ⁾</code>. These are often stored as rows in a parameter matrix <code>Θ</code>.</li><li>Estimate the <strong>probability <code>p̂ₖ</code> that the instance belongs to class <code>k</code></strong> by applying the <strong>softmax function</strong> (also called normalized exponential) to the scores.<ul><li><strong>Equation 4-20: Softmax function</strong>
<code>p̂ₖ = σ(s(x))ₖ = exp(sₖ(x)) / Σⱼ exp(sⱼ(x))</code> (sum over all classes <code>j=1</code> to <code>K</code>)<ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> It takes a vector of arbitrary scores <code>s(x)</code> for all classes, computes the exponential of each score (making them all positive), and then normalizes them by dividing by their sum. The result is a set of probabilities (<code>p̂ₖ</code>) that are all between 0 and 1 and sum up to 1 across all classes. The class with the highest initial score <code>sₖ(x)</code> will get the highest probability <code>p̂ₖ</code>.</li></ul></li></ul></li></ol></li><li><p><strong>Prediction (Equation 4-21, page 149):</strong>
The classifier predicts the class <code>k</code> that has the highest estimated probability <code>p̂ₖ</code> (which is simply the class with the highest score <code>sₖ(x)</code>).
<code>ŷ = argmaxₖ p̂ₖ</code></p><ul><li>Softmax Regression predicts only one class at a time (mutually exclusive classes). It&rsquo;s multiclass, not multioutput.</li></ul></li><li><p><strong>Training and Cost Function (Cross Entropy - page 149):</strong></p><ul><li><strong>Objective:</strong> Estimate a high probability for the <em>target class</em> and low probabilities for other classes.</li><li><strong>Cost Function (Cross Entropy - Equation 4-22):</strong>
<code>J(Θ) = - (1/m) * Σᵢ Σₖ yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾)</code><ul><li><code>yₖ⁽ⁱ⁾</code> is the target probability that instance <code>i</code> belongs to class <code>k</code> (usually 1 if it&rsquo;s the target class, 0 otherwise).</li><li><code>p̂ₖ⁽ⁱ⁾</code> is the model&rsquo;s estimated probability that instance <code>i</code> belongs to class <code>k</code>.</li><li><em>What it&rsquo;s ultimately trying to achieve:</em> This cost function penalizes the model when it estimates a low probability for the actual target class. It&rsquo;s a common measure for how well a set of estimated class probabilities matches the target classes.</li><li>The sidebar on &ldquo;Cross Entropy&rdquo; (page 150) gives some information theory background – it measures the average number of bits needed to encode events based on your probability estimates vs. the true probabilities. Lower is better.</li><li>When there are only two classes (K=2), this cross-entropy cost function is equivalent to the log loss for Logistic Regression.</li></ul></li><li><strong>Gradient Vector (Equation 4-23, page 150):</strong>
<code>∇_θ⁽ᵏ⁾ J(Θ) = (1/m) * Σᵢ (p̂ₖ⁽ⁱ⁾ - yₖ⁽ⁱ⁾) * x⁽ⁱ⁾</code>
This gives the gradient for the parameter vector <code>θ⁽ᵏ⁾</code> of a specific class <code>k</code>. Again, very similar form to previous gradient equations!
You compute this for every class, then use an optimization algorithm (like GD) to find the parameter matrix <code>Θ</code> that minimizes the cost.</li></ul></li><li><p><strong>Using Softmax Regression in Scikit-Learn (page 150):</strong></p><ul><li><code>LogisticRegression</code> can perform Softmax Regression by setting:<ul><li><code>multi_class="multinomial"</code></li><li><code>solver="lbfgs"</code> (or another solver that supports multinomial, like &ldquo;sag&rdquo; or &ldquo;newton-cg&rdquo;)</li><li>It also applies ℓ₂ regularization by default (controlled by <code>C</code>).</li></ul></li><li>Example: Classify Iris flowers into all 3 classes using petal length and width.
<code>X = iris["data"][:, (2, 3)]</code>
<code>y = iris["target"]</code>
<code>softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10)</code>
<code>softmax_reg.fit(X, y)</code></li><li>To predict: <code>softmax_reg.predict([[5, 2]])</code> might give <code>array([2])</code> (Iris virginica).</li><li>To get probabilities: <code>softmax_reg.predict_proba([[5, 2]])</code> might give <code>[[6.3e-07, 0.057, 0.942]]</code>, meaning 94.2% prob for class 2 (virginica), 5.8% for class 1 (versicolor), and near 0% for class 0 (setosa).</li><li><strong>Figure 4-25 (page 151)</strong> shows the decision boundaries. They are linear between any two classes.</li></ul></li></ul><p>And that brings us to the end of the core content of Chapter 4! We&rsquo;ve gone from simple Linear Regression to Polynomial Regression, learned about diagnosing model fit with learning curves, seen how to regularize models to prevent overfitting (Ridge, Lasso, Elastic Net, Early Stopping), and finally explored Logistic and Softmax Regression for classification tasks.</p><p>The recurring theme for training, especially for models without closed-form solutions, is defining a cost function that captures how &ldquo;bad&rdquo; our model is, and then using an iterative algorithm like Gradient Descent to find the model parameters that minimize this cost. The specific form of the cost function and how predictions are made change from model to model, but the underlying optimization principle is often very similar.</p><p>This chapter is dense, but the concepts are absolutely key for understanding how models actually <em>learn</em>. How are you feeling about Logistic and Softmax Regression? Any particular part of &ldquo;what they are trying to achieve&rdquo; that needs more light?</p><hr><h1 id=glossary>Glossary</h1><p>You&rsquo;ve hit on a really neat and visual way to understand the difference between L1 (Lasso) and L2 (Ridge) regularization! Let&rsquo;s break down that intuition about the &ldquo;shapes&rdquo; and &ldquo;corners.&rdquo;</p><p>Imagine we have a model with just two weights, <code>θ₁</code> and <code>θ₂</code>. We want to find the values of <code>θ₁</code> and <code>θ₂</code> that minimize our main cost function (let&rsquo;s say MSE), <em>subject to</em> some penalty on the size of these weights.</p><p><strong>Visualizing the Penalties (Loss Functions for Weights)</strong></p><p>Think of the penalty term as defining a &ldquo;budget&rdquo; or a &ldquo;constraint region&rdquo; for our weights. The optimization process is trying to find the best MSE it can, <em>while staying within or close to this budget defined by the penalty.</em></p><ol><li><p><strong>L₂ Penalty (Ridge): <code>θ₁² + θ₂² ≤ C</code></strong></p><ul><li>The equation <code>θ₁² + θ₂² = constant</code> describes a <strong>circle</strong> (or a sphere/hypersphere in higher dimensions) centered at the origin (<code>θ₁=0, θ₂=0</code>).</li><li>So, the L2 penalty encourages the weights <code>(θ₁, θ₂)</code> to stay within a circular region around the origin.</li><li><strong>Shape:</strong> Smooth and round. No sharp corners.</li><li>Imagine the contours of this penalty function: they are concentric circles.</li></ul></li><li><p><strong>L₁ Penalty (Lasso): <code>|θ₁| + |θ₂| ≤ C</code></strong></p><ul><li>The equation <code>|θ₁| + |θ₂| = constant</code> describes a <strong>diamond</strong> (or a rotated square in 2D, and a similar shape with &ldquo;pointy&rdquo; corners on the axes in higher dimensions).</li><li>So, the L1 penalty encourages the weights <code>(θ₁, θ₂)</code> to stay within a diamond-shaped region around the origin.</li><li><strong>Shape:</strong> Has sharp corners that lie <em>on the axes</em>. For our 2D example, the corners are at points like (C, 0), (-C, 0), (0, C), and (0, -C).</li><li>Imagine the contours of this penalty function: they are concentric diamonds.</li></ul></li></ol><p><strong>Visualizing the Optimization Process (Figure 4-19)</strong></p><p>Now, let&rsquo;s consider the main cost function, the MSE. The contours of the MSE (if we ignore the penalty for a moment) are typically elliptical. The center of these ellipses is the point where MSE is minimized <em>without any regularization</em> – let&rsquo;s call this the &ldquo;unconstrained optimum.&rdquo;</p><p>The regularization process is trying to find a point that:
a. Is on the lowest possible MSE contour (meaning good fit to the data).
b. Satisfies the &ldquo;budget&rdquo; imposed by the L1 or L2 penalty (meaning weights are small).</p><p>This can be visualized as finding the point where an MSE ellipse first &ldquo;touches&rdquo; the boundary of the penalty region (the circle for L2, the diamond for L1).</p><ul><li><p><strong>With L₂ Penalty (Ridge - bottom-right plot in Figure 4-19):</strong></p><ul><li>Imagine an expanding MSE ellipse (as we try to get lower MSE) until it just kisses the circular L2 penalty region.</li><li>Because the circle is smooth and round, the point where the ellipse touches it can be anywhere on the circle&rsquo;s boundary.</li><li>It&rsquo;s very unlikely that this touch point will be <em>exactly</em> on an axis (e.g., where <code>θ₁ = 0</code> or <code>θ₂ = 0</code>).</li><li><strong>Result:</strong> The L2 penalty shrinks both <code>θ₁</code> and <code>θ₂</code> towards zero, but it&rsquo;s rare for either of them to become <em>exactly</em> zero. The optimization path (white dots) goes smoothly towards a point where both weights are small but likely non-zero.</li></ul></li><li><p><strong>With L₁ Penalty (Lasso - top-right plot in Figure 4-19):</strong></p><ul><li>Imagine an expanding MSE ellipse until it just kisses the diamond-shaped L1 penalty region.</li><li>Because the diamond has sharp corners <em>on the axes</em>, it&rsquo;s much more likely that the ellipse will first touch the diamond at one of these corners.</li><li>If it touches at a corner on, say, the <code>θ₁</code>-axis, then at that point, <code>θ₂</code> will be <em>exactly zero</em>.</li><li><strong>Result:</strong> The L1 penalty also shrinks weights towards zero, but because of its shape, it often forces some weights to become <em>exactly zero</em>. This is the &ldquo;feature selection&rdquo; property. The optimization path (white dots) is shown hitting the <code>θ₂=0</code> axis and then &ldquo;rolling down the gutter&rdquo; along that axis.</li></ul></li></ul><p><strong>Why the &ldquo;Corners&rdquo; Matter for Hitting Zero:</strong></p><p>Think about the gradient (the direction of steepest descent) of the penalty term itself:</p><ul><li><strong>L₂ Penalty:</strong> The gradient of <code>θ₁² + θ₂²</code> always points directly towards the origin. It smoothly pulls all weights smaller.</li><li><strong>L₁ Penalty:</strong> The gradient of <code>|θ₁| + |θ₂|</code> is a bit trickier.<ul><li>If <code>θ₁ > 0</code>, the derivative w.r.t. <code>θ₁</code> is 1.</li><li>If <code>θ₁ &lt; 0</code>, the derivative w.r.t. <code>θ₁</code> is -1.</li><li>If <code>θ₁ = 0</code>, it&rsquo;s undefined (this is the &ldquo;corner&rdquo;).
This means the L1 penalty applies a constant &ldquo;force&rdquo; (either +1 or -1) pulling a weight towards zero, regardless of how small the weight already is (as long as it&rsquo;s not zero). Once a weight hits zero, this &ldquo;force&rdquo; can effectively keep it there if the pull from the MSE term isn&rsquo;t strong enough to move it away.
In contrast, for L2, the &ldquo;force&rdquo; <code>2θ₁</code> pulling <code>θ₁</code> towards zero gets weaker as <code>θ₁</code> gets smaller. So, it might get very close to zero but not quite hit it unless the MSE term also strongly favors zero.</li></ul></li></ul><p><strong>Intuitive Summary:</strong></p><ul><li><strong>L₂ (Ridge) is like saying:</strong> &ldquo;Try to make your weights small, and distribute the &lsquo;smallness&rsquo; somewhat evenly. Keep them inside this round ball.&rdquo; It pulls weights towards zero but doesn&rsquo;t aggressively zero them out.</li><li><strong>L₁ (Lasso) is like saying:</strong> &ldquo;Try to make your weights small, and if possible, make some of them <em>exactly</em> zero. Keep them inside this pointy diamond. It&rsquo;s okay to focus your &lsquo;budget&rsquo; on just a few important weights and eliminate the others.&rdquo;</li></ul><p>The &ldquo;corners&rdquo; of the L1 penalty region are key because they represent solutions where one or more weights are exactly zero. The optimization process, in trying to balance minimizing MSE with staying within the L1 budget, often finds its optimal solution precisely at one of these corners.</p><p>Does that help clarify why the <em>shape</em> of the penalty (round vs. pointy with corners on axes) leads to these different behaviors in terms of zeroing out weights?</p></div></div></article></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer></body></html>