<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul></nav></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><a href=#notes-for-chapter-2>Notes for Chapter 2</a></li><li><a href=#glossary>Glossary</a><ul><li><a href=#rmse--mae-a-deep-dive>RMSE , MAE a Deep dive</a></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 2: End-to-End Machine Learning Project</h1><span class=reading-time><em>23 min read</em></span><div class=book-details><div class=book-content><h1 id=notes-for-chapter-2>Notes for Chapter 2</h1><p>Alright class, buckle up! Last time, we got a bird&rsquo;s-eye view of the Machine Learning landscape – the different continents, climates, and major landmarks. Today, with Chapter 2, &ldquo;End-to-End Machine Learning Project,&rdquo; we&rsquo;re grabbing our hiking boots and actually trekking through one of these landscapes. This is where the rubber meets the road!</p><p>The book says we&rsquo;re pretending to be a recently hired data scientist at a real estate company. Our mission? To predict median housing prices in California. This chapter is <em>fantastic</em> because it walks you through the practical steps. It’s less about the theory of one specific algorithm and more about the <em>process</em> of doing ML in the real world.</p><p><strong>(Page 35: The 8 Steps & Working with Real Data)</strong></p><p>The chapter lays out 8 key steps, and we&rsquo;ll follow them closely:</p><ol><li>Look at the big picture.</li><li>Get the data.</li><li>Discover and visualize the data to gain insights.</li><li>Prepare the data for Machine Learning algorithms.</li><li>Select a model and train it.</li><li>Fine-tune your model.</li><li>Present your solution.</li><li>Launch, monitor, and maintain your system.</li></ol><p>It also rightly emphasizes using <strong>real-world data</strong>. Artificial datasets are fine for understanding a specific algorithm, but real data? That&rsquo;s where you learn about the messiness, the missing values, the quirks – the stuff that makes this field challenging and fun! The book lists some great places to find datasets: UC Irvine, Kaggle, AWS datasets, etc. (Page 36).</p><p>For our project, we&rsquo;ll use the <strong>California Housing Prices dataset</strong> (Figure 2-1, page 36). It&rsquo;s from the 1990 census – a time when, as the book cheekily notes, &ldquo;a nice house in the Bay Area was still affordable.&rdquo; Ha! The map shows housing prices across California, with population density. It&rsquo;s a good, rich dataset for learning.</p><p><strong>(Page 37-41: Look at the Big Picture)</strong></p><p>This is step one, and it&rsquo;s absolutely crucial. Before you write a single line of code, you need to understand the <em>why</em>.</p><ul><li><p><strong>Frame the Problem:</strong></p><ul><li>Your boss wants a model of housing prices. But <em>why</em>? What’s the <strong>business objective</strong>? Is it to decide where to invest? Is it to advise clients? Knowing this shapes everything: how you frame the problem, what algorithms you pick, how you measure success, and how much effort you put into tweaking.</li><li>The book says our model&rsquo;s output (median housing price for a district) will be fed into <em>another</em> ML system (Figure 2-2, page 38). This downstream system decides if investing in an area is worthwhile. So, getting our price prediction right directly impacts revenue. That makes it critical!</li><li>The book also introduces the idea of <strong>Pipelines</strong> (page 38). ML systems are often sequences of data processing components. Data flows from one to the next. This modularity is great for organization and robustness but needs monitoring.</li><li>What&rsquo;s the <strong>current solution</strong> (if any)? The boss says experts currently estimate prices manually – costly, time-consuming, and often off by >20%. This gives us a baseline performance to beat and justifies building an ML model.</li><li>Now, we frame it technically (page 39):<ul><li><strong>Supervised, Unsupervised, or Reinforcement?</strong> We have labeled examples (districts with their median housing prices). So, it’s <strong>supervised learning</strong>.</li><li><strong>Classification or Regression?</strong> We&rsquo;re predicting a value (price). So, it’s a <strong>regression task</strong>.</li><li>More specifically, it&rsquo;s <strong>multiple regression</strong> (using multiple features like population, median income to predict the price) and <strong>univariate regression</strong> (predicting a single value – the median housing price – per district). If we were predicting multiple values (e.g., price and rental yield), it&rsquo;d be multivariate regression.</li><li><strong>Batch or Online Learning?</strong> There&rsquo;s no continuous flow of new data, and the dataset is small enough. So, <strong>batch learning</strong> is fine.</li></ul></li></ul></li><li><p><strong>Select a Performance Measure (Page 39):</strong></p><ul><li><p>How do we know if our model is good? For regression, a common measure is <strong>Root Mean Square Error (RMSE)</strong>. Equation 2-1 shows the formula.<figure><img src=/bookshelf/hands-on-ml/ch-2-eq-1.png alt=image width=350></figure></p><ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> RMSE gives us a sense of how much error our system typically makes in its predictions, in the same units as the thing we&rsquo;re predicting (dollars, in this case).
It penalizes larger errors more heavily because of the squaring.</li></ul></li><li><p>The book then introduces some common <strong>Notations</strong> (page 40), which are standard in ML:</p><ul><li><code>m</code>: Number of instances.</li><li><code>x⁽ⁱ⁾</code>: Feature vector for the i-th instance (all input attributes like longitude, population, etc., for one district).</li><li><code>y⁽ⁱ⁾</code>: Label for the i-th instance (the actual median house price for that district).</li><li><code>X</code> (capital X): Matrix containing all feature vectors for all instances (one row per district).</li><li><code>h</code>: The prediction function (our model), also called the hypothesis.</li><li><code>ŷ⁽ⁱ⁾</code> (y-hat): The predicted value for the i-th instance, i.e., <code>h(x⁽ⁱ⁾)</code>.</li></ul></li><li><p>What if RMSE isn&rsquo;t ideal? If you have many outliers, you might prefer <strong>Mean Absolute Error (MAE)</strong> (Equation 2-2, page 41).<figure><img src=/bookshelf/hands-on-ml/ch-2-eq-2.png alt=image width=350></figure></p><ul><li><em>What it&rsquo;s trying to achieve:</em> MAE measures the average absolute difference. It&rsquo;s less sensitive to outliers than RMSE because it doesn&rsquo;t square the errors.</li></ul></li><li><p>The book connects these to different <strong>norms</strong>: RMSE is related to the Euclidean norm (l₂ norm – your standard straight-line distance), and MAE to the Manhattan norm (l₁ norm – like distance in a city grid). Generally, higher norm indices focus more on large values. RMSE (l₂) is usually preferred when outliers are rare (like in a bell-shaped distribution).(more details at the end of the chapter glossary)</p></li></ul></li><li><p><strong>Check the Assumptions (Page 42):</strong></p><ul><li>Super important sanity check! We <em>assume</em> the downstream system needs actual price values. What if it actually converts prices into categories like &ldquo;cheap,&rdquo; &ldquo;medium,&rdquo; &ldquo;expensive&rdquo;? Then our regression task is overkill; it should have been a classification task!</li><li>Fortunately, the book says our colleagues confirm they need actual prices. Phew!</li></ul></li></ul><p><strong>(Page 42-50: Get the Data)</strong></p><p>Time to get our hands dirty!</p><ul><li><p><strong>Create the Workspace (Page 42-45):</strong></p><ul><li>This is about setting up your Python environment. The book walks through installing Python, Jupyter, NumPy, pandas, Matplotlib, and Scikit-Learn.</li><li>It <em>strongly recommends</em> using an <strong>isolated environment</strong> (like <code>virtualenv</code> or <code>conda env</code>). This is crucial in real projects to avoid library version conflicts. Think of it as a clean, dedicated lab space for each experiment.</li><li>Then, it shows how to start Jupyter and create a new notebook (Figure 2-3, 2-4).</li></ul></li><li><p><strong>Download the Data (Page 46):</strong></p><ul><li>In real life, data might be in a database, spread across files, etc. Here, it&rsquo;s simpler: a single compressed file.</li><li>The book provides a nifty Python function <code>fetch_housing_data()</code> to download and extract the data. Why a function? Automation! If data changes regularly, you can re-run the script.</li></ul></li><li><p><strong>Take a Quick Look at the Data Structure (Page 47-50):</strong></p><ul><li>First, a function <code>load_housing_data()</code> using pandas to load the CSV into a DataFrame.</li><li><code>.head()</code> (Figure 2-5): Shows the top 5 rows. We see attributes like longitude, latitude, housing_median_age, total_rooms, etc. 10 attributes in total.</li><li><code>.info()</code> (Figure 2-6): Super useful!<ul><li>20,640 instances (rows) – fairly small for ML, but good for learning.</li><li><code>total_bedrooms</code> has only 20,433 non-null values. This means 207 districts are missing this data. We&rsquo;ll need to handle this!</li><li>Most attributes are <code>float64</code> (numerical), but <code>ocean_proximity</code> is an <code>object</code>. Since it&rsquo;s from a CSV, it&rsquo;s likely text, probably categorical.</li></ul></li><li><code>.value_counts()</code> on <code>ocean_proximity</code> (page 48) confirms it&rsquo;s categorical: <code>&lt;1H OCEAN</code>, <code>INLAND</code>, etc., with counts for each.</li><li><code>.describe()</code> (Figure 2-7, page 49): Shows summary statistics for numerical attributes (count, mean, std, min, 25th/50th/75th percentiles, max).<ul><li><code>std</code> (standard deviation) tells us how spread out the values are.</li><li>Percentiles (quartiles) give us a sense of the distribution. 25% of districts have <code>housing_median_age</code> &lt; 18.</li></ul></li><li><strong>Histograms</strong> (page 49, Figure 2-8 on page 50): A histogram for each numerical attribute.
This helps us see:<ol><li><strong>Median Income:</strong> Doesn&rsquo;t look like USD. The book clarifies it&rsquo;s scaled (roughly tens of thousands of dollars) and capped at 15 (for high incomes) and 0.5 (for low incomes). Preprocessing is common.</li><li><strong>Housing Median Age & Median House Value:</strong> Also capped. The <code>median_house_value</code> cap (our target variable!) could be a problem. If the model learns prices never go above $500k, it can&rsquo;t predict higher. We might need to get uncapped data or remove these districts.</li><li><strong>Different Scales:</strong> Attributes have very different ranges (e.g., <code>total_rooms</code> vs. <code>median_income</code>). Many ML algorithms don&rsquo;t like this. We&rsquo;ll need <strong>feature scaling</strong>.</li><li><strong>Tail-Heavy Distributions:</strong> Many histograms are skewed (e.g., extend far to the right). This can make it harder for algorithms to detect patterns. We might try transforming them (e.g., log transform) later.<figure><img src=/bookshelf/hands-on-ml/ch-2-img-1.png alt=image width=750></figure></li></ol></li></ul></li></ul><p><strong>(Page 51-55: Create a Test Set)</strong></p><p>&ldquo;Wait!&rdquo; says the scorpion icon (page 51). &ldquo;Before you look at the data any further, you need to create a test set, put it aside, and never look at it.&rdquo;</p><ul><li><strong>Why so early?</strong> Our brains are amazing pattern detectors. If we look too closely at the <em>entire</em> dataset, including what will be our test set, we might subconsciously pick up on patterns in the test data. This is <strong>data snooping bias</strong>. When we later evaluate our model on this &ldquo;seen&rdquo; test set, our performance estimate will be too optimistic. The model won&rsquo;t generalize as well in the real world.</li><li><strong>How to create it?</strong> Theoretically, pick ~20% randomly and set aside.<ul><li>The book gives a simple <code>split_train_test</code> function (page 52).</li><li>Problem: Running it again gives a <em>different</em> test set. Over time, your algorithm (or you!) might see the whole dataset.</li><li>Solutions:<ol><li>Save the test set on the first run.</li><li>Set a random seed (<code>np.random.seed(42)</code> – 42 is the &ldquo;Answer to the Ultimate Question of Life, the Universe, and Everything,&rdquo; a common tongue-in-cheek seed).</li></ol></li><li>But these break if you fetch an <em>updated</em> dataset.</li><li><strong>Better solution for stable splits:</strong> Use a unique, immutable instance identifier. Compute a hash of the ID, and if the hash is below a threshold (e.g., 20% of max hash value), it goes into the test set. This ensures consistency even if the dataset is refreshed (new instances go to train/test consistently, old instances stay put).<ul><li>The housing dataset doesn&rsquo;t have an ID column. Simplest is to use row index (if data is only ever appended). Or create an ID from stable features (e.g., latitude + longitude, though page 53 notes this can cause issues if coarse).</li></ul></li></ul></li><li><strong>Scikit-Learn&rsquo;s <code>train_test_split</code> (page 53):</strong> Simpler, has <code>random_state</code> for reproducibility. Can also split multiple datasets (e.g., features and labels) consistently.</li><li><strong>Stratified Sampling (Page 53-55):</strong><ul><li>Random sampling is fine for large datasets. For smaller ones, you risk <strong>sampling bias</strong>. Imagine surveying 1000 people: random sampling might accidentally give you 60% men, not representative of the population.</li><li><strong>Stratified sampling</strong> ensures the test set is representative of important subgroups (strata).</li><li>Experts say <code>median_income</code> is very important. We want our test set to have a similar income distribution to the full dataset.</li><li>Since <code>median_income</code> is continuous, we first create an <strong>income category attribute</strong> (<code>pd.cut()</code>) with 5 categories (Figure 2-9, page 54). Not too many strata, each large enough.</li><li>Then use Scikit-Learn&rsquo;s <code>StratifiedShuffleSplit</code> to sample, ensuring similar proportions of each income category in train and test sets.</li><li>Figure 2-10 (page 55) shows stratified sampling is much better than random sampling at preserving income category proportions.</li><li>Finally, drop the temporary <code>income_cat</code> attribute.</li><li>This attention to test set creation is critical but often overlooked!</li></ul></li></ul><p><strong>(Page 56-61: Discover and Visualize the Data to Gain Insights)</strong></p><p>Now, working <em>only</em> with the training set (or a copy, <code>strat_train_set.copy()</code>):</p><ul><li><p><strong>Visualizing Geographical Data (Page 56-57):</strong></p><ul><li>Scatter plot of <code>longitude</code> vs. <code>latitude</code> (Figure 2-11). Looks like California, but hard to see patterns.</li><li>Setting <code>alpha=0.1</code> (Figure 2-12) reveals high-density areas (Bay Area, LA, San Diego, Central Valley).</li><li>More advanced plot (Figure 2-13, page 57): Circle radius = population, color = median house price. Clearly shows prices are higher near coasts and in dense areas.</li></ul></li><li><p><strong>Looking for Correlations (Page 58-60):</strong></p><ul><li><strong>Standard correlation coefficient (Pearson&rsquo;s r):</strong> Ranges from -1 (strong negative correlation) to 1 (strong positive correlation). 0 means no <em>linear</em> correlation.</li><li>Use <code>.corr()</code> method on the DataFrame.</li><li><code>corr_matrix["median_house_value"].sort_values(ascending=False)</code> shows <code>median_income</code> has the strongest positive correlation (0.68) with <code>median_house_value</code>.</li><li>Figure 2-14 (page 59) shows examples: correlation only captures <em>linear</em> relationships. Nonlinear relationships can have r=0.</li><li><strong><code>scatter_matrix</code> function (pandas.plotting):</strong> Plots every numerical attribute against every other (Figure 2-15, page 60). Diagonal shows histograms. Focus on attributes promising for predicting <code>median_house_value</code>.</li><li>Zoom in on <code>median_income</code> vs. <code>median_house_value</code> (Figure 2-16, page 61):<ul><li>Strong upward trend.</li><li>Price cap at $500k clearly visible.</li><li>Other less obvious horizontal lines (data quirks around $450k, $350k). Might want to remove these districts to prevent model learning these quirks.</li></ul></li></ul></li><li><p><strong>Experimenting with Attribute Combinations (Page 61):</strong></p><ul><li>Sometimes, combining attributes makes more sense.</li><li><code>total_rooms</code> isn&rsquo;t useful without <code>households</code>. So, create <code>rooms_per_household</code>.</li><li>Similarly, <code>bedrooms_per_room</code> and <code>population_per_household</code>.</li><li>Check correlations again (page 62). <code>bedrooms_per_room</code> is more negatively correlated with price than <code>total_bedrooms</code> or <code>total_rooms</code>. Lower bedroom/room ratio (fewer bedrooms for a given house size, or larger rooms) often means more expensive.<figure><img src=/bookshelf/hands-on-ml/ch-2-img-2.png alt=image width=1050></figure></li></ul></li></ul><p>This exploration is iterative. Get a prototype, analyze its output, come back here.</p><p><strong>(Page 62-71: Prepare the Data for Machine Learning Algorithms)</strong></p><p>Time to write functions for data transformations. Why functions?</p><ul><li>Reproducibility.</li><li>Build a library of transformations.</li><li>Use in live system for new data.</li></ul><p>First, revert to a clean training set and separate predictors (<code>housing</code>) from labels (<code>housing_labels</code>).</p><ul><li><p><strong>Data Cleaning (Missing Values - page 63):</strong></p><ul><li>We saw <code>total_bedrooms</code> has missing values. Options:<ol><li>Get rid of corresponding districts: <code>housing.dropna(subset=["total_bedrooms"])</code></li><li>Get rid of the whole attribute: <code>housing.drop("total_bedrooms", axis=1)</code></li><li>Set values to something (zero, mean, median): <code>median = housing["total_bedrooms"].median()</code>, then <code>housing["total_bedrooms"].fillna(median, inplace=True)</code></li></ol></li><li>If using option 3, compute median on <em>training set</em> and save it to fill missing values in test set and live data.</li><li><strong>Scikit-Learn&rsquo;s <code>SimpleImputer</code> (page 63):</strong><ul><li>Create an imputer: <code>imputer = SimpleImputer(strategy="median")</code></li><li>It only works on numerical attributes, so drop <code>ocean_proximity</code> first: <code>housing_num = housing.drop("ocean_proximity", axis=1)</code></li><li>Fit the imputer to numerical training data: <code>imputer.fit(housing_num)</code> (computes medians).</li><li>The medians are stored in <code>imputer.statistics_</code>.</li><li>Transform the training set: <code>X = imputer.transform(housing_num)</code> (returns NumPy array).</li><li>Convert back to DataFrame if needed.</li></ul></li><li><strong>Scikit-Learn Design Principles (sidebar, page 64-65):</strong><ul><li><strong>Consistency:</strong> Estimators, Transformers, Predictors.</li><li><strong>Estimators:</strong> Any object that can estimate params from data (e.g., <code>imputer</code>). <code>fit(data, [labels])</code> method. Hyperparameters set in constructor.</li><li><strong>Transformers:</strong> Estimators that can transform data (e.g., <code>imputer</code>). <code>transform(data)</code> method. Also <code>fit_transform()</code> (often optimized).</li><li><strong>Predictors:</strong> Estimators that can make predictions (e.g., <code>LinearRegression</code>). <code>predict(data)</code> method. <code>score(data, labels)</code> method.</li><li><strong>Inspection:</strong> Hyperparameters are public instance vars (e.g., <code>imputer.strategy</code>). Learned params end with <code>_</code> (e.g., <code>imputer.statistics_</code>).</li><li><strong>Nonproliferation of classes:</strong> Uses NumPy arrays, basic Python types.</li><li><strong>Composition:</strong> Easy to combine (e.g., <code>Pipeline</code>).</li><li><strong>Sensible defaults.</strong></li></ul></li></ul></li><li><p><strong>Handling Text and Categorical Attributes (Page 65-67):</strong></p><ul><li><code>ocean_proximity</code> is categorical.</li><li><strong><code>OrdinalEncoder</code> (Scikit-Learn >= 0.20):</strong> Converts categories to numbers (0, 1, 2&mldr;). (Page 66).<ul><li>Problem: ML algorithms assume nearby numbers are more similar (e.g., 0 and 1 more similar than 0 and 4). This isn&rsquo;t true for <code>ocean_proximity</code>.</li></ul></li><li><strong><code>OneHotEncoder</code> (Scikit-Learn):</strong> Common solution (page 67). Creates one binary attribute per category (dummy variables). Only one is &ldquo;hot&rdquo; (1), others &ldquo;cold&rdquo; (0).<ul><li>Output is a SciPy <strong>sparse matrix</strong> (efficient for many categories, as it only stores non-zero locations). Can convert to dense with <code>.toarray()</code>.</li></ul></li><li>If many categories, one-hot encoding creates many features. Alternatives: replace with numerical related features (e.g., distance to ocean) or use <strong>embeddings</strong> (learnable low-dimensional vectors, Ch 13 & 17).</li></ul></li><li><p><strong>Custom Transformers (Page 68):</strong></p><ul><li>For custom cleanup or attribute combination.</li><li>Create a class, implement <code>fit()</code>, <code>transform()</code>, <code>fit_transform()</code>.</li><li>Add <code>TransformerMixin</code> (gets <code>fit_transform()</code> for free) and <code>BaseEstimator</code> (gets <code>get_params()</code>, <code>set_params()</code> for hyperparameter tuning, if no <code>*args</code>, <code>**kwargs</code> in constructor).</li><li>Example: <code>CombinedAttributesAdder</code> class to add <code>rooms_per_household</code> etc. Has a hyperparameter <code>add_bedrooms_per_room</code>.</li></ul></li><li><p><strong>Feature Scaling (Page 69):</strong></p><ul><li>Crucial! ML algos often perform badly if numerical inputs have very different scales.</li><li>Two common ways:<ol><li><strong>Min-max scaling (Normalization):</strong> Values shifted/rescaled to range 0-1. Subtract min, divide by (max-min). Scikit-Learn: <code>MinMaxScaler</code>.</li><li><strong>Standardization:</strong> Subtract mean, divide by std dev. Resulting distribution has zero mean, unit variance. Not bounded to a specific range. Less affected by outliers. Scikit-Learn: <code>StandardScaler</code>.</li></ol></li><li><strong>Important:</strong> Fit scalers on <em>training data only</em>! Then use them to transform train set, test set, and new data.</li></ul></li><li><p><strong>Transformation Pipelines (Page 70-71):</strong></p><ul><li>Many data prep steps need to be in order. Scikit-Learn&rsquo;s <code>Pipeline</code> helps.</li><li>Example <code>num_pipeline</code> for numerical attributes (page 70): <code>SimpleImputer</code> -> <code>CombinedAttributesAdder</code> -> <code>StandardScaler</code>.</li><li>Call <code>pipeline.fit_transform(housing_num)</code>.</li><li><strong><code>ColumnTransformer</code> (Scikit-Learn >= 0.20):</strong> Applies different transformers to different columns. Super convenient! (Page 71)<ul><li>Takes list of (name, transformer, column_list) tuples.</li><li>Applies <code>num_pipeline</code> to numerical columns, <code>OneHotEncoder</code> to categorical.</li><li>Concatenates outputs. Handles sparse/dense matrix mix.</li><li>This gives one preprocessing pipeline for all data!</li></ul></li></ul></li></ul><p><strong>(Page 72-74: Select and Train a Model)</strong></p><p>Finally, the exciting part!</p><ul><li><p><strong>Training and Evaluating on the Training Set (Page 72):</strong></p><ul><li><strong>Linear Regression:</strong><ul><li><code>lin_reg = LinearRegression()</code></li><li><code>lin_reg.fit(housing_prepared, housing_labels)</code></li><li>Try on a few instances: predictions aren&rsquo;t great.</li><li>RMSE on whole training set: $68,628. Not good (most house values are $120k-$265k). This is <strong>underfitting</strong>. Features might not be good enough, or model too simple.</li></ul></li><li><strong>Decision Tree Regressor (more complex model, Ch 6):</strong><ul><li><code>tree_reg = DecisionTreeRegressor()</code></li><li><code>tree_reg.fit(housing_prepared, housing_labels)</code></li><li>Evaluate on training set: RMSE = 0.0! Perfect? (Page 73)</li><li>No, much more likely it badly <strong>overfit</strong> the data.</li><li>Don&rsquo;t touch test set yet! Use part of training set for validation.</li></ul></li></ul></li><li><p><strong>Better Evaluation Using Cross-Validation (Page 73-74):</strong></p><ul><li>Option 1: <code>train_test_split</code> training set into smaller train/validation.</li><li>Option 2 (better): <strong>K-fold cross-validation</strong>.<ul><li>Scikit-Learn&rsquo;s <code>cross_val_score</code>. Splits training set into K folds (e.g., 10). Trains K times, each time using a different fold for evaluation and other K-1 folds for training. Returns K scores.</li><li><strong>Scoring function:</strong> <code>cross_val_score</code> expects utility (higher is better), not cost (lower is better). So use <code>scoring="neg_mean_squared_error"</code>. Then take <code>np.sqrt(-scores)</code>.</li><li>Decision Tree CV scores (page 74): Mean RMSE ~71,407. Standard deviation ~2,439. Much worse than 0! And worse than Linear Regression.</li><li>Linear Regression CV scores: Mean RMSE ~69,052. Std dev ~2,732.</li><li>Decision Tree is overfitting badly.</li></ul></li><li><strong>Random Forest Regressor (Ensemble Learning, Ch 7):</strong><ul><li>Trains many Decision Trees on random subsets of features, averages predictions.</li><li>Skip code details (similar to others).</li><li>Training set RMSE (page 75, <code>forest_rmse</code>): ~18,603.</li><li>CV scores: Mean RMSE ~50,182. Std dev ~2,097.</li><li>Much better! But training score is still much lower than validation scores -> still overfitting. Solutions: simplify model, regularize, get more data.</li></ul></li><li>Goal: Shortlist 2-5 promising models (SVMs, neural nets, etc.) without too much hyperparameter tweaking yet.</li><li><strong>Save your models!</strong> Use <code>joblib</code> (better for large NumPy arrays) or <code>pickle</code>. Save hyperparameters, CV scores, predictions.</li></ul></li></ul><p><strong>(Page 75-78: Fine-Tune Your Model)</strong></p><p>Now, take your shortlisted models and fine-tune their hyperparameters.</p><ul><li><p><strong>Grid Search (Page 76):</strong></p><ul><li>Tedious to do manually. Use Scikit-Learn&rsquo;s <code>GridSearchCV</code>.</li><li>Tell it which hyperparameters to try and what values.</li><li>It uses cross-validation to evaluate all combinations.</li><li>Example <code>param_grid</code> for <code>RandomForestRegressor</code>.</li><li>It will explore (3x4 + 2x3) = 18 combinations, each trained 5 times (for cv=5). 90 rounds of training!</li><li><code>grid_search.best_params_</code> gives the best combination.</li><li><code>grid_search.best_estimator_</code> gives the best model retrained on the full training set (if <code>refit=True</code>, default).</li><li>Evaluation scores for each combo are in <code>grid_search.cv_results_</code>.</li><li>Example result (page 77): RMSE 49,682. Better than default 50,182.</li><li>Can treat data prep steps as hyperparameters too! (e.g., <code>add_bedrooms_per_room</code> in your custom transformer).</li></ul></li><li><p><strong>Randomized Search (Page 78):</strong></p><ul><li><code>RandomizedSearchCV</code>. Good for large hyperparameter search spaces.</li><li>Instead of trying all combos, evaluates a given number of random combinations.</li><li>Benefits: explores more values per hyperparameter if run long enough; more control over budget.</li></ul></li><li><p><strong>Ensemble Methods (Page 78):</strong></p><ul><li>Combine best models. Often performs better than any single model, especially if they make different types of errors. (More in Ch 7).</li></ul></li><li><p><strong>Analyze the Best Models and Their Errors (Page 78-79):</strong></p><ul><li>Gain insights! <code>RandomForestRegressor</code> can show <strong>feature importances</strong>.</li><li>List importances with attribute names (page 79). <code>median_income</code> is most important. <code>INLAND</code> (categorical feature) is surprisingly important.</li><li>Might decide to drop less useful features.</li><li>Look at specific errors your system makes. Why? How to fix?</li></ul></li></ul><p><strong>(Page 79-80: Evaluate Your System on the Test Set)</strong></p><p>The moment of truth! After all tweaking, evaluate the final model on the test set.</p><ul><li>Get predictors (<code>X_test</code>) and labels (<code>y_test</code>) from <code>strat_test_set</code>.</li><li>Run <code>full_pipeline.transform(X_test)</code> (NOT <code>fit_transform</code> – don&rsquo;t fit to test set!).</li><li><code>final_model = grid_search.best_estimator_</code></li><li><code>final_predictions = final_model.predict(X_test_prepared)</code></li><li><code>final_mse = mean_squared_error(y_test, final_predictions)</code></li><li><code>final_rmse = np.sqrt(final_mse)</code> -> e.g., 47,730.</li><li>Point estimate might not be enough. Compute a <strong>95% confidence interval</strong> using <code>scipy.stats.t.interval()</code>.</li><li>If performance is worse than CV scores (often is due to tuning on validation set), <strong>resist temptation to tweak based on test set results!</strong> Those tweaks are unlikely to generalize.</li></ul><p><strong>(Page 80-83: Launch, Monitor, and Maintain Your System)</strong></p><p>You got approval to launch!</p><ul><li><strong>Prelaunch:</strong> Present solution, highlight learnings, assumptions, limitations. Document. Create clear visualizations.</li><li><strong>Launch:</strong> Polish code, write docs, tests. Deploy model.<ul><li>Save trained pipeline (e.g., with <code>joblib</code>).</li><li>Load in production environment.</li><li>Maybe wrap in a web service that your app queries via REST API (Figure 2-17, page 81). Easier upgrades, scaling.</li><li>Or deploy on cloud (e.g., Google Cloud AI Platform).</li></ul></li><li><strong>Monitor:</strong> This is not the end!<ul><li>Check live performance regularly. Trigger alerts if it drops.</li><li>Models &ldquo;rot&rdquo; over time as the world changes. (Cats and dogs don&rsquo;t mutate, but camera tech and breed popularity do!)</li><li>Infer performance from downstream metrics (e.g., sales of recommended products).</li><li>Or use human raters / crowdsourcing (Amazon Mechanical Turk) for tasks needing human judgment.</li><li>Monitor input data quality (malfunctioning sensors, stale data from other teams).</li></ul></li><li><strong>Maintain:</strong><ul><li>Automate as much as possible:<ul><li>Collect fresh data regularly and label it.</li><li>Script to retrain model and fine-tune hyperparameters.</li><li>Script to evaluate new vs. old model on updated test set, deploy if better.</li></ul></li><li>Keep backups of models and datasets. Easy rollback. Compare new models to old.</li><li>Evaluate on specific subsets of test data for deeper insights (e.g., recent data, specific input types like inland vs. ocean districts).</li></ul></li></ul><p>ML involves a lot of infrastructure! First project is a lot of effort. Once it&rsquo;s in place, future projects are faster.</p><p><strong>(Page 83-84: Try It Out! & Exercises)</strong>
The chapter ends by encouraging you to try this whole process on a dataset you&rsquo;re interested in (e.g., from Kaggle). And then lists some great exercises based on the housing dataset. Definitely try these!</p><hr><h1 id=glossary>Glossary</h1><h2 id=rmse--mae-a-deep-dive>RMSE , MAE a Deep dive</h2><p>Think about it this way: both RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) are trying to tell us, on average, <strong>how &ldquo;far off&rdquo; our model&rsquo;s predictions are from the actual values.</strong> The &ldquo;how far off&rdquo; part is where the idea of &ldquo;distance&rdquo; comes in, and norms are just formal mathematical ways of defining distance.</p><p>Let&rsquo;s look at a single prediction error first. Suppose the actual house price (<code>y</code>) is $300,000 and our model predicts (<code>ŷ</code>) $250,000. The error is $50,000.</p><ul><li><p><strong>MAE (Mean Absolute Error) and the l₁ norm (Manhattan Distance):</strong></p><ul><li>MAE takes the <em>absolute value</em> of this error: |$250,000 - $300,000| = $50,000.</li><li>Then it averages these absolute errors over all your predictions.</li><li>The <strong>l₁ norm</strong> (read as &ldquo;L-one norm&rdquo;) of a vector (think of a vector of errors <code>[e₁, e₂, ..., eₘ]</code>) is the sum of the absolute values of its components: <code>|e₁| + |e₂| + ... + |eₘ|</code>. The MAE is just this sum divided by <code>m</code> (the number of errors).</li><li><strong>Why &ldquo;Manhattan Distance&rdquo;?</strong> Imagine you&rsquo;re in a city like Manhattan where streets form a grid. To get from point A to point B, you can&rsquo;t cut diagonally through buildings. You have to travel along the streets (say, 3 blocks east and 4 blocks north). The total distance is 3 + 4 = 7 blocks. This is the l₁ distance. You&rsquo;re summing the absolute differences along each axis (the &ldquo;east-west&rdquo; error and the &ldquo;north-south&rdquo; error, if you will). In our error context, we only have one &ldquo;axis&rdquo; of error for each prediction, and MAE sums these absolute &ldquo;street-block&rdquo; errors.</li></ul></li><li><p><strong>RMSE (Root Mean Square Error) and the l₂ norm (Euclidean Distance):</strong></p><ul><li>RMSE takes the error ($50,000), <strong>squares it</strong> (($50,000)² = 2,500,000,000), then averages these squared errors, and finally takes the <strong>square root</strong> of that average.</li><li>The <strong>l₂ norm</strong> (read as &ldquo;L-two norm&rdquo;) of a vector of errors <code>[e₁, e₂, ..., eₘ]</code> is <code>√(e₁² + e₂² + ... + eₘ²)</code>.</li><li><strong>Why &ldquo;Euclidean Distance&rdquo;?</strong> This is the &ldquo;as the crow flies&rdquo; distance you learned in geometry using the Pythagorean theorem: <code>distance = √(Δx² + Δy²)</code>. If you have a vector of errors, the l₂ norm is like calculating the length of that vector in a multi-dimensional space. It&rsquo;s our everyday understanding of straight-line distance.</li><li>When RMSE squares the errors, it gives much more weight to larger errors. An error of 10 becomes 100, but an error of 2 becomes 4. The error of 10 contributes 25 times more to the sum of squares than the error of 2 (100 vs 4), whereas for MAE, it would only contribute 5 times more (10 vs 2). The square root at the end brings the measurement back into the original units (e.g., dollars).</li></ul></li></ul><p><strong>So, what&rsquo;s the ultimate goal of connecting them to norms?</strong></p><ol><li><p><strong>Generalization:</strong> Norms are a general mathematical concept for measuring the &ldquo;size&rdquo; or &ldquo;length&rdquo; of vectors. By recognizing that RMSE and MAE are related to specific norms (l₂ and l₁ respectively), it places them within a broader mathematical framework. There are other norms too (l₀, l∞, lₚ in general), and each has different properties and might be useful in different contexts for measuring error or regularizing models.</p><ul><li>For instance, the l₀ norm counts the number of non-zero elements in a vector.</li><li>The l∞ (L-infinity) norm gives the maximum absolute value in a vector (it focuses <em>only</em> on the largest error).</li></ul></li><li><p><strong>Understanding Properties:</strong> Knowing the underlying norm helps us understand the properties of the error metric.</p><ul><li>Because <strong>RMSE uses the l₂ norm (squaring)</strong>, it&rsquo;s more sensitive to outliers. One very large error, when squared, can dominate the RMSE.</li><li>Because <strong>MAE uses the l₁ norm (absolute value)</strong>, it&rsquo;s more robust to outliers. A large error is just a large error; it&rsquo;s not disproportionately magnified.</li></ul></li><li><p><strong>Consistency in Terminology:</strong> In more advanced ML literature, you&rsquo;ll often see error functions or regularization terms described using norm notation (e.g., &ldquo;l₁ regularization&rdquo; or &ldquo;l₂ regularization&rdquo;). Understanding this connection early on helps you navigate that.</p></li></ol><p><strong>Let&rsquo;s try a simple analogy:</strong></p><p>Imagine you have two friends, Alex (l₁) and Beth (l₂), and you ask them to measure how &ldquo;messy&rdquo; a room is based on how far items are from their correct places.</p><ul><li><strong>Alex (l₁ / MAE):</strong> For each misplaced item, Alex measures how far it is from its spot (e.g., book is 2 feet off, sock is 3 feet off). Alex just adds up these distances (2+3=5 feet of &ldquo;mess&rdquo;). Alex treats every foot of displacement equally.</li><li><strong>Beth (l₂ / RMSE):</strong> For each misplaced item, Beth measures the distance, <em>squares it</em> (book 2²=4, sock 3²=9), adds them up (4+9=13), and then might take a square root. Beth gets <em>really</em> upset by items that are <em>very</em> far off, because squaring makes those large distances much bigger. A sock 10 feet away (10²=100) is much, much worse to Beth than 5 socks each 2 feet away (5 * 2² = 20).</li></ul><p>So, when the book says &ldquo;RMSE corresponds to the Euclidean norm&rdquo; and &ldquo;MAE corresponds to the l₁ norm,&rdquo; it&rsquo;s essentially saying:</p><ul><li>RMSE measures error in a way that emphasizes large errors more (like a straight-line distance calculation that involves squaring).</li><li>MAE measures error in a way that treats all magnitudes of error more evenly (like adding up block-by-block distances).</li></ul><p>Did that make it a bit clearer? The key is that they are both ways to quantify &ldquo;how far off&rdquo; you are, but they &ldquo;feel&rdquo; or &ldquo;react&rdquo; to large errors differently because of their mathematical structure, which is captured by these different types of norms.</p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer></body></html>