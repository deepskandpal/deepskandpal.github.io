<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><a href=#an-overview-of-transformer-models>An Overview of Transformer Models</a><ul><li><a href=#the-inputs-and-outputs-of-a-trained-transformer-llm>The Inputs and Outputs of a Trained Transformer LLM</a></li><li><a href=#autoregressive-generation>Autoregressive Generation</a></li></ul></li><li><a href=#the-components-of-the-forward-pass>The Components of the Forward Pass</a></li><li><a href=#choosing-a-single-token-from-the-probability-distribution-samplingdecoding>Choosing a Single Token from the Probability Distribution (Sampling/Decoding)</a></li><li><a href=#parallel-token-processing-and-context-size>Parallel Token Processing and Context Size</a></li><li><a href=#speeding-up-generation-by-caching-keys-and-values>Speeding Up Generation by Caching Keys and Values</a></li><li><a href=#inside-the-transformer-block>Inside the Transformer Block</a><ul><li><a href=#attention-is-all-you-need-the-deep-dive>Attention is All You Need: The Deep Dive</a></li></ul></li><li><a href=#recent-improvements-to-the-transformer-architecture>Recent Improvements to the Transformer Architecture</a><ul><li><a href=#more-efficient-attention>More Efficient Attention</a></li><li><a href=#the-modern-transformer-block>The Modern Transformer Block</a></li><li><a href=#positional-embeddings-rope>Positional Embeddings (RoPE)</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 3: Looking Inside Large Language Models</h1><span class=reading-time><em>15 min read</em></span><div class=book-details><div class=book-content><p>Let&rsquo;s begin our deep dive into <strong>Chapter 3: Looking Inside Large Language Models</strong>. We&rsquo;ll proceed cover to cover, just as you asked.</p><p>Now, we address the main event: what happens <em>after</em> tokenization? How does a model like the Transformer take those initial embeddings and generate coherent, often brilliant, text? That&rsquo;s our mission for today.</p><p>As the book mentions, we&rsquo;ll start by loading our model, just to have it ready. This is the same <code>microsoft/Phi-3-mini-4k-instruct</code> model we&rsquo;ve seen before. It&rsquo;s a powerful yet manageable model that&rsquo;s perfect for our hands-on exploration.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoModelForCausalLM, AutoTokenizer, pipeline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load model and tokenizer</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;microsoft/Phi-3-mini-4k-instruct&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;microsoft/Phi-3-mini-4k-instruct&#34;</span>,
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cuda&#34;</span>,
</span></span><span style=display:flex><span>    torch_dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>    trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create a pipeline</span>
</span></span><span style=display:flex><span>generator <span style=color:#f92672>=</span> pipeline(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;text-generation&#34;</span>,
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model,
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>=</span>tokenizer,
</span></span><span style=display:flex><span>    return_full_text<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
</span></span><span style=display:flex><span>    do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>With our tools at the ready, let&rsquo;s start with a high-level view and peel back the layers one by one.</p><h2 id=an-overview-of-transformer-models>An Overview of Transformer Models</h2><h3 id=the-inputs-and-outputs-of-a-trained-transformer-llm>The Inputs and Outputs of a Trained Transformer LLM</h3><p>At the highest level of abstraction, a generative LLM is a system that takes a text prompt as input and produces a text generation as output. Think of it as a highly sophisticated text completion machine. Figure 3-1 shows this perfectly: you provide a prompt, and the Transformer LLM generates a relevant completion.</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-1.png alt="Figure 3-1" width=700></figure><p>However, a crucial point often missed is that this generation is not instantaneous. The model doesn&rsquo;t produce the entire email in one go. As Figure 3-2 shows below, the process is sequential.</p><p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-2.png alt="Figure 3-2" width=700></figure>The model generates <strong>one token at a time</strong>. For the prompt about the gardening mishap, it first generates &ldquo;Dear&rdquo;, then &ldquo;Sarah&rdquo;, then a comma, then a newline character (<code>\n</code>), and so on.</p><p>This brings us to one of the most important concepts for generative models.</p><h3 id=autoregressive-generation>Autoregressive Generation</h3><p>Look at Figure 3-3 below. It shows that after the model generates the first token (&ldquo;Dear&rdquo;), that token is appended to the original prompt.</p><p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-3.png alt="Figure 3-3" width=700></figure>This new, slightly longer sequence (&ldquo;Write an email&mldr; Dear&rdquo;) becomes the input for the <em>next</em> step. The model then processes this new input to generate the <em>next</em> token (&ldquo;Sarah&rdquo;).</p><p>This process, where a model&rsquo;s own previous outputs are fed back in as inputs for subsequent steps, is called <strong>autoregression</strong>. This is <em>the</em> core mechanism of generative LLMs. They are, fundamentally, autoregressive models. This is what we&rsquo;re seeing when we run the <code>generator</code> pipeline from the book:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&#34;</span>
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> generator(prompt)
</span></span><span style=display:flex><span>print(output[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;generated_text&#39;</span>])
</span></span></code></pre></div><p>The output begins, token by token, until it hits the <code>max_new_tokens</code> limit we set.</p><p><code>Subject: My Sincere Apologies for the Gardening Mishap...</code></p><p>The software wrapper around the neural network (in this case, the <code>pipeline</code>) handles this loop for us, but it&rsquo;s essential to know it&rsquo;s happening.</p><h2 id=the-components-of-the-forward-pass>The Components of the Forward Pass</h2><p>So, what&rsquo;s inside this &ldquo;Transformer LLM&rdquo; box that allows for this token-by-token generation? As we can see in Figure 3-4 below, there are three main components working in concert during a single &ldquo;forward pass&rdquo; (the process of taking an input and producing an output):</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-4.png alt="Figure 3-4" width=700></figure><ol><li><strong>Tokenizer</strong>: We know this from Chapter 2. It converts the input text into a sequence of token IDs.</li><li><strong>Stack of Transformer Blocks</strong>: This is the computational core of the model. It&rsquo;s a deep stack of identical layers that process the token embeddings. This is where the &ldquo;magic&rdquo; of understanding context happens.</li><li><strong>Language Modeling (LM) Head</strong>: This is the final layer. It takes the processed information from the Transformer blocks and converts it into a usable output: a probability score for <em>every single token</em> in the vocabulary.</li></ol><p>Figure 3-5 beautifully illustrates the connection we established in Chapter 2. The tokenizer has a vocabulary (a lookup table of token IDs to token strings), and the model has a corresponding <strong>token embeddings</strong> matrix. For every token ID from the tokenizer, we fetch its embedding vector to feed into the model.</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-5.png alt="Figure 3-5" width=700></figure><p>The computation flows from top to bottom. The input token IDs are converted to embeddings, which then pass through the entire stack of Transformer blocks. Finally, as shown in Figure 3-6 below, the LM head takes the final processed vector and outputs a probability distribution over the entire vocabulary. In this example, &ldquo;Dear&rdquo; has a 40% probability of being the next token, &ldquo;Title&rdquo; has 13%, and so on.</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-6.png alt="Figure 3-6" width=700></figure><p>Let&rsquo;s look at the model structure itself, as the book does on page 8. When we print the <code>model</code> variable, we see the architecture laid out for us.</p><pre tabindex=0><code>Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(...)
    )
    ...
  )
  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)
)
</code></pre><p>Let&rsquo;s break this down like a senior scientist:</p><ul><li><code>(embed_tokens)</code>: This is our embedding matrix. It has <strong>32,064</strong> rows, one for each token in the vocabulary. Each token is represented by a vector of <strong>3,072</strong> dimensions. This is the <code>d_model</code> or model dimension.</li><li><code>(layers)</code>: This is our stack of Transformer blocks. We see it&rsquo;s a <code>ModuleList</code> containing <strong>32 identical <code>Phi3DecoderLayer</code></strong> blocks.</li><li><code>(lm_head)</code>: This is our language modeling head. It&rsquo;s a simple <code>Linear</code> layer. Notice its dimensions: it takes an input vector of size <code>in_features=3072</code> (the output from the last Transformer block) and projects it to an output vector of size <code>out_features=32064</code> (one score for each token in the vocabulary). This output vector is what we call the <strong>logits</strong>.</li></ul><h2 id=choosing-a-single-token-from-the-probability-distribution-samplingdecoding>Choosing a Single Token from the Probability Distribution (Sampling/Decoding)</h2><p>The LM head gives us logits, which can be converted to probabilities (usually via a softmax function). Now, how do we pick just one token from this distribution? This is the <strong>decoding strategy</strong>.</p><ul><li><strong>Greedy Decoding</strong>: The simplest method, as shown in Figure 3-7 below, is to always pick the token with the highest probability. This is fast and deterministic, but often leads to repetitive and boring text. This is what happens when we set <code>do_sample=False</code> or <code>temperature=0</code>.</li></ul><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-7.png alt="Figure 3-7" width=700></figure><ul><li><strong>Sampling</strong>: A better approach is to <em>sample</em> from the distribution. A token with a 40% probability gets picked 40% of the time. This introduces randomness and creativity. We&rsquo;ll explore this more in Chapter 6.</li></ul><p>Let&rsquo;s trace this with the book&rsquo;s code example. We can get the raw output (logits) from the LM head.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;The capital of France is&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Tokenize the input prompt</span>
</span></span><span style=display:flex><span>input_ids <span style=color:#f92672>=</span> tokenizer(prompt, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#34;cuda&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Get the output of the model before the lm_head</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Note: model.model gets the raw Transformer block outputs</span>
</span></span><span style=display:flex><span>model_output <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>model(input_ids) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Get the output of the lm_head (the logits)</span>
</span></span><span style=display:flex><span>lm_head_output <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>lm_head(model_output[<span style=color:#ae81ff>0</span>])
</span></span></code></pre></div><p>The <code>lm_head_output</code> tensor has a shape of <code>[1, 6, 32064]</code>. This means:</p><ul><li><code>1</code>: A batch size of 1 (one prompt).</li><li><code>6</code>: Sequence length of 6 tokens (&ldquo;The&rdquo;, &ldquo;capital&rdquo;, &ldquo;of&rdquo;, &ldquo;France&rdquo;, &ldquo;is&rdquo;, and a special start token).</li><li><code>32064</code>: The logit score for each token in the vocabulary.</li></ul><p>We only care about predicting the <em>next</em> token, so we look at the logits for the <em>last</em> input token (<code>is</code>). We can get this with <code>lm_head_output[0, -1]</code>. Then, we find the index (token ID) with the highest score using <code>.argmax()</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Get the token ID with the highest score from the last position&#39;s logits</span>
</span></span><span style=display:flex><span>token_id <span style=color:#f92672>=</span> lm_head_output[<span style=color:#ae81ff>0</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>argmax(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Decode the ID back to text</span>
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>decode(token_id)
</span></span></code></pre></div><p>And the output is, unsurprisingly, <code>Paris</code>.</p><h2 id=parallel-token-processing-and-context-size>Parallel Token Processing and Context Size</h2><p>A key reason Transformers superseded older architectures like RNNs is their ability to process tokens in parallel. As Figure 3-8 intuits below, when you input a prompt, the model creates a separate processing &ldquo;stream&rdquo; or &ldquo;track&rdquo; for each token simultaneously.</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-8.png alt="Figure 3-8" width=700></figure><p>There&rsquo;s a limit to how many tokens can be processed at once, known as the <strong>context length</strong> or context window. Our <code>Phi-3-mini-4k-instruct</code> model has a 4K (4096) context length.</p><p>As Figure 3-9 shows below, each of these parallel streams takes an embedding vector as input and, after passing through all the Transformer blocks, produces a final output vector. Crucially, for text generation, <strong>we only use the output vector of the very last token</strong> to feed into the LM head.</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-9.png alt="Figure 3-9" width=700></figure><p>&ldquo;Wait,&rdquo; you might ask, &ldquo;if we only use the last output, why do we bother computing all the previous ones?&rdquo; That&rsquo;s an excellent question. The answer lies in the <strong>attention mechanism</strong>. The computation of the final stream <em>depends on</em> the intermediate calculations from all the previous streams. They talk to each other inside each Transformer block.</p><h2 id=speeding-up-generation-by-caching-keys-and-values>Speeding Up Generation by Caching Keys and Values</h2><p>Remember our autoregressive loop? Generate a token, append it, re-process everything. This is incredibly inefficient! When generating the 100th token, we would re-calculate the streams for the first 99 tokens from scratch.</p><p>This is where the <strong>Key-Value (KV) Cache</strong> comes in. It&rsquo;s a critical optimization. Inside the attention mechanism (which we&rsquo;ll dissect next), certain intermediate results called &ldquo;Keys&rdquo; and &ldquo;Values&rdquo; are calculated for each token. Instead of re-calculating them every time, we can cache them.</p><p>As Figure 3-10 visualizes below, when generating the second token, we don&rsquo;t re-run the calculation for the first. We retrieve its cached Keys and Values and only perform the full computation for the new token.</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-10.png alt="Figure 3-10" width=700></figure><p>The book&rsquo;s <code>%%timeit</code> example is a fantastic demonstration of this. Generating 100 tokens:</p><ul><li>With KV Cache (<code>use_cache=True</code>): <strong>4.5 seconds</strong></li><li>Without KV Cache (<code>use_cache=False</code>): <strong>21.8 seconds</strong></li></ul><p>A dramatic, nearly 5x speedup! This is why KV caching is enabled by default and is essential for making LLM inference practical.</p><h2 id=inside-the-transformer-block>Inside the Transformer Block</h2><p>Now, let&rsquo;s zoom in further. Figure 3-11 shows the processing flow: an embedding goes into Block 1, the output of Block 1 goes into Block 2, and so on.</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-11.png alt="Figure 3-11" width=700></figure><p>According to Figure 3-12 below, each Transformer block is composed of two main sub-layers:</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-12.png alt="Figure 3-12" width=700></figure><ol><li><strong>The Attention Layer</strong>: Its job is to incorporate context. It allows a token to &ldquo;look at&rdquo; other tokens in the sequence and pull in relevant information.</li><li><strong>The Feedforward Layer (FFN)</strong>: This is a standard neural network that acts as the model&rsquo;s &ldquo;thinking&rdquo; or processing workhorse. This is where a lot of the learned knowledge is stored and applied.</li></ol><p>The &ldquo;Shawshank Redemption&rdquo; example in Figure 3-13 gives a good intuition for the FFN&rsquo;s role. The knowledge that &ldquo;Redemption&rdquo; follows &ldquo;The Shawshank&rdquo; is likely stored within the weights of these FFNs across the model&rsquo;s layers.</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-13.png alt="Figure 3-13" width=700></figure><p>The attention layer&rsquo;s role is all about context. In the sentence &ldquo;The dog chased the squirrel because <strong>it</strong> was fast,&rdquo; the attention mechanism helps the model figure out that &ldquo;<strong>it</strong>&rdquo; refers to the &ldquo;squirrel,&rdquo; not the &ldquo;dog.&rdquo; It does this by enriching the vector representation of &ldquo;it&rdquo; with information from the &ldquo;squirrel&rdquo; vector, as conceptualized in Figure 3-14.</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-14.png alt="Figure 3-14" width=700></figure><h3 id=attention-is-all-you-need-the-deep-dive>Attention is All You Need: The Deep Dive</h3><p>Let&rsquo;s break down the attention mechanism inside a single <strong>attention head</strong>, following Figures 3-15 and 3-16. There are two main steps:</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-15.png alt="Figure 3-15" width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-16.png alt="Figure 3-16" width=700></figure><ol><li><strong>Relevance Scoring</strong>: For the current token we&rsquo;re processing, calculate a score for every <em>other</em> token in the context that tells us how relevant it is.</li><li><strong>Combining Information</strong>: Create a new vector for the current token by taking a weighted sum of all the other token vectors, using the relevance scores as weights.</li></ol><p>To do this, we use the famous <strong>Query, Key, Value (QKV)</strong> model. Imagine this:</p><ul><li>You are the <strong>Query</strong>: the current token, asking for information.</li><li>Every other token is a <strong>Key</strong>: a label for the information it holds.</li><li>Every other token also has a <strong>Value</strong>: the actual information it holds.</li></ul><p>The process, shown in Figures 3-18 through 3-21, is:</p><ol><li><strong>Projection</strong>: The input vectors are multiplied by three separate weight matrices (learned during training) to create the Q, K, and V vectors for each token (see Figure 3-19).</li><li><strong>Scoring</strong>: You (the Query) compare yourself to every other token&rsquo;s Key. This is done with a dot product: <code>Q · K^T</code>. The result is a set of raw scores. After a softmax function, these become the attention scores (our relevance scores), summing to 1 (see Figure 3-20).</li><li><strong>Combining</strong>: These scores are then used to create a weighted sum of all the Value vectors. A token with a high score contributes more of its Value to the final output. The result is the new, context-enriched vector for the current position (see Figure 3-21).</li></ol><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-19.png alt="Figure 3-19" width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-20.png alt="Figure 3-20" width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-21.png alt="Figure 3-21" width=700></figure><p>Let me draw this core QKV logic with a diagram to make it even clearer.</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph Input Vectors for each token
        direction LR
        I1(Token 1 Vector)
        I2(Token 2 Vector)
        I3(Token 3 Vector)
    end

    subgraph Projection Matrices
        direction LR
        W_Q(Query Matrix Wq)
        W_K(Key Matrix Wk)
        W_V(Value Matrix Wv)
    end

    subgraph &#34;Step 1: Create Q, K, V vectors&#34;
        direction LR
        Q1(Q1)
        K1(K1)
        V1(V1)
        Q2(Q2)
        K2(K2)
        V2(V2)
        Q3(Q3)
        K3(K3)
        V3(V3)
    end

    I1 -- x Wq --&gt; Q1
    I1 -- x Wk --&gt; K1
    I1 -- x Wv --&gt; V1
    I2 -- x Wq --&gt; Q2
    I2 -- x Wk --&gt; K2
    I2 -- x Wv --&gt; V2
    I3 -- x Wq --&gt; Q3
    I3 -- x Wk --&gt; K3
    I3 -- x Wv --&gt; V3

    subgraph &#34;Step 2: Score (e.g., for Token 3)&#34;
        S1(Score_3_1 = Q3 • K1)
        S2(Score_3_2 = Q3 • K2)
        S3(Score_3_3 = Q3 • K3)
        SM(Softmax)
        S1 &amp; S2 &amp; S3 --&gt; SM
    end

    subgraph &#34;Step 3: Combine based on Scores&#34;
        O3(Output Vector for Token 3)
    end

    SM -- Weights --&gt; O3
    V1 &amp; V2 &amp; V3 -- Vectors to be weighted --&gt; O3

    linkStyle 11 stroke:#ff0000,stroke-width:2px;
    linkStyle 12 stroke:#0000ff,stroke-width:2px;
    linkStyle 13 stroke:#00ff00,stroke-width:2px;
</code></pre><p>This entire QKV calculation is done in parallel inside multiple <strong>attention heads</strong>, allowing the model to focus on different kinds of relationships (e.g., one head for syntax, another for semantics) simultaneously.</p><h2 id=recent-improvements-to-the-transformer-architecture>Recent Improvements to the Transformer Architecture</h2><p>The original Transformer was from 2017. The field has evolved. Let&rsquo;s cover the modern enhancements mentioned in the book.</p><h3 id=more-efficient-attention>More Efficient Attention</h3><p>As models got bigger, the <code>N^2</code> complexity of full attention became a bottleneck.</p><ul><li><strong>Local/Sparse Attention</strong>: As shown in Figure 3-23, instead of attending to all previous tokens, a token might only attend to a small, local window or a &ldquo;strided&rdquo; pattern. This is much faster but can lose global context. GPT-3 cleverly alternates between full and sparse attention layers.</li></ul><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-23.png alt="Figure 3-23" width=700></figure><ul><li><strong>Multi-Query and Grouped-Query Attention (MQA/GQA)</strong>: Full multi-head attention (MHA) has separate K and V matrices for every head. This uses a lot of memory for the KV Cache.<ul><li><strong>MQA</strong>: A major optimization where all heads <em>share</em> a single Key and Value matrix (see Figure 3-27). This drastically reduces the KV cache size but can hurt model quality.</li><li><strong>GQA</strong>: A happy medium. Heads are divided into groups, and heads <em>within a group</em> share K and V matrices (see Figure 3-28). This is used by models like Llama 2 and Phi-3, offering a great balance of speed and quality.</li></ul></li></ul><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-27.png alt="Figure 3-27" width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-28.png alt="Figure 3-28" width=700></figure><p>Here&rsquo;s a diagram illustrating the evolution from MHA to GQA.</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph &#34;Multi-Head Attention (MHA)&#34;
        H1_MHA(&#34;Head 1 &lt;br/&gt; Q1, K1, V1&#34;)
        H2_MHA(&#34;Head 2 &lt;br/&gt; Q2, K2, V2&#34;)
        H3_MHA(&#34;Head 3 &lt;br/&gt; Q3, K3, V3&#34;)
        H4_MHA(&#34;Head 4 &lt;br/&gt; Q4, K4, V4&#34;)
    end

    subgraph &#34;Multi-Query Attention (MQA)&#34;
        H1_MQA(&#34;Head 1 &lt;br/&gt; Q1&#34;) --&gt; KV_MQA(&#34;Shared K, V&#34;)
        H2_MQA(&#34;Head 2 &lt;br/&gt; Q2&#34;) --&gt; KV_MQA
        H3_MQA(&#34;Head 3 &lt;br/&gt; Q3&#34;) --&gt; KV_MQA
        H4_MQA(&#34;Head 4 &lt;br/&gt; Q4&#34;) --&gt; KV_MQA
    end

    subgraph &#34;Grouped-Query Attention (GQA)&#34;
        subgraph &#34;Group 1&#34;
            H1_GQA(&#34;Head 1 &lt;br/&gt; Q1&#34;) --&gt; KV1_GQA(&#34;Shared K1, V1&#34;)
            H2_GQA(&#34;Head 2 &lt;br/&gt; Q2&#34;) --&gt; KV1_GQA
        end
        subgraph &#34;Group 2&#34;
            H3_GQA(&#34;Head 3 &lt;br/&gt; Q3&#34;) --&gt; KV2_GQA(&#34;Shared K2, V2&#34;)
            H4_GQA(&#34;Head 4 &lt;br/&gt; Q4&#34;) --&gt; KV2_GQA
        end
    end

    H1_MHA -.-&gt; H1_MQA
    H1_MQA -.-&gt; H1_GQA
</code></pre><ul><li><strong>FlashAttention</strong>: Not an algorithmic change, but a brilliant re-implementation. It optimizes the way Q, K, and V matrices are moved between the GPU&rsquo;s fast SRAM and slower HBM, avoiding bottlenecks and providing massive speedups for both training and inference.</li></ul><h3 id=the-modern-transformer-block>The Modern Transformer Block</h3><p>Figure 3-29 shows the original block, which used &ldquo;post-normalization&rdquo; (Add & Norm <em>after</em> the sub-layer). Figure 3-30 shows a modern block, like in Llama or Phi-3, with key differences:</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-29.png alt="Figure 3-29" width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-30.png alt="Figure 3-30" width=700></figure><ol><li><strong>Pre-Normalization</strong>: The normalization layer (like RMSNorm) is applied <em>before</em> the attention and FFN layers. This leads to more stable training.</li><li><strong>RMSNorm</strong>: A simpler, faster alternative to the original LayerNorm.</li><li><strong>SwiGLU</strong>: A more advanced activation function for the FFN, replacing the original ReLU.</li><li><strong>Rotary Positional Embeddings (RoPE)</strong>.</li></ol><h3 id=positional-embeddings-rope>Positional Embeddings (RoPE)</h3><p>Transformers are inherently &ldquo;bags of vectors&rdquo;; they don&rsquo;t know the order of tokens. We need to inject positional information.
The old way was to add a positional embedding to the token embedding at the very beginning. But this is inflexible, especially when &ldquo;packing&rdquo; multiple documents into one context, as shown in Figure 3-31.</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-31.png alt="Figure 3-31" width=700></figure><p><strong>RoPE</strong> is a clever solution. Instead of adding a vector, it <em>rotates</em> the Query and Key vectors based on their absolute position. This is done <em>inside</em> the attention layer, just before the <code>Q · K</code> multiplication (as shown in Figures 3-32 and 3-33). This method has proven to be incredibly effective, as it elegantly encodes both absolute and relative position information.</p><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-32.png alt="Figure 3-32" width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-3/figure-3-33.png alt="Figure 3-33" width=700></figure><h2 id=summary>Summary</h2><p>And that brings us to the end of a very dense but crucial chapter. We have peered deep inside the Transformer.</p><p>We&rsquo;ve seen that generation is an <strong>autoregressive, token-by-token loop</strong>. We dissected the forward pass into its three main components: the <strong>tokenizer</strong>, the <strong>stack of Transformer blocks</strong>, and the <strong>LM head</strong>. We learned that this process is parallelized across tokens and that the <strong>KV cache</strong> is a vital optimization to speed it up.</p><p>Most importantly, we opened up the Transformer block itself and demystified its two workhorses: the <strong>feedforward network</strong> for storing knowledge and the <strong>attention mechanism</strong> for incorporating context. We went deep into the QKV model of attention and looked at modern improvements like GQA and FlashAttention. Finally, we touched on architectural tweaks like pre-normalization and the powerful Rotary Positional Embeddings (RoPE).</p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>