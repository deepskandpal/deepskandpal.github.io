<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><a href=#arxivs-articles-computation-and-language>ArXiv&rsquo;s Articles: Computation and Language</a></li><li><a href=#a-common-pipeline-for-text-clustering>A Common Pipeline for Text Clustering</a><ul><li><a href=#embedding-documents>Embedding Documents</a></li><li><a href=#reducing-the-dimensionality-of-embeddings>Reducing the Dimensionality of Embeddings</a></li><li><a href=#cluster-the-reduced-embeddings>Cluster the Reduced Embeddings</a></li><li><a href=#inspecting-the-clusters>Inspecting the Clusters</a></li></ul></li><li><a href=#from-text-clustering-to-topic-modeling>From Text Clustering to Topic Modeling</a></li><li><a href=#bertopic-a-modular-topic-modeling-framework>BERTopic: A Modular Topic Modeling Framework</a></li><li><a href=#adding-a-special-lego-block-representation-models>Adding a Special Lego Block: Representation Models</a><ul><li><a href=#keybertinspired>KeyBERTInspired</a></li><li><a href=#maximal-marginal-relevance-mmr>Maximal Marginal Relevance (MMR)</a></li><li><a href=#the-text-generation-lego-block>The Text Generation Lego Block</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 5: Text Clustering and Topic Modeling</h1><span class=reading-time><em>11 min read</em></span><div class=book-details><div class=book-content><p>The goal is simple but profound: to automatically group similar texts together based on their meaning. As <strong>Figure 5-1</strong> illustrates, we start with a heap of unstructured documents and aim to produce clusters where documents about &ldquo;pets&rdquo; are in one group, &ldquo;sports&rdquo; in another, and &ldquo;food&rdquo; in a third. This is immensely powerful for quick data exploration, finding unexpected themes, or even pre-categorizing data for later supervised tasks.</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-1.png alt="Figure 5-1. Clustering unstructured textual data." width=700></figure><p>As we move from clustering to topic modeling, our goal refines slightly. We don&rsquo;t just want the groups; we want a meaningful description for each group. <strong>Figure 5-2</strong> shows this transition: the &ldquo;pets&rdquo; cluster is described by keywords like <code>pet</code>, <code>dog</code>, <code>cat</code>, etc. This gives us a human-understandable label for the abstract group.</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-2.png alt="Figure 5-2. Topic modeling is a way to give meaning to clusters of textual documents." width=700></figure><p>This chapter is a masterclass in combining different types of models. We&rsquo;ll see how modern embedding models (encoder-only), classical NLP techniques (bag-of-words), and even powerful generative models (decoder-only) can be chained together in a pipeline to create something truly impressive.</p><h2 id=arxivs-articles-computation-and-language>ArXiv&rsquo;s Articles: Computation and Language</h2><p>To make this tangible, we need a dataset. The book chooses a fantastic one: a collection of 44,949 abstracts from ArXiv&rsquo;s &ldquo;Computation and Language&rdquo; (<code>cs.CL</code>) section. This is perfect because it&rsquo;s real, complex data, and it&rsquo;s relevant to our field.</p><p>Let&rsquo;s start by loading the data, just as the book does.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Load data from Hugging Face</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;maartengr/arxiv_nlp&#34;</span>)[<span style=color:#e6db74>&#34;train&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Extract metadata</span>
</span></span><span style=display:flex><span>abstracts <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#34;abstracts&#34;</span>]
</span></span><span style=display:flex><span>titles <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#34;titles&#34;</span>]
</span></span></code></pre></div><p>We now have our collection of abstracts ready. Our mission is to find the hidden topics within them.</p><h2 id=a-common-pipeline-for-text-clustering>A Common Pipeline for Text Clustering</h2><p>The book lays out a very common and effective three-step pipeline for modern text clustering. This is our foundational recipe:</p><ol><li><strong>Embed:</strong> Convert all documents into numerical vectors (embeddings).</li><li><strong>Reduce:</strong> Lower the dimensionality of these embeddings.</li><li><strong>Cluster:</strong> Group the low-dimensional points together.</li></ol><p>Let&rsquo;s walk through each step.</p><h3 id=embedding-documents>Embedding Documents</h3><p>Our first step, shown in <strong>Figure 5-3</strong>, is to convert the 44,949 abstracts into 44,949 embedding vectors. <strong>What are we trying to achieve here?</strong> We want vectors where the distance between them reflects semantic similarity. Abstracts about &ldquo;machine translation&rdquo; should be close to each other in this vector space, and far from abstracts about &ldquo;phonetics.&rdquo;</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-3.png alt="Figure 5-3. Step 1: We convert documents to embeddings using an embedding model." width=700></figure><p>Choosing the right embedding model is key. The book wisely points to the MTEB (Massive Text Embedding Benchmark) leaderboard. For this task, we want a model that performs well on clustering and is reasonably fast. The book selects <code>thenlper/gte-small</code>, a great choice that balances performance and size.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers <span style=color:#f92672>import</span> SentenceTransformer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create an embedding for each abstract</span>
</span></span><span style=display:flex><span>embedding_model <span style=color:#f92672>=</span> SentenceTransformer(<span style=color:#e6db74>&#34;thenlper/gte-small&#34;</span>)
</span></span><span style=display:flex><span>embeddings <span style=color:#f92672>=</span> embedding_model<span style=color:#f92672>.</span>encode(abstracts, 
</span></span><span style=display:flex><span>                                      show_progress_bar<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>After running this, let&rsquo;s check the shape of our result.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Check the dimensions of the resulting embeddings</span>
</span></span><span style=display:flex><span>embeddings<span style=color:#f92672>.</span>shape
</span></span></code></pre></div><p>Output: <code>(44949, 384)</code></p><p>This tells us we have 44,949 vectors, each with 384 dimensions. This is our high-dimensional representation of the dataset.</p><h3 id=reducing-the-dimensionality-of-embeddings>Reducing the Dimensionality of Embeddings</h3><p>Now, why do we need this step? It comes down to a concept data scientists know well: the <strong>curse of dimensionality</strong>. In high-dimensional spaces (like 384-D), distances become less meaningful, and everything seems far apart. This makes it very difficult for clustering algorithms to find dense regions.</p><p><strong>Our goal</strong> is to compress this 384-D space into a much lower-dimensional one (e.g., 5-D) while preserving as much of the global structure as possible. We want the points that were close in 384-D to remain close in 5-D. <strong>Figure 5-4</strong> and <strong>Figure 5-5</strong> visualize this compression perfectly.</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-4.png alt="Figure 5-4. Dimensionality reduction allows data in high-dimensional space to be compressed to a lower-dimensional space." width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-5.png alt="Figure 5-5. Step 2: The embeddings are reduced to a lower-dimensional space using dimensionality reduction." width=700></figure><p>The book chooses <strong>UMAP</strong> (Uniform Manifold Approximation and Projection) over the classic PCA. This is a deliberate choice. UMAP is excellent at preserving the non-linear structures and local/global relationships in the data, which is exactly what we need.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> umap <span style=color:#f92672>import</span> UMAP
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># We reduce the input embeddings from 384 dimensions to 5 dimensions</span>
</span></span><span style=display:flex><span>umap_model <span style=color:#f92672>=</span> UMAP(n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, 
</span></span><span style=display:flex><span>                  min_dist<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0</span>, 
</span></span><span style=display:flex><span>                  metric<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cosine&#39;</span>, 
</span></span><span style=display:flex><span>                  random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>reduced_embeddings <span style=color:#f92672>=</span> umap_model<span style=color:#f92672>.</span>fit_transform(embeddings)
</span></span></code></pre></div><p>Let&rsquo;s break down these parameters, as a senior DS would:</p><ul><li><code>n_components=5</code>: We&rsquo;re projecting down to 5 dimensions. This is a sweet spot for capturing structure without being too high-dimensional for the clustering algorithm.</li><li><code>min_dist=0.0</code>: This encourages UMAP to create very tight, dense clusters, which is great for our purpose.</li><li><code>metric='cosine'</code>: We use cosine similarity because it&rsquo;s generally more effective than Euclidean distance for high-dimensional text embeddings.</li><li><code>random_state=42</code>: This makes our UMAP results reproducible.</li></ul><h3 id=cluster-the-reduced-embeddings>Cluster the Reduced Embeddings</h3><p>This is the final step in our pipeline, as shown in <strong>Figure 5-6</strong>. We now have 44,949 points in a 5-D space. <strong>Our goal is to assign a cluster label to each point.</strong></p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-6.png alt="Figure 5-6. Step 3: We cluster the documents using the embeddings with reduced dimensionality." width=700></figure><p>The book makes another excellent point by contrasting centroid-based clustering (like k-means) with density-based clustering (<strong>Figure 5-7</strong>).</p><ul><li><strong>K-means</strong> requires you to specify the number of clusters (<code>k</code>) beforehand. We don&rsquo;t know <code>k</code>! It also forces every single point into a cluster.</li><li><strong>HDBSCAN</strong> (Hierarchical Density-Based Spatial Clustering of Applications with Noise) is a much better fit. It automatically determines the number of clusters based on density and, crucially, it allows for <strong>outliers</strong>. It identifies points that don&rsquo;t belong to any dense cluster and labels them as such (typically with <code>-1</code>). Since our ArXiv dataset likely contains many niche or unique papers, this ability to handle outliers is perfect.</li></ul><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-7.png alt="Figure 5-7. The clustering algorithm not only impacts how clusters are generated but also how they are interpreted." width=700></figure><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> hdbscan <span style=color:#f92672>import</span> HDBSCAN
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># We fit the model and extract the clusters</span>
</span></span><span style=display:flex><span>hdbscan_model <span style=color:#f92672>=</span> HDBSCAN(min_cluster_size<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>, 
</span></span><span style=display:flex><span>                        metric<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;euclidean&#34;</span>, 
</span></span><span style=display:flex><span>                        cluster_selection_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;eom&#34;</span>)
</span></span><span style=display:flex><span>                        <span style=color:#f92672>.</span>fit(reduced_embeddings)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>clusters <span style=color:#f92672>=</span> hdbscan_model<span style=color:#f92672>.</span>labels_
</span></span></code></pre></div><p><code>min_cluster_size=50</code> tells the algorithm to not even consider a group of points a cluster unless it has at least 50 members. This helps avoid tiny, noisy micro-clusters.</p><p>When we check <code>len(set(clusters))</code>, we find we&rsquo;ve generated <strong>156 clusters</strong>.</p><h3 id=inspecting-the-clusters>Inspecting the Clusters</h3><p>Now we have our clusters, but what do they mean? The book shows how to do a quick manual inspection. Let&rsquo;s look at the first three documents from Cluster 0.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Print first three documents in cluster 0</span>
</span></span><span style=display:flex><span>cluster <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> index <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>where(clusters<span style=color:#f92672>==</span>cluster)[<span style=color:#ae81ff>0</span>][:<span style=color:#ae81ff>3</span>]:
</span></span><span style=display:flex><span>    print(abstracts[index][:<span style=color:#ae81ff>300</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;... </span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>The output shows documents about &ldquo;statistical machine translation from English text to American Sign Language (ASL)&rdquo;. It seems <strong>Cluster 0 is about Sign Language Translation</strong>. This is the core of exploratory analysis!</p><p>For a bird&rsquo;s-eye view, we can create the 2D scatter plot shown in <strong>Figure 5-8</strong>. We re-run UMAP, this time with <code>n_components=2</code> for visualization, and plot the points, coloring them by their cluster ID. The gray points are the outliers identified by HDBSCAN.</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-8.png alt="Figure 5-8. The generated clusters (colored) and outliers (gray) are represented as a 2D visualization." width=700></figure><h2 id=from-text-clustering-to-topic-modeling>From Text Clustering to Topic Modeling</h2><p>Manually inspecting clusters is insightful but not scalable. This is where we transition to <strong>Topic Modeling</strong>. The goal is to automatically generate a representation for each cluster.</p><p>Instead of a single label like &ldquo;Sign Language,&rdquo; traditional topic modeling (like Latent Dirichlet Allocation, or LDA) represents a topic as a distribution of words, as shown in <strong>Figures 5-9 and 5-10</strong>. The top words in the distribution act as the topic&rsquo;s description.</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-9.png alt="Figure 5-9. A topic is represented as a distribution over words." width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-10.png alt="Figure 5-10. Keywords are extracted based on their distribution over a single topic." width=700></figure><h2 id=bertopic-a-modular-topic-modeling-framework>BERTopic: A Modular Topic Modeling Framework</h2><p>This is the star of the chapter. <strong>BERTopic</strong> is a brilliant framework that formalizes and extends the pipeline we just built.</p><p>As <strong>Figure 5-11</strong> shows, the first part of BERTopic is exactly what we did: <strong>SBERT (Embed) -> UMAP (Reduce) -> HDBSCAN (Cluster)</strong>.</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-11.png alt="Figure 5-11. The first part of BERTopic’s pipeline is to create clusters of semantically similar documents." width=700></figure><p>The second part is where it gets clever. How does it represent the topics? It uses a classical NLP technique: <strong>c-TF-IDF</strong>. Let me break this down, because it&rsquo;s the secret sauce.</p><ol><li><strong>Bag-of-Words</strong>: First, for all documents <em>within a single cluster</em>, it creates a giant bag-of-words. It&rsquo;s not per-document, but per-cluster (<strong>Figure 5-13</strong>). This gives us the <strong>Term Frequency (TF)</strong> for each word <em>within its class (c)</em>. This is the &ldquo;c-TF&rdquo; part.</li></ol><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-12.png alt="Figure 5-12. A bag-of-words counts the number of times each word appears inside a document." width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-13.png alt="Figure 5-13. Generating c-TF by counting the frequency of words per cluster instead of per document." width=700></figure><ol start=2><li><strong>Inverse Document Frequency (IDF)</strong>: It then calculates how common each word is across <em>all</em> clusters. Words that are common everywhere (like <code>the</code>, <code>and</code>, <code>model</code>) are uninformative and get a low IDF score. Words that are frequent in one cluster but rare elsewhere (like <code>asr</code> or <code>phonetic</code>) are very informative and get a high IDF score (<strong>Figure 5-14</strong>).</li></ol><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-14.png alt="Figure 5-14. Creating a weighting scheme." width=700></figure><ol start=3><li><strong>c-TF-IDF Score</strong>: The final score for a word in a topic is <code>c-TF * IDF</code>. This score is high for words that are frequent within a topic but rare overall. These are the perfect keywords to describe the topic!</li></ol><p>The full BERTopic pipeline is shown in <strong>Figure 5-16</strong>. It&rsquo;s this beautiful marriage of modern deep learning for semantic clustering and classic, interpretable TF-IDF for topic representation.</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-15.png alt="Figure 5-15. The second part of BERTopic’s pipeline is representing the topics: the calculation of the c-TF-IDF." width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-16.png alt="Figure 5-16. The full pipeline of BERTopic, roughly, consists of two steps, clustering and topic representation." width=700></figure><p>The true power of BERTopic, as emphasized in <strong>Figure 5-17</strong>, is its <strong>modularity</strong>. It&rsquo;s built like Lego blocks. Don&rsquo;t like HDBSCAN? Swap it for k-means. Have a better embedding model? Plug it in. This makes the framework incredibly powerful and future-proof.</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-17.png alt="Figure 5-17. The modularity of BERTopic is a key component and allows you to build your own topic model." width=700></figure><p>Let&rsquo;s run it. We can even reuse the components we already trained.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> bertopic <span style=color:#f92672>import</span> BERTopic
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Train our model with our previously defined models</span>
</span></span><span style=display:flex><span>topic_model <span style=color:#f92672>=</span> BERTopic(
</span></span><span style=display:flex><span>    embedding_model<span style=color:#f92672>=</span>embedding_model,
</span></span><span style=display:flex><span>    umap_model<span style=color:#f92672>=</span>umap_model,
</span></span><span style=display:flex><span>    hdbscan_model<span style=color:#f92672>=</span>hdbscan_model,
</span></span><span style=display:flex><span>    verbose<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>fit(abstracts, embeddings)
</span></span></code></pre></div><p>We can now inspect the topics programmatically.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Get a summary of the topics</span>
</span></span><span style=display:flex><span>topic_model<span style=color:#f92672>.</span>get_topic_info()
</span></span></code></pre></div><p>This gives us a table with the topic ID, the number of documents (Count), and a default Name made from the top 4 keywords. We can see Topic -1 (the outliers), Topic 0 (<code>speech_asr_recognition_end</code>), Topic 1 (<code>medical_clinical_biomedical_pa</code>), and so on.</p><h2 id=adding-a-special-lego-block-representation-models>Adding a Special Lego Block: Representation Models</h2><p>The default c-TF-IDF representation is great, but we can do even better. BERTopic allows us to add new &ldquo;Lego blocks&rdquo; on top of the pipeline to <strong>rerank or relabel</strong> the topics. This is the concept of a <strong>Representation Model</strong> (<strong>Figure 5-20</strong>).</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-18.png alt="Figure 5-18. The output when we visualize documents and topics." width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-19.png alt="Figure 5-19. Fine-tune the topic representations by reranking the original c-TF-IDF distributions." width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-20.png alt="Figure 5-20. The reranker (representation) block sits on top of the c-TF-IDF representation." width=700></figure><h3 id=keybertinspired>KeyBERTInspired</h3><p>This method improves the keyword-based representation. <strong>What&rsquo;s the goal?</strong> To find keywords that are not just frequent, but are also semantically very similar to the overall meaning of the topic.
As shown in <strong>Figure 5-22</strong>, it calculates the average embedding of all documents in a topic. Then, it finds candidate keywords (from c-TF-IDF) whose own embeddings are most similar to this average topic embedding. This cleans up the keyword list beautifully. The comparison table on page 34 shows the &ldquo;Original&rdquo; vs &ldquo;Updated&rdquo; keywords, and the updated ones are much more coherent.</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-21.png alt="Figure 5-21. After applying the c-TF-IDF weighting, topics can be fine-tuned with a wide variety of representation models." width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-22.png alt="Figure 5-22. KeyBERTInspired representation model procedure." width=700></figure><h3 id=maximal-marginal-relevance-mmr>Maximal Marginal Relevance (MMR)</h3><p><strong>What&rsquo;s the goal here?</strong> To improve diversity. A topic might have keywords like <code>summaries</code>, <code>summary</code>, <code>summarization</code>. They are redundant. MMR is an algorithm that selects a list of keywords that are both relevant to the topic <em>and</em> diverse from each other. The table on page 36 clearly shows this—the updated topic for &ldquo;summarization&rdquo; is much more diverse and informative.</p><h3 id=the-text-generation-lego-block>The Text Generation Lego Block</h3><p>This is the ultimate upgrade. Why just have a list of keywords when we can have a full, human-readable label? <strong>Our goal: use a generative LLM to create a short, descriptive title for each topic.</strong></p><p>This is incredibly efficient. Instead of running the LLM on millions of documents, we run it just once per topic (156 times in our case). As <strong>Figure 5-23</strong> shows, we construct a prompt for the LLM that includes the top keywords and a few representative documents from the topic, and then we ask it: &ldquo;Based on this, what&rsquo;s a short label for this topic?&rdquo;</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-23.png alt="Figure 5-23. Use text generative LLMs and prompt engineering to create labels for topics from keywords and documents related to each topic." width=700></figure><p>The book shows examples with both Flan-T5 and GPT-3.5. The results with GPT-3.5 (page 40) are stunningly good:</p><ul><li>Topic 0 (speech, asr, recognition) becomes &ldquo;<strong>Leveraging External Data for Improving Low-Res&mldr;</strong>&rdquo;</li><li>Topic 1 (medical, clinical) becomes &ldquo;<strong>Improved Representation Learning for Biomedica&mldr;</strong>&rdquo;</li><li>Topic 3 (translation, nmt) becomes &ldquo;<strong>Neural Machine Translation</strong>&rdquo;</li></ul><p>These are not just keywords; they are meaningful, high-quality topic labels.</p><p>With these amazing labels, we can now create the final, beautiful visualization from <strong>Figure 5-24</strong>, using the <code>datamapplot</code> package. It plots the documents and adds the generated labels, giving us an interpretable map of the entire dataset.</p><figure><img src=/images/hands-on-large-language-models/chapter-5/fig-5-24.png alt="Figure 5-24. The top 20 topics visualized." width=700></figure><h2 id=summary>Summary</h2><p>And that&rsquo;s Chapter 5. We&rsquo;ve gone on a complete journey through unsupervised learning with modern tools.</p><p>We started with a foundational <strong>three-step clustering pipeline</strong>: Embed, Reduce, and Cluster. We made deliberate, expert choices at each step: a strong <code>SentenceTransformer</code> model, <code>UMAP</code> for dimensionality reduction, and <code>HDBSCAN</code> for robust, density-based clustering.</p><p>We then saw how <strong>BERTopic</strong> elegantly packages this pipeline and enhances it with an interpretable <strong>c-TF-IDF</strong> representation to define topics.</p><p>Finally, and most powerfully, we explored the modular &ldquo;Lego block&rdquo; nature of BERTopic by adding <strong>representation models</strong> to refine our topics. We used <code>KeyBERTInspired</code> and <code>MMR</code> to improve keyword quality and diversity, and we capped it all off by using a powerful <strong>generative LLM</strong> to create high-quality, human-readable labels.</p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>