<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><a href=#the-sentiment-of-movie-reviews>The Sentiment of Movie Reviews</a></li><li><a href=#text-classification-with-representation-models>Text Classification with Representation Models</a><ul><li><a href=#model-selection>Model Selection</a></li><li><a href=#using-a-task-specific-model>Using a Task-Specific Model</a></li><li><a href=#classification-tasks-that-leverage-embeddings>Classification Tasks That Leverage Embeddings</a></li><li><a href=#what-if-we-do-not-have-labeled-data>What If We Do Not Have Labeled Data?</a></li></ul></li><li><a href=#text-classification-with-generative-models>Text Classification with Generative Models</a><ul><li><a href=#using-the-text-to-text-transfer-transformer-flan-t5>Using the Text-to-Text Transfer Transformer (Flan-T5)</a></li><li><a href=#chatgpt-for-classification>ChatGPT for Classification</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 4: Text Classification</h1><span class=reading-time><em>11 min read</em></span><div class=book-details><div class=book-content><p>As the book opens, it states a simple goal: <strong>classification is about assigning a label or class to some input text.</strong> This is a cornerstone of NLP. Think about it:</p><ul><li>Is this email spam or not? (Spam detection)</li><li>Is this customer review positive, negative, or neutral? (Sentiment analysis)</li><li>Is this news article about sports, politics, or technology? (Topic classification)</li></ul><p><strong>Figure 4-1</strong> gives us the high-level picture: text goes in, the model does its thing, and a category comes out.</p><p>The real meat of this chapter, and what makes it so relevant today, is that we can tackle this problem with two different families of models, as shown in <strong>Figure 4-2</strong>:</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-2.png alt="Figure 4-2. Although both representation and generative models can be used for classification, their approaches differ." width=700></figure><ol><li><strong>Representation Models (Encoder-style, like BERT):</strong> These models are designed to understand text and output numerical representations. For classification, they typically output a final class ID (e.g., <code>1</code> for &ldquo;Returns&rdquo;).</li><li><strong>Generative Models (Decoder-style, like GPT, or Encoder-Decoder like T5):</strong> These models are designed to generate text. To get a class out of them, we have to cleverly prompt them to output the <em>text</em> of the label (e.g., the string &ldquo;Returns&rdquo;).</li></ol><p>We will explore both paths. But first, we need a problem to solve and data to solve it with.</p><h3 id=the-sentiment-of-movie-reviews>The Sentiment of Movie Reviews</h3><p>The book wisely chooses a classic and intuitive dataset: <code>rotten_tomatoes</code>. The task is simple: determine if a movie review is positive or negative.</p><p>Let&rsquo;s load the data using the <code>datasets</code> library from Hugging Face. This is a standard workflow you&rsquo;ll use constantly.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load our data</span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;rotten_tomatoes&#34;</span>)
</span></span></code></pre></div><p>When we inspect <code>data</code>, we get the following structure:</p><pre tabindex=0><code>DatasetDict({
    train: Dataset({
        features: [&#39;text&#39;, &#39;label&#39;],
        num_rows: 8530
    })
    validation: Dataset({
        features: [&#39;text&#39;, &#39;label&#39;],
        num_rows: 1066
    })
    test: Dataset({
        features: [&#39;text&#39;, &#39;label&#39;],
        num_rows: 1066
    })
})
</code></pre><p>This is great. The data is already split for us into <code>train</code>, <code>validation</code>, and <code>test</code> sets. As good practitioners, we&rsquo;ll use the <code>train</code> set for any training, and the <code>test</code> set for our final evaluation. The <code>validation</code> set is there if we need to do hyperparameter tuning, to avoid &ldquo;peeking&rdquo; at the final test set.</p><p>Let&rsquo;s look at a couple of examples. The label <code>1</code> is positive, and <code>0</code> is negative.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># data[&#34;train&#34;][0, -1]</span>
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;text&#39;</span>: [<span style=color:#e6db74>&#34;the rock is destined to be the 21st century&#39;s new conan...&#34;</span>, 
</span></span><span style=display:flex><span>          <span style=color:#e6db74>&#39;things really get weird , though not particularly scary...&#39;</span>], 
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;label&#39;</span>: [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]}
</span></span></code></pre></div><p>The task is clear. Now, let&rsquo;s see how our first family of models tackles it.</p><h3 id=text-classification-with-representation-models>Text Classification with Representation Models</h3><p>As we discussed in previous chapters, representation models like BERT are phenomenal at understanding language. They are typically pre-trained on a massive corpus of text (like Wikipedia) to learn the nuances of language. Then, they are <strong>fine-tuned</strong> for specific tasks.</p><p><strong>Figure 4-3</strong> shows this beautifully. We start with a base foundation model (like BERT). We can then fine-tune it for a specific task like classification, creating a <strong>task-specific model</strong>. Alternatively, we can fine-tune it to produce high-quality <strong>general-purpose embeddings</strong>.</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-3.png alt="Figure 4-3. A foundation model is fine-tuned for specific tasks; for instance, to perform classification" width=700></figure><p>In this chapter, we&rsquo;re not doing the fine-tuning ourselves (we&rsquo;ll save that for Chapters 10 and 11). Instead, we&rsquo;ll use models that others have already fine-tuned for us. <strong>Figure 4-4</strong> shows our two strategies:</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-4.png alt="Figure 4-4. Perform classification directly with a task-specific model or indirectly with general-purpose embeddings" width=700></figure><ol><li><strong>Directly use a task-specific model</strong> that outputs a classification.</li><li><strong>Use an embedding model</strong> to convert text to vectors, and then train a simple, classical classifier (like logistic regression) on those vectors.</li></ol><h4 id=model-selection>Model Selection</h4><p>The Hugging Face Hub is vast. <strong>Figure 4-5</strong> gives a glimpse into the timeline of just the BERT-like models. How do we choose? The book gives some solid baselines like <code>bert-base-model</code> or <code>roberta-base-model</code>.</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-5.png alt="Figure 4-5. A timeline of common BERT-like model releases. These are considered foundation models" width=700></figure><p>For our first approach, we&rsquo;ll use a <strong>task-specific model</strong>: <code>cardiffnlp/twitter-roberta-base-sentiment-latest</code>. This is a RoBERTa model that was fine-tuned for sentiment on Tweets. It&rsquo;s an interesting choice because we&rsquo;re testing how well it <strong>generalizes</strong> from the short, informal language of Twitter to our movie review dataset.</p><h4 id=using-a-task-specific-model>Using a Task-Specific Model</h4><p>We can use the <code>pipeline</code> abstraction from <code>transformers</code>, which makes inference incredibly simple.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> pipeline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Path to our HF model</span>
</span></span><span style=display:flex><span>model_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;cardiffnlp/twitter-roberta-base-sentiment-latest&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load model into pipeline</span>
</span></span><span style=display:flex><span>pipe <span style=color:#f92672>=</span> pipeline(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;text-classification&#34;</span>, <span style=color:#75715e># We specify the task</span>
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model_path,
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>=</span>model_path, <span style=color:#75715e># Good practice to be explicit</span>
</span></span><span style=display:flex><span>    return_all_scores<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    device<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cuda:0&#34;</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>As <strong>Figure 4-6</strong> illustrates, when we pass text to this pipeline, it first gets tokenized, then processed by the model, which outputs the classification. The power of subword tokenization, as shown in <strong>Figure 4-7</strong>, means the model can handle words it&rsquo;s never seen before by breaking them down into known sub-parts.</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-6.png alt="Figure 4-6. An input sentence is first fed to a tokenizer before it can be processed by the task-specific model" width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-7.png alt="Figure 4-7. By breaking down an unknown word into tokens, word embeddings can still be generated" width=700></figure><p>Now, let&rsquo;s run this pipeline on our entire test set and evaluate it.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tqdm <span style=color:#f92672>import</span> tqdm
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers.pipelines.pt_utils <span style=color:#f92672>import</span> KeyDataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run inference</span>
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#75715e># The model outputs scores for &#39;Negative&#39;, &#39;Neutral&#39;, &#39;Positive&#39;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># We are interested in Negative (index 0) vs Positive (index 2)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> output <span style=color:#f92672>in</span> tqdm(pipe(KeyDataset(data[<span style=color:#e6db74>&#34;test&#34;</span>], <span style=color:#e6db74>&#34;text&#34;</span>)), total<span style=color:#f92672>=</span>len(data[<span style=color:#e6db74>&#34;test&#34;</span>])):
</span></span><span style=display:flex><span>    negative_score <span style=color:#f92672>=</span> output[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;score&#34;</span>]
</span></span><span style=display:flex><span>    positive_score <span style=color:#f92672>=</span> output[<span style=color:#ae81ff>2</span>][<span style=color:#e6db74>&#34;score&#34;</span>]
</span></span><span style=display:flex><span>    assignment <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax([negative_score, positive_score]) <span style=color:#75715e># 0 for negative, 1 for positive</span>
</span></span><span style=display:flex><span>    y_pred<span style=color:#f92672>.</span>append(assignment)
</span></span></code></pre></div><p>Now we have our predictions (<code>y_pred</code>) and the true labels (<code>data["test"]["label"]</code>). Let&rsquo;s evaluate. The book provides a handy function.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> classification_report
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>evaluate_performance</span>(y_true, y_pred):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Create and print the classification report&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    performance <span style=color:#f92672>=</span> classification_report(
</span></span><span style=display:flex><span>        y_true, y_pred, target_names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;Negative Review&#34;</span>, <span style=color:#e6db74>&#34;Positive Review&#34;</span>]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    print(performance)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>evaluate_performance(data[<span style=color:#e6db74>&#34;test&#34;</span>][<span style=color:#e6db74>&#34;label&#34;</span>], y_pred)
</span></span></code></pre></div><p>This gives us the classification report:</p><pre tabindex=0><code>               precision    recall  f1-score   support

Negative Review       0.76      0.88      0.81       533
Positive Review       0.86      0.72      0.78       533

       accuracy                           0.80      1066
      macro avg       0.81      0.80      0.80      1066
   weighted avg       0.81      0.80      0.80      1066
</code></pre><p>To really understand this, let&rsquo;s quickly break down the key metrics, which the book visualizes in <strong>Figure 4-9</strong>:</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-9.png alt="Figure 4-9. The classification report describes several metrics for evaluating a model's performance" width=700></figure><ul><li><strong>Precision</strong>: Of all the times we predicted &ldquo;Positive&rdquo;, how many were actually positive? (Here, 86%). This is about not making false positive mistakes.</li><li><strong>Recall</strong>: Of all the reviews that were actually &ldquo;Positive&rdquo;, how many did we find? (Here, 72%). This is about not missing any actual positives.</li><li><strong>F1-score</strong>: The harmonic mean of precision and recall. It&rsquo;s a single number that balances both concerns.</li><li><strong>Weighted Avg F1-score</strong>: 0.80. This is a very respectable score, especially considering the model was trained on Twitter data, not movie reviews! This shows the power of pre-training.</li></ul><h4 id=classification-tasks-that-leverage-embeddings>Classification Tasks That Leverage Embeddings</h4><p>What if a ready-made, task-specific model doesn&rsquo;t exist for our problem? Do we have to do a full, expensive fine-tuning run? No!</p><p>This is where the second strategy from <strong>Figure 4-4</strong> comes in. We can use a model that&rsquo;s great at creating general-purpose embeddings. This acts as a powerful <strong>feature extractor</strong>. Then, we can train a very simple, lightweight classifier on these features. The process, shown in <strong>Figure 4-10</strong>, separates feature extraction from classification.</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-10.png alt="Figure 4-10. The feature extraction step and classification steps are separated" width=700></figure><p>For this, we&rsquo;ll use the <code>sentence-transformers/all-mpnet-base-v2</code> model, a top performer on embedding benchmarks.</p><p><strong>Step 1: Create Embeddings</strong> (as in <strong>Figure 4-11</strong>)</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-11.png alt="Figure 4-11. In step 1, we use the embedding model to extract the features and convert the input text into embeddings" width=700></figure><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers <span style=color:#f92672>import</span> SentenceTransformer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> SentenceTransformer(<span style=color:#e6db74>&#34;sentence-transformers/all-mpnet-base-v2&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Convert text to embeddings</span>
</span></span><span style=display:flex><span>train_embeddings <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>encode(data[<span style=color:#e6db74>&#34;train&#34;</span>][<span style=color:#e6db74>&#34;text&#34;</span>], show_progress_bar<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>test_embeddings <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>encode(data[<span style=color:#e6db74>&#34;test&#34;</span>][<span style=color:#e6db74>&#34;text&#34;</span>], show_progress_bar<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>The shape of <code>train_embeddings</code> will be <code>(8530, 768)</code>, meaning we have a 768-dimensional vector for each of our 8,530 training reviews.</p><p><strong>Step 2: Train a Classifier</strong> (as in <strong>Figure 4-12</strong>)</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-12.png alt="Figure 4-12. Using the embeddings as our features, we train a logistic regression model on our training data" width=700></figure><p>Now we treat this like a classic ML problem. The embeddings are our features (<code>X</code>), and the labels are our targets (<code>y</code>).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LogisticRegression
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Train a logistic regression on our train embeddings</span>
</span></span><span style=display:flex><span>clf <span style=color:#f92672>=</span> LogisticRegression(random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>clf<span style=color:#f92672>.</span>fit(train_embeddings, data[<span style=color:#e6db74>&#34;train&#34;</span>][<span style=color:#e6db74>&#34;label&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Predict previously unseen instances</span>
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> clf<span style=color:#f92672>.</span>predict(test_embeddings)
</span></span><span style=display:flex><span>evaluate_performance(data[<span style=color:#e6db74>&#34;test&#34;</span>][<span style=color:#e6db74>&#34;label&#34;</span>], y_pred)
</span></span></code></pre></div><p>The result:</p><pre tabindex=0><code>               precision    recall  f1-score   support

Negative Review       0.85      0.86      0.85       533
Positive Review       0.86      0.85      0.85       533
...
   weighted avg       0.85      0.85      0.85      1066
</code></pre><p>An F1-score of <strong>0.85!</strong> We actually <em>improved</em> our performance by using general-purpose embeddings and a simple classifier. This is a powerful and flexible technique.</p><h4 id=what-if-we-do-not-have-labeled-data>What If We Do Not Have Labeled Data?</h4><p>This is where things get really interesting. What if we don&rsquo;t have <em>any</em> labeled data? This is called <strong>zero-shot classification</strong>. The core idea, shown in <strong>Figure 4-13</strong>, is to classify text against a set of candidate labels that the model has never been explicitly trained on.</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-13.png alt="Figure 4-13. In zero-shot classification, we have no labeled data, only the labels themselves" width=700></figure><p>How do we do this with embeddings? The book shows a brilliant trick. Instead of using abstract labels like <code>0</code> and <code>1</code>, we create descriptive sentences for our labels.</p><ul><li><code>0</code> becomes <code>"A negative movie review"</code></li><li><code>1</code> becomes <code>"A positive movie review"</code></li></ul><p>Then, as <strong>Figure 4-14</strong> illustrates, we embed both our documents <em>and</em> our descriptive labels into the same vector space.</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-14.png alt="Figure 4-14. To embed the labels, we first need to give them a description, such as 'a negative movie review'" width=700></figure><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Create embeddings for our labels</span>
</span></span><span style=display:flex><span>label_embeddings <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>encode([<span style=color:#e6db74>&#34;A negative review&#34;</span>, <span style=color:#e6db74>&#34;A positive review&#34;</span>])
</span></span></code></pre></div><p>Now, for each document embedding, we can calculate its <strong>cosine similarity</strong> to each of the label embeddings. Cosine similarity measures the angle between two vectors, as visualized in <strong>Figure 4-15</strong>. A smaller angle (higher similarity) means they are closer in meaning. We simply assign the label with the highest similarity.</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-15.png alt="Figure 4-15. Cosine similarity calculation between document and label embeddings" width=700></figure><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics.pairwise <span style=color:#f92672>import</span> cosine_similarity
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Find the best matching label for each document</span>
</span></span><span style=display:flex><span>sim_matrix <span style=color:#f92672>=</span> cosine_similarity(test_embeddings, label_embeddings)
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax(sim_matrix, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>evaluate_performance(data[<span style=color:#e6db74>&#34;test&#34;</span>][<span style=color:#e6db74>&#34;label&#34;</span>], y_pred)
</span></span></code></pre></div><p>The result:</p><pre tabindex=0><code>               precision    recall  f1-score   support

Negative Review       0.78      0.77      0.78       533
Positive Review       0.77      0.79      0.78       533
...
   weighted avg       0.78      0.78      0.78      1066
</code></pre><p>An F1-score of <strong>0.78</strong>! This is astonishing. Without a single labeled example, just by describing our labels, we achieved a result that&rsquo;s almost as good as our fully supervised models. This is a testament to the power of high-quality embeddings.</p><h3 id=text-classification-with-generative-models>Text Classification with Generative Models</h3><p>Now we switch to the other family of models. These are sequence-to-sequence models. They don&rsquo;t output a class ID; they output text. To make them classify, we need to guide them with an instruction, or <strong>prompt</strong>. This iterative process of refining the instruction is called <strong>prompt engineering</strong>, shown beautifully in <strong>Figure 4-18</strong>.</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-18.png alt="Figure 4-18. Prompt engineering allows prompts to be updated to improve the output generated by the model" width=700></figure><h4 id=using-the-text-to-text-transfer-transformer-flan-t5>Using the Text-to-Text Transfer Transformer (Flan-T5)</h4><p>First, we&rsquo;ll look at an open-source generative model. T5 is an <strong>encoder-decoder</strong> model (<strong>Figure 4-19</strong>). Its big innovation was framing every NLP task as a text-to-text problem during its fine-tuning phase (<strong>Figures 4-20 and 4-21</strong>). The Flan-T5 models were further fine-tuned on many instruction-based tasks, making them great at following prompts.</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-19.png alt="Figure 4-19. The T5 architecture is similar to the original Transformer model, a decoder-encoder architecture" width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-20.png alt="Figure 4-20. In the first step of training, namely pretraining, the T5 model needs to predict masks in the input text" width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-21.png alt="Figure 4-21. By converting specific tasks to textual instructions, the T5 model can be trained on a variety of tasks" width=700></figure><p>Let&rsquo;s load the small version.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pipe <span style=color:#f92672>=</span> pipeline(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;text2text-generation&#34;</span>,
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;google/flan-t5-small&#34;</span>,
</span></span><span style=display:flex><span>    device<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cuda:0&#34;</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>We can&rsquo;t just feed it the review. We have to create a prompt. Let&rsquo;s prefix every review with a question.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Prepare our data</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Is the following sentence positive or negative? &#34;</span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> example: {<span style=color:#e6db74>&#34;t5&#34;</span>: prompt <span style=color:#f92672>+</span> example[<span style=color:#e6db74>&#39;text&#39;</span>]})
</span></span></code></pre></div><p>Now we run inference, but we have to parse the text output.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> output <span style=color:#f92672>in</span> tqdm(pipe(KeyDataset(data[<span style=color:#e6db74>&#34;test&#34;</span>], <span style=color:#e6db74>&#34;t5&#34;</span>)), total<span style=color:#f92672>=</span>len(data[<span style=color:#e6db74>&#34;test&#34;</span>])):
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> output[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;generated_text&#34;</span>]
</span></span><span style=display:flex><span>    y_pred<span style=color:#f92672>.</span>append(<span style=color:#ae81ff>0</span> <span style=color:#66d9ef>if</span> text<span style=color:#f92672>.</span>lower() <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;negative&#34;</span> <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p>The result is an F1-score of <strong>0.84</strong>. This is very strong and shows that prompting generative models is a highly effective classification strategy.</p><h4 id=chatgpt-for-classification>ChatGPT for Classification</h4><p>Finally, we&rsquo;ll look at a closed-source, decoder-only model: ChatGPT. We access it via an API. The book explains its training process, including instruction tuning (<strong>Figure 4-22</strong>) and the crucial preference tuning (RLHF) step (<strong>Figure 4-23</strong>).</p><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-22.png alt="Figure 4-22. Manually labeled data consisting of an instruction (prompt) and output was used to train the model" width=700></figure><figure><img src=/images/hands-on-large-language-models/chapter-4/fig-4-23.png alt="Figure 4-23. Manually ranked preference data was used to generate the final model, ChatGPT" width=700></figure><p>First, we set up our client.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> openai
</span></span><span style=display:flex><span><span style=color:#75715e># You need to get your own key from OpenAI</span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> openai<span style=color:#f92672>.</span>OpenAI(api_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;YOUR_KEY_HERE&#34;</span>)
</span></span></code></pre></div><p>We then write a much more detailed prompt, telling the model exactly what to do and what format to return. This is a powerful form of prompt engineering.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;Predict whether the following document is a positive or negative movie review:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[DOCUMENT]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>If it is positive return 1 and if it is negative return 0. Do not give any other answers.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Example call</span>
</span></span><span style=display:flex><span>document <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;unpretentious , charming , quirky , original&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># chatgpt_generation(prompt, document) # a helper function defined in the book</span>
</span></span></code></pre></div><p>After running this on the test set and converting the string outputs (<code>"1"</code> or <code>"0"</code>) to integers, we get the evaluation:</p><pre tabindex=0><code>               precision    recall  f1-score   support

Negative Review       0.87      0.97      0.92       533
Positive Review       0.96      0.86      0.91       533
...
   weighted avg       0.92      0.91      0.91      1066
</code></pre><p>An F1-score of <strong>0.91!</strong> A remarkable performance. However, the book makes a critical point for any data scientist: with a closed-source model, we don&rsquo;t know its training data. It&rsquo;s possible the <code>rotten_tomatoes</code> dataset was part of its training, which would make this evaluation invalid. This is a key consideration when working with proprietary models.</p><h3 id=summary>Summary</h3><p>This chapter was our first deep dive into a practical application, and what a journey it was.</p><p>We saw that text classification can be tackled with two families of models.</p><ul><li><strong>Representation models</strong> are excellent, either used directly as fine-tuned classifiers (F1=0.80) or as powerful feature extractors for classical ML models (F1=0.85). We even saw their incredible flexibility with zero-shot classification via cosine similarity (F1=0.78).</li><li><strong>Generative models</strong> are also formidable classifiers when guided by good <strong>prompt engineering</strong>. We saw strong results from an open-source encoder-decoder model like Flan-T5 (F1=0.84) and state-of-the-art performance from a closed-source model like ChatGPT (F1=0.91), with important caveats.</li></ul></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>