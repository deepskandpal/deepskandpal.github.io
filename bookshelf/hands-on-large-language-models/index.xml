<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hands-On Large Language Models: Language Understanding and Generation on 404EngineerNotFound</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/</link><description>Recent content in Hands-On Large Language Models: Language Understanding and Generation on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 21 Feb 2024 10:00:00 +0000</lastBuildDate><atom:link href="https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 8: Retrieval Augmented Generation</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-8/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-8/</guid><description>&lt;p>Excellent! A very good choice. Retrieval Augmented Generation, or RAG, is one of the most impactful and practical applications of LLMs today. It directly addresses one of the biggest challenges with generative models: their tendency to &amp;ldquo;hallucinate&amp;rdquo; or make things up, and their lack of knowledge beyond their training data.&lt;/p>
&lt;p>So, before we jump headfirst into Chapter 8, which is all about &lt;strong>Semantic Search and Retrieval-Augmented Generation&lt;/strong>, let&amp;rsquo;s do exactly what you suggested: lay out the &lt;strong>exact basics needed to study RAG&lt;/strong>. Think of these as the foundational pillars upon which RAG is built. Many of these we&amp;rsquo;ve touched on in Chapters 1 and 2, but let&amp;rsquo;s crystallize them.&lt;/p></description></item></channel></rss>