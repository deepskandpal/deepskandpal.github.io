<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hands-On Large Language Models: Language Understanding and Generation on 404EngineerNotFound</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/</link><description>Recent content in Hands-On Large Language Models: Language Understanding and Generation on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 21 Feb 2024 10:00:00 +0000</lastBuildDate><atom:link href="https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 2: Tokens and Embeddings</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-2/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-2/</guid><description>&lt;p>As we&amp;rsquo;ve established, tokens and embeddings are the fundamental ways language models interact with and understand text. Without them, the magic simply doesn&amp;rsquo;t happen. Figure 2-1 in the book clearly shows this two-step process: raw text is first broken into &lt;strong>tokens&lt;/strong>, and then these tokens are converted into numerical &lt;strong>embeddings&lt;/strong> that capture their meaning.&lt;/p>
&lt;p>Let&amp;rsquo;s begin with the first part of this process.&lt;/p>
&lt;hr>
&lt;h2 id="section-1-llm-tokenization">Section 1: LLM Tokenization&lt;/h2>
&lt;p>The way most of us interact with LLMs, like chatbots, we see text coming out, seemingly word by word. This output, and more importantly, the input the model receives, is processed in chunks called &lt;strong>tokens&lt;/strong>.&lt;/p></description></item><item><title>Chapter 3: Looking Inside Large Language Models</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-3/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-3/</guid><description>&lt;p>Let&amp;rsquo;s begin our deep dive into &lt;strong>Chapter 3: Looking Inside Large Language Models&lt;/strong>. We&amp;rsquo;ll proceed cover to cover, just as you asked.&lt;/p>
&lt;p>Now, we address the main event: what happens &lt;em>after&lt;/em> tokenization? How does a model like the Transformer take those initial embeddings and generate coherent, often brilliant, text? That&amp;rsquo;s our mission for today.&lt;/p>
&lt;p>As the book mentions, we&amp;rsquo;ll start by loading our model, just to have it ready. This is the same &lt;code>microsoft/Phi-3-mini-4k-instruct&lt;/code> model we&amp;rsquo;ve seen before. It&amp;rsquo;s a powerful yet manageable model that&amp;rsquo;s perfect for our hands-on exploration.&lt;/p></description></item><item><title>Chapter 8: Retrieval Augmented Generation</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-8/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-8/</guid><description>&lt;h2 id="prerequisites-for-understanding-rag">Prerequisites for Understanding RAG&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Large Language Models (LLMs) as Generative Engines (from Chapter 1):&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Core Idea:&lt;/strong> LLMs, especially decoder-only architectures like GPT, are fundamentally &lt;em>next-token predictors&lt;/em>. Given some input text (a prompt), they generate a plausible continuation.&lt;/li>
&lt;li>&lt;strong>The &amp;ldquo;Knowledge&amp;rdquo; Limitation:&lt;/strong> Their knowledge is &amp;ldquo;frozen&amp;rdquo; at the time of their last training. They don&amp;rsquo;t know about events or information that occurred after that.&lt;/li>
&lt;li>&lt;strong>The Hallucination Problem:&lt;/strong> Because they are so good at generating fluent, confident-sounding text, they can sometimes generate incorrect or nonsensical information with high confidence. They are trying to complete a pattern, not necessarily state a verified fact from an internal database.&lt;/li>
&lt;li>&lt;strong>What RAG tries to achieve here:&lt;/strong> Provide the LLM with &lt;em>fresh, relevant, and factual information&lt;/em> at the time of generation to guide its output and make it more accurate and less prone to hallucination.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Embeddings: The Language of Meaning (from Chapter 2):&lt;/p></description></item></channel></rss>