<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hands-On Large Language Models: Language Understanding and Generation on 404EngineerNotFound</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/</link><description>Recent content in Hands-On Large Language Models: Language Understanding and Generation on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 21 Feb 2024 10:00:00 +0000</lastBuildDate><atom:link href="https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 2: Tokens and Embeddings</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-2/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-2/</guid><description>&lt;p>As we&amp;rsquo;ve established, tokens and embeddings are the fundamental ways language models interact with and understand text. Without them, the magic simply doesn&amp;rsquo;t happen. Figure 2-1 in the book clearly shows this two-step process: raw text is first broken into &lt;strong>tokens&lt;/strong>, and then these tokens are converted into numerical &lt;strong>embeddings&lt;/strong> that capture their meaning.&lt;/p>
&lt;p>Let&amp;rsquo;s begin with the first part of this process.&lt;/p>
&lt;hr>
&lt;h2 id="section-1-llm-tokenization">Section 1: LLM Tokenization&lt;/h2>
&lt;p>The way most of us interact with LLMs, like chatbots, we see text coming out, seemingly word by word. This output, and more importantly, the input the model receives, is processed in chunks called &lt;strong>tokens&lt;/strong>.&lt;/p></description></item><item><title>Chapter 3: Looking Inside Large Language Models</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-3/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-3/</guid><description>&lt;p>Now, we address the main event: what happens &lt;em>after&lt;/em> tokenization? How does a model like the Transformer take those initial embeddings and generate coherent, often brilliant, text? That&amp;rsquo;s our mission for today.&lt;/p>
&lt;p>As the book mentions, we&amp;rsquo;ll start by loading our model, just to have it ready. This is the same &lt;code>microsoft/Phi-3-mini-4k-instruct&lt;/code> model we&amp;rsquo;ve seen before. It&amp;rsquo;s a powerful yet manageable model that&amp;rsquo;s perfect for our hands-on exploration.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> torch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> transformers &lt;span style="color:#f92672">import&lt;/span> AutoModelForCausalLM, AutoTokenizer, pipeline
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Load model and tokenizer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tokenizer &lt;span style="color:#f92672">=&lt;/span> AutoTokenizer&lt;span style="color:#f92672">.&lt;/span>from_pretrained(&lt;span style="color:#e6db74">&amp;#34;microsoft/Phi-3-mini-4k-instruct&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>model &lt;span style="color:#f92672">=&lt;/span> AutoModelForCausalLM&lt;span style="color:#f92672">.&lt;/span>from_pretrained(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;microsoft/Phi-3-mini-4k-instruct&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> device_map&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;cuda&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> torch_dtype&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> trust_remote_code&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Create a pipeline&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>generator &lt;span style="color:#f92672">=&lt;/span> pipeline(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;text-generation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model&lt;span style="color:#f92672">=&lt;/span>model,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tokenizer&lt;span style="color:#f92672">=&lt;/span>tokenizer,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> return_full_text&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> max_new_tokens&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">50&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> do_sample&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With our tools at the ready, let&amp;rsquo;s start with a high-level view and peel back the layers one by one.&lt;/p></description></item><item><title>Chapter 4: Text Classification</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-4/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-4/</guid><description>&lt;p>As the book opens, it states a simple goal: &lt;strong>classification is about assigning a label or class to some input text.&lt;/strong> This is a cornerstone of NLP. Think about it:&lt;/p>
&lt;ul>
&lt;li>Is this email spam or not? (Spam detection)&lt;/li>
&lt;li>Is this customer review positive, negative, or neutral? (Sentiment analysis)&lt;/li>
&lt;li>Is this news article about sports, politics, or technology? (Topic classification)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Figure 4-1&lt;/strong> gives us the high-level picture: text goes in, the model does its thing, and a category comes out.&lt;/p></description></item><item><title>Chapter 8: Retrieval Augmented Generation</title><link>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-8/</link><pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-8/</guid><description>&lt;h2 id="prerequisites-for-understanding-rag">Prerequisites for Understanding RAG&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Large Language Models (LLMs) as Generative Engines (from Chapter 1):&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Core Idea:&lt;/strong> LLMs, especially decoder-only architectures like GPT, are fundamentally &lt;em>next-token predictors&lt;/em>. Given some input text (a prompt), they generate a plausible continuation.&lt;/li>
&lt;li>&lt;strong>The &amp;ldquo;Knowledge&amp;rdquo; Limitation:&lt;/strong> Their knowledge is &amp;ldquo;frozen&amp;rdquo; at the time of their last training. They don&amp;rsquo;t know about events or information that occurred after that.&lt;/li>
&lt;li>&lt;strong>The Hallucination Problem:&lt;/strong> Because they are so good at generating fluent, confident-sounding text, they can sometimes generate incorrect or nonsensical information with high confidence. They are trying to complete a pattern, not necessarily state a verified fact from an internal database.&lt;/li>
&lt;li>&lt;strong>What RAG tries to achieve here:&lt;/strong> Provide the LLM with &lt;em>fresh, relevant, and factual information&lt;/em> at the time of generation to guide its output and make it more accurate and less prone to hallucination.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Embeddings: The Language of Meaning (from Chapter 2):&lt;/p></description></item></channel></rss>