<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><a href=#prerequisites-for-understanding-rag>Prerequisites for Understanding RAG</a></li><li><a href=#overview-of-semantic-search-and-rag>Overview of Semantic Search and RAG</a></li><li><a href=#semantic-search-with-language-models>Semantic Search with Language Models</a></li><li><a href=#reranking>Reranking</a></li><li><a href=#retrieval-evaluation-metrics>Retrieval Evaluation Metrics</a></li><li><a href=#retrieval-augmented-generation-rag>Retrieval-Augmented Generation (RAG)</a></li><li><a href=#summary-of-rag>Summary of RAG</a></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 8: Retrieval Augmented Generation</h1><span class=reading-time><em>13 min read</em></span><div class=book-details><div class=book-content><h2 id=prerequisites-for-understanding-rag>Prerequisites for Understanding RAG</h2><ol><li><p>Large Language Models (LLMs) as Generative Engines (from Chapter 1):</p><ul><li><strong>Core Idea:</strong> LLMs, especially decoder-only architectures like GPT, are fundamentally <em>next-token predictors</em>. Given some input text (a prompt), they generate a plausible continuation.</li><li><strong>The &ldquo;Knowledge&rdquo; Limitation:</strong> Their knowledge is &ldquo;frozen&rdquo; at the time of their last training. They don&rsquo;t know about events or information that occurred after that.</li><li><strong>The Hallucination Problem:</strong> Because they are so good at generating fluent, confident-sounding text, they can sometimes generate incorrect or nonsensical information with high confidence. They are trying to complete a pattern, not necessarily state a verified fact from an internal database.</li><li><strong>What RAG tries to achieve here:</strong> Provide the LLM with <em>fresh, relevant, and factual information</em> at the time of generation to guide its output and make it more accurate and less prone to hallucination.</li></ul></li><li><p>Embeddings: The Language of Meaning (from Chapter 2):</p><ul><li><strong>Core Idea:</strong> Embeddings are numerical representations (vectors) of text (words, sentences, documents) in a high-dimensional space.</li><li><strong>Semantic Similarity:</strong> The crucial property is that texts with similar meanings will have embeddings that are &ldquo;close&rdquo; to each other in this vector space. &ldquo;The cat is furry&rdquo; will be closer to &ldquo;My feline is fluffy&rdquo; than to &ldquo;The car is fast.&rdquo;</li><li><strong>How they are made:</strong> We saw word2vec, and more advanced models like BERT or Sentence-Transformers produce these.</li><li><strong>What RAG tries to achieve here:</strong> Use embeddings to find pieces of text from a knowledge source that are semantically similar (i.e., relevant) to a user&rsquo;s query.</li></ul></li><li><p>Dense Retrieval / Semantic Search (Implicit in Chapter 2, core to Chapter 8):</p><ul><li><strong>Core Idea:</strong> The process of finding the most relevant documents (or text chunks) from a large collection (a &ldquo;text archive&rdquo; or &ldquo;knowledge base&rdquo;) in response to a user query, based on semantic similarity of their embeddings.</li><li><strong>The Mechanism:</strong><ol><li><strong>Indexation:</strong> Convert all documents/chunks in your knowledge base into embeddings and store them (often in a specialized &ldquo;vector database&rdquo;).</li><li><strong>Querying:</strong> When a user asks a question, convert that question into an embedding.</li><li><strong>Search:</strong> Compare the query embedding with all the document embeddings in your index and retrieve the &ldquo;nearest neighbors&rdquo; – the documents whose embeddings are closest to the query embedding.</li></ol></li><li><strong>What RAG tries to achieve here:</strong> This is the &ldquo;Retrieval&rdquo; part of RAG. It&rsquo;s the engine that pulls out the relevant context.</li></ul></li><li><p>Chunking Long Texts (Practical aspect for Retrieval, hinted at in Chapter 8):</p><ul><li><strong>Core Idea:</strong> LLMs have a finite context window (the maximum number of tokens they can process). You often can&rsquo;t feed an entire large document to an embedding model or into the final LLM prompt.</li><li><strong>The Solution:</strong> Break down large documents into smaller, manageable &ldquo;chunks&rdquo; (e.g., paragraphs, sentences, or fixed-size token blocks). Each chunk then gets its own embedding.</li><li><strong>What RAG tries to achieve here:</strong> Ensure that the retrieval step can pinpoint specific relevant pieces of information from large documents, rather than just getting a vague embedding of the whole document.</li></ul></li><li><p>Prompt Engineering (Rudimentary in Chapter 1, full focus in Chapter 6):</p><ul><li><strong>Core Idea:</strong> Crafting effective input prompts to guide an LLM to produce the desired output.</li><li><strong>Providing Context:</strong> A key technique in prompt engineering is giving the LLM relevant context along with the instruction or question.</li><li><strong>What RAG tries to achieve here:</strong> RAG automates the process of finding <em>highly relevant</em> context (from the retrieval step) and then constructs a new prompt that includes this context along with the original user query to feed into the generative LLM.</li></ul></li></ol><p>So, to recap the prerequisites:</p><ul><li>LLMs generate text but can hallucinate.</li><li>Embeddings capture meaning and allow similarity comparison.</li><li>Dense retrieval uses embeddings to find relevant text for a query.</li><li>Chunking makes large documents searchable.</li><li>Prompt engineering is how we tell an LLM what to do, and RAG uses it to provide retrieved context.</li></ul><p>With these building blocks in mind, we are perfectly set to explore Chapter 8!</p><hr><p><strong>Chapter 8: Semantic Search and Retrieval-Augmented Generation.</strong></p><p><em>(Gestures to an imaginary slide with the chapter title)</em></p><p>Search engines were some of the very first large-scale applications of language models. Google announced using BERT for its search way back, calling it a &ldquo;huge leap.&rdquo; Microsoft Bing followed suit. Why? Because these models enabled <strong>semantic search</strong> – searching by <em>meaning</em>, not just keywords.</p><p>But then came the generative models, like ChatGPT. People started asking them questions expecting factual answers. And while they&rsquo;re fluent, they aren&rsquo;t always correct. This led to the problem of &ldquo;hallucinations.&rdquo; One of the best ways to combat this is <strong>Retrieval-Augmented Generation (RAG)</strong> – building systems that retrieve relevant information <em>before</em> generating an answer. This is one of the hottest applications of LLMs right now.</p><h2 id=overview-of-semantic-search-and-rag>Overview of Semantic Search and RAG</h2><p>Chapter 8 looks at three broad categories:</p><ol><li><p><strong>Dense Retrieval:</strong> This is what we just discussed as a prerequisite. It relies on embeddings. You embed your query, you embed your documents (or chunks of documents), and you find the ones whose embeddings are closest to your query&rsquo;s embedding. (Figure 8-1 in the book shows this: query -> dense retrieval -> ranked documents).</p><ul><li><strong>What it&rsquo;s trying to achieve:</strong> Find semantically relevant documents from a corpus.</li></ul></li><li><p><strong>Reranking:</strong> Often, search is a pipeline. A first-stage retriever (maybe keyword-based, or a fast dense retriever) gets a bunch of potentially relevant documents. A reranker then takes this smaller set and the original query, and <em>scores</em> the relevance of each document much more carefully, reordering them. (Figure 8-2 shows query + initial results -> reranker -> improved order of results).</p><ul><li><strong>What it&rsquo;s trying to achieve:</strong> Improve the quality and ordering of search results from an initial, possibly less precise, retrieval step.</li></ul></li><li><p><strong>Retrieval-Augmented Generation (RAG):</strong> This is where we combine search with generation. The LLM doesn&rsquo;t just rely on its internal knowledge; it&rsquo;s <em>augmented</em> with retrieved information. (Figure 8-3 shows query -> RAG system -> answer + cited sources).</p><ul><li><strong>What it&rsquo;s trying to achieve:</strong> Generate factual, grounded answers by providing the LLM with relevant context from external sources, reducing hallucinations, and enabling &ldquo;chat with your data&rdquo; scenarios.</li></ul></li></ol><p>Let&rsquo;s dive deeper into these.</p><h2 id=semantic-search-with-language-models>Semantic Search with Language Models</h2><p><em>(Again, imagine Figure 8-4: texts as points in space, similar texts are closer)</em>
This is the core idea we&rsquo;ve built up. Embeddings project text into a space where distance equals dissimilarity. When a user queries, we embed the query into this same space and find the nearest document embeddings (Figure 8-5).</p><ul><li><p><strong>Caveats of Dense Retrieval (page 328-329):</strong></p><ul><li>What if no good results exist? The system might still return the &ldquo;least bad&rdquo; ones. We might need a similarity threshold.</li><li>What if the query and best result aren&rsquo;t <em>truly</em> semantically similar, just share some keywords? This is why embedding models for retrieval are often fine-tuned on question-answer pairs (more on this in Chapter 10).</li><li>Keyword matching is still good for exact phrases. Hybrid search (semantic + keyword) is often best.</li><li>Domain specificity: A model trained on Wikipedia might not do well on legal texts.</li></ul></li><li><p><strong>Chunking Long Texts (page 330-333):</strong></p><ul><li><strong>Why?</strong> LLMs have limited context windows.</li><li><strong>One vector per document?</strong> You could embed just the title, or average all chunk embeddings. Not ideal as you lose a lot of specific information.</li><li><strong>Multiple vectors per document (Better!):</strong> Chunk the document (sentences, paragraphs, fixed-size, overlapping chunks as shown in Figures 8-7, 8-8, 8-9) and embed each chunk. Your search index then contains <em>chunk</em> embeddings. This allows for more precise retrieval.<ul><li>Strategies include: each sentence, each paragraph, or overlapping chunks to preserve context across chunk boundaries (Figure 8-10).</li></ul></li></ul></li><li><p><strong>Nearest Neighbor Search vs. Vector Databases (page 333-334):</strong></p><ul><li>For small archives, calculating all distances (e.g., with NumPy) is fine.</li><li>For millions of vectors, you need optimized <strong>Approximate Nearest Neighbor (ANN)</strong> search libraries like FAISS or Annoy. They are fast, can use GPUs.</li><li><strong>Vector Databases</strong> (e.g., Weaviate, Pinecone, ChromaDB) are even more sophisticated. They allow adding/deleting vectors without rebuilding the whole index, filtering, and more complex querying beyond just vector distance (Figure 8-11).</li></ul></li><li><p><strong>Fine-tuning Embedding Models for Dense Retrieval (page 334-336):</strong></p><ul><li>Just like in classification (Chapter 4), we can fine-tune embedding models specifically for retrieval.</li><li><strong>Goal:</strong> Make embeddings of relevant query-document pairs <em>closer</em> and irrelevant pairs <em>farther</em>.</li><li><strong>Training Data:</strong> (Query, Relevant Document) pairs as positive examples, and (Query, Irrelevant Document) pairs as negative examples.</li><li>(Figure 8-12 shows before fine-tuning: &ldquo;Interstellar release date&rdquo; and &ldquo;Interstellar cast&rdquo; might be equally close to a document about Interstellar&rsquo;s premiere. Figure 8-13 shows after fine-tuning: &ldquo;Interstellar release date&rdquo; is much closer, &ldquo;Interstellar cast&rdquo; is pushed away.)</li></ul></li></ul><h2 id=reranking>Reranking</h2><p>This is often a second stage in a search pipeline.</p><ul><li><strong>How reranking models work (Figure 8-15):</strong> They are often <strong>cross-encoders</strong>. The query AND a candidate document are fed <em>together</em> into the LLM (like BERT). The model then outputs a relevance score (e.g., 0 to 1). This is more computationally expensive than dense retrieval (where query and documents are embedded separately), so it&rsquo;s typically done on a smaller, shortlisted set of documents.</li><li><strong>Example (page 337-338):</strong> The book shows using Cohere&rsquo;s Rerank endpoint. If a keyword search (BM25) brings up some results, the reranker can significantly improve their order by understanding the semantic relevance more deeply.</li></ul><h2 id=retrieval-evaluation-metrics>Retrieval Evaluation Metrics</h2><p>How do we know if our search system is good? We need:</p><ol><li>A text archive.</li><li>A set of queries.</li><li><strong>Relevance judgments:</strong> For each query, which documents in the archive are actually relevant? (Figure 8-16)</li></ol><ul><li><strong>Mean Average Precision (MAP):</strong> A popular metric.<ul><li><strong>Precision@k:</strong> Out of the top k results, how many are relevant?</li><li><strong>Average Precision (AP) for a single query:</strong> (Figures 8-20, 8-21, 8-22 show this). It rewards systems that rank relevant documents higher. If a relevant document is at rank 1, AP is 1.0. If it&rsquo;s at rank 3 (with 2 irrelevant ones before it), AP might be 0.33. If there are multiple relevant documents, it averages the precision at each relevant document&rsquo;s position.</li><li><strong>Mean Average Precision (MAP):</strong> The average of AP scores across <em>all</em> queries in your test set (Figure 8-23). This gives a single number to compare systems.</li><li>Another common metric is <strong>nDCG</strong> (normalized discounted cumulative gain), which handles graded relevance (some documents can be more relevant than others).</li></ul></li></ul><hr><p>Now, the main event for this session!</p><h2 id=retrieval-augmented-generation-rag>Retrieval-Augmented Generation (RAG)</h2><p><em>(Imagine Figure 8-24: A diagram showing Question -> 1) Retrieval -> 2) Grounded Generation -> Answer)</em></p><p>This is the industry&rsquo;s leading method to tackle LLM hallucinations and ground them in specific, up-to-date knowledge.</p><ul><li><p><strong>What it is:</strong> A system that first <em>retrieves</em> relevant information from a knowledge source and then uses that information to <em>augment</em> the prompt given to a generative LLM, which then produces the final answer.</p></li><li><p><strong>Why it&rsquo;s great:</strong></p><ul><li>Reduces hallucinations.</li><li>Improves factuality.</li><li>Allows LLMs to use information beyond their training data (e.g., internal company documents, recent news).</li><li>Enables &ldquo;chat with your data&rdquo; applications.</li></ul></li><li><p><strong>The Basic RAG Pipeline (Figure 8-24):</strong></p><ol><li><strong>Retrieval Step:</strong> User asks a question. This question is used to query a knowledge base (using dense retrieval, keyword search, or hybrid). The top N relevant document chunks are retrieved.</li><li><strong>Grounded Generation Step:</strong> The original question AND the retrieved document chunks are combined into a new, augmented prompt. This prompt is then fed to a generative LLM to produce the final answer. The LLM is instructed to use the provided context. (Figure 8-25 shows this with sources cited, Figure 8-26 shows the context being added to the prompt).</li></ol></li><li><p><strong>Example: Grounded Generation with an LLM API (page 351):</strong></p><ul><li>The book shows using Cohere&rsquo;s <code>co.chat</code> endpoint which has built-in RAG capabilities.</li><li>You provide the <code>message</code> (query) and <code>documents</code> (retrieved chunks).</li><li>The LLM generates an answer and can even provide citations pointing to which parts of the retrieved documents support its answer.</li></ul></li><li><p><strong>Example: RAG with Local Models (page 352-355):</strong></p><ul><li>This demonstrates the flow if you&rsquo;re building it yourself.</li><li><strong>Load Generation Model:</strong> e.g., a quantized Phi-3 using <code>llama-cpp-python</code> and LangChain.</li><li><strong>Load Embedding Model:</strong> e.g., <code>BAAI/bge-small-en-v1.5</code>.</li><li><strong>Create Vector Database:</strong> Use FAISS (or ChromaDB, etc.) to index your document chunks with their embeddings.</li><li><strong>The RAG Prompt:</strong> This is crucial. It typically looks something like:<pre tabindex=0><code>&lt;|user|&gt;
Relevant information:
{context}  &lt;-- This is where retrieved chunks go

Provide a concise answer the following question using the
relevant information provided above:
{question} &lt;--- This is the original user question
&lt;|end|&gt;
&lt;|assistant|&gt;
</code></pre></li><li><strong>LangChain&rsquo;s <code>RetrievalQA</code> chain</strong> can orchestrate this: it takes the LLM, the retriever (from the vector DB), and the prompt template.</li></ul></li><li><p><strong>Advanced RAG Techniques (page 355-357):</strong></p><ul><li><strong>Query Rewriting:</strong> If the user&rsquo;s question is verbose or conversational (e.g., &ldquo;I need an essay on dolphins, where do they live?&rdquo;), an LLM can rewrite it into a more effective search query (&ldquo;Where do dolphins live&rdquo;).</li><li><strong>Multi-Query RAG:</strong> For questions like &ldquo;Compare Nvidia&rsquo;s financial results in 2020 vs. 2023,&rdquo; the system might generate multiple search queries (&ldquo;Nvidia 2020 financial results&rdquo;, &ldquo;Nvidia 2023 financial results&rdquo;) and then synthesize the information.</li><li><strong>Multi-Hop RAG:</strong> For questions requiring sequential reasoning (e.g., &ldquo;Who are the largest car manufacturers in 2023? Do they each make EVs?&rdquo;).<ol><li>Search: &ldquo;largest car manufacturers 2023&rdquo; -> Gets Toyota, VW, Hyundai.</li><li>Search: &ldquo;Toyota electric vehicles&rdquo;, &ldquo;VW electric vehicles&rdquo;, &ldquo;Hyundai electric vehicles&rdquo;.</li></ol></li><li><strong>Query Routing:</strong> If you have multiple knowledge bases (e.g., HR documents in Notion, customer data in Salesforce), an LLM can decide which source to query based on the question.</li><li><strong>Agentic RAG:</strong> This is where RAG starts to look like the agents we&rsquo;ll discuss more (or that the book covers in Chapter 7, which you&rsquo;ve skipped for now!). The LLM becomes more autonomous, deciding which tools (search, specific databases, etc.) to use and in what order. Cohere&rsquo;s Command R+ is good at this.</li></ul></li><li><p><strong>RAG Evaluation (page 357-358):</strong></p><ul><li>How do you know your RAG system is good? It&rsquo;s not just about search relevance.</li><li>The paper &ldquo;Evaluating verifiability in generative search engines&rdquo; suggests axes like:<ul><li><strong>Fluency:</strong> Is the generated text smooth and cohesive?</li><li><strong>Perceived Utility:</strong> Is the answer helpful and informative?</li><li><strong>Citation Recall:</strong> Are statements supported by the cited sources?</li><li><strong>Citation Precision:</strong> Do the citations actually support the statements they&rsquo;re linked to?</li></ul></li><li><strong>LLM-as-a-Judge:</strong> Using another capable LLM to evaluate the RAG output.</li><li><strong>Ragas:</strong> A software library for this. It looks at:<ul><li><strong>Faithfulness:</strong> Is the answer consistent with the provided context?</li><li><strong>Answer Relevance:</strong> Is the answer relevant to the question?</li></ul></li></ul></li></ul><h2 id=summary-of-rag>Summary of RAG</h2><p>RAG is a powerful technique that combines the strengths of information retrieval with the generative capabilities of LLMs.</p><ul><li>It <strong>retrieves</strong> relevant information.</li><li>It <strong>augments</strong> the LLM&rsquo;s prompt with this information.</li><li>It allows the LLM to <strong>generate</strong> more factual, grounded, and up-to-date responses.</li><li>It&rsquo;s key to reducing hallucinations and making LLMs more trustworthy and useful in real-world applications.</li></ul><p>Phew! That was a deep dive into RAG and its foundations. It&rsquo;s a cornerstone of modern LLM applications. It&rsquo;s about giving your LLM a library card and teaching it how to read relevant books before answering your question!</p><p>What are your thoughts? Does this give you a clearer picture of what RAG is trying to achieve and how it goes about it?</p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script></body></html>