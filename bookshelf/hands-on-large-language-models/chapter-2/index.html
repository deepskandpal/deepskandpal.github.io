<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><a href=#section-1-llm-tokenization>Section 1: LLM Tokenization</a></li><li><a href=#section-2-token-embeddings>Section 2: Token Embeddings</a></li><li><a href=#section-3-text-embeddings-for-sentences-and-whole-documents>Section 3: Text Embeddings (for Sentences and Whole Documents)</a></li><li><a href=#section-4-word-embeddings-beyond-llms>Section 4: Word Embeddings Beyond LLMs</a></li><li><a href=#chapter-2-summary>Chapter 2 Summary</a></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 2: Tokens and Embeddings</h1><span class=reading-time><em>11 min read</em></span><div class=book-details><div class=book-content><p>As we&rsquo;ve established, tokens and embeddings are the fundamental ways language models interact with and understand text. Without them, the magic simply doesn&rsquo;t happen. Figure 2-1 in the book clearly shows this two-step process: raw text is first broken into <strong>tokens</strong>, and then these tokens are converted into numerical <strong>embeddings</strong> that capture their meaning.</p><p>Let&rsquo;s begin with the first part of this process.</p><hr><h2 id=section-1-llm-tokenization>Section 1: LLM Tokenization</h2><p>The way most of us interact with LLMs, like chatbots, we see text coming out, seemingly word by word. This output, and more importantly, the input the model receives, is processed in chunks called <strong>tokens</strong>.</p><ul><li><strong>How Tokenizers Prepare the Inputs to the Language Model</strong> (Page 68-69)<ul><li>When you send a prompt to an LLM, it doesn&rsquo;t see the raw string of characters directly.</li><li>First, it passes through a <strong>tokenizer</strong>. This component breaks your input text into smaller pieces.</li><li><em>Visual Example (Figure 2-3):</em> The book uses the OpenAI tokenizer playground. If you input &ldquo;Have the bards who preceded me left any theme unsung?&rdquo;, the tokenizer might break it down into something like: <code>Have</code>, <code>the</code>, <code>bard</code>, <code>s</code>, <code>who</code>, <code>preced</code>, <code>ed</code>, <code>me</code>, <code>left</code>, <code>any</code>, <code>theme</code>, <code>unsung</code>, <code>?</code>. (Note: The exact tokenization depends on the specific tokenizer used by the model.) Each of these is a token.</li></ul></li></ul><figure><img src=figure-2-3.png alt="Figure 2-3. A tokenizer breaks down text into words or parts of words before the model processes the text. It does so according to a specific method and training procedure (from https://oreil.ly/ovUWO)." width=700></figure><ul><li><strong>Downloading and Running an LLM (Code Example Deep Dive)</strong> (Page 69-73)<ul><li>Let&rsquo;s revisit the code example where we load a model (e.g., Phi-3-mini) and its tokenizer using the <code>transformers</code> library.<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Conceptual code structure from the book</span>
</span></span><span style=display:flex><span><span style=color:#75715e># from transformers import AutoModelForCausalLM, AutoTokenizer</span>
</span></span><span style=display:flex><span><span style=color:#75715e># model = AutoModelForCausalLM.from_pretrained(...)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># tokenizer = AutoTokenizer.from_pretrained(...)</span>
</span></span></code></pre></div></li><li>When we prepare an input prompt, like:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.&lt;|assistant|&gt;&#34;</span>
</span></span><span style=display:flex><span>input_ids <span style=color:#f92672>=</span> tokenizer(prompt, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)<span style=color:#f92672>.</span>input_ids
</span></span></code></pre></div></li><li>The variable <code>input_ids</code> doesn&rsquo;t contain the text string. It contains a sequence of integers (a tensor), like <code>tensor([[ 1, 14350, 385, ...]])</code>.<ul><li><em>Concept (Figure 2-4):</em> Each integer is a unique ID mapping to a specific token in the tokenizer&rsquo;s vocabulary. The LLM operates on these numerical IDs.</li></ul></li></ul></li></ul><figure><img src=/images/hands-on-large-language-models/chapter-2/figure-2-4-tokenizer-process.png alt="Figure 2-4: A tokenizer processes the input prompt and prepares the actual input into the language model: a list of token IDs." width=700></figure><pre><code>*   **Decoding Token IDs:** To see the actual tokens, we use `tokenizer.decode()` on individual IDs.
    ```python
    # for id_val in input_ids[0]:
    #     print(tokenizer.decode(id_val))
    ```
    This would print each token on a new line, showing how the original prompt was segmented.
    *   **Observations from the book's example (page 72-73):**
        *   Special tokens like `&lt;s&gt;` (ID 1 for Phi-3) often mark the beginning of the text.
        *   Some tokens are full words (e.g., &quot;Write&quot;, &quot;an&quot;, &quot;email&quot;).
        *   Many tokens are *subwords* (e.g., &quot;apolog&quot;, &quot;izing&quot;, &quot;trag&quot;, &quot;ic&quot;). This allows the model to handle a vast vocabulary and new words by combining known subwords.
        *   Punctuation usually gets its own token (e.g., &quot;.&quot;).
        *   **Space Handling:** Crucially, the space character itself often isn't a distinct token. Instead, subword tokens might have a prefix (like `Ġ` in GPT-2's tokenizer, or just implied by the algorithm) indicating they are preceded by a space or are at the start of a word. Tokens without this prefix attach directly to the previous one.
*   **Output Side:** The LLM generates a sequence of *output token IDs*. The tokenizer is then used again to convert these IDs back to human-readable text. Figure 2-5 illustrates this decoding process.
</code></pre><ul><li><p><strong>How Does the Tokenizer Break Down Text?</strong> (Page 74-75)
Three key factors dictate tokenization:</p><ol><li><strong>Tokenization Method:</strong> Chosen by the model creators. Common methods include:<ul><li><strong>Byte Pair Encoding (BPE):</strong> Starts with individual characters and iteratively merges the most frequent pair of bytes (or characters) to form new tokens. Used by GPT models.</li><li><strong>WordPiece:</strong> Similar to BPE, but merges pairs based on which new token maximizes the likelihood of the training data. Used by BERT.</li><li><strong>SentencePiece:</strong> Treats text as a sequence of Unicode characters and uses BPE or unigram language modeling to find optimal subword units. It&rsquo;s language-agnostic.</li><li><strong>Unigram Language Modeling:</strong> Starts with a large vocabulary of potential subwords and iteratively removes those that contribute least to the overall likelihood of the corpus, until the desired vocabulary size is reached.</li></ul></li><li><strong>Tokenizer Design Choices:</strong><ul><li><em>Vocabulary Size:</em> How many unique tokens will the tokenizer know? (e.g., 30k, 50k, 100k+).</li><li><em>Special Tokens:</em> <code>[CLS]</code>, <code>[SEP]</code>, <code>[PAD]</code>, <code>[UNK]</code>, <code>&lt;s></code>, <code>&lt;|user|></code>, etc., each serving a specific purpose.</li></ul></li><li><strong>Training Dataset:</strong> The tokenizer is <em>trained</em> on a large corpus of text to build its vocabulary and learn the merge rules (for BPE/WordPiece) or probabilities (for Unigram). A tokenizer trained on English will differ from one trained on Python code.</li></ol></li><li><p><strong>Word Versus Subword Versus Character Versus Byte Tokens</strong> (Page 75-78)
A nice visual is Figure 2-6, showing different ways to tokenize &ldquo;Have the bards who preceded&mldr;&rdquo;</p><ul><li><strong>Word Tokens:</strong><ul><li><em>Pros:</em> Conceptually simple.</li><li><em>Cons:</em> Large vocabulary (every word form is unique), struggles with out-of-vocabulary (OOV) words, can&rsquo;t represent nuances within words (e.g., &ldquo;apology&rdquo; vs &ldquo;apologize&rdquo;). Less common now for LLMs.</li></ul></li><li><strong>Subword Tokens:</strong> (Most common for modern LLMs)<ul><li><em>Pros:</em> Balances vocabulary size and expressiveness. Can represent OOV words by breaking them into known subwords. More efficient for model context length.</li><li><em>Cons:</em> Can sometimes break words in unintuitive ways.</li></ul></li><li><strong>Character Tokens:</strong><ul><li><em>Pros:</em> Smallest possible vocabulary, no OOV words.</li><li><em>Cons:</em> Sequences become very long, making it harder for the model to learn long-range dependencies. Less information per token.</li></ul></li><li><strong>Byte Tokens:</strong><ul><li><em>Pros:</em> Truly &ldquo;tokenization-free&rdquo; in a sense, handles all Unicode characters naturally. Good for highly multilingual or noisy data. GPT-2&rsquo;s BPE uses byte-level operations as a fallback.</li><li><em>Cons:</em> Sequences can also be long, less semantically meaningful units initially.</li></ul></li></ul></li><li><p><strong>Comparing Trained LLM Tokenizers</strong> (Page 78-91)
This is a fantastic section in the book that builds intuition. The key takeaway is that <strong>different models use different tokenizers, and these choices profoundly impact how the model &ldquo;sees&rdquo; and processes text.</strong></p><ul><li>The book takes a sample string with capitalization, emojis, non-English text, code-like syntax, and numbers.</li><li><strong>BERT (uncased):</strong> Loses capitalization, newlines. Emojis/Chinese become <code>[UNK]</code>.</li><li><strong>BERT (cased):</strong> Preserves case, but &ldquo;CAPITALIZATION&rdquo; might become 8 tokens.</li><li><strong>GPT-2:</strong> Handles newlines, capitalization. Emojis/Chinese are represented by multiple byte-level fallback tokens (often shown as <code>�</code> but reconstructible). Handles whitespace more granularly.</li><li><strong>Flan-T5 (SentencePiece):</strong> Loses newlines/whitespace tokens, emojis/Chinese become <code>&lt;unk></code>.</li><li><strong>GPT-4:</strong> Very efficient. Fewer tokens for many words. Has specific tokens for longer sequences of whitespace. Understands Python keywords like <code>elif</code> as single tokens.</li><li><strong>StarCoder2 (code-focused):</strong> Special tokens for code context (<code>&lt;filename></code>). Each <em>digit</em> is a separate token (e.g., &ldquo;600&rdquo; -> &ldquo;6&rdquo;, &ldquo;0&rdquo;, &ldquo;0&rdquo;).</li><li><strong>Galactica (science-focused):</strong> Special tokens for citations (<code>[START_REF]</code>), reasoning (<code>&lt;work></code>). Handles tabs as single tokens.</li><li><strong>Phi-3/Llama 2:</strong> Use special chat tokens like <code>&lt;|user|></code>, <code>&lt;|assistant|></code>.</li><li><em>The side-by-side comparison table on pages 90-91 is incredibly illustrative.</em></li></ul></li><li><p><strong>Tokenizer Properties (Recap of factors affecting tokenization)</strong> (Page 91-93)</p><ol><li><strong>Tokenization methods</strong> (BPE, WordPiece, etc.).</li><li><strong>Tokenizer parameters</strong> (vocab size, special tokens, handling of capitalization).</li><li><strong>The domain of the data</strong> (English text vs. code vs. multilingual).<ul><li>Example: A text-focused tokenizer might tokenize code indentation inefficiently (e.g., four spaces as four separate tokens), while a code-focused tokenizer might have a single token for &ldquo;four spaces.&rdquo; This makes the model&rsquo;s job easier for code generation.</li></ul></li></ol></li></ul><hr><h2 id=section-2-token-embeddings>Section 2: Token Embeddings</h2><p>Now that we have tokenized our text into a sequence of token IDs, the next step is to convert these IDs into meaningful numerical representations – <strong>embeddings</strong>.</p><ul><li><strong>A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer</strong> (Page 95)<ul><li>When an LLM is pretrained, it learns an <strong>embedding vector</strong> for each token in its tokenizer&rsquo;s vocabulary.</li><li>This collection of vectors forms an <strong>embedding matrix</strong> (or embedding layer) within the model (as shown in Figure 2-7).</li></ul></li></ul><figure><img src=/images/hands-on-large-language-models/chapter-2/figure-2-7-token-embeddings.png alt="Figure 2-7. A language model holds an embedding vector associated with each token in its tokenizer." width=700></figure><pre><code>*   Initially, these vectors are random, but during pretraining, their values are adjusted so that tokens with similar meanings or that appear in similar contexts have similar embedding vectors. These are *static* embeddings – each token ID always maps to the same initial vector.
</code></pre><ul><li><strong>Creating Contextualized Word Embeddings with Language Models</strong> (Page 96-99)<ul><li>Static token embeddings are just the starting point. The real power comes when the language model processes these static embeddings <em>in the context of the entire input sequence</em>.</li><li>The output of the main Transformer blocks (before the final prediction layer) for each input token is its <strong>contextualized word embedding</strong>. This means the embedding for &ldquo;bank&rdquo; in &ldquo;river bank&rdquo; will be different from &ldquo;bank&rdquo; in &ldquo;money bank.&rdquo;</li><li><em>Visual (Figure 2-8 & 2-9):</em> Raw, static token embeddings go into the language model. The model&rsquo;s layers process these, considering the whole sequence (via attention, which we&rsquo;ll detail in Chapter 3). The output for each token position is a new, richer, contextualized embedding vector.</li></ul></li></ul><figure><img src=/images/hands-on-large-language-models/chapter-2/figure-2-8-contextualized-embeddings.png alt="Figure 2-8. Language models produce contextualized token embeddings that improve on raw, static token embeddings." width=700></figure><pre><code>*   *Code Example (using DeBERTa, page 97):*
    ```python
    # from transformers import AutoModel, AutoTokenizer
    # tokenizer_deberta = AutoTokenizer.from_pretrained(&quot;microsoft/deberta-base&quot;)
    # model_deberta = AutoModel.from_pretrained(&quot;microsoft/deberta-v3-xsmall&quot;)
    # tokens_deberta = tokenizer_deberta('Hello world', return_tensors='pt')
    # output_embeddings = model_deberta(**tokens_deberta)[0]
    # print(output_embeddings.shape) # e.g., torch.Size([1, 4, 384])
    ```
    This `output_embeddings` tensor contains the contextualized embeddings for each token (including special ones like `[CLS]` and `[SEP]`). These are the representations used for downstream tasks or further generation.
</code></pre><hr><h2 id=section-3-text-embeddings-for-sentences-and-whole-documents>Section 3: Text Embeddings (for Sentences and Whole Documents)</h2><p>While token embeddings are crucial for the internal workings of LLMs, many applications need a single vector representation for an entire sentence, paragraph, or document.</p><ul><li><strong>Concept (Figure 2-10):</strong> An embedding model takes a piece of text and outputs a single vector that captures its overall meaning.</li><li><strong>Methods:</strong><ul><li>A simple approach is to average the contextualized token embeddings of all tokens in the text.</li><li>However, high-quality text embedding models (often called <strong>Sentence Transformers</strong> or bi-encoders, building on architectures like SBERT) are specifically trained for this task. They often outperform simple averaging.</li></ul></li><li><strong>Using <code>sentence-transformers</code> library (page 101-102):</strong><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># from sentence_transformers import SentenceTransformer</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sbert_model = SentenceTransformer(&#34;sentence-transformers/all-mpnet-base-v2&#34;)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># sentence_vector = sbert_model.encode(&#34;This is the best movie ever!&#34;)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># print(sentence_vector.shape) # e.g., (768,)</span>
</span></span></code></pre></div>This gives a single vector for the entire sentence. These sentence/document embeddings are powerful for:<ul><li>Semantic search (as we&rsquo;ll see in Chapter 8)</li><li>Clustering (Chapter 5)</li><li>Classification (Chapter 4)</li><li>Text similarity tasks.</li></ul></li></ul><hr><h2 id=section-4-word-embeddings-beyond-llms>Section 4: Word Embeddings Beyond LLMs</h2><p>This section briefly revisits older but foundational embedding techniques, primarily to set the stage for understanding <strong>contrastive learning</strong>, a key concept for training modern embedding models (which we&rsquo;ll hit hard in Chapter 10).</p><ul><li><p><strong>Using pretrained Word Embeddings (word2vec, GloVe)</strong> (Page 103)</p><ul><li>Libraries like <code>gensim</code> allow you to download and use classic pretrained word embeddings.</li><li>Example: Loading <code>glove-wiki-gigaword-50</code> and finding similar words to &ldquo;king&rdquo;.<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># import gensim.downloader as api</span>
</span></span><span style=display:flex><span><span style=color:#75715e># glove_model_gensim = api.load(&#34;glove-wiki-gigaword-50&#34;)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># print(glove_model_gensim.most_similar(&#34;king&#34;))</span>
</span></span></code></pre></div></li></ul></li><li><p><strong>The Word2vec Algorithm and Contrastive Training</strong> (Page 103-107)</p><ul><li><em>Recap:</em> Word2vec learns embeddings by predicting context words (Skip-gram) or a target word from its context (CBOW).</li><li><em>Key Idea: Contrastive Approach.</em><ol><li><strong>Positive Pairs:</strong> Words that actually appear near each other in the training text (e.g., &ldquo;Thou&rdquo; and &ldquo;shalt&rdquo; in the Dune example, Figure 2-11).</li><li><strong>Negative Pairs:</strong> Words that <em>don&rsquo;t</em> usually appear together. These are generated by taking a context word and pairing it with a randomly sampled word from the vocabulary (Figure 2-13).</li><li><strong>Training Objective:</strong> The model (a simple neural network) is trained to output a high score for positive pairs and a low score for negative pairs. During this process, the embedding vectors for each word (which start random, Figure 2-15) are adjusted (Figure 2-16).</li></ol></li><li><em>Relevance:</em> This &ldquo;learning by contrast&rdquo; is fundamental. It teaches the model not just what words are similar, but also what makes them distinct.</li></ul></li><li><p><strong>Embeddings for Recommendation Systems</strong> (Page 108-113)</p><ul><li>A practical example of applying word2vec-like thinking beyond text.</li><li><strong>Idea:</strong> Treat <em>songs</em> as &ldquo;words&rdquo; and <em>playlists</em> as &ldquo;sentences.&rdquo;</li><li>If two songs frequently appear in the same playlists, their learned &ldquo;song embeddings&rdquo; will be similar.</li><li><em>Dataset:</em> The book uses playlists from US radio stations (Figure 2-17).</li><li><em>Process:</em><ol><li>Prepare data: List of playlists, where each playlist is a list of song IDs.</li><li>Train a <code>Word2Vec</code> model from <code>gensim</code> on these playlists.</li><li>Use <code>model.wv.most_similar()</code> to find songs similar to a given song ID.</li></ol></li><li>The example (pages 109-113) shows this working well for recommending similar artists/genres.</li></ul></li></ul><hr><h2 id=chapter-2-summary>Chapter 2 Summary</h2><p>To quickly summarize the key takeaways from Chapter 2:</p><ul><li><strong>Tokenizers</strong> are the first crucial step, breaking raw text into token IDs. The choice of tokenizer and its parameters significantly affects how an LLM processes information. Subword tokenization is the most common.</li><li><strong>Token Embeddings</strong> are the initial (often static) numerical representations of these tokens, learned during pretraining.</li><li>Language models then create <strong>Contextualized Word Embeddings</strong>, which are dynamic and reflect the meaning of a token within its specific sentence.</li><li><strong>Text Embeddings</strong> provide a single vector for an entire sentence or document, crucial for many downstream tasks and often produced by specialized models like Sentence Transformers.</li><li>The principles of <strong>Contrastive Learning</strong> (as seen in word2vec and later in SBERT) are fundamental for training effective embedding models by teaching them similarity and dissimilarity.</li><li>These embedding concepts are versatile and can be applied beyond text, for example, in recommendation systems.</li></ul></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>