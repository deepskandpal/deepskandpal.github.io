<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul></nav></header><main><article class=book-single><h1>Chapter 3: Data Engineering Fundamentals</h1><span class=reading-time><em>25 min read</em></span><div class=book-details><div class=book-content><p>As the book says, &ldquo;The rise of ML in recent years is tightly coupled with the rise of big data.&rdquo; At FAANG, this is our daily reality. Our ML systems, from recommendation engines to fraud detection, are built on vast, complex data landscapes. If you don&rsquo;t have a solid grasp of data engineering, you&rsquo;re going to struggle, no matter how fancy your model architecture is.</p><p>This chapter is dense with terminology and concepts that might seem overwhelming if you&rsquo;re new to large-scale data systems. But don&rsquo;t worry, we&rsquo;ll break it down. The goal here is to give you a &ldquo;steady piece of land to stand on.&rdquo;</p><p>We&rsquo;ll cover:</p><ul><li>Data Sources: Where does our data even come from?</li><li>Data Formats: How is it stored? What are the trade-offs?</li><li>Data Models: How is it structured and represented?</li><li>Data Storage Engines & Processing: Databases, transactional vs. analytical workloads, ETL.</li><li>Modes of Dataflow: How does data move between different parts of our systems?</li><li>Batch vs. Stream Processing: Two fundamental paradigms for handling data.</li></ul><p>This chapter is critical for ML system design interviews. Understanding data is non-negotiable. Let&rsquo;s get started!</p><h2 id=page-49-chapter-introduction-the-data-deluge>Page 49 (Chapter Introduction): The Data Deluge</h2><p>The intro page sets the scene:</p><ul><li>ML&rsquo;s growth is linked to big data.</li><li>Large data systems are complex, even without ML. Full of acronyms, evolving standards, diverse tools. It can feel like every company does it differently.</li><li>This chapter provides the basics of data engineering.</li></ul><p>The roadmap mentioned:</p><ul><li>Sources of data.</li><li>Formats for storage.</li><li>Structure of data (data models).</li><li>Databases (storage engines) for transactional and analytical processing.</li><li>Passing data across multiple processes and services.</li></ul><p>This chapter really emphasizes that data engineering is a distinct discipline that ML practitioners must understand.</p><h2 id=pages-50-52-data-sources--where-it-all-begins>Pages 50-52: Data Sources – Where It All Begins</h2><p>An ML system ingests data from various sources. Understanding these sources helps use the data more efficiently and anticipate challenges.</p><h3 id=user-input-data-page-50>User Input Data (Page 50)</h3><p>Data explicitly provided by users.</p><ul><li><strong>Examples</strong>: Text typed into a search bar, images/videos uploaded, form submissions.</li><li><strong>Challenges</strong>:<ul><li><em>Malformatted</em>: &ldquo;If it&rsquo;s even remotely possible for users to input wrong data, they are going to do it.&rdquo; This is a golden rule! Text too long/short, text in numerical fields, wrong file formats.</li><li>Requires heavy-duty validation and processing.</li><li><em>User Patience is Low</em>: Users expect immediate results from their input. This implies a need for fast processing.</li></ul></li></ul><p><strong>FAANG Perspective</strong>: Input validation is a huge deal. For example, search queries need sanitization, image uploads need format checks and virus scans. Latency is critical – a slow search box is a bad user experience.</p><h3 id=system-generated-data-pages-50-51>System-Generated Data (Pages 50-51)</h3><p>Data generated by your system&rsquo;s components.</p><ul><li><strong>Examples</strong>: Logs (memory usage, services called, job results), model predictions.</li><li><strong>Purpose</strong>: Visibility into system health, debugging, improving the application. Essential &ldquo;when something is on fire.&rdquo;</li><li><strong>Characteristics</strong>:<ul><li>Less likely to be malformatted than user input.</li><li>Processing doesn&rsquo;t always need to be immediate (e.g., hourly/daily log processing is often fine).</li><li>However, you might want faster processing for &ldquo;interesting&rdquo; events (footnote 1: &ldquo;interesting&rdquo; often means &ldquo;catastrophic&rdquo; – like a system crash or a runaway cloud bill!). This is where real-time alerting on logs comes in.</li></ul></li><li><strong>Challenges with Logs</strong>:<ul><li><em>Volume</em>: &ldquo;Log everything you can&rdquo; is common practice for debugging ML systems, leading to massive log volumes.</li><li><em>Signal vs. Noise</em>: Hard to find useful information. Services like Logstash, Datadog, Logz.io (often using ML themselves) help process and analyze logs.</li><li><em>Storage Cost</em>: Store logs only as long as useful. Low-access, cheaper storage (e.g., AWS S3 Glacier vs. S3 Standard – footnote 2 notes a 5x cost difference for much higher retrieval latency) can be used for older logs.</li></ul></li></ul><p><strong>FAANG Perspective</strong>: We generate petabytes of logs daily. Effective log management, aggregation, and analysis are critical for SREs (Site Reliability Engineers) and ML engineers alike to debug production issues.</p><h3 id=user-behavior-data-page-51>User Behavior Data (Page 51)</h3><p>System-generated data specifically recording user actions.</p><ul><li><strong>Examples</strong>: Clicks, suggestions chosen, scrolling, zooming, ignoring pop-ups, time spent on page.</li><li><strong>Crucial for ML</strong>: This is the raw material for training many models (recommenders, personalization, engagement prediction).</li><li><strong>Privacy Concerns</strong>: Even if system-generated, it&rsquo;s considered user data and subject to privacy regulations (e.g., GDPR, CCPA). Footnote 3 has a great anecdote: an ML engineer says his team only uses browsing/purchase history, not &ldquo;personal data&rdquo; like age/location. The author rightly points out that browsing/purchase history is extremely personal!</li></ul><p><strong>FAANG Perspective</strong>: This is gold. But handling it ethically and in compliance with regulations is paramount. Data anonymization and aggregation are key, but true anonymization is very hard.</p><h3 id=internal-databases-page-52>Internal Databases (Page 52)</h3><p>Generated by various services and enterprise applications within the company.</p><ul><li><strong>Examples</strong>: Inventory, customer relationship management (CRM), user accounts.</li><li><strong>Usage in ML</strong>:<ul><li>Directly by models (e.g., a model might need current user subscription status).</li><li>By components of an ML system (e.g., Amazon search: ML model detects intent for &ldquo;frozen,&rdquo; then system checks internal inventory database for &ldquo;Frozen&rdquo; movie vs. &ldquo;frozen foods&rdquo; availability before ranking).</li></ul></li></ul><p><strong>FAANG Perspective</strong>: These are often the &ldquo;source of truth&rdquo; for many entities. Integrating them into ML pipelines reliably and efficiently is a common data engineering task.</p><h3 id=third-party-data-page-52>Third-Party Data (Page 52)</h3><p>The &ldquo;wonderfully weird world.&rdquo;</p><ul><li><strong>First-party data</strong>: What your company collects about its own users/customers. (This is the best, you own it, you know its provenance).</li><li><strong>Second-party data</strong>: Data from another company on their customers, which they make available to you (usually for a fee).</li><li><strong>Third-party data</strong>: Companies collect data about the public (not their direct customers) and sell it.</li><li><strong>Collection Mechanisms</strong>: Historically, unique advertiser IDs on phones (IDFA on iOS, AAID on Android) made it easy to aggregate activity across apps, websites, check-ins. This data is (hopefully) anonymized.</li><li><strong>Types of Data</strong>: Social media activity, purchase history, web browsing, car rentals, political leaning, demographics (e.g., &ldquo;men, age 25-34, tech workers, Bay Area&rdquo;).</li><li><strong>Use Cases</strong>: Inferring correlations (people liking Brand A also like Brand B), helpful for recommenders. Usually sold pre-cleaned and processed.</li><li><strong>Privacy Pushback</strong>:<ul><li>Apple&rsquo;s IDFA opt-in (early 2021) significantly reduced third-party data on iPhones, forcing companies to focus more on first-party data (footnote 4).</li><li>Advertisers seek workarounds (e.g., CAID device fingerprinting in China - footnote 5).</li></ul></li></ul><p><strong>FAANG Perspective</strong>: While first-party data is king, third-party data can be useful for cold-start problems or enriching user profiles. However, the privacy and ethical implications are significant, and reliance on it is decreasing due to regulations and platform changes.</p><p>Understanding your data sources tells you about its likely quality, freshness, volume, and any constraints (privacy, cost, processing needs).</p><h2 id=pages-53-57-data-formats--how-data-is-represented-for-storage-and-transmission>Pages 53-57: Data Formats – How Data is Represented for Storage and Transmission</h2><p>Once you have data, you need to store it (&ldquo;persist&rdquo; it). The format matters for cost, access speed, and ease of use. Key questions to consider:</p><ul><li>Storing multimodal data (images + text)?</li><li>Cheap and fast access?</li><li>Storing complex models for cross-hardware compatibility?</li></ul><p><strong>Data Serialization</strong>: Converting a data structure/object state into a format that can be stored/transmitted and later reconstructed.</p><p><strong>Table 3-1: Common Data Formats</strong></p><table><thead><tr><th style=text-align:left>Format</th><th style=text-align:left>Binary/Text</th><th style=text-align:left>Human-readable</th><th style=text-align:left>Example Use Cases</th></tr></thead><tbody><tr><td style=text-align:left>JSON</td><td style=text-align:left>Text</td><td style=text-align:left>Yes</td><td style=text-align:left>Everywhere</td></tr><tr><td style=text-align:left>CSV</td><td style=text-align:left>Text</td><td style=text-align:left>Yes</td><td style=text-align:left>Everywhere</td></tr><tr><td style=text-align:left>Parquet</td><td style=text-align:left>Binary</td><td style=text-align:left>No</td><td style=text-align:left>Hadoop, Amazon Redshift</td></tr><tr><td style=text-align:left>Avro</td><td style=text-align:left>Binary primary</td><td style=text-align:left>No</td><td style=text-align:left>Hadoop</td></tr><tr><td style=text-align:left>Protobuf</td><td style=text-align:left>Binary primary</td><td style=text-align:left>No</td><td style=text-align:left>Google, TensorFlow (TFRecord)</td></tr><tr><td style=text-align:left>Pickle</td><td style=text-align:left>Binary</td><td style=text-align:left>No</td><td style=text-align:left>Python, PyTorch serialization (models)</td></tr></tbody></table><p>Key characteristics to consider: human readability, access patterns (how data is read/written - footnote 6), text vs. binary (impacts file size).</p><p>Let&rsquo;s look at a few in detail:</p><h3 id=json-javascript-object-notation-page-54>JSON (JavaScript Object Notation) (Page 54)</h3><ul><li>Ubiquitous, language-independent, human-readable.</li><li>Key-value pair paradigm, handles different levels of structuredness.<ul><li>Structured: <code>{"firstName": "Boatie", "address": {"city": "Port Royal"}}</code></li><li>Unstructured blob: <code>{"text": "Boatie McBoatFace, aged 12, is vibing..."}</code></li></ul></li><li><strong>Pain points</strong>:<ul><li>Schema evolution is painful once committed. Changing a schema in existing JSON files is hard.</li><li>Text files = take up a lot of space (more on this later).</li></ul></li></ul><p><strong>FAANG Perspective</strong>: JSON is incredibly common for API responses, configs, and semi-structured data logging. Its human readability is a big plus for debugging.</p><h3 id=row-major-versus-column-major-format-pages-54-55-figure-3-1>Row-Major Versus Column-Major Format (Pages 54-55, Figure 3-1)</h3><ul><li><strong>CSV (Comma-Separated Values)</strong>: Row-major. Consecutive elements in a row are stored together in memory.<pre tabindex=0><code>Example1_Feat1, Example1_Feat2, ... Example1_FeatN
Example2_Feat1, Example2_Feat2, ... Example2_FeatN
</code></pre></li><li><strong>Parquet</strong>: Column-major (Columnar). Consecutive elements in a column are stored together.<pre tabindex=0><code>Example1_Feat1, Example2_Feat1, ... ExampleM_Feat1
Example1_Feat2, Example2_Feat2, ... ExampleM_Feat2
</code></pre></li><li><strong>Performance implications</strong> (due to sequential data access being efficient):<ul><li><em>Row-major (CSV)</em>: Better for accessing entire rows/examples (e.g., &ldquo;get all examples from today&rdquo;). Faster writes when adding new individual examples.</li><li><em>Column-major (Parquet)</em>: Better for accessing specific columns/features (e.g., &ldquo;get timestamps for all examples&rdquo;). More flexible column-based reads, especially with many features. If you have 1000 features but only need 4 (time, location, distance, price), Parquet lets you read just those 4 columns directly. With CSV, you&rsquo;d often read all 1000 and then filter.</li></ul></li><li><strong>Overall</strong>: Row-major for write-heavy workloads or row-based reads. Column-major for read-heavy, analytical workloads needing specific columns.</li></ul><p><strong>FAANG Perspective</strong>: Columnar formats like Parquet (and ORC) are the standard for data warehouses and data lakes (e.g., in S3, GCS) because analytical queries usually operate on subsets of columns. This leads to huge I/O savings and faster query performance. Compression is also more effective on columnar data.</p><h3 id=numpy-versus-pandas-page-56-figure-3-2-a-common-gotcha>NumPy Versus pandas (Page 56, Figure 3-2): A Common Gotcha!</h3><ul><li>Many don&rsquo;t realize: pandas DataFrame is built around a columnar format (inspired by R&rsquo;s data frame).</li><li>NumPy ndarrays are row-major by default (though configurable).</li><li>People coming from NumPy often treat DataFrames like ndarrays, accessing by row, and find it slow.</li><li><strong>Figure 3-2 Performance</strong>:<ul><li>Iterating pandas DataFrame by column: 0.07 seconds.</li><li>Iterating pandas DataFrame by row (<code>df.iloc[i]</code>): 2.41 seconds (MUCH SLOWER!).</li><li>Converting to NumPy array (<code>df.to_numpy()</code>) and iterating by row: 0.019 seconds (FAST!).</li><li>Iterating NumPy array by column: 0.005 seconds (FASTEST, as expected for columnar data if NumPy array was C-contiguous/columnar, but it&rsquo;s likely F-contiguous/row-major here, so this illustrates pandas&rsquo; overhead).</li></ul></li></ul><p><strong>Self-Correction/Teaching Point</strong>: The code snippet <code>df_np[:, j]</code> in Figure 3-2 iterates through a NumPy array column by column. If <code>df_np</code> is row-major (NumPy default), this is non-contiguous access, which should be slower than row-wise access. The 0.005s vs 0.019s suggests the test DataFrame might have few rows and many columns, or there&rsquo;s something subtle about <code>to_numpy()</code> memory layout or caching effects. The main takeaway is pandas row iteration is slow due to its internal structure and overhead. Use vectorized operations in pandas, or convert to NumPy for row-wise loops if absolutely necessary. The author&rsquo;s &ldquo;Just pandas Things&rdquo; GitHub repo (footnote 7) is a good resource for these quirks.</p><h3 id=text-versus-binary-format-page-57-figure-3-3>Text Versus Binary Format (Page 57, Figure 3-3)</h3><ul><li>CSV, JSON are text files (plain text, human-readable).</li><li>Parquet is a binary file (0s and 1s, machine-readable).</li><li><strong>Binary files are more compact</strong>:<ul><li>Example: Storing the number 1000000.<ul><li>Text file: 7 characters, 7 bytes (if 1 byte/char).</li><li>Binary file (int32): 32 bits = 4 bytes. (Significant saving!)</li></ul></li></ul></li><li><strong>Figure 3-3 Illustration (interviews.csv)</strong>:<ul><li>CSV (text): 17,654 rows, 10 columns. File size: 14 MB.</li><li>Parquet (binary): Same data. File size: 6 MB. (Over 2x smaller).</li></ul></li><li>AWS recommends Parquet: &ldquo;up to 2x faster to unload and consumes up to 6x less storage in Amazon S3, compared to text formats&rdquo; (footnote 8). This is due to efficient encoding and compression schemes that work well with columnar data.</li></ul><p><strong>FAANG Perspective</strong>: For large datasets, binary columnar formats (Parquet, ORC) are almost always preferred over text formats like CSV/JSON for storage in data lakes due to space savings, query performance, and schema evolution support. Text formats are fine for smaller files, human inspection, or system interchange where readability is key.</p><h2 id=pages-58-66-data-models--structuring-your-data>Pages 58-66: Data Models – Structuring Your Data</h2><p>Data models describe how data is represented and the relationships between data elements. This choice affects system build and the problems you can solve.</p><h3 id=relational-model-pages-59-61>Relational Model (Pages 59-61)</h3><ul><li>Invented by Edgar F. Codd (1970), still dominant.</li><li>Data organized into relations (tables), each a set of tuples (rows).</li><li><strong>Key Property (Figure 3-4)</strong>: Relations are unordered (rows and columns can be shuffled, it&rsquo;s still the same relation). Stored in formats like CSV/Parquet.</li><li><strong>Normalization</strong>: Reducing redundancy, improving integrity.<ul><li>Example (Tables 3-2, 3-3, 3-4): Book data.<ul><li>Initial Book relation (Table 3-2): Title, Author, Format, Publisher, Country, Price. Duplicates publisher info (Banana Press, UK) for different books/formats. If &ldquo;Banana Press&rdquo; changes to &ldquo;Pineapple Press&rdquo;, multiple rows need updates.</li><li>Normalized:<ul><li>Book relation (Table 3-3): Title, Author, Format, Publisher_ID, Price.</li><li>Publisher relation (Table 3-4): Publisher_ID, Publisher, Country.</li></ul></li><li>Now, if publisher name changes, only one row in Publisher table needs update. Standardizes spelling, easier to translate values.</li></ul></li></ul></li><li><strong>Downside of Normalization</strong>: Data spread across tables. Retrieving full info requires joins, which can be expensive for large tables.</li><li><strong>Relational Databases & SQL (Structured Query Language)</strong>:<ul><li>SQL is the most popular query language.</li><li>Declarative: You specify <em>what</em> data you want (pattern, conditions, transformations like join, sort, group, aggregate), not <em>how</em> to get it.</li><li>Imperative (like Python): You specify the steps.</li><li>Database system has a query optimizer to figure out the execution plan (break query, methods for each part, order of execution). This is hard! ML is even being used to improve query optimizers (footnote 14, Neo).</li><li>SQL is Turing-complete (with additions), but complex queries can be &ldquo;nightmarish&rdquo; (footnote 12, 700-line SQL query).</li></ul></li></ul><p><strong>FAANG Perspective</strong>: Relational databases (PostgreSQL, MySQL, Spanner, Aurora) are workhorses for many transactional systems and structured data stores. SQL is a fundamental skill. Understanding query plans (EXPLAIN) is key for performance tuning.</p><h3 id=aside-from-declarative-data-systems-to-declarative-ml-systems-page-62>Aside: From Declarative Data Systems to Declarative ML Systems (Page 62)</h3><ul><li>Inspired by SQL&rsquo;s success, &ldquo;Declarative ML&rdquo; aims to abstract away model construction/tuning.</li><li>User declares feature schema and task; system finds the best model.</li><li>Examples: Ludwig (Uber), H2O AutoML.<ul><li>Ludwig: User can specify model structure (layers, units) on top of schema.</li><li>H2O AutoML: No need to specify structure/hyperparameters; it experiments and picks best. Example code shows simple API: <code>aml.train(x=x, y=y, training_frame=train)</code>.</li></ul></li><li><strong>Limitation</strong>: Abstracts away model development (often the easier part now with commoditized models). Hard parts remain: feature engineering, data processing, evaluation, drift detection, continual learning.</li></ul><p><strong>Self-Correction</strong>: Declarative ML is great for baselining and for users who aren&rsquo;t ML experts, but for complex, high-stakes production systems at FAANG, engineers often need finer-grained control.</p><h3 id=nosql-not-only-sql-pages-63-65>NoSQL (Not Only SQL) (Pages 63-65)</h3><ul><li>Movement against relational model&rsquo;s restrictions (strict schema, schema management pain - #1 reason for Couchbase adoption, footnote 16). SQL can be hard for specialized apps.</li><li>Many NoSQL systems now also support relational models/SQL.</li><li>Two major types discussed: Document and Graph.</li></ul><h4 id=document-model-pages-63-64>Document Model (Pages 63-64)</h4><ul><li>Built around &ldquo;documents&rdquo; (often JSON, XML, or binary like BSON).</li><li>Each document has a unique key. Collection of documents ~ table, document ~ row.</li><li><strong>Flexibility</strong>: Documents in a collection can have different schemas (unlike rows in a relational table).</li><li><strong>&ldquo;Schemaless&rdquo; is Misleading</strong>: The reading application usually assumes some structure. Responsibility shifts from write-time schema enforcement (relational) to read-time schema interpretation (document).</li><li>Example (Examples 3-1, 3-2, 3-3): Book data as JSON documents. All info for one book (Harry Potter) is in one document, including &ldquo;Sold as&rdquo; array for formats/prices.</li><li><strong>Better Locality</strong>: All info for a book is in one place, easier retrieval than joining multiple relational tables.</li><li><strong>Worse for Joins/Cross-Document Queries</strong>: Finding all books under $25 requires reading all documents, extracting prices, comparing. Less efficient than SQL <code>WHERE price &lt; 25</code>.</li><li>Many DBs (PostgreSQL, MySQL) now support both relational and document models.</li></ul><p><strong>FAANG Perspective</strong>: Document databases (MongoDB, DynamoDB) are great for use cases with self-contained data items, flexible schemas, and high scalability needs (e.g., user profiles, product catalogs where attributes vary widely).</p><h4 id=graph-model-page-65>Graph Model (Page 65)</h4><ul><li>Data as a graph: nodes and edges (relationships).</li><li>Prioritizes relationships between data items.</li><li>Example (Figure 3-5): Social network. Nodes: person, city, country. Edges: lives_in, born_in, coworker, friend, within.</li><li><strong>Efficient for Relationship-Based Queries</strong>: &ldquo;Find everyone born in USA.&rdquo; Start at &ldquo;USA&rdquo; node, traverse <code>within</code> and <code>born_in</code> edges to find &ldquo;person&rdquo; nodes.</li><li>Hard to do this easily in SQL or document model if #hops is unknown/variable (e.g., 3 hops from Zhenzhong Xu to USA, 2 from Chloe He).</li></ul><p><strong>FAANG Perspective</strong>: Graph databases (Neo4j, Amazon Neptune) shine for use cases like social networks, knowledge graphs, fraud detection (rings of fraudsters), recommendation (users-who-bought-this-also-bought). Query languages like Cypher or Gremlin are used.</p><p>Picking the Right Model: Crucial for simplifying development. Many queries easy in one model are hard in another.</p><h3 id=structured-versus-unstructured-data-page-66-table-3-5>Structured Versus Unstructured Data (Page 66, Table 3-5)</h3><ul><li><strong>Structured Data</strong>: Follows a predefined data model/schema (e.g., <code>name=string(50)</code>, <code>age=int(0-200)</code>). Easy to analyze (e.g., average age).<ul><li><em>Disadvantage</em>: Schema changes require retrospective updates, can cause bugs (e.g., new &rsquo;email&rsquo; field; or null ages becoming 0, confusing ML model - footnote 18&rsquo;s anecdote, solved by using -1).</li></ul></li><li><strong>Unstructured Data</strong>: No predefined schema. Usually text, but can be numbers, dates, images, audio (e.g., log files).<ul><li><em>Advantage</em>: Appealing when business reqs change, or data from many sources can&rsquo;t conform to one schema.</li><li>May still have intrinsic patterns (e.g., CSV-like log lines: <code>Lisa,43</code>). But no guarantee all lines follow it.</li></ul></li><li><strong>Storage Options</strong>:<ul><li>Schema-enforced storage can only store conforming data.</li><li>Schema-less storage can store any data (e.g., convert all to bytestrings).</li></ul></li><li><strong>Data Warehouse</strong>: Repository for structured data (processed, ready to use).</li><li><strong>Data Lake</strong>: Repository for unstructured (or raw) data, often before processing.</li></ul><p><strong>Table 3-5 Differences:</strong></p><table><thead><tr><th style=text-align:left>Feature</th><th style=text-align:left>Structured</th><th style=text-align:left>Unstructured</th></tr></thead><tbody><tr><td style=text-align:left>Schema</td><td style=text-align:left>Clearly defined</td><td style=text-align:left>Doesn&rsquo;t have to follow a schema</td></tr><tr><td style=text-align:left>Search/Analyze</td><td style=text-align:left>Easy</td><td style=text-align:left>(Implied harder until structure is imposed) Fast arrival</td></tr><tr><td style=text-align:left>Data Handling</td><td style=text-align:left>Specific schema only</td><td style=text-align:left>Any source</td></tr><tr><td style=text-align:left>Schema Changes</td><td style=text-align:left>Lots of trouble</td><td style=text-align:left>Worry shifted to downstream apps</td></tr><tr><td style=text-align:left>Stored In</td><td style=text-align:left>Data warehouses</td><td style=text-align:left>Data lakes</td></tr></tbody></table><p><strong>FAANG Perspective</strong>: The distinction is fluid. &ldquo;Schema-on-read&rdquo; (data lakes) vs. &ldquo;schema-on-write&rdquo; (warehouses). The trend is towards data lakehouses (e.g., Databricks, Snowflake) combining flexibility of lakes with management features of warehouses. Raw data lands in lake, then curated/structured versions are created.</p><h2 id=pages-67-71-data-storage-engines-and-processing>Pages 67-71: Data Storage Engines and Processing</h2><p>Data formats/models = interface. Storage engines (databases) = implementation on machines. Two main workload types:</p><h3 id=transactional-and-analytical-processing-pages-67-69>Transactional and Analytical Processing (Pages 67-69)</h3><ul><li><strong>Transaction</strong>: Digital world: any action (tweet, ride order, model upload, YouTube watch). Inserted as generated, occasionally updated/deleted.</li><li><strong>Online Transaction Processing (OLTP)</strong>:<ul><li>Needs to be fast (low latency) for users. High availability. If system can&rsquo;t process, transaction fails.</li><li><strong>Transactional Databases</strong>: Designed for OLTP. Often associated with ACID properties (Atomicity, Consistency, Isolation, Durability - definitions on page 68 are standard).<ul><li><em>Atomicity</em>: All steps succeed or all fail (e.g., payment fails, driver not assigned).</li><li><em>Consistency</em>: Transactions follow predefined rules (e.g., valid user).</li><li><em>Isolation</em>: Concurrent transactions appear isolated (e.g., two users don&rsquo;t book same driver simultaneously).</li><li><em>Durability</em>: Committed transaction persists despite system failure (e.g., phone dies, ride still coming).</li></ul></li><li>Not all need ACID. Some find it too restrictive. BASE (Basically Available, Soft state, Eventual consistency) is an alternative, &ldquo;even more vague&rdquo; (Kleppmann, footnote 20).</li><li>Often row-major (transactions processed as units).</li></ul></li><li><strong>Online Analytical Processing (OLAP)</strong>:<ul><li>For analytical questions (e.g., &ldquo;average ride price in SF in Sept?&rdquo;). Requires aggregating columns across many rows.</li><li><strong>Analytical Databases</strong>: Designed for this. Efficient with queries from different viewpoints. Often columnar.</li></ul></li><li><strong>OLTP/OLAP are Outdated Terms? (Figure 3-6, Google Trends)</strong>:<ul><li>Separation was due to tech limits (hard to do both well). This is closing.</li><li>Transactional DBs handling analytical queries (e.g., CockroachDB).</li><li>Analytical DBs handling transactional queries (e.g., Apache Iceberg, DuckDB).</li></ul></li><li><strong>Traditional OLTP/OLAP</strong>: Storage and processing tightly coupled. Often meant same data stored multiple times for different query types.</li><li><strong>Modern Paradigm</strong>: Decouple Storage from Processing (Compute). Data in one place, different processing layers on top. (Google BigQuery, Snowflake, IBM, Teradata - footnote 21).</li><li><strong>&ldquo;Online&rdquo; is Overloaded</strong>: Used to mean &ldquo;internet-connected,&rdquo; then &ldquo;in production.&rdquo; Data world: speed of processing/availability (online, nearline, offline - footnote 22).</li></ul><p><strong>FAANG Perspective</strong>: The decoupling of storage (e.g., S3, GCS) and compute (e.g., Spark, Presto, BigQuery) is a dominant architecture. It provides flexibility, scalability, and cost-efficiency. ACID is critical for financial transactions, eventual consistency is often fine for social media feeds.</p><h3 id=etl-extract-transform-and-load-pages-70-71-figure-3-7>ETL: Extract, Transform, and Load (Pages 70-71, Figure 3-7)</h3><ul><li>Early days: relational data, mostly structured. ETL was data warehousing process. Still relevant for ML.</li><li>General purpose processing/aggregating data into desired shape/format.</li><li><strong>Extract</strong>: Get data from sources. Validate, reject corrupted/malformatted data. Notify sources of rejected data. Crucial first step.</li><li><strong>Transform</strong>: Meaty part. Join, clean, standardize values (Male/Female vs M/F vs 1/2), transpose, deduplicate, sort, aggregate, derive new features, more validation.</li><li><strong>Load</strong>: Decide how/how often to load transformed data into target (file, DB, warehouse).</li><li><strong>Figure 3-7</strong>: Shows sources (DB, App, Flat files) -> ETL -> Targets (Data warehouse, Feature store, DB).</li><li><strong>Rise of ELT (Extract, Load, Transform)</strong>:<ul><li>Internet/hardware boom -> easy to collect massive, evolving data. Schemas changed.</li><li>Idea: Store all raw data in a data lake first (fast arrival, little pre-processing). Applications pull and process as needed.</li><li>Problem with ELT as data grows: Inefficient to search massive raw data. (Footnote 23: storage cost is rarely a problem now, but processing cost/time is).</li></ul></li><li><strong>Trend</strong>: Cloud/standardized infra -> committing to predefined schema becomes feasible again.</li><li><strong>Hybrid</strong>: Data Lakehouse (Databricks, Snowflake). Flexibility of lakes + management of warehouses.</li></ul><p><strong>FAANG Perspective</strong>: ETL/ELT pipelines are the backbone of our data infrastructure. Building robust, scalable, and maintainable ETLs (often using Spark, Beam, Airflow) is a core data engineering function. Feature stores are becoming common &ldquo;targets&rdquo; for ML features.</p><h2 id=pages-72-77-modes-of-dataflow--how-data-moves>Pages 72-77: Modes of Dataflow – How Data Moves</h2><p>In production, data isn&rsquo;t in one process; it flows between many. How does it pass if processes don&rsquo;t share memory?</p><h3 id=data-passing-through-databases-page-72>Data Passing Through Databases (Page 72)</h3><ul><li>Easiest way: Process A writes to DB, Process B reads from DB.</li><li><strong>Limitations</strong>:<ul><li>Both processes need access to same DB (infeasible if different companies).</li><li>DB read/writes can be slow, unsuitable for low-latency apps (most consumer-facing ones).</li></ul></li></ul><p><strong>FAANG Perspective</strong>: Used for asynchronous tasks or when latency isn&rsquo;t paramount. E.g., a batch job updates a model quality table, a dashboard service reads it.</p><h3 id=data-passing-through-services-request-driven-pages-73-74>Data Passing Through Services (Request-Driven) (Pages 73-74)</h3><ul><li>Direct network communication. Process A requests data from Process B; B returns it.</li><li><strong>Service-Oriented Architecture (SOA) / Microservices</strong>:<ul><li>Process B is a &ldquo;service&rdquo; A can call. B can also call A if A is a service.</li><li>Can be different companies (e.g., investment firm service calls stock exchange service for prices).</li><li>Can be components of one app (microservices). Allows independent development, testing, maintenance.</li></ul></li><li><strong>ML Example: Ride-Sharing Price Optimization (Lyft)</strong>:<ul><li>Services: Driver Management (available drivers), Ride Management (requested rides), Price Optimization.</li><li>Price Optimization service needs data from other two to predict optimal price (supply/demand). It requests this data. (Footnote 24: in practice, might use cached data, refresh periodically).</li></ul></li><li><strong>Popular Styles: REST (Representational State Transfer) vs. RPC (Remote Procedure Call)</strong>:<ul><li><em>REST</em>: For network requests. Often public APIs.</li><li><em>RPC</em>: Make remote call look like local function call. Often internal services in same org/datacenter. (Kleppmann, footnote 25).</li><li>RESTful = implements REST architecture. HTTP is an implementation, not same as REST (footnote 26).</li></ul></li></ul><p><strong>FAANG Perspective</strong>: Microservices are ubiquitous. REST for external/public APIs, gRPC (an RPC framework) very common for internal service-to-service communication due to efficiency and strong typing.</p><h3 id=data-passing-through-real-time-transport-event-driven-pages-74-77>Data Passing Through Real-Time Transport (Event-Driven) (Pages 74-77)</h3><ul><li><strong>Motivation (Ride-sharing example, Figure 3-8)</strong>: If Price Optimization, Driver Mgmt, Ride Mgmt all need data from each other via requests, it becomes a complex web. With hundreds/thousands of services, this is a bottleneck.</li><li>Request-driven is synchronous: Target service must be listening. If Driver Mgmt is down, Price Opt. keeps resending, times out. Response lost if Price Opt. goes down. Cascading failures.</li><li><strong>Broker/Event Bus (Figure 3-9)</strong>: Services communicate via a central broker.<ul><li>Driver Mgmt makes a prediction, broadcasts it (an event) to broker.</li><li>Other services wanting this data get it from broker.</li></ul></li><li>Technically, a DB can be a broker. But slow for low-latency. So, use in-memory storage for brokering (real-time transports).</li><li>Event-driven architecture: Better for data-heavy systems. Request-driven for logic-heavy.</li><li><strong>Common Types</strong>:<ul><li><em>Pub/Sub (Publish-Subscribe)</em>: Apache Kafka, Amazon Kinesis.<ul><li>Services publish events to topics.</li><li>Services subscribe to topics to read events. Producers don&rsquo;t care about consumers.</li><li><em>Retention Policy (Figure 3-10)</em>: Events kept in in-memory transport for a period (e.g., 7 days), then deleted or moved to permanent storage (e.g., S3). This is key for Kafka&rsquo;s design – it&rsquo;s a durable commit log.</li></ul></li><li><em>Message Queue</em>: Apache RocketMQ, RabbitMQ.<ul><li>Event (message) often has intended consumers. Queue gets message to right consumers.</li></ul></li></ul></li><li>Both Kafka/RabbitMQ are very popular (Figure 3-11, Stackshare). (Footnote 27: Mitch Seymour&rsquo;s Kafka/otters animation is great!)</li></ul><p><strong>FAANG Perspective</strong>: Kafka is a cornerstone of many real-time data pipelines for logging, metrics, event sourcing, stream processing. It enables decoupling of services and resilience.</p><h2 id=pages-78-79-batch-processing-versus-stream-processing>Pages 78-79: Batch Processing Versus Stream Processing</h2><p>Two paradigms for processing data based on its nature (historical vs. in-flight).</p><h3 id=batch-processing>Batch Processing</h3><ul><li>Data in storage (DBs, lakes, warehouses) = historical data.</li><li>Processed in batch jobs (kicked off periodically, e.g., daily job for average surge charge).</li><li>Distributed systems like MapReduce, Spark process batch data efficiently.</li><li><strong>Use in ML</strong>: Compute features that change less often (static features). E.g., driver&rsquo;s overall rating (if hundreds of rides, one more doesn&rsquo;t change it much day-to-day).</li></ul><h3 id=stream-processing>Stream Processing</h3><ul><li>Data in real-time transports (Kafka, Kinesis) = streaming data.</li><li>Computation on this data. Can be periodic (shorter periods, e.g., every 5 mins) or triggered (e.g., user requests ride -> process stream for available drivers).</li><li><strong>Low latency</strong>: Process data as generated, without writing to DB first.</li><li><strong>Efficiency</strong>:<ul><li>Myth: Less efficient than batch (can&rsquo;t use Spark/MapReduce). Not always true.</li><li>Streaming tech (Apache Flink) is scalable, distributed (parallel computation).</li></ul></li><li><strong>Strength</strong>: Stateful computation. Example: 30-day user engagement trial. Batch: recompute over last 30 days daily. Stream: compute on new day&rsquo;s data, join with older computation (state). Avoids redundancy.</li><li><strong>Use in ML</strong>: Compute features that change quickly (dynamic features / streaming features). E.g., drivers available right now, rides requested last minute, median price of last 10 rides in area. Essential for optimal real-time predictions.</li><li><strong>Both Batch and Stream Features Needed</strong>: Many problems need both. Need infra to process both and join them for ML models (preview of Chapter 7).</li><li><strong>Stream Computation Engines</strong>:<ul><li>Kafka&rsquo;s built-in stream processing is limited (various data sources).</li><li>ML streaming features often need complex queries (joins, aggregations).</li><li>Need efficient stream processing engines: Apache Flink, KSQL (Kafka SQL), Spark Streaming.</li><li>Flink, KSQL more recognized, nice SQL abstraction for data scientists.</li></ul></li><li>Stream processing is harder: Unbounded data, variable rates/speeds.</li><li>Argument (Flink maintainers, footnote 28): Batch is a special case of streaming. (i.e., a bounded stream). Stream engines can unify both.</li></ul><p><strong>FAANG Perspective</strong>: This unification is a powerful trend (e.g., Apache Beam model). Lambda architectures (separate batch/stream paths) are complex; Kappa architectures (all stream) are simpler if feasible. Choosing the right features (static, dynamic, or both) is key for model performance and system complexity.</p><h2 id=pages-79-80-summary-of-chapter-3>Pages 79-80: Summary of Chapter 3</h2><p>This chapter built on Chapter 2&rsquo;s emphasis on data&rsquo;s importance. Key takeaways:</p><ul><li><strong>Data Formats</strong>: Choose wisely for future use. Row-major vs. column-major, text vs. binary have pros/cons.</li><li><strong>Data Models</strong>: Relational (SQL), Document, Graph. All widely used, each suited for different tasks. Structured (writer assumes schema) vs. Unstructured (reader assumes schema) is fluid.</li><li><strong>Storage & Processing</strong>: Traditionally coupled (OLTP DBs for transactional, OLAP for analytical). Decoupling storage/compute is the trend. Hybrid DBs emerging.</li><li><strong>Modes of Dataflow</strong>: Databases (slow, simple), Services (request-driven, microservices), Real-time Transports (event-driven, Kafka/RabbitMQ for async, low latency).</li><li><strong>Batch vs. Stream Processing</strong>: Historical data -> batch jobs -> static features. Streaming data -> stream engines -> dynamic features. Stream engines can potentially unify both.</li></ul><p>With data systems figured out, next chapter is about collecting data and creating training data!</p><h2 id=interview-questions--page-references-chapter-3>Interview Questions & Page References (Chapter 3)</h2><p>As promised, here&rsquo;s a list of potential interview questions related to Chapter 3, with page numbers for where you can find relevant concepts in the book:</p><h3 id=general-data-understanding>General Data Understanding:</h3><ul><li>&ldquo;Describe the different types of data sources you might encounter in an ML project and their characteristics.&rdquo; (p. 50-52)</li><li>&ldquo;What are the challenges associated with user-input data? How would you handle them?&rdquo; (p. 50)</li><li>&ldquo;Why is system-generated log data important? What are the challenges in managing it?&rdquo; (p. 50-51)</li><li>&ldquo;What are the privacy considerations for user behavior data?&rdquo; (p. 51, esp. footnote 3)</li><li>&ldquo;Explain the difference between first-party, second-party, and third-party data. What are the trends affecting third-party data?&rdquo; (p. 52)</li></ul><h3 id=data-formats>Data Formats:</h3><ul><li>&ldquo;What is data serialization? Name some common data formats and their use cases.&rdquo; (p. 53, Table 3-1)</li><li>&ldquo;Compare and contrast row-major (e.g., CSV) and column-major (e.g., Parquet) data formats. When would you choose one over the other?&rdquo; (p. 54-55, Figure 3-1)</li><li>&ldquo;Why might iterating over pandas DataFrame rows be slow? How does its internal storage format relate to this?&rdquo; (p. 56, Figure 3-2)</li><li>&ldquo;Discuss the trade-offs between text formats (like JSON/CSV) and binary formats (like Parquet).&rdquo; (p. 57, Figure 3-3)</li><li>&ldquo;How would you choose a data format for storing a large dataset intended for analytical queries?&rdquo; (Implied: Parquet, p. 55, 57)</li></ul><h3 id=data-models>Data Models:</h3><ul><li>&ldquo;What is the relational data model? Explain the concept of normalization and its pros/cons.&rdquo; (p. 59-60, Tables 3-2 to 3-4)</li><li>&ldquo;What does it mean for SQL to be a declarative language?&rdquo; (p. 61)</li><li>&ldquo;What is NoSQL? Describe the document model and its advantages/disadvantages compared to the relational model.&rdquo; (p. 63-64, Examples 3-1 to 3-3)</li><li>&ldquo;When would a graph data model be appropriate? Give an example.&rdquo; (p. 65, Figure 3-5)</li><li>&ldquo;Explain the difference between structured and unstructured data. What are data lakes and data warehouses?&rdquo; (p. 66, Table 3-5)</li></ul><h3 id=data-storage-engines--processing>Data Storage Engines & Processing:</h3><ul><li>&ldquo;What are OLTP and OLAP? How do their requirements differ?&rdquo; (p. 67-69)</li><li>&ldquo;What are ACID properties? Why are they important for transactional databases?&rdquo; (p. 68)</li><li>&ldquo;Why are the terms OLTP/OLAP becoming outdated? What is the significance of decoupling storage and compute?&rdquo; (p. 69, Figure 3-6)</li><li>&ldquo;Describe the ETL process. What are the key steps?&rdquo; (p. 70-71, Figure 3-7)</li><li>&ldquo;What is ELT, and how does it relate to data lakes?&rdquo; (p. 71)</li></ul><h3 id=modes-of-dataflow>Modes of Dataflow:</h3><ul><li>&ldquo;Describe different ways data can be passed between processes in a production system.&rdquo; (p. 72)</li><li>&ldquo;When is passing data through databases suitable/unsuitable?&rdquo; (p. 72)</li><li>&ldquo;Explain request-driven data passing and its connection to microservices. What are REST and RPC?&rdquo; (p. 73-74)</li><li>&ldquo;What is event-driven architecture for data passing? Describe real-time transports like pub/sub (Kafka) and message queues.&rdquo; (p. 74-77, Figures 3-8 to 3-11)</li><li>&ldquo;Why might an event-driven architecture be preferred over a request-driven one for a system with many services?&rdquo; (p. 75)</li></ul><h3 id=batch-vs-stream-processing>Batch vs. Stream Processing:</h3><ul><li>&ldquo;Compare batch processing and stream processing. What types of data and features are typically associated with each?&rdquo; (p. 78)</li><li>&ldquo;What are the advantages of stream processing, especially concerning latency and stateful computation?&rdquo; (p. 78)</li><li>&ldquo;Name some stream processing engines. Why are they necessary for complex streaming ML features?&rdquo; (p. 79)</li><li>&ldquo;How can batch and stream processing be combined in an ML system?&rdquo; (p. 79, though more in Ch7)</li></ul></div></div></article></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer></body></html>