<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul></nav></header><main><article class=book-single><h1>Chapter 2: Introduction to Machine Learning Systems Design</h1><div class=book-details><div class=book-content><p>This chapter will guide us through:</p><ul><li><strong>Objectives</strong>: Why are we building this system in the first place?</li><li><strong>Requirements</strong>: What qualities must our system possess? (Think reliability, scalability).</li><li><strong>Iterative Process</strong>: Spoiler: building ML systems is rarely a straight line.</li><li><strong>Problem Framing</strong>: How do we turn a vague business problem into a concrete ML task? This is crucial.</li><li>And finally, a bit of a philosophical discussion: <strong>Data vs. Algorithms</strong>.</li></ul><p>You&rsquo;ll see a recurring theme: <em>&ldquo;Not so soon!&rdquo;</em> Before you jump to coding a model, there&rsquo;s a lot of critical thinking and planning. This upfront work is what separates a successful, production-ready ML system from a science project. This chapter is packed with concepts that will come up in ML system design interviews.</p><p>Let&rsquo;s get to it!</p><h2 id=page-1-of-chapter-2-text-reiteration-and-roadmap>Page 1 (of Chapter 2 text): Reiteration and Roadmap</h2><p>This page just sets the stage, reminding us of the holistic view from Chapter 1. The key components are:</p><ul><li>Business requirements</li><li>Data stack</li><li>Infrastructure</li><li>Deployment</li><li>Monitoring</li><li>And, importantly, the stakeholders involved.</li></ul><p>The chapter will flow from high-level objectives down to the specifics of framing the ML task. The author teases: <em>&ldquo;The difficulty of your job can change significantly depending on how you frame your problem.&rdquo;</em> This is an understatement! A good problem framing can save you months of effort.</p><h2 id=pages-2-4-business-and-ml-objectives--the-why>Pages 2-4: Business and ML Objectives – The &ldquo;Why&rdquo;</h2><p>This is the absolute starting point for any ML project, especially in a business context like FAANG.</p><h3 id=ml-objectives-vs-business-objectives>ML Objectives vs. Business Objectives:</h3><ul><li><strong>Data Scientists often focus on ML Objectives</strong>: These are metrics we can directly measure from our model – accuracy, precision, recall, F1-score, RMSE, inference latency. We get excited about tweaking a model to push accuracy from 94% to 94.2%.</li><li><strong>Companies care about Business Objectives</strong>: The truth is, most businesses don&rsquo;t care about that 0.2% accuracy bump unless it translates into something meaningful for them. This means:<ul><li>Increasing revenue (e.g., more sales, higher ad click-through rates)</li><li>Reducing costs (e.g., less fraud, more efficient operations)</li><li>Improving user engagement (e.g., more time on site, higher monthly active users)</li><li>Increasing customer satisfaction.</li></ul></li></ul><p>The book quotes Milton Friedman: <em>&ldquo;The social responsibility of business is to increase its profits.&rdquo;</em> While this is a specific economic viewpoint, the underlying message for us is that ML projects within a business context must ultimately contribute to the business&rsquo;s success.</p><h3 id=the-pitfall>The Pitfall:</h3><p>A very common failure pattern, as the book highlights (citing Eugene Yan&rsquo;s excellent post), is when data science teams get lost in &ldquo;hacking ML metrics&rdquo; without connecting them to business impact. If your manager can&rsquo;t see how your fancy model is helping the business, the project (and sometimes the team) might get cut.</p><p><strong>FAANG Perspective</strong>: This is a constant conversation. PMs will always ask, <em>&ldquo;How does this new model move our North Star metric?&rdquo;</em> If you propose a project, you need to have a hypothesis about its business impact.</p><h3 id=mapping-ml-metrics-to-business-metrics-this-is-key>Mapping ML Metrics to Business Metrics: This is key.</h3><ul><li><strong>Easy Mappings</strong>: For ad click-through rate (CTR) prediction, an increase in model AUC (an ML metric) often directly correlates with increased ad revenue (a business metric). For fraud detection, better precision/recall directly means less money lost.</li><li><strong>Harder Mappings / Custom Metrics</strong>:<ul><li>Netflix&rsquo;s &ldquo;take-rate&rdquo;: (quality plays) / (recommendations shown). They found a higher take-rate correlated with more streaming hours and lower subscription cancellations (business metrics). This is a great example of a derived metric that bridges the ML and business worlds.</li><li>Recommender Systems: An e-commerce site wants to move from batch to online recommendations. Hypothesis: online recommendations are more relevant -> higher purchase-through rate. Experiment: Show X% improvement in predictive accuracy (ML metric) from online. Historically, Y% accuracy improvement led to Z% purchase-through rate increase (business metric).</li></ul></li></ul><p><strong>A/B Testing is Crucial</strong>: Often, the only way to truly know if your ML model is improving business metrics is through rigorous A/B testing. You might have a model with slightly worse offline ML metrics (e.g., accuracy) that performs better in an A/B test on the actual business KPI. The A/B test is usually the ultimate decider.</p><h3 id=the-ai-powered-hype-vs-roi-return-on-investment>The &ldquo;AI-Powered&rdquo; Hype vs. ROI (Return on Investment):</h3><ul><li>Many companies want to say they&rsquo;re &ldquo;AI-powered&rdquo; because it&rsquo;s trendy. But ML isn&rsquo;t magic. It doesn&rsquo;t transform businesses overnight.</li><li>Google&rsquo;s success with ML came from decades of investment.</li></ul><h3 id=maturity-matters-figure-2-1-this-graph-from-algorithmia-is-insightful>Maturity Matters (<em>Figure 2-1</em>): This graph from Algorithmia is insightful.</h3><ul><li>Companies sophisticated in ML (models in production > 5 years) can often deploy new models in &lt; 30 days (almost 75% of them).</li><li>Those just starting often take > 30 days (60% of them).</li><li>This shows that mature MLOps pipelines, experienced teams, and established processes significantly speed up development and deployment, leading to better ROI.</li></ul><p><strong>Interview Tip</strong>: When asked to design a system, implicitly consider the company&rsquo;s maturity. A startup will have different constraints and capabilities than Google.</p><h3 id=attribution-in-complex-systems>Attribution in Complex Systems:</h3><p>Sometimes ML is just one small cog in a huge machine. For a cybersecurity threat detection system, an ML model might detect anomalies, which then go through rule-based filters, then human review, then an automated blocking process. If a threat gets through, was it the ML model&rsquo;s fault? Hard to say. This makes direct attribution of business impact tricky.</p><p><strong>Self-Correction/Interview Tip</strong>: In an ML system design interview, the first thing you should clarify is the business objective. <em>&ldquo;What are we trying to achieve for the business? What is the key metric we want to move?&rdquo;</em> This shows you&rsquo;re thinking about the big picture, not just the tech.</p><h2 id=pages-5-7-requirements-for-ml-systems--the-what>Pages 5-7: Requirements for ML Systems – The &ldquo;What&rdquo;</h2><p>Once objectives are clear, we define system requirements. These are often non-functional requirements that dictate the quality and robustness of our system. The book focuses on four:</p><ol><li><p><strong>Reliability (Page 5)</strong>: The system should perform its correct function at the desired level of performance, even in the face of adversity (hardware/software faults, human error).</p><ul><li>&ldquo;Correctness&rdquo; is tricky for ML: A traditional software system might crash or throw an error. ML systems can fail silently. Google Translate might give you a grammatically correct sentence that means the opposite of the original, and if you don&rsquo;t know the target language, you&rsquo;d never know!</li><li>How do we know a prediction is wrong if we don&rsquo;t have ground truth for new, unseen data in real-time? This leads to the need for robust monitoring (Chapter 8).</li><li><strong>FAANG Perspective</strong>: Silent failures are a nightmare. Imagine a recommendation system silently starts recommending inappropriate content. Robust monitoring and alerting are paramount.</li></ul></li><li><p><strong>Scalability (Pages 6-7)</strong>: The system&rsquo;s ability to handle growth. Growth can occur in several dimensions:</p><ul><li><em>Model Complexity</em>: From logistic regression (1GB RAM) to a 100M parameter neural net (16GB RAM).</li><li><em>Traffic Volume</em>: From 10k daily requests to 1-10M daily requests.</li><li><em>Model Count</em>:<ul><li>One model for one use case (e.g., trending hashtag detection).</li><li>Adding more models for the same use case (e.g., NSFW filter, bot filter).</li><li>One model per customer in enterprise scenarios (author mentions a startup with 8,000 models for 8,000 customers). This is common in B2B SaaS where models are fine-tuned on customer-specific data.</li></ul></li><li><em>Resource Scaling</em>:<ul><li>Scaling up: Bigger machine. Scaling out: More machines (footnote 8).</li><li>Autoscaling: Automatically adjusting resources (e.g., GPUs) based on demand. Peak might need 100 GPUs, off-peak only 10. Keeping 100 GPUs always on is expensive.</li><li>Autoscaling is tricky! Amazon&rsquo;s Prime Day autoscaling failure example (costing $72M-$99M in an hour) shows even the giants can struggle.</li></ul></li><li><em>Artifact Management</em>: Managing 100 models is vastly different from 1. Manual monitoring/retraining is not feasible. Need automated MLOps pipelines, code generation for reproducibility.</li><li>The book cross-references later chapters covering distributed training, model optimization, resource management, experiment tracking, and development environments, all related to scalability.</li><li><strong>Interview Tip</strong>: Always consider scalability. <em>&ldquo;What happens if traffic 10x-es? What if the number of items to recommend 10x-es?&rdquo;</em> Be prepared to discuss strategies like sharding, caching, load balancing, and autoscaling.</li></ul></li><li><p><strong>Maintainability (Page 7)</strong>: Making it easy for various teams (ML engineers, DevOps, Subject Matter Experts - SMEs) to operate, debug, and evolve the system.</p><ul><li>Teams have different backgrounds, tools, programming languages.</li><li>Needs:<ul><li>Well-structured workloads and infrastructure.</li><li>Good documentation (often overlooked but critical!).</li><li>Versioning of code, data, and artifacts (models).</li><li>Reproducibility: Can someone else reproduce your model and results if you leave?</li><li>Collaborative debugging without finger-pointing.</li></ul></li><li>The book mentions &ldquo;Team Structure&rdquo; (page 334) in this context.</li><li><strong>FAANG Perspective</strong>: Maintainability is huge for long-term cost of ownership. Complex, poorly documented systems become &ldquo;tribal knowledge&rdquo; nightmares. We invest heavily in tools and processes for versioning, reproducibility (e.g., model registries, feature stores), and clear ownership.</li></ul></li><li><p><strong>Adaptability (Pages 7-8)</strong>: The system&rsquo;s ability to evolve with:</p><ul><li>Shifting data distributions (data drift, concept drift): What users search for changes, spam tactics evolve. (Covered in &ldquo;Data Distribution Shifts&rdquo; on page 237).</li><li>Changing business requirements.</li><li>The system needs capacity to discover opportunities for improvement and allow updates without service interruption. (Covered in &ldquo;Continual Learning&rdquo; on page 264).</li><li>Tightly linked to maintainability.</li><li><strong>Interview Tip</strong>: <em>&ldquo;How will your system adapt to changing user behavior or new product features?&rdquo;</em> This leads to discussions of online learning, retraining frequency, and monitoring for drift.</li></ul></li></ol><p>These four requirements are often interconnected and sometimes involve trade-offs. For instance, a highly scalable system might introduce complexity that challenges maintainability.</p><h2 id=pages-8-10-iterative-process--the-how-often>Pages 8-10: Iterative Process – The &ldquo;How Often&rdquo;</h2><p>Forget the idea of a linear, one-shot ML project: Collect data -> Train model -> Deploy -> Done. It&rsquo;s a myth!</p><p>ML development is <strong>iterative</strong> and often never-ending. (Footnote 10: &ldquo;a property of traditional software&rdquo; – true, but often more pronounced in ML due to data dependency). Once in production, it needs continuous monitoring and updating.</p><h3 id=example-workflow-ad-prediction---steps-1-13-this-is-a-fantastic-illustration-of-the-cyclical-nature>Example Workflow (Ad Prediction - Steps 1-13): This is a fantastic illustration of the cyclical nature:</h3><ol><li>Choose metric (e.g., impressions).</li><li>Collect data/labels.</li><li>Feature engineering.</li><li>Train models.</li><li>Error analysis -> realize labels are wrong -> relabel.</li><li>Train again.</li><li>Error analysis -> model always predicts &ldquo;no ad&rdquo; (due to 99.99% negative labels - class imbalance!) -> collect more positive data.</li><li>Train again.</li><li>Model good on old test data, bad on recent data (stale!) -> update with recent data.</li><li>Train again.</li><li>Deploy.</li><li>Business feedback: Revenue decreasing! Ads shown, but few clicks. Original metric (impressions) was wrong. Change to optimize for click-through rate.</li><li>Go to step 1. (And as footnote 11 hilariously adds: <em>&ldquo;Praying and crying not featured, but present through the entire process.&rdquo;</em> So true.)</li></ol><h3 id=figure-2-2-simplified-iterative-cycle><em>Figure 2-2</em> (Simplified Iterative Cycle):</h3><ol><li><strong>Project Scoping</strong>: Goals, objectives, constraints, stakeholders, resources. (Covered in Ch1 and earlier in Ch2, also Ch11 for team organization).</li><li><strong>Data Engineering</strong>: Handling sources/formats (Ch3), curating training data (sampling, labeling - Ch4).</li><li><strong>ML Model Development</strong>: Feature extraction (Ch5), model selection, training, evaluation (Ch6). (This is the &ldquo;sexy&rdquo; part often overemphasized in courses).</li><li><strong>Deployment</strong>: Making model accessible to users (Ch7). <em>&ldquo;Like writing, you never reach done, but you reach a point where you have to put it out there.&rdquo;</em></li><li><strong>Monitoring and Continual Learning</strong>: Performance decay, adapting to changes (Ch8 & Ch9).</li><li><strong>Business Analysis</strong>: Evaluate model against business goals, generate insights, decide to kill/scope new projects. (This closes the loop back to Step 1).</li></ol><p>Notice the dashed arrows in <em>Figure 2-2</em>: you can jump between these stages. Error analysis in model development (3) might send you back to data engineering (2). Business analysis (6) might redefine project scope (1).</p><p>The perspective of an ML platform engineer or DevOps engineer might differ; they&rsquo;d focus more on infrastructure setup.</p><p><strong>Self-Correction/Interview Tip</strong>: When outlining your approach in an ML system design interview, always frame it as an iterative process. Mention baselines, prototyping, feedback loops, and phased rollouts. This demonstrates a mature understanding of real-world ML development.</p><h2 id=pages-11-19-framing-ml-problems--the-what-exactly>Pages 11-19: Framing ML Problems – The &ldquo;What Exactly&rdquo;</h2><p>This is where your expertise as an ML engineer truly shines. You take a business problem and translate it into something ML can solve.</p><h3 id=business-problem-vs-ml-problem>Business Problem vs. ML Problem:</h3><ul><li><strong>Boss&rsquo;s request</strong>: <em>&ldquo;Rival bank uses ML to speed up customer service by 2x. Do that.&rdquo;</em> This is a business problem, not an ML problem.</li><li><strong>ML problem needs</strong>: Inputs, Outputs, Objective Function.</li><li><strong>Your job</strong>: Investigate. Bottleneck is routing requests to Accounting, Inventory, HR, IT.</li><li><strong>ML Framing</strong>: Predict which department a request should go to.<ul><li><em>Input</em>: Customer request text.</li><li><em>Output</em>: Predicted department (one of four).</li><li><em>Task Type</em>: Classification.</li><li><em>Objective function</em>: Minimize difference between predicted and actual department.</li></ul></li></ul><h3 id=types-of-ml-tasks-page-12-14-figure-2-3-the-models-output-dictates-the-task-type>Types of ML Tasks (Page 12-14, <em>Figure 2-3</em>): The model&rsquo;s output dictates the task type.</h3><ul><li><p><strong>Classification vs. Regression</strong>:</p><ul><li><em>Classification</em>: Output is a category (e.g., spam/not-spam, cat/dog/mouse).</li><li><em>Regression</em>: Output is a continuous value (e.g., house price, temperature).</li><li>They can be interchanged (<em>Figure 2-4</em>):<ul><li>House price regression -> classification by bucketing prices (e.g., &lt;$100k, $100k-$200k).</li><li>Email spam classification -> regression by outputting a spamminess score (0-1), then thresholding.</li></ul></li><li><strong>FAANG Perspective</strong>: Choosing classification vs. regression can have subtle implications for user experience and error analysis. Sometimes a score is more nuanced than a hard class label.</li></ul></li><li><p><strong>Within Classification</strong>:</p><ul><li><em>Binary Classification</em>: Two classes (e.g., fraud/not-fraud, toxic/not-toxic). Simplest. F1, confusion matrices are intuitive.</li><li><em>Multiclass Classification</em>: More than two classes, an example belongs to exactly one (e.g., classifying an image as cat OR dog OR bird).</li><li><em>High Cardinality Classification</em>: Many classes (e.g., thousands of diseases, tens of thousands of product categories).<ul><li><strong>Challenge 1: Data Collection.</strong> Often need many examples per class (author suggests ~100). For 1000 classes, that&rsquo;s 100,000 examples. Difficult for rare classes.</li><li><strong>Strategy: Hierarchical Classification.</strong> First classify into broad categories (e.g., electronics, fashion), then a second model classifies into subcategories (e.g., shoes, shirts within fashion). This is common in product categorization at Amazon or e-commerce sites.</li></ul></li><li><em>Multilabel Classification</em>: An example can belong to multiple classes simultaneously (e.g., an article can be about &rsquo;tech&rsquo; AND &lsquo;finance&rsquo;).<ul><li>This is often the trickiest!</li><li><em>Labeling</em>: Annotator disagreement is common (see Chapter 4). One says 2 labels, another says 1.</li><li><em>Prediction</em>: If model outputs probabilities [0.45 (tech), 0.2 (ent), 0.02 (fin), 0.33 (pol)], how many labels do you pick? The top one? Top two? This requires careful thresholding or learning the number of labels.</li><li><em>Two Approaches</em>:<ol><li>Treat as multiclass with a multi-hot encoded target vector (e.g., [0, 1, 1, 0] for entertainment and finance).</li><li>Train multiple binary classifiers (one for each topic: &ldquo;is it tech?&rdquo;, &ldquo;is it entertainment?&rdquo;). This is often simpler to manage and interpret.</li></ol></li></ul></li></ul></li></ul><p><strong>Interview Tip</strong>: Clearly state your problem framing, including the task type. Be ready to justify why you chose that framing and discuss alternatives.</p><h3 id=multiple-ways-to-frame-a-problem-pages-15-16-figures-2-5-2-6-next-app-prediction>Multiple Ways to Frame a Problem (Pages 15-16, <em>Figures 2-5, 2-6</em>): Next App Prediction</h3><ul><li><strong>Problem</strong>: Predict the app a user will most likely open next.</li><li><strong>Naive Framing (Multiclass Classification - <em>Figure 2-5</em>)</strong>:<ul><li><em>Input</em>: User features (demographics, past apps), Environment features (time, location).</li><li><em>Output</em>: A probability distribution vector of size N (for N apps on the phone). [P(App0), P(App1), &mldr;, P(AppN-1)].</li><li><strong>Bad because</strong>: If a new app is installed (N changes), the output layer of your neural network changes. You have to retrain from scratch or at least a significant part of the model.</li></ul></li><li><strong>Better Framing (Regression - <em>Figure 2-6</em>)</strong>:<ul><li><em>Input</em>: User features, Environment features, AND App features (category of app, metadata).</li><li><em>Output</em>: A single score (0-1) indicating likelihood of opening that specific app given the context.</li><li>To recommend, you make N predictions (one for each app, feeding its features into the model) and pick the app with the highest score.</li><li><strong>Good because</strong>: If a new app is installed, you just featurize it and score it with the existing model. No retraining needed immediately. This is much more scalable and practical for dynamic environments like app stores.</li><li><strong>FAANG Perspective</strong>: This &ldquo;Siamese&rdquo; network-like approach (where you compare user context to item features) is very common in large-scale recommendation and search ranking systems.</li></ul></li></ul><h3 id=objective-functions-page-16-aka-loss-functions>Objective Functions (Page 16): AKA Loss Functions.</h3><p>The function the model tries to minimize during training.</p><ul><li>For supervised learning, it compares model outputs to ground truth labels.</li><li>Examples: RMSE or MAE for regression, Logistic Loss (Log Loss) for binary classification, Cross-Entropy for multiclass classification.</li><li>The book gives a Python snippet for cross-entropy: <code>p</code> is ground truth (e.g., <code>[0,0,0,1]</code>), <code>q</code> is model prediction (e.g., <code>[0.45,0.2,0.02,0.33]</code>).</li><li>Most ML engineers use standard loss functions; deriving novel ones requires deeper math.</li><li><strong>Important Note (footnote 12)</strong>: These mathematical objective functions are different from the business/ML objectives we discussed earlier. The hope is that optimizing the mathematical loss function will improve the ML objectives, which in turn will improve business objectives. This chain isn&rsquo;t always perfect!</li></ul><h3 id=decoupling-objectives-pages-17-18-what-if-you-have-multiple-potentially-conflicting-goals>Decoupling Objectives (Pages 17-18): What if you have multiple, potentially conflicting, goals?</h3><ul><li><strong>Example: Newsfeed Ranking.</strong><ul><li><em>Initial goal</em>: Maximize user engagement (clicks). Objectives: Filter spam, filter NSFW, rank by click likelihood.</li><li><em>Problem</em>: Prioritizing engagement alone leads to extreme content (clickbait, outrage). (Footnote 13 cites Facebook/YouTube examples).</li><li><em>New goal</em>: Maximize engagement AND minimize extreme views/misinformation.</li><li><em>New objectives</em>: Filter spam, filter NSFW, filter misinformation, rank by quality, rank by engagement.</li><li>Now, quality and engagement might conflict. A high-quality post might be boring; an engaging post might be low-quality.</li></ul></li><li><strong>Approach 1: Combined Loss Function</strong>:<ul><li><code>loss = alpha * quality_loss + beta * engagement_loss</code></li><li>Train one model to minimize this combined loss.</li><li><em>Problem</em>: Tuning alpha and beta is tricky. Every time you adjust them (e.g., business decides quality is more important), you have to retrain the entire model.</li></ul></li><li><strong>Approach 2: Decouple - Train Separate Models</strong>:<ul><li><code>quality_model</code>: Minimizes <code>quality_loss</code>, predicts <code>quality_score</code>.</li><li><code>engagement_model</code>: Minimizes <code>engagement_loss</code>, predicts <code>engagement_score</code>.</li><li>Combine scores at inference time: <code>final_score = alpha * quality_score + beta * engagement_score</code>.</li><li><em>Advantages</em>:<ul><li>You can tweak alpha and beta without retraining models. Much more agile.</li><li>Easier maintenance: Spam techniques evolve faster than quality perception. The spam filter (part of quality) might need frequent updates, while the core engagement model might be more stable. Decoupling allows different maintenance schedules.</li></ul></li><li><strong>FAANG Perspective</strong>: Decoupling objectives into separate models or model components is a very common and powerful pattern in complex production systems like search ranking, ad ranking, and feed ranking. It allows for modularity, independent iteration on components, and easier tuning of business trade-offs.</li></ul></li></ul><h2 id=pages-19-22-mind-versus-data--the-big-debate>Pages 19-22: Mind Versus Data – The Big Debate</h2><p>This is a fascinating, ongoing discussion in the ML community. What&rsquo;s more important for progress?</p><ul><li><strong>The Premise</strong>: <em>&ldquo;More Data Usually Beats Better Algorithms&rdquo;</em> (Anand Rajaraman, footnote 16). Many successes in the last decade (AlexNet, BERT, GPT) relied heavily on massive datasets. Companies increasingly focus on managing and improving their data.</li><li><strong>The Debate</strong>:<ul><li><strong>&ldquo;Mind&rdquo; Camp</strong>: Emphasizes inductive biases, intelligent architectural designs, causal inference.<ul><li>Judea Pearl (Turing Award winner): &ldquo;Mind over Data,&rdquo; &ldquo;Data is profoundly dumb.&rdquo; He controversially tweeted that data-centric ML folks might be jobless in 3-5 years.</li><li>Christopher Manning: Huge data + simple algorithm = &ldquo;incredibly bad learners.&rdquo; Structure helps learn from less data.</li></ul></li><li><strong>&ldquo;Data&rdquo; (and Compute) Camp</strong>:<ul><li>Richard Sutton (&ldquo;The Bitter Lesson&rdquo;): General methods leveraging computation ultimately win by a large margin. Trying to bake in human domain knowledge gives short-term gains, but scaling computation is the long-term winner.</li><li>Peter Norvig (on Google Search): &ldquo;We don&rsquo;t have better algorithms. We just have more data.&rdquo; (This is from &ldquo;The Unreasonable Effectiveness of Data,&rdquo; Halevy, Norvig, Pereira).</li></ul></li><li>Monica Rogati&rsquo;s &ldquo;Data Science Hierarchy of Needs&rdquo; (<em>Figure 2-7</em>): Data is foundational. You can&rsquo;t do ML/AI (top of pyramid) without the lower levels: collect, move/store, explore/transform, aggregate/label.
<img src=/bookshelf/design-ml-system/ch-2-fig-2-7.png alt=image><ul><li><strong>Self-Correction</strong>: This hierarchy is a fantastic mental model. Often, companies want to jump to &ldquo;AI&rdquo; without solid data foundations, which is a recipe for failure.</li></ul></li></ul></li><li><strong>The Reality</strong>:<ul><li>Data is essential (for now): Quality and quantity matter.</li><li>Dataset sizes are exploding (<em>Figure 2-8</em>): Language model datasets grew from PTB (few million tokens) to Text8 (100M) to One Billion Word (0.8B tokens, 2013) to GPT-2 (10B tokens) to GPT-3 (500B tokens).</li><li><strong>Caveat</strong>: More data isn&rsquo;t always better. Low-quality data (outdated, incorrect labels) can hurt performance. This emphasizes the &ldquo;quality&rdquo; aspect.</li><li>The debate isn&rsquo;t whether finite data is necessary (it is), but whether it&rsquo;s sufficient, or if &ldquo;mind&rdquo; offers a more efficient path. If we had infinite data, we could just look up answers. A lot of data is different from infinite data.</li></ul></li></ul><p><strong>Interview Tip</strong>: While you won&rsquo;t be asked to solve this philosophical debate, understanding it shows you&rsquo;re aware of the field&rsquo;s trends. You can articulate the practical importance of good data pipelines and also appreciate the research into more data-efficient or structured models.</p><h2 id=pages-22-23-summary>Pages 22-23: Summary</h2><p>Chapter 2 provides a crucial introduction to the ML system design process:</p><ul><li><strong>Start with &ldquo;Why&rdquo;</strong>: Business objectives must drive ML project selection and definition. Translate business needs into ML objectives.</li><li><strong>Define Requirements</strong>: Systems need to be reliable, scalable, maintainable, and adaptable.</li><li><strong>Embrace Iteration</strong>: ML system development is a continuous cycle, not a one-shot deal.</li><li><strong>The Role of Data is Undeniable</strong>: While the &ldquo;mind vs. data&rdquo; debate continues, practical ML today heavily relies on access to large amounts of high-quality data. The book will devote significant attention to data questions.</li><li><strong>Building Blocks</strong>: Complex ML systems are made of simpler components. The following chapters will zoom into these, starting with data engineering.</li></ul><p>If any of this feels abstract, the book promises concrete examples in later chapters. This chapter provides the mental framework you need to start thinking like an ML Systems Engineer. The principles here – tying to business value, defining non-functional requirements, iterative development, and careful problem framing – are universal.</p><p>Okay, that&rsquo;s Chapter 2! A lot to digest, but incredibly important. What are your initial thoughts? Any of these points particularly resonate with your experiences or spark questions? This stuff is the bread and butter of what we do in production ML.</p></div></div></article></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer></body></html>