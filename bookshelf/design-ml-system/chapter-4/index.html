<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents></nav></nav></aside><article class=book-single><h1>Chapter 4: Training Data</h1><span class=reading-time><em>26 min read</em></span><div class=book-details><div class=book-content><p>Okay, class, let&rsquo;s settle in. We&rsquo;ve journeyed through the high-level overview of ML systems, the design process, and the nitty-gritty of data engineering fundamentals. Now, we arrive at a topic that is, in many ways, the heart of supervised machine learning: <strong>Chapter 4: Training Data.</strong></p><p>The author makes a critical point right at the start: ML curricula often skew heavily towards modeling – the &ldquo;fun&rdquo; part. But as anyone who&rsquo;s worked in production ML at FAANG or elsewhere knows, &ldquo;spending days wrangling with a massive amount of malformatted data that doesn’t even fit into your machine’s memory is frustrating&rdquo; but <em>essential</em>. Bad data can sink your entire ML operation, no matter how brilliant your model.</p><p>This chapter shifts focus from the <em>systems perspective</em> of data (Chapter 3) to the <em>data science perspective</em>. We&rsquo;re going to cover:</p><ul><li><strong>Sampling techniques:</strong> How to select data for training.</li><li><strong>Labeling challenges:</strong> The pains of hand labels, label multiplicity, and the beauty of natural labels.</li><li><strong>Handling lack of labels:</strong> Weak supervision, semi-supervision, transfer learning, active learning.</li><li><strong>Class imbalance:</strong> Why it&rsquo;s a problem and how to tackle it.</li><li><strong>Data augmentation:</strong> Creating more data from what you have.</li></ul><p>A key terminology point: The book uses &ldquo;training data&rdquo; instead of &ldquo;training dataset&rdquo; because &ldquo;dataset&rdquo; implies finite and stationary. Production data is rarely either (hello, &ldquo;Data Distribution Shifts&rdquo; on page 237, which we&rsquo;ll cover much later). Creating training data, like everything else in ML systems, is an iterative process.</p><p>Let&rsquo;s begin!</p><hr><p><strong>Page 81 (Chapter Introduction): The Importance and Pain of Data</strong></p><p>The intro reiterates the core message:</p><ul><li>Data is &ldquo;messy, complex, unpredictable, and potentially treacherous.&rdquo;</li><li>Handling it well saves time and headaches. This chapter is about techniques to obtain/create <em>good</em> training data.</li><li>&ldquo;Training data&rdquo; here encompasses all data for development: train, validation, and test splits.</li></ul><p><strong>A Crucial Word of Caution (Page 82, top): Data is full of potential biases!</strong></p><ul><li>Biases can creep in during collection, sampling, or labeling.</li><li>Historical data can embed human biases.</li><li>ML models trained on biased data <em>will perpetuate those biases</em>.</li><li><strong>&ldquo;Use data but don’t trust it too much!&rdquo;</strong> This is a mantra every ML practitioner should live by. Always be skeptical, always question your data.</li></ul><hr><p><strong>Pages 82-87: Sampling – Choosing Your Data Wisely</strong></p><p>Sampling is often overlooked in coursework but is integral to the ML lifecycle. It happens when:</p><ul><li>Creating training data from all possible real-world data.</li><li>Creating train/validation/test splits from a given dataset.</li><li>Sampling events for monitoring.</li></ul><p><strong>Why sample?</strong></p><ol><li><strong>Necessity:</strong> You rarely have access to <em>all</em> possible data. Your training data is inherently a sample.</li><li><strong>Feasibility:</strong> Processing <em>all</em> accessible data might be too time-consuming or resource-intensive.</li><li><strong>Efficiency:</strong> Quick experiments on small subsets can validate a new model&rsquo;s promise before full-scale training (footnote 1: even for large models, experimenting with dataset sizes reveals its effect).</li></ol><p>Understanding sampling helps avoid bias and improve efficiency. Two families:</p><ol><li><p><strong>Nonprobability Sampling (Pages 83-84):</strong> Selection <em>not</em> based on probability criteria. Often driven by convenience, leading to selection biases (footnote 2, Heckman).</p><ul><li><strong>Convenience sampling:</strong> Selected based on availability. Popular because it&rsquo;s easy.<ul><li><em>Example: Language models</em> often trained on easily collected Wikipedia, Common Crawl, Reddit, not necessarily representative of <em>all</em> text.</li><li><em>Example: Sentiment analysis data</em> from IMDB/Amazon reviews. Biased towards those willing to leave reviews, not representative of all users.</li><li><em>Example: Self-driving car data</em> initially from sunny Phoenix/Bay Area, less from rainy Kirkland (footnote 3). Model might be great in sun, poor in rain.</li></ul></li><li><strong>Snowball sampling:</strong> Future samples selected based on existing ones (e.g., scrape Twitter accounts, then accounts they follow, etc.).</li><li><strong>Judgment sampling:</strong> Experts decide what to include.</li><li><strong>Quota sampling:</strong> Select based on quotas for slices (e.g., survey: 100 responses from &lt;30yo, 100 from 30-60yo, etc., regardless of actual age distribution).</li><li><strong>Usefulness:</strong> Quick way to get initial data. For reliable models, probability-based sampling is preferred.</li><li><em>FAANG Perspective:</em> Convenience sampling is often how initial datasets are gathered for new problem domains, but we&rsquo;re acutely aware of the biases and strive to get more representative data over time.</li></ul></li><li><p><strong>Random Sampling (Probability-Based) (Pages 84-87):</strong></p><ul><li><p><strong>Simple Random Sampling (Page 84):</strong> All samples in the population (footnote 4: &ldquo;statistical population&rdquo; = potentially infinite set of all possible samples) have equal selection probability.</p><ul><li>Advantage: Easy to implement.</li><li>Drawback: Rare categories might be missed. If a class is 0.01% of data, a 1% random sample will likely miss it. Model might assume rare class doesn&rsquo;t exist.</li></ul></li><li><p><strong>Stratified Sampling (Page 84):</strong></p><ul><li>Divide population into groups (strata) you care about. Sample from each group separately.</li><li>Example: To sample 1% from data with classes A and B, sample 1% of class A <em>and</em> 1% of class B. Ensures rare classes are included.</li><li>Drawback: Not always possible if groups are hard to define or samples belong to multiple groups (multilabel tasks - footnote 5).</li><li><em>FAANG Perspective:</em> Stratified sampling is crucial for creating representative validation/test sets, especially when dealing with imbalanced classes or ensuring coverage across different user segments (e.g., regions, demographics).</li></ul></li><li><p><strong>Weighted Sampling (Page 85):</strong> Each sample given a weight determining its selection probability.</p><ul><li>Example: Samples A, B, C with desired probabilities 50%, 30%, 20% get weights 0.5, 0.3, 0.2.</li><li>Leverages domain expertise (e.g., more recent data is more valuable, give it higher weight).</li><li>Corrects for distribution mismatch (e.g., your data is 25% red, 75% blue; real world is 50/50 red/blue. Give red samples 3x weight of blue during sampling).</li><li>Python: <code>random.choices(population, weights, k)</code>.</li><li><strong>Related concept: Sample Weights (in training, not sampling to create dataset):</strong> Assigns &ldquo;importance&rdquo; to training samples. Higher weight samples affect the loss function more. Can significantly change decision boundaries (Figure 4-1 from scikit-learn).</li><li><em>FAANG Perspective:</em> Weighted sampling is used to upweight important/rare data. Sample weights in training are used for cost-sensitive learning or to prioritize certain types of errors.</li></ul></li><li><p><strong>Reservoir Sampling (Page 86, Figure 4-2):</strong> For streaming data when you don&rsquo;t know total N, can&rsquo;t fit all in memory, but want to sample <code>k</code> items such that each has equal selection probability.</p><ul><li>Algorithm:<ol><li>Fill reservoir (array of size <code>k</code>) with first <code>k</code> elements.</li><li>For each incoming <code>n</code>-th element (where <code>n > k</code>):<ul><li>Generate random integer <code>i</code> from <code>1</code> to <code>n</code>.</li><li>If <code>1 &lt;= i &lt;= k</code>, replace reservoir element at index <code>i</code> with the <code>n</code>-th element. Else, do nothing.</li></ul></li></ol></li><li>Ensures:<ul><li>Every tweet/element has an equal probability of being selected <em>at any point in time</em>.</li><li>If stopped, current reservoir is a fair sample.</li></ul></li><li><code>n</code>-th element has <code>k/n</code> probability of being <em>in</em> the reservoir. Each element <em>already in</em> reservoir has <code>k/n</code> probability of <em>staying</em> (or more precisely, <code>1 - (k/n)*(1/k)</code> = <code>(n-1)/n</code> probability of one specific item in reservoir <em>not</em> being replaced by the nth item, which then combines with prior probabilities). The math ensures uniform probability for all seen items.</li><li>Figure 4-2 visualizes this.</li><li><em>FAANG Perspective:</em> Essential for sampling from massive, unbounded streams (e.g., sampling search queries for analysis, sampling events for monitoring from a firehose).</li></ul></li><li><p><strong>Importance Sampling (Page 87):</strong> Sample from a distribution <code>Q(x)</code> (proposal/importance distribution, easy to sample from) when you actually want to sample from <code>P(x)</code> (target distribution, hard to sample from). Weigh sample <code>x</code> from <code>Q(x)</code> by <code>P(x)/Q(x)</code>.</p><ul><li>Requires <code>Q(x) > 0</code> whenever <code>P(x) != 0</code>.</li><li>Equation: <code>E_P[x] = E_Q[x * P(x)/Q(x)]</code>.</li><li>Use in ML: Policy-based reinforcement learning. Estimate value of new policy (P) using rewards from old policy (Q) and reweight.</li><li><em>FAANG Perspective:</em> Used in some advanced modeling scenarios, off-policy evaluation in RL, and in areas like Bayesian inference (though less common in typical production ML pipelines).</li></ul></li></ul></li></ol><hr><p><strong>Pages 88-97: Labeling – The Quest for Ground Truth</strong></p><p>Most production ML is supervised, needing labeled data. Quality/quantity of labels heavily impacts performance.</p><ul><li>Andrej Karpathy anecdote: Recruiter asked how long he&rsquo;d need a labeling team. His reply: &ldquo;How long do we need an engineering team?&rdquo; Labeling is now a core, ongoing function.</li></ul><ol><li><p><strong>Hand Labels (Pages 88-90):</strong> The classic approach.</p><ul><li><strong>Challenges:</strong><ul><li><strong>Expensive:</strong> Especially with Subject Matter Expertise (SME). Spam classification: 20 crowdworkers, 15 mins training. Chest X-rays: board-certified radiologists (limited, expensive).</li><li><strong>Privacy Threat:</strong> Someone looks at your data. Can&rsquo;t ship patient records or confidential financials to third-party labelers. Might need on-premise annotators.</li><li><strong>Slow:</strong> Phonetic speech transcription can take 400x utterance duration (footnote 7). 1 hour speech = 400 hours (3 months) labeling. Author&rsquo;s colleagues waited almost a year for lung cancer X-ray labels.</li><li><strong>Slow Iteration:</strong> If task/data changes, relabeling is slow, model becomes less adaptive. E.g., sentiment model (NEG/POS) needs to add ANGRY class -> relabel, collect more ANGRY examples.</li></ul></li><li><strong>Label Multiplicity / Ambiguity (Page 89, Table 4-1):</strong> Multiple annotators, different expertise/accuracy -> conflicting labels for same instance.<ul><li>Entity recognition example: &ldquo;Darth Sidious, known simply as the Emperor, was a Dark Lord of the Sith who reigned over the galaxy as Galactic Emperor of the First Galactic Empire.&rdquo;<ul><li>Annotator 1: 3 entities.</li><li>Annotator 2: 6 entities (more granular, e.g., &ldquo;Dark Lord&rdquo; and &ldquo;Sith&rdquo; separate).</li><li>Annotator 3: 4 entities.</li></ul></li><li>Which to train on? Models will perform differently.</li><li>Disagreements common, especially with high SME needed (footnote 8: if obvious, no SME needed). What is &ldquo;human-level performance&rdquo; if experts disagree?</li><li><strong>Mitigation:</strong> Clear problem definition (e.g., &ldquo;pick longest substring for entities&rdquo;) and annotator training.</li></ul></li><li><strong>Data Lineage (Page 90):</strong> Track origin of samples and labels.<ul><li>Critical if using multiple sources/annotators.</li><li>Example: Model trained on 100k good samples. Add 1M crowdsourced samples (lower accuracy). Performance <em>decreases</em>. If data mixed, hard to debug.</li><li>Helps flag bias, debug models. If model fails on recent data, investigate its acquisition. Often, it&rsquo;s bad labels, not bad model.</li></ul></li><li><em>FAANG Perspective:</em> We have large in-house and vendor labeling operations. Managing quality, cost, throughput, and privacy is a huge operational challenge. Data lineage is critical for regulatory compliance and debugging. Adjudication (resolving labeler disagreements) is a standard part of the process.</li></ul></li><li><p><strong>Natural Labels (Behavioral Labels) (Pages 91-93):</strong> Labels inferred from system or user behavior, no human annotation needed.</p><ul><li>Model predictions automatically/partially evaluated by system.</li><li>Examples:<ul><li>Google Maps ETA: At trip end, actual travel time is known.</li><li>Stock price prediction: After 2 mins, actual price is known.</li><li><strong>Recommender Systems (canonical):</strong> User clicks (POSITIVE label) or doesn&rsquo;t click after time (NEGATIVE label) on recommendation.</li><li>Many tasks framed as recommendation (e.g., ad CTR prediction = recommend relevant ads).</li></ul></li><li>Can be set up: Google Translate allows community to submit alternative translations (becomes labels for next iteration, after review). Facebook &ldquo;Like&rdquo; button provides feedback for newsfeed ranking.</li><li><strong>Common in Industry (Figure 4-3):</strong> Survey of 86 companies: 63% use natural labels. (Percentages don&rsquo;t sum to 1 as companies use multiple sources). Likely because it&rsquo;s cheaper/easier to start with.</li><li><strong>Implicit vs. Explicit Labels (Page 92):</strong><ul><li>Recommendation not clicked after time = <em>implicit</em> negative label.</li><li>User downvotes recommendation = <em>explicit</em> negative label.</li></ul></li><li><strong>Feedback Loop Length (Pages 92-93):</strong> Time from prediction to feedback.<ul><li>Short loops (minutes): Many recommenders (Amazon related products, Twitter follows).</li><li>Longer loops (hours/days/weeks): Recommending blog posts, YouTube videos, Stitch Fix clothes (feedback after user tries them on).</li><li><strong>Different Types of User Feedback (sidebar, page 93):</strong> Ecommerce example:<ul><li>Clicking product (high volume, weaker signal, fast feedback).</li><li>Adding to cart.</li><li>Buying product (lower volume, stronger signal, business-correlated).</li><li>Rating/reviewing.</li><li>Returning product.</li><li>Optimizing for clicks vs. purchases is a common trade-off. Depends on use case, discuss with stakeholders.</li></ul></li><li><strong>Choosing Window Length:</strong> Speed vs. accuracy. Short window = faster labels, but might prematurely label as negative. Twitter Ads study (footnote 10): most clicks in 5 mins, but some hours later. Short window underestimates true CTR.</li><li>Long feedback loops (weeks/months): Fraud detection (dispute window 1-3 months). Good for quarterly reports, bad for quick issue detection. A faulty fraud model can bankrupt a small business if issues take months to fix.</li></ul></li><li><em>FAANG Perspective:</em> Natural labels are heavily used for large-scale systems (search, ads, recommendations). Designing for good feedback collection is key. Understanding feedback delay and its impact on evaluation is critical.</li></ul></li><li><p><strong>Handling the Lack of Labels (Pages 94-101, Table 4-2):</strong> What to do when hand labels are too hard and natural labels are absent/insufficient.</p><ul><li><p><strong>Table 4-2 Summary:</strong></p><table><thead><tr><th style=text-align:left>Method</th><th style=text-align:left>How</th><th style=text-align:left>Ground Truths Required?</th></tr></thead><tbody><tr><td style=text-align:left>Weak Supervision</td><td style=text-align:left>(Noisy) heuristics to generate labels</td><td style=text-align:left>No, but small set recommended to guide heuristic dev.</td></tr><tr><td style=text-align:left>Semi-supervision</td><td style=text-align:left>Structural assumptions, seed labels</td><td style=text-align:left>Yes, small initial set as seeds</td></tr><tr><td style=text-align:left>Transfer Learning</td><td style=text-align:left>Pretrained models from another task</td><td style=text-align:left>No (zero-shot). Yes (fine-tuning, often fewer than scratch)</td></tr><tr><td style=text-align:left>Active Learning</td><td style=text-align:left>Label samples most useful to model</td><td style=text-align:left>Yes</td></tr></tbody></table></li><li><p><strong>Weak Supervision (Pages 95-97):</strong> Use heuristics (SME rules) to label data.</p><ul><li><strong>Snorkel (Stanford AI Lab, footnote 11):</strong> Popular open-source tool.</li><li><strong>Labeling Functions (LFs):</strong> Functions encoding heuristics.<ul><li>Example: <code>if "pneumonia" in nurse_note: return "EMERGENT"</code></li><li>Types of LFs: Keyword, regex, DB lookup, outputs of other models.</li></ul></li><li>LFs produce <em>noisy</em> labels. Multiple LFs can conflict. Need to combine, denoise, reweight LFs (Figure 4-4 shows this high-level process).</li><li>Theoretically no hand labels needed, but small set recommended to guide LF development/accuracy check.</li><li><strong>Benefits:</strong><ul><li>Privacy (write LFs on small, cleared subset; apply to all data without looking).</li><li>SME expertise versioned, reused, shared.</li><li>Adaptive (data/reqs change -> reapply LFs).</li></ul></li><li>Known as <strong>programmatic labeling</strong>. Table 4-3 compares with hand labeling (cost, privacy, speed, adaptivity).</li><li><strong>Case Study (Stanford Medicine, footnote 13, Figure 4-5):</strong> Weakly supervised X-ray models (1 radiologist, 8hrs writing LFs) comparable to fully supervised (almost a year hand labeling). Models improved with more unlabeled data (LFs applied). LFs reused across tasks (CXR, EXR). (Footnote 14: study used 18-20 LFs; author has seen hundreds).</li><li><strong>Why still need ML models if heuristics work?</strong> LFs might not cover all samples. Train ML on LF-labeled data to generalize to samples not covered by any LF.</li><li>Powerful, but not perfect (labels can be too noisy). Good for bootstrapping.</li><li><em>FAANG Perspective:</em> Weak supervision is gaining traction for problems with limited labels or high SME cost. Combining heuristics with small amounts of gold data is common.</li></ul></li><li><p><strong>Semi-Supervision (Pages 98-99):</strong> Leverages structural assumptions to generate new labels from a small initial set of labeled data.</p><ul><li>Used since &rsquo;90s (footnote 16, Blum & Mitchell co-training).</li><li><strong>Self-training (classic):</strong><ol><li>Train model on existing labeled data.</li><li>Predict on unlabeled data.</li><li>Add high-confidence predictions (with their predicted labels) to training set.</li><li>Retrain. Repeat.</li></ol></li><li><strong>Similarity-based:</strong> Assume similar samples have similar labels.<ul><li>Obvious similarity: Twitter hashtags. Label #AI as CS. If #ML, #BigData in same MIT CSAIL profile (Figure 4-6), label them CS too.</li><li>Complex similarity: Use clustering or k-NN.</li></ul></li><li><strong>Perturbation-based (popular):</strong> Small perturbations to sample shouldn&rsquo;t change label. Apply small noise to images or word embeddings. (More in &ldquo;Perturbation&rdquo; on page 114).</li><li>Can reach fully supervised performance with fewer labels (footnote 17).</li><li><strong>Consideration with limited data:</strong> How much for evaluation vs. training? Small eval set -> overfitting to it. Large eval set -> less data for training boost. Common trade-off: use reasonably large eval set, then continue training champion model on that eval data too.</li><li><em>FAANG Perspective:</em> Used when labeled data is scarce. Self-training and consistency regularization (perturbation-based) are common techniques.</li></ul></li><li><p><strong>Transfer Learning (Pages 99-100):</strong> Reuse model from one task (base task) as starting point for another (downstream task).</p><ul><li>Base task usually has abundant/cheap data (e.g., language modeling: predict next token from vast text corpora like books, Wikipedia - footnote 18 token definition).</li><li><strong>Usage:</strong><ul><li><strong>Zero-shot:</strong> Use base model directly on downstream task.</li><li><strong>Fine-tuning:</strong> Make small changes to base model (e.g., continue training on downstream data - footnote 19 Howard & Ruder ULMFiT).</li></ul></li><li><strong>Prompting (footnote 20):</strong> Modify inputs with a template for base model. E.g., for QA with GPT-3:
<code>Q: When was US founded? A: July 4, 1776.</code>
<code>Q: Who wrote Dec of Ind? A: Thomas Jefferson.</code>
<code>Q: What year Alex Hamilton born? A: [GPT-3 outputs year]</code></li><li>Appealing for tasks with few labels. Boosts performance even with many labels.</li><li>Enabled many apps (ImageNet pretrained object detectors; BERT/GPT-3 for text - footnote 21). Lowers entry barrier.</li><li><strong>Trend:</strong> Larger pretrained base model -> better downstream performance. Training large models (GPT-3) costs tens of millions USD. Future: few companies train huge models, rest use/fine-tune them.</li><li><em>FAANG Perspective:</em> Transfer learning is THE dominant paradigm in NLP and increasingly in CV. We train massive foundation models and fine-tune them for various tasks. This saves immense labeling effort and compute.</li></ul></li><li><p><strong>Active Learning (Query Learning) (Pages 101-102):</strong> Improve label efficiency. Model (active learner) chooses which unlabeled samples to send to annotators.</p><ul><li>Label samples most <em>helpful</em> to model.</li><li><strong>Uncertainty measurement (most straightforward):</strong> Label examples model is least certain about (hoping to clarify decision boundary). E.g., classification: samples with lowest predicted class probability. (Figure 4-7 from Burr Settles, footnote 22: toy example, 30 random labels = 70% acc; 30 active learning labels = 90% acc).</li><li><strong>Query-by-committee (ensemble method, footnote 23 Ch6):</strong> Multiple models vote on samples. Label samples committee disagrees on most.</li><li>Other heuristics: samples giving highest gradient updates or reducing loss most. (Settles 2010 survey).</li><li><strong>Data regimes for samples:</strong><ul><li>Synthesized (model generates uncertain input points - footnote 24 Angluin).</li><li>Stationary pool of unlabeled data.</li><li>Real-world stream (production data).</li></ul></li><li>Author most excited about active learning with real-time, changing data (Chapter 1, Chapter 8). Adapts faster.</li><li><em>FAANG Perspective:</em> Active learning is used to prioritize labeling efforts, especially when budgets are constrained or labeling is a bottleneck. It&rsquo;s often integrated with human-in-the-loop systems.</li></ul></li></ul></li></ol><hr><p><strong>Pages 102-113: Class Imbalance – The Uneven Playing Field</strong></p><p>A very common real-world problem.</p><ul><li><strong>Definition (Page 102):</strong> Classification: substantial difference in number of samples per class. (E.g., 99.99% normal lung X-rays, 0.01% cancerous).</li><li>Regression: can also happen with skewed continuous labels. (Eugene Yan&rsquo;s healthcare bills example, footnote 25: median bill low, 95th percentile astronomical. 100% error on $250 bill ($250 vs $500) okay; 100% error on $10k bill ($10k vs $20k) not. Might need to optimize for 95th percentile prediction even if overall metrics suffer).</li></ul><ol><li><p><strong>Challenges of Class Imbalance (Page 103-104, Figure 4-8):</strong> ML (esp. Deep Learning) works well with balanced data, struggles with imbalance.</p><ul><li><strong>Figure 4-8 (Andrew Ng image):</strong> ML works well when distribution is like <code>[Cat, Dog, Chair, Bike, Person]</code> (balanced). Not so well when like <code>[Effusion, Atelectasis, Mass, Consolidation, Hernia]</code> (highly imbalanced medical findings).</li><li><strong>Reason 1: Insufficient signal for minority class.</strong> Becomes few-shot learning problem. If no instances, model assumes class doesn&rsquo;t exist.</li><li><strong>Reason 2: Model exploits simple heuristic.</strong> E.g., lung cancer: always predict majority class (normal) -> 99.99% accuracy (footnote 27: why accuracy is bad for imbalance). Hard for gradient descent to beat this trivial solution.</li><li><strong>Reason 3: Asymmetric error costs.</strong> Misclassifying cancerous X-ray (rare) far more dangerous than misclassifying normal lung (common). Standard loss functions treat all samples equally.</li><li><strong>Imbalance is the norm in real world (Page 104):</strong> Author shocked after school (balanced datasets) to find this. Rare events are often more interesting/dangerous.<ul><li>Examples: Fraud detection (6.8c per $100 is fraud - footnote 29), churn prediction, disease screening, resume screening (98% eliminated initially - footnote 30), object detection (most generated bounding boxes are background).</li></ul></li><li><strong>Other causes:</strong> Sampling bias (spam: 85% of all email is spam, but filtered before DB, so dataset has little spam - footnote 31), labeling errors.</li><li><em>Always examine data to understand causes of imbalance.</em></li></ul></li><li><p><strong>Handling Class Imbalance (Pages 105-113):</strong> Extensively studied (footnote 32). Sensitivity varies by task complexity, imbalance level (footnote 33). Binary easier than multiclass. Deep NNs (10+ layers in 2017) better on imbalanced data than shallower ones (footnote 34).</p><ul><li><p>Some argue: don&rsquo;t &ldquo;fix&rdquo; imbalance if it&rsquo;s real-world. Good model should learn it. Challenging.</p></li><li><p>Three approaches:</p></li><li><p><strong>A. Using the Right Evaluation Metrics (Page 106-108):</strong> Most important first step!</p><ul><li>Overall accuracy/error rate: Insufficient. Dominated by majority class.</li><li>Example: CANCER (positive, 10% of data) vs. NORMAL (negative, 90%).<ul><li>Model A (Table 4-4): Predicts 10/100 CANCER, 890/900 NORMAL. Overall Accuracy = (10+890)/1000 = 0.9.</li><li>Model B (Table 4-5): Predicts 90/100 CANCER, 810/900 NORMAL. Overall Accuracy = (90+810)/1000 = 0.9.</li><li>Both 90% accurate, but Model B much better for CANCER detection.</li></ul></li><li><strong>Better choice: Per-class accuracy.</strong> Model A CANCER acc: 10%. Model B CANCER acc: 90%.</li><li><strong>Precision, Recall, F1 (sidebar, Table 4-6):</strong> For binary tasks, measure performance wrt positive class (footnote 35: scikit-learn <code>pos_label</code>). Asymmetric (values change if you swap positive/negative class).<ul><li>Precision = TP / (TP + FP) (Of those predicted positive, how many were actually positive?)</li><li>Recall (Sensitivity, True Positive Rate) = TP / (TP + FN) (Of all actual positives, how many did we find?)</li><li>F1 = 2 * (Precision * Recall) / (Precision + Recall) (Harmonic mean)</li><li>Table 4-7: Model A (CANCER positive): P=0.5, R=0.1, F1=0.17. Model B: P=0.5, R=0.9, F1=0.64. Clearly shows B is better.</li></ul></li><li><strong>ROC Curve (Receiver Operating Characteristics) (Page 108, Figure 4-9):</strong><ul><li>Classification often outputs probability. Threshold (e.g., 0.5) converts to class.</li><li>Plot True Positive Rate (Recall) vs. False Positive Rate (1 - Specificity) for different thresholds.</li><li>Perfect model: Line at top (TPR=1). Random: Diagonal. Closer to top-left = better.</li><li><strong>AUC (Area Under Curve):</strong> Measures area under ROC. Larger = better.</li></ul></li><li><strong>Precision-Recall Curve (Page 108):</strong> ROC focuses on positive class, doesn&rsquo;t show negative class performance. Davis & Goadrich (footnote 36) argue PR curve is more informative for heavy imbalance.</li><li><em>FAANG Perspective:</em> For imbalanced problems, we <em>never</em> rely on overall accuracy. We look at Precision, Recall, F1 for the classes of interest, AUC-ROC, AUC-PR. Confusion matrices are essential.</li></ul></li><li><p><strong>B. Data-Level Methods: Resampling (Pages 109-110):</strong> Modify training data distribution.</p><ul><li><strong>Undersampling:</strong> Remove instances from majority class. Simplest: random removal.</li><li><strong>Oversampling:</strong> Add instances to minority class. Simplest: random replication.</li><li>Figure 4-10 (Rafael Alencar, footnote 37) visualizes this.</li><li><strong>Tomek links (undersampling, footnote 38):</strong> Find close pairs from opposite classes, remove majority class sample. Clears decision boundary, but might make model less robust (loses subtlety of true boundary). For low-dim data.</li><li><strong>SMOTE (Synthetic Minority Oversampling TEchnique, footnote 39):</strong> Synthesize new minority samples by convex combinations of existing ones (footnote 40 linear). For low-dim data.</li><li>Sophisticated methods (Near-Miss, one-sided selection - footnote 41) need distance calcs, expensive for high-dim data/features (e.g., NNs).</li><li><strong>CRITICAL: Never evaluate on resampled validation/test data!</strong> Model will overfit to resampled distribution. Evaluate on original, true distribution.</li><li>Risks: Undersampling loses data. Oversampling (replication) overfits.</li><li><strong>Two-phase learning (footnote 42):</strong> Train on resampled data (e.g., undersample majority to N instances per class). Fine-tune on original data.</li><li><strong>Dynamic sampling (footnote 43 Pouyanfar):</strong> Oversample low-performing classes, undersample high-performing ones during training. Show model less of what it knows, more of what it doesn&rsquo;t.</li><li><em>FAANG Perspective:</em> Resampling is common, but needs care. SMOTE is popular. Often combined with algorithm-level methods. Evaluation on original distribution is key.</li></ul></li><li><p><strong>C. Algorithm-Level Methods (Pages 110-112):</strong> Keep data intact, alter algorithm (usually loss function) to be robust to imbalance.</p><ul><li>Prioritize learning instances we care about by giving them higher weight in loss.</li><li><code>L(X; θ) = (1/N) * Σ_x L(x; θ)</code> (standard average loss). Treats all instances equally.</li><li><strong>Cost-sensitive learning (Elkan 2001, footnote 44, Table 4-8):</strong> Misclassification costs vary. <code>C_ij</code> = cost if class <code>i</code> classified as <code>j</code>. <code>C_ii = 0</code>. If classifying POS as NEG is 2x costly as NEG as POS, <code>C_10 = 2 * C_01</code>.<ul><li><code>L(x; θ) = Σ_j C_ij * P(j|x; θ)</code> (loss for instance <code>x</code> of class <code>i</code> is weighted average of costs for possible predicted classes <code>j</code>).</li><li>Problem: Manually define cost matrix, task/scale dependent.</li></ul></li><li><strong>Class-balanced loss (Page 112):</strong> Punish model for misclassifying minority classes.<ul><li>Vanilla: Weight class <code>i</code> by <code>W_i = N_total / N_i</code> (rarer class = higher weight).</li><li><code>L(x; θ) = W_i * Σ_j P(j|x; θ) * Loss(x, j)</code> (where <code>x</code> is instance of class <code>i</code>).</li><li>Sophisticated: Consider overlap among samples (effective number of samples - footnote 45 Cui et al.).</li></ul></li><li><strong>Focal Loss (Lin et al. 2017, footnote 46, Figure 4-11):</strong> Incentivize model to focus on hard-to-classify samples. Adjust loss: if sample has lower probability of being right, give it higher weight.<ul><li>Figure 4-11 shows Focal Loss (FL) vs. Cross Entropy (CE). FL reduces loss more for well-classified examples, focusing on hard ones. <code>γ</code> parameter controls focusing rate.</li></ul></li><li>Ensembles can help (footnote 47), but not their primary purpose. (Covered in Ch6).</li><li><em>FAANG Perspective:</em> Modifying loss functions (class weighting, focal loss) is very common for imbalanced problems, especially in deep learning. It&rsquo;s often more effective and easier to implement than complex resampling if you have large data.</li></ul></li></ul></li></ol><hr><p><strong>Pages 113-117: Data Augmentation – Creating More from Less (or More from More!)</strong></p><p>Increase amount of training data. Traditionally for limited data (medical imaging). Now useful even with lots of data (robustness to noise, adversarial attacks). Standard in CV, finding way into NLP. Format-dependent.</p><ol><li><p><strong>Simple Label-Preserving Transformations (Page 114):</strong></p><ul><li><strong>Computer Vision:</strong> Randomly modify image, preserve label. Crop, flip, rotate, invert, erase part. Rotated dog is still a dog. PyTorch, TF, Keras support this. AlexNet (footnote 48): generated on CPU while GPU trains on previous batch (computationally &ldquo;free&rdquo;).</li><li><strong>NLP (Table 4-9):</strong> Randomly replace word with similar one (synonym dictionary, or close in embedding space), assume meaning/sentiment preserved.<ul><li><code>I'm so happy to see you.</code> -> <code>I'm so glad to see you.</code> / <code>...see y'all.</code> / <code>I'm very happy...</code></li><li>Quick way to double/triple training data.</li></ul></li><li><em>FAANG Perspective:</em> Standard practice in CV. For NLP, synonym replacement, back-translation (translate Eng->Fre->Eng) are common. Need to be careful not to change meaning too much.</li></ul></li><li><p><strong>Perturbation (Pages 114-116):</strong> Also label-preserving, but sometimes used to trick models, so gets own section.</p><ul><li>NNs sensitive to noise.</li><li><strong>CV:</strong> Small noise can cause misclassification. <strong>One-pixel attack</strong> (Su et al., footnote 49, Figure 4-12): Changing one pixel misclassifies many CIFAR-10/ImageNet images.</li><li><strong>Adversarial attacks:</strong> Using deceptive data to trick NNs. Adding noise is common.</li><li><strong>Adversarial augmentation/training (footnote 53):</strong> Add noisy samples to training data -> helps model recognize weak spots, improve performance (footnote 51 Goodfellow). Noise can be random or found by search (DeepFool, footnote 52, finds min noise for misclassification).</li><li><strong>NLP:</strong> Less common (random chars -> gibberish). But perturbation used for robustness. BERT (footnote 54): 15% tokens chosen; of these, 10% replaced with random words (1.5% total tokens become nonsensical, e.g., &ldquo;My dog is apple&rdquo;). Small performance boost.</li><li>Chapter 6 covers perturbation for evaluation too.</li><li><em>FAANG Perspective:</em> Adversarial training is important for security-sensitive models (spam, fraud, face recognition) and for improving general robustness.</li></ul></li><li><p><strong>Data Synthesis (Pages 116-117):</strong> Sidestep expensive/slow/private data collection by synthesizing it. Still far from synthesizing <em>all</em> data, but can boost performance.</p><ul><li><strong>NLP Templates (Table 4-10):</strong> Bootstrap chatbot training data.<ul><li>Template: <code>Find me a [CUISINE] restaurant within [NUMBER] miles of [LOCATION].</code></li><li>Fill with lists of cuisines, numbers, locations -> thousands of queries.</li></ul></li><li><strong>CV Mixup (Zhang et al. ICLR 2018, footnote 55):</strong> Combine existing examples with discrete labels to make continuous labels.<ul><li><code>x' = γ*x1 + (1-γ)*x2</code> (e.g., <code>x1</code>=DOG (0), <code>x2</code>=CAT (1)).</li><li>Label for <code>x'</code> = <code>γ*0 + (1-γ)*1</code>.</li><li>Improves generalization, reduces memorization of corrupt labels, robust to adversarial examples, stabilizes GAN training.</li></ul></li><li><strong>NNs to synthesize data (e.g., CycleGAN, footnote 56 Sandfort):</strong> Exciting research, not yet popular in production. Adding CycleGAN images to CT segmentation improved performance.</li><li>CV Augmentation Survey (Shorten & Khoshgoftaar 2019).</li><li><em>FAANG Perspective:</em> Templating is used for bootstrapping NLU models. Mixup and related techniques (CutMix, CutOut) are standard in CV training. GAN-based synthesis is still mostly research but promising for rare data or privacy-preserving data generation.</li></ul></li></ol><hr><p><strong>Page 118: Summary of Chapter 4</strong></p><p>Training data is foundational. Bad data = bad models. Invest time/effort to curate/create it.</p><ul><li><strong>Sampling:</strong> Nonprobability (convenience) vs. Random (simple, stratified, weighted, reservoir, importance).</li><li><strong>Labeling:</strong><ul><li>Most ML is supervised. Natural labels (delivery times, recommender clicks) are great, but often delayed (feedback loop length).</li><li>Hand labels: Expensive, slow, privacy issues, label multiplicity. Data lineage is key.</li><li>Lack of labels: Weak supervision (heuristics, Snorkel), semi-supervision (self-training, similarity, perturbation), transfer learning (pretrained models), active learning (querying for most useful labels).</li></ul></li><li><strong>Class Imbalance:</strong> Norm in real world. Hard for ML. Handle by: right metrics (Precision/Recall/F1, ROC/PR AUC), resampling (over/under, SMOTE), algorithm changes (cost-sensitive loss, focal loss).</li><li><strong>Data Augmentation:</strong> Increase data (simple transforms, perturbation/adversarial, synthesis/mixup/templates). Improves performance, generalization, robustness.</li></ul><p>Next: Feature extraction (Chapter 5).</p><hr><p><strong>Interview Questions & Page References (Chapter 4):</strong></p><ol><li><p><strong>General Training Data Concepts:</strong></p><ul><li>&ldquo;Why is handling training data well so critical in ML projects?&rdquo; (p. 81)</li><li>&ldquo;What are some potential sources of bias in training data, and why is it important to be aware of them?&rdquo; (p. 82)</li><li>&ldquo;Why does the author prefer the term &rsquo;training data&rsquo; over &rsquo;training dataset&rsquo; in the context of production ML?&rdquo; (p. 81)</li></ul></li><li><p><strong>Sampling:</strong></p><ul><li>&ldquo;Why is sampling necessary or helpful in the ML workflow?&rdquo; (p. 82)</li><li>&ldquo;Describe different nonprobability sampling methods and their potential biases. Give examples where they might be used.&rdquo; (p. 83-84)</li><li>&ldquo;Compare simple random sampling with stratified sampling. When would you prefer stratified sampling?&rdquo; (p. 84)</li><li>&ldquo;What is weighted sampling, and how can it be used to leverage domain expertise or correct for distribution mismatches?&rdquo; (p. 85)</li><li>&ldquo;Explain reservoir sampling. When is it particularly useful?&rdquo; (p. 86, Figure 4-2)</li><li>&ldquo;What is importance sampling and where might it be applied in ML?&rdquo; (p. 87)</li></ul></li><li><p><strong>Labeling:</strong></p><ul><li>&ldquo;What are the main challenges associated with acquiring hand labels?&rdquo; (p. 88)</li><li>&ldquo;What is label multiplicity? How can disagreements among annotators be minimized?&rdquo; (p. 89, Table 4-1)</li><li>&ldquo;Explain the concept of data lineage and its importance.&rdquo; (p. 90)</li><li>&ldquo;What are natural labels (behavioral labels)? Give some examples. How do they compare to hand labels?&rdquo; (p. 91)</li><li>&ldquo;Discuss the concept of feedback loop length for natural labels and its implications. Provide examples of short and long feedback loops.&rdquo; (p. 92-93)</li><li>&ldquo;Explain the difference between implicit and explicit labels.&rdquo; (p. 92)</li></ul></li><li><p><strong>Handling Lack of Labels:</strong></p><ul><li>&ldquo;Describe weak supervision. How do Labeling Functions (LFs) work in tools like Snorkel?&rdquo; (p. 95-97, Figure 4-4, Table 4-3)</li><li>&ldquo;What are the advantages of programmatic labeling (weak supervision) over hand labeling?&rdquo; (p. 96, Table 4-3)</li><li>&ldquo;What is semi-supervised learning? Describe self-training and perturbation-based methods.&rdquo; (p. 98-99, Figure 4-6)</li><li>&ldquo;Explain transfer learning. What are base models, downstream tasks, fine-tuning, and prompting?&rdquo; (p. 99-100)</li><li>&ldquo;What is active learning? How can uncertainty measurement or query-by-committee be used to select samples for labeling?&rdquo; (p. 101-102, Figure 4-7)</li></ul></li><li><p><strong>Class Imbalance:</strong></p><ul><li>&ldquo;What is class imbalance, and why does it make learning difficult for ML models?&rdquo; (p. 102-104, Figure 4-8)</li><li>&ldquo;Give some real-world examples of tasks with class imbalance.&rdquo; (p. 104)</li><li>&ldquo;Why is overall accuracy an insufficient metric for tasks with class imbalance? What are better alternatives?&rdquo; (p. 106-108, Tables 4-4, 4-5, 4-7, Figure 4-9)</li><li>&ldquo;Explain Precision, Recall, and F1-score. Why are they useful for imbalanced datasets?&rdquo; (p. 107, Table 4-6)</li><li>&ldquo;Describe data-level methods for handling class imbalance, such as oversampling and undersampling (including SMOTE and Tomek links).&rdquo; (p. 109-110, Figure 4-10)</li><li>&ldquo;What are algorithm-level methods for class imbalance? Explain cost-sensitive learning, class-balanced loss, and focal loss.&rdquo; (p. 110-112, Table 4-8, Figure 4-11)</li><li>&ldquo;When resampling training data, what is a critical consideration for model evaluation?&rdquo; (p. 110)</li></ul></li><li><p><strong>Data Augmentation:</strong></p><ul><li>&ldquo;What is data augmentation, and why is it used?&rdquo; (p. 113)</li><li>&ldquo;Describe simple label-preserving transformations for image and text data.&rdquo; (p. 114, Table 4-9)</li><li>&ldquo;What is perturbation in the context of data augmentation? How does it relate to adversarial attacks and adversarial training?&rdquo; (p. 114-116, Figure 4-12)</li><li>&ldquo;Explain some data synthesis techniques like using templates for NLP or mixup for CV.&rdquo; (p. 116-117, Table 4-10)</li></ul></li></ol><p>This chapter is packed with practical techniques essential for any ML engineer. Getting the training data right is often more than half the battle! Any questions on these topics?</p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><script src=/js/search.js></script></body></html>