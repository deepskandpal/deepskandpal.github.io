<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul></nav></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><a href=#page-2-mlops-vs-ml-systems-design--the-books-philosophy>Page 2: MLOps vs. ML Systems Design & The Book&rsquo;s Philosophy</a></li><li><a href=#pages-3-8-when-to-use-machine-learning-the-litmus-test>Pages 3-8: When to Use Machine Learning (The Litmus Test)</a></li><li><a href=#pages-9-11-machine-learning-use-cases>Pages 9-11: Machine Learning Use Cases</a></li><li><a href=#page-12-more-enterprise-use-cases--intro-to-understanding-ml-systems>Page 12: More Enterprise Use Cases & Intro to Understanding ML Systems</a></li><li><a href=#pages-12-21-machine-learning-in-research-versus-in-production>Pages 12-21: Machine Learning in Research Versus in Production</a><ul><li><a href=#different-stakeholders-and-requirements-page-13-14>Different stakeholders and requirements (Page 13-14):</a></li><li><a href=#criticism-of-ml-leaderboards-page-15>Criticism of ML Leaderboards (Page 15):</a></li><li><a href=#computational-priorities-page-15-18>Computational priorities (Page 15-18):</a></li><li><a href=#data-page-18-19>Data (Page 18-19):</a></li><li><a href=#fairness-page-19-20>Fairness (Page 19-20):</a></li><li><a href=#interpretability-page-20-21>Interpretability (Page 20-21):</a></li><li><a href=#discussion-page-21-why-production-focus-matters>Discussion (Page 21): Why production focus matters?</a></li></ul></li><li><a href=#pages-22-23-machine-learning-systems-versus-traditional-software>Pages 22-23: Machine Learning Systems Versus Traditional Software</a></li><li><a href=#page-23-summary>Page 23: Summary</a></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 1: Overview of Machine Learning Systems</h1><span class=reading-time><em>15 min read</em></span><div class=book-details><div class=book-content><p><a href=https://aistudio.google.com/prompts/19Kbu6lIbeuASE0ER7Hc3puDMRZYY6_Z4>Prompt</a></p><p>The chapter opens with a great example: Google Translate&rsquo;s multilingual neural machine translation system back in 2016. This was a landmark – deep learning making a massive, tangible impact at scale. It showed the world the power of modern ML.</p><p>Since then, as the book says, ML has &ldquo;found its way into almost every aspect of our lives.&rdquo; This is true. At FAANG, we see this daily – from the recommendations you get, to how your photos are enhanced, to how we optimize our data centers.</p><p>But here&rsquo;s the first crucial takeaway for any ML design interview, and indeed, for your day-to-day work:</p><blockquote><p>&ldquo;Many people, when they hear &lsquo;machine learning system,&rsquo; think of just the ML algorithms&mldr; However, the algorithm is only a small part of an ML system in production.&rdquo;</p></blockquote><p>This is <strong>HUGE</strong>. I can&rsquo;t stress this enough. In interviews, if you only talk about model architectures and training loops, you&rsquo;re missing 90% of the picture. The system includes:</p><ul><li><strong>Business Requirements</strong>: Why are we even building this? What problem does it solve?</li><li><strong>User Interface</strong>: How do users interact with it? How do developers interact with it?</li><li><strong>Data Stack</strong>: Ingestion, storage, processing, versioning – the lifeblood.</li><li><strong>Development, Monitoring, Updating Logic</strong>: This is MLOps territory.</li><li><strong>Infrastructure</strong>: The compute, storage, and networking that makes it all run.</li></ul><p><figure><img src=/bookshelf/design-ml-system/ch-1-fig-1.png alt=image width=750></figure><em>Figure 1-1</em> (which we&rsquo;ll see on the next page) visually breaks this down and shows how the book will cover these components. We&rsquo;ll keep coming back to this idea of the holistic system.</p><p><strong>Self-Correction/Interview Tip</strong>: When an interviewer asks you to design an ML system, your first thoughts should be about the business problem, the users, and the data, not immediately about whether to use a Transformer or a ResNet.</p><h2 id=page-2-mlops-vs-ml-systems-design--the-books-philosophy>Page 2: MLOps vs. ML Systems Design & The Book&rsquo;s Philosophy</h2><p>This page introduces two key terms:</p><ul><li><strong>MLOps</strong>: This comes from DevOps. It&rsquo;s about operationalizing ML – deploying, monitoring, maintaining. It&rsquo;s a set of tools and best practices. Think CI/CD for ML, model registries, monitoring dashboards.</li><li><strong>ML Systems Design</strong>: This is the approach the book (and this cohort) takes. It&rsquo;s a holistic view of MLOps, considering all components and stakeholders to meet objectives. It&rsquo;s the &ldquo;architecture&rdquo; level thinking.</li></ul><p><em>Figure 1-1</em> is really important here.
You see ML system users and business requirements feeding in (Chapters 1 & 2).
You see ML system developers and the entire book dedicated to them.
The &ldquo;ML system&rdquo; box itself has:</p><ul><li>Data (Chapters 3 & 4)</li><li>Feature Engineering (Chapter 5)</li><li>ML Algorithms (Chapter 6 – notice it&rsquo;s just one chapter!)</li><li>Evaluation (also Chapter 6)</li><li>Deployment, monitoring, updating of logics (Chapters 7, 8 & 9)</li><li>Infrastructure (Chapter 10)</li></ul><p>And Chapter 11 covers the overarching aspects like stakeholders, ethics, etc. (though ethics is woven throughout).</p><p>The book explicitly states it won&rsquo;t cover specific algorithms in detail. Why?</p><blockquote><p>Because algorithms change, they get outdated. The framework for building robust, scalable, maintainable ML systems – that&rsquo;s timeless.</p></blockquote><p>This is exactly what FAANG companies look for: people who can think in terms of systems and principles, not just the latest hot model.</p><p><strong>FAANG Perspective</strong>: At places like Google, Meta, Amazon, we have dedicated MLOps platform teams building tools, but product-focused ML engineers are the ones using these tools within a systems design framework to solve specific business problems. They need to understand the whole lifecycle.</p><h2 id=pages-3-8-when-to-use-machine-learning-the-litmus-test>Pages 3-8: When to Use Machine Learning (The Litmus Test)</h2><p>This is arguably the most critical section for any ML project&rsquo;s inception and a very common starting point for ML design interviews. Before you design anything, you must ask: <strong>Is ML necessary or cost-effective?</strong></p><p>The book offers a fantastic five-part definition: &ldquo;Machine learning is an approach to:</p><ol><li><p><strong>(1) Learn</strong>: The system must have the capacity to learn. A relational database isn&rsquo;t an ML system because you explicitly define relationships. ML systems infer them, usually from data. For supervised learning (e.g., predicting Airbnb prices), you need input-output pairs.</p><ul><li><strong>Interview Relevance</strong>: <em>&ldquo;How does your system learn from new data?&rdquo;</em></li></ul></li><li><p><strong>(2) Complex patterns</strong>: ML shines when patterns are too complex to hand-code. Predicting if a fair die roll is a 6? No pattern. Predicting stock prices? Complex patterns (hopefully!). Sorting listings by state based on zip code? Lookup table. Predicting Airbnb rental price from many features? ML! This is Andrej Karpathy&rsquo;s &ldquo;Software 2.0&rdquo; idea – you provide data, the system writes the &ldquo;code&rdquo; (the learned patterns).</p><ul><li><strong>FAANG Nuance</strong>: We often deal with problems where the rules are unknown or too numerous to list, like ranking search results or newsfeed items. This is prime ML territory.</li></ul></li><li><p><strong>(3) Existing data</strong>: ML needs data to learn from. No data, no ML (usually). The book mentions predicting taxes without tax data – impossible.</p><ul><li><em>Zero-shot learning</em> is an interesting case: it makes predictions for tasks it wasn&rsquo;t directly trained on, but it was trained on related tasks/data. So, it still needs some data.</li><li><em>Fake-it-til-you-make-it</em>: Launch with humans making predictions, collect that data, then train an ML model. This is a common bootstrapping strategy for new products.</li><li><strong>Interview Relevance</strong>: <em>&ldquo;What data would you use? How would you collect it? What if you don&rsquo;t have labeled data initially?&rdquo;</em></li></ul></li><li><p><strong>(4) Predictions</strong>: ML solves problems requiring predictive answers. &ldquo;What will the weather be?&rdquo; &ldquo;What movie will a user watch next?&rdquo; Even compute-intensive problems can be reframed: &ldquo;What would the outcome of this complex rendering process look like?&rdquo; (approximate it with ML).</p></li><li><p><strong>(5) Unseen data</strong>: The patterns learned must generalize to new, unseen data. If your model trained on 2008 app download data (Koi Pond!) tries to predict 2020 downloads, it&rsquo;ll fail. Technically, training and unseen data should come from similar distributions. If not, your model will perform poorly (hello, monitoring and retraining, Chapters 8 & 9!).</p><ul><li><strong>Interview Relevance</strong>: This leads directly to discussions of data drift, concept drift, and the importance of robust evaluation and monitoring.</li></ul></li></ol><p>The book then lists additional characteristics where ML solutions shine:</p><ol start=6><li><strong>It&rsquo;s repetitive</strong>: ML algorithms (especially deep learning) often need many examples. Repetitive tasks provide these.</li><li><strong>The cost of wrong predictions is cheap (usually)</strong>: A bad movie recommendation? User ignores it. A self-driving car makes a wrong turn? Catastrophic. This influences your choice of problem and acceptable error rates. However, even for high-stakes, if ML on average outperforms humans (like statistically safer self-driving cars), it can be viable.</li><li><strong>It&rsquo;s at scale</strong>: ML often needs significant upfront investment (data, compute, talent). This is justified if you&rsquo;re making many predictions (e.g., sorting millions of emails, routing thousands of support tickets). &ldquo;At scale&rdquo; also implies lots of data for training.</li><li><strong>The patterns are constantly changing</strong>: Spam emails evolve. Fashion trends change. ML models can be updated with new data to adapt, unlike hardcoded rules. This is where &ldquo;Continual Learning&rdquo; (page 264, Chapter 9) comes in.</li></ol><p><strong>When NOT to use ML</strong>:</p><ul><li><strong>It&rsquo;s unethical</strong>: Automated grader biases (page 341) – we&rsquo;ll cover ethics in detail.</li><li><strong>Simpler solutions do the trick</strong>: (Chapter 6) ALWAYS start with a non-ML baseline. If a heuristic works, great!<ul><li><strong>FAANG Mantra</strong>: <em>&ldquo;Start simple.&rdquo;</em> A common failure mode for junior engineers is over-engineering.</li></ul></li><li><strong>It&rsquo;s not cost-effective.</strong></li></ul><p>A crucial caution: Don&rsquo;t dismiss new tech just because it&rsquo;s not cost-effective now. Early adoption can be a competitive advantage later. And sometimes, you can break a big problem into smaller pieces, using ML for just one part.</p><p><strong>Interview Gold</strong>: The &ldquo;When to use ML&rdquo; checklist is your first filter for any ML system design question. Always discuss these trade-offs. If you propose an ML solution, be ready to defend why ML is appropriate over a simpler, non-ML approach.</p><h2 id=pages-9-11-machine-learning-use-cases>Pages 9-11: Machine Learning Use Cases</h2><p>This section is a tour of where ML is making an impact. It&rsquo;s good for broadening your understanding of the applications.</p><p><strong>Consumer Applications</strong>:</p><ul><li>Search engines (Google)</li><li>Recommender systems (Netflix, Amazon)</li><li>Predictive typing (your phone&rsquo;s keyboard)</li><li>Photo enhancement</li><li>Biometric authentication (fingerprint, face ID)</li><li>Machine translation (the author&rsquo;s personal anecdote about parents using Google Translate is lovely)</li><li>Smart assistants (Alexa, Google Assistant)</li><li>Smart security cameras (pet detection, uninvited guests)</li><li>At-home health monitoring (fall detection)</li></ul><p><strong>Enterprise Applications</strong>: The book rightly states these are the majority of ML use cases.</p><ul><li>Often have stricter accuracy requirements but can be more forgiving on latency compared to consumer apps (e.g., 0.1% improvement in resource allocation for Google can save millions, even if the system takes a few seconds to run).</li><li><em>Figure 1-3 (Algorithmia 2020 survey)</em>: Shows diverse enterprise uses:<ul><li><em>Internal</em>: Reducing costs (38%), generating customer insights (37%), internal processing automation (30%).</li><li><em>External</em>: Improving customer experience (34%), retaining customers (29%), interacting with customers (28%).</li></ul></li><li>Specific examples:<ul><li><strong>Fraud detection</strong>: Anomaly detection on transactions. (Very common, one of the oldest ML uses in enterprise).</li><li><strong>Price optimization</strong>: Dynamic pricing for ads, flights, ride-sharing. Maximize margin/revenue.</li><li><strong>Demand forecasting</strong>: For inventory, resource allocation.</li><li><strong>Customer acquisition</strong>: Identifying potential customers, targeted ads, optimizing discounts.</li><li><strong>Churn prediction</strong>: Predicting when customers (or employees!) might leave.</li></ul></li></ul><p><strong>Self-Correction/Interview Tip</strong>: Having a few diverse use cases in your back pocket is useful. If an interviewer asks, &ldquo;Give me an example of an ML system you find interesting,&rdquo; you can draw from these. It also helps you think about the types of problems ML can solve (classification, regression, anomaly detection, etc.).</p><h2 id=page-12-more-enterprise-use-cases--intro-to-understanding-ml-systems>Page 12: More Enterprise Use Cases & Intro to Understanding ML Systems</h2><p>More enterprise examples:</p><ul><li>Automated support ticket classification: Route tickets to the right department faster.</li><li>Brand monitoring: Sentiment analysis on brand mentions.</li><li>Healthcare: Skin cancer detection, diabetes diagnosis (often through a provider, due to accuracy/privacy needs).</li></ul><p>This page then transitions to the next major section: <strong>Understanding Machine Learning Systems</strong>. The key motivation here is that ML systems are different from:</p><ul><li>ML in research (or academia).</li><li>Traditional software.</li></ul><p>And these differences necessitate the kind of thinking this book promotes.</p><h2 id=pages-12-21-machine-learning-in-research-versus-in-production>Pages 12-21: Machine Learning in Research Versus in Production</h2><p>This is a <strong>CRITICAL</strong> section, especially for those coming from academia or who have mostly worked on Kaggle-like projects. ML in the wild is a different beast. <em>Table 1-1</em> summarizes key differences:<figure><img src=/bookshelf/design-ml-system/ch-1-table-1.png alt=image width=750></figure></p><ul><li><strong>Requirements</strong><ul><li><em>Research:</em> State-of-the-art model performance on benchmarks</li><li><em>Production:</em> Different stakeholders, often conflicting requirements</li></ul></li><li><strong>Computational Priority</strong><ul><li><em>Research:</em> Fast training, high throughput</li><li><em>Production:</em> Fast inference, low latency</li></ul></li><li><strong>Data</strong><ul><li><em>Research:</em> Static, benchmark datasets</li><li><em>Production:</em> Constantly shifting, messy, real-world data</li></ul></li><li><strong>Fairness</strong><ul><li><em>Research:</em> Often not a focus</li><li><em>Production:</em> Must be considered</li></ul></li><li><strong>Interpretability</strong><ul><li><em>Research:</em> Often not a focus</li><li><em>Production:</em> Must be considered (often a requirement)</li></ul></li></ul><p>Let&rsquo;s break these down:</p><h3 id=different-stakeholders-and-requirements-page-13-14>Different stakeholders and requirements (Page 13-14):</h3><ul><li><strong>Research</strong>: Usually a single objective – e.g., SOTA on a benchmark. Researchers might use complex techniques for marginal gains.</li><li><strong>Production</strong>: Multiple stakeholders with conflicting needs. The restaurant recommender example is classic:<ul><li><em>ML Engineers</em>: Want best model for user clicks (maybe complex, needs more data).</li><li><em>Sales Team</em>: Want model recommending expensive restaurants (more service fees).</li><li><em>Product Team</em>: Want low latency (&lt;100ms), as latency drops orders.</li><li><em>ML Platform Team</em>: Worried about scaling, want to pause updates to improve platform.</li><li><em>Manager</em>: Wants to maximize margin (maybe cut the ML team if costs are too high!).</li></ul></li></ul><p>The book mentions decoupling objectives (page 41) – we&rsquo;ll get there. For now, understand that production ML is about trade-offs. Is a 100ms latency a must-have or a nice-to-have? This determines if Model A or B is even viable.</p><p><em>Ensembling</em>: Popular in competitions (Netflix Prize), but often too complex, slow, or hard to interpret for production. A small performance lift might not justify the operational overhead.</p><h3 id=criticism-of-ml-leaderboards-page-15>Criticism of ML Leaderboards (Page 15):</h3><ul><li>Hard steps (data collection, problem formulation) are often done for you.</li><li>Multiple hypothesis testing: With many teams, some might get good results by chance.</li><li>Misaligned incentives: Drive for accuracy at the expense of compactness, fairness, energy efficiency (Ethayarajh & Jurafsky).</li></ul><p><strong>FAANG Perspective</strong>: This stakeholder wrangling is daily life. Product Managers, Engineering Managers, ML Engineers, and sometimes legal/policy teams all have a say. Clear communication and defining priorities are key.</p><h3 id=computational-priorities-page-15-18>Computational priorities (Page 15-18):</h3><ul><li><strong>Research</strong>: Focus on fast training and high throughput (samples/sec during training).</li><li><strong>Production</strong>: Focus on fast inference and low latency (time from query to result).</li></ul><p><strong>Terminology Clash (Page 16)</strong>: The book uses &ldquo;latency&rdquo; to mean &ldquo;response time&rdquo; (what the client sees, including network/queueing delays). This is common ML community usage.</p><p><strong>Latency vs. Throughput (<em>Figure 1-4, Page 17</em>)</strong>:</p><ul><li><em>Single query processing</em>: Higher latency = lower throughput.</li><li><em>Batched query processing</em>: Can increase throughput, but also individual query latency (waiting for a batch to fill). This is a common trade-off.</li></ul><p><strong>Impact of Latency</strong>: Real-world examples (Akamai: 100ms delay = 7% conversion drop; Booking.com: 30% latency increase = 0.5% conversion cost; Google: >3s load time, users leave). Users are impatient!</p><p><strong>Latency is a distribution (Page 18)</strong>: Don&rsquo;t just use average! Averages hide outliers. Use percentiles:</p><ul><li><em>p50 (median)</em>: 50% of requests are faster/slower.</li><li><em>p90, p95, p99</em>: Show tail latencies. These outliers might affect your most valuable customers (e.g., Amazon customers with large purchase histories). Product requirements are often &ldquo;p99 latency &lt; X ms.&rdquo;</li></ul><p><strong>Interview Relevance</strong>: Be ready to discuss latency/throughput trade-offs, batching strategies, and how you&rsquo;d measure and set SLOs for latency (e.g., &ldquo;p95 latency for recommendations must be under 200ms&rdquo;).</p><h3 id=data-page-18-19>Data (Page 18-19):</h3><ul><li><strong>Research</strong>: Often clean, well-formatted, static benchmark datasets. Known quirks, public scripts for processing.</li><li><strong>Production</strong>: Data is MESSY!<ul><li>Noisy, unstructured, constantly shifting.</li><li>Biased (and you might not know how).</li><li>Labels can be sparse, imbalanced, incorrect.</li><li>Changing requirements mean updating labels.</li><li>Privacy and regulatory concerns (GDPR, CCPA).</li><li>Data is constantly generated by users, systems, third parties.</li></ul></li></ul><p><em>Figure 1-5 (Karpathy&rsquo;s &ldquo;Amount of sleep lost over&mldr;&rdquo; graphic)</em>: In PhD, sleep lost over models/algorithms. At Tesla (production), sleep lost over datasets. This is profoundly true.</p><p><strong>FAANG Perspective</strong>: Data engineering, data quality, data governance, and data privacy are massive efforts. Often, more engineering time is spent on the data pipeline than on the model itself.</p><h3 id=fairness-page-19-20>Fairness (Page 19-20):</h3><ul><li><strong>Research</strong>: Often an afterthought.</li><li><strong>Production</strong>: CRITICAL. ML models encode past biases from data. Deployed at scale, they can discriminate at scale.</li><li>Examples: Loan applications biased by zip code, resume ranking biased by name spelling, mortgage rates based on biased credit scores.</li><li>Cathy O&rsquo;Neil&rsquo;s &ldquo;Weapons of Math Destruction&rdquo; is a must-read.</li><li>Misclassifying minority groups might have a small impact on overall accuracy metrics but a huge impact on those individuals/groups.</li><li>McKinsey study (2019): Only 13% of large companies mitigating algorithmic bias. This is changing, but slowly. We&rsquo;ll cover Responsible AI in Chapter 11.</li></ul><p><strong>Interview Relevance</strong>: Expect questions on fairness. <em>&ldquo;How would you detect bias in your model? How would you mitigate it?&rdquo;</em> This is table stakes now.</p><h3 id=interpretability-page-20-21>Interpretability (Page 20-21):</h3><ul><li>Geoffrey Hinton&rsquo;s AI surgeon dilemma: 90% cure rate AI surgeon (black box) vs. 80% human surgeon (explainable). Who do you choose? (When the author asked execs, it was 50/50).</li><li><strong>Research</strong>: Often not incentivized if focus is pure performance.</li><li><strong>Production</strong>: Often a requirement.<ul><li><em>For users/business leaders</em>: To trust the model, detect biases.</li><li><em>For developers</em>: To debug and improve the model.</li><li><em>Legal</em>: &ldquo;Right to explanation&rdquo; in some jurisdictions (e.g., GDPR).</li></ul></li><li>2019 Stanford HAI report: Only 19% of large companies working on explainability. Again, this is improving.</li></ul><p><strong>FAANG Nuance</strong>: For critical systems (e.g., fraud, content moderation, medical), interpretability is non-negotiable. Techniques like SHAP, LIME are used, but simpler, inherently interpretable models are often preferred if performance is comparable.</p><h3 id=discussion-page-21-why-production-focus-matters>Discussion (Page 21): Why production focus matters?</h3><ul><li>Most companies can&rsquo;t afford pure research without business application.</li><li>The &ldquo;bigger, better&rdquo; ML models (e.g., large language models) require massive data and compute, often tens of millions of dollars.</li><li>As ML becomes more accessible, demand for productionizing ML grows.</li><li>The vast majority of ML-related jobs are in productionizing ML. This is why you&rsquo;re here!</li></ul><h2 id=pages-22-23-machine-learning-systems-versus-traditional-software>Pages 22-23: Machine Learning Systems Versus Traditional Software</h2><p>If ML is part of software engineering (SWE), why not just use existing SWE best practices?
Good idea! ML production would be better if ML experts were also strong software engineers. Many SWE tools are useful.</p><p>However, ML has unique challenges:</p><ul><li><strong>Code + Data + Artifacts</strong>:<ul><li><em>SWE</em>: Assumes code and data are separate. Focus on modularity.</li><li><em>ML</em>: Systems are tightly coupled: code (training scripts, inference logic), data (features, labels), and artifacts (trained models).</li><li>Trend: &ldquo;Best data wins&rdquo; over &ldquo;best algorithm.&rdquo; So, focus shifts to improving data.</li><li>Data changes quickly => ML apps need to adapt quickly => faster dev/deploy cycles.</li></ul></li><li><strong>Testing and Versioning</strong>:<ul><li><em>SWE</em>: Test and version code.</li><li><em>ML</em>: Must test and version code <em>and data</em>. Versioning large datasets is hard. How to know if a data sample is &ldquo;good&rdquo; or &ldquo;bad&rdquo;?</li><li>Not all data samples are equal: A scan of a cancerous lung is more valuable if you have 1M normal lung scans and only 1k cancerous ones.</li><li>Indiscriminate data acceptance can hurt performance or lead to data poisoning attacks (footnote 31).</li></ul></li><li><strong>Model Size</strong>:<ul><li>As of 2022, models with billions of parameters are common (e.g., LLMs). Require GBs of RAM.</li><li>This might seem quaint in the future (like the 32MB RAM of the Apollo moon computer).</li><li>Deploying large models, especially on edge devices (Chapter 7), is a massive engineering challenge.</li></ul></li><li><strong>Speed (Inference Latency)</strong>:<ul><li>How to run these large models fast enough? An autocompletion model slower than typing is useless.</li></ul></li><li><strong>Monitoring and Debugging</strong>:<ul><li>Non-trivial for complex, black-box models. Hard to know what went wrong or get alerted quickly.</li></ul></li></ul><p>The good news (Page 23): These challenges are being tackled. BERT (2018) was initially seen as too big/slow (340M params, 1.35GB). By 2020, it was in &ldquo;almost every English search on Google.&rdquo; Progress is rapid.</p><p><strong>Interview Relevance</strong>: Understanding these unique challenges of ML (data-centricity, versioning data+model, model size/latency, monitoring complexity) distinguishes a candidate who has thought about production issues from one who has only trained models in a lab.</p><h2 id=page-23-summary>Page 23: Summary</h2><p>This opening chapter gives you the lay of the land:</p><ul><li>ML is widely used, especially in enterprise.</li><li>Knowing when and when not to use ML is crucial.</li><li>ML in production is very different from ML in research (stakeholders, compute, data, fairness, interpretability).</li><li>ML systems have unique challenges compared to traditional software (data entanglement, versioning, model size, monitoring).</li></ul><p>The key theme of the book, and this cohort, is a holistic system approach. We don&rsquo;t just look at algorithms; we look at all components working together.</p></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer></body></html>