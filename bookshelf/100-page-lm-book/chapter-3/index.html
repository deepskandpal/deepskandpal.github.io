<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><a href=#what-this-chapter-is-ultimately-trying-to-achieve>What this chapter is ultimately trying to achieve</a></li><li><a href=#31-elman-rnn-simple-recurrent-neural-network>3.1 Elman RNN (Simple Recurrent Neural Network)</a></li><li><a href=#32-mini-batch-gradient-descent-revisited-for-sequences>3.2 Mini-Batch Gradient Descent (Revisited for Sequences)</a></li><li><a href=#33-programming-an-rnn-in-pytorch>3.3 Programming an RNN (in PyTorch)</a></li><li><a href=#34-rnn-as-a-language-model>3.4 RNN as a Language Model</a></li><li><a href=#35-embedding-layer-deeper-dive-with-nnembedding>3.5 Embedding Layer (Deeper Dive with <code>nn.Embedding</code>)</a></li><li><a href=#36-training-an-rnn-language-model-the-full-loop-in-pytorch>3.6 Training an RNN Language Model (The Full Loop in PyTorch)</a></li><li><a href=#37-dataset-and-dataloader-pytorch-utilities>3.7 Dataset and DataLoader (PyTorch Utilities)</a></li><li><a href=#38-training-data-and-loss-computation-for-language-modeling>3.8 Training Data and Loss Computation (for Language Modeling)</a></li></ul></li></ul></nav></nav></aside><article class=book-single><h1>Chapter 3: Recurrent Neural Network</h1><span class=reading-time><em>9 min read</em></span><div class=book-details><div class=book-content><h2 id=what-this-chapter-is-ultimately-trying-to-achieve>What this chapter is ultimately trying to achieve</h2><p>To introduce a type of neural network specifically designed to process sequences of data (like words in a sentence) one element at a time, while maintaining an internal &ldquo;memory&rdquo; or &ldquo;state&rdquo; that captures information from previous elements in the sequence. This &ldquo;memory&rdquo; allows RNNs to understand context that spans multiple tokens, which is something BoW or simple n-gram models struggle with significantly.</p><p>Let&rsquo;s break down the key concepts:</p><h2 id=31-elman-rnn-simple-recurrent-neural-network>3.1 Elman RNN (Simple Recurrent Neural Network)</h2><ul><li><p><strong>What it&rsquo;s ultimately trying to achieve:</strong>
To process a sequence of inputs (e.g., word embeddings) step-by-step, and at each step, produce an output and update an internal <strong>hidden state</strong>. This hidden state acts as a compressed summary of the sequence seen so far.</p></li><li><p><strong>The Core Idea (The Loop):</strong>
Imagine a standard neural network unit. Now, give it a loop: the output of the unit at a given time step <code>t</code> (specifically, its hidden state <code>h_t</code>) is fed back into the unit as an <em>additional</em> input at the next time step <code>t+1</code>, along with the actual next input from the sequence <code>x_{t+1}</code>.</p><ul><li><strong>Input:</strong> At each time step <code>t</code>, the RNN unit takes two things:<ol><li>The current input from the sequence, <code>x_t</code> (e.g., the embedding of the current word).</li><li>The hidden state from the <em>previous</em> time step, <code>h_{t-1}</code>.</li></ol></li><li><strong>Calculation:</strong> Inside the unit, these inputs are typically transformed by weight matrices and an activation function (often <code>tanh</code> in classic RNNs) to produce:<ol><li>The new hidden state for the current time step, <code>h_t</code>.</li><li>(Optionally) An output for the current time step, <code>y_t</code>. For language modeling, this <code>y_t</code> would be related to predicting the <em>next</em> word.</li></ol></li><li><strong>Formula (Conceptual):</strong>
<code>h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)</code> (Hidden state update)
<code>y_t = W_hy * h_t + b_y</code> (Output at time t, often passed through softmax for probabilities)
Where <code>W_hh</code>, <code>W_xh</code>, <code>W_hy</code> are weight matrices and <code>b_h</code>, <code>b_y</code> are bias terms. These weights are <em>shared</em> across all time steps, which is key to how RNNs generalize.</li></ul></li><li><p><strong>Visualizing It:</strong>
You can &ldquo;unroll&rdquo; an RNN in time. It looks like a chain of identical network units, where the hidden state from one unit is passed to the next.</p></li><li><p><strong>Realism and Challenges:</strong></p><ul><li><strong>Vanishing/Exploding Gradients:</strong> When training RNNs with backpropagation through time (BPTT), gradients can become very small (vanish) or very large (explode) as they are propagated back through many time steps. This makes it hard for simple RNNs to learn long-range dependencies (e.g., connecting a word at the beginning of a long sentence to a word at the end). ReLU helps with vanishing gradients compared to tanh/sigmoid in deep <em>feedforward</em> nets, but the recurrent nature still poses challenges. LSTMs and GRUs (which are more advanced RNN variants, not deeply covered in a 100-page book but important to know about) were developed to mitigate this.</li></ul></li></ul><h2 id=32-mini-batch-gradient-descent-revisited-for-sequences>3.2 Mini-Batch Gradient Descent (Revisited for Sequences)</h2><ul><li><p><strong>What it&rsquo;s ultimately trying to achieve:</strong>
To efficiently train RNNs (and other large models) by processing multiple sequences in parallel within each training step, rather than one sequence at a time or the entire dataset at once.</p></li><li><p><strong>The Setup for Sequences:</strong>
When we feed data to an RNN, it&rsquo;s often in the shape of <code>(batch_size, sequence_length, embedding_dimensionality)</code>.</p><ul><li><code>batch_size</code>: The number of sequences processed together.</li><li><code>sequence_length</code>: The number of tokens in each sequence (sequences are often padded to be the same length in a batch).</li><li><code>embedding_dimensionality</code>: The size of the vector representing each token.</li></ul></li><li><p><strong>Why it&rsquo;s important:</strong>
Processing batches leverages the parallel processing capabilities of modern hardware (like GPUs), making training much faster. It also provides a more stable estimate of the gradient compared to processing single examples (stochastic gradient descent).</p></li></ul><h2 id=33-programming-an-rnn-in-pytorch>3.3 Programming an RNN (in PyTorch)</h2><ul><li><p><strong>What it&rsquo;s ultimately trying to achieve:</strong>
To translate the mathematical concept of an RNN unit and a stack of RNN layers into working code.</p></li><li><p><strong>Key PyTorch Components:</strong></p><ul><li><code>nn.Module</code>: The base class for all neural network modules in PyTorch. Our RNN unit and the full RNN model will inherit from this.</li><li><code>nn.Parameter</code>: Wraps a tensor to tell PyTorch that it&rsquo;s a learnable model parameter (like the weight matrices <code>W_hh</code>, <code>W_xh</code>).</li><li>The <code>__init__</code> method: Where you define the layers and parameters of your model.</li><li>The <code>forward</code> method: Where you define how the input data flows through the layers to produce an output. For an RNN, this will involve a loop over the time steps of the input sequence.</li></ul></li><li><p><strong>Implementing the ElmanRNNUnit:</strong></p><ul><li>Initialize weight matrices (<code>Uh</code> for hidden-to-hidden, <code>Wh</code> for input-to-hidden) and a bias term (<code>b</code>).</li><li>The <code>forward</code> method takes current input <code>x</code> and previous hidden state <code>h_prev</code> and computes <code>h_new = tanh(x @ Wh + h_prev @ Uh + b)</code>.</li></ul></li><li><p><strong>Implementing the full ElmanRNN (stacking layers):</strong></p><ul><li>The <code>ElmanRNN</code> class would contain a list of <code>ElmanRNNUnit</code> instances (one for each layer).</li><li>Its <code>forward</code> method would:<ol><li>Initialize hidden states for all layers (usually to zeros).</li><li>Loop through each token (time step <code>t</code>) in the input sequences of the batch.</li><li>For each token, loop through each RNN layer:<ul><li>The input to the first layer is the token&rsquo;s embedding.</li><li>The input to subsequent layers is the hidden state from the layer below at the same time step.</li><li>Each layer updates its hidden state.</li></ul></li><li>Collect the outputs (usually the hidden states of the last layer at each time step).</li></ol></li></ul></li></ul><h2 id=34-rnn-as-a-language-model>3.4 RNN as a Language Model</h2><ul><li><p><strong>What it&rsquo;s ultimately trying to achieve:</strong>
To use the RNN architecture to perform the core language modeling task: predicting the next token in a sequence.</p></li><li><p><strong>The Architecture:</strong></p><ol><li><strong>Embedding Layer:</strong> Converts input token IDs into dense embedding vectors. This is often <code>nn.Embedding</code> in PyTorch.</li><li><strong>RNN Layers:</strong> One or more RNN layers (like our <code>ElmanRNN</code>) process the sequence of embeddings and output a sequence of hidden states (usually from the final RNN layer).</li><li><strong>Output (Linear) Layer / Classification Head:</strong> A fully connected linear layer takes the RNN&rsquo;s hidden state output at each time step <code>t</code> and transforms it into a vector of logits, where the size of this vector is the vocabulary size.</li><li><strong>Softmax (implicitly with CrossEntropyLoss):</strong> These logits are then (conceptually, often combined within the loss function) passed through a softmax function to get probabilities for each word in the vocabulary being the <em>next</em> word.</li></ol></li><li><p><strong>Training:</strong></p><ul><li><strong>Input Sequence:</strong> A sequence of token IDs, e.g., <code>[token_A, token_B, token_C]</code>.</li><li><strong>Target Sequence:</strong> The input sequence shifted by one position, e.g., <code>[token_B, token_C, token_D]</code>.</li><li>At each time step <code>t</code>, the model processes <code>input_token_t</code> and its goal is to output a high probability for <code>target_token_t</code>.</li><li>The <strong>cross-entropy loss</strong> is calculated between the predicted probability distribution and the actual target token at each position, and then averaged.</li></ul></li></ul><h2 id=35-embedding-layer-deeper-dive-with-nnembedding>3.5 Embedding Layer (Deeper Dive with <code>nn.Embedding</code>)</h2><ul><li><p><strong>What it&rsquo;s ultimately trying to achieve:</strong>
To provide a learnable lookup table that maps discrete token indices (integers) to dense, continuous-valued embedding vectors.</p></li><li><p><strong>How it works in PyTorch (<code>nn.Embedding</code>):</strong></p><ul><li>When you create <code>nn.Embedding(vocab_size, emb_dim)</code>, PyTorch initializes a weight matrix of shape <code>(vocab_size, emb_dim)</code> with random values. Each row <code>i</code> of this matrix is the embedding vector for token ID <code>i</code>.</li><li>When you pass a tensor of token IDs to this layer, it simply looks up and returns the corresponding rows (embedding vectors).</li><li>These embedding vectors are <strong>learnable parameters</strong>. During training, gradients flow back to them, and they get updated to better represent the tokens for the given task.</li><li><code>padding_idx</code>: You can specify an index to be treated as a padding token. The embedding for this token will be a zero vector and (importantly) will not be updated during training.</li></ul></li></ul><h2 id=36-training-an-rnn-language-model-the-full-loop-in-pytorch>3.6 Training an RNN Language Model (The Full Loop in PyTorch)</h2><ul><li><p><strong>What it&rsquo;s ultimately trying to achieve:</strong>
To put all the pieces together – data preparation, model instantiation, loss function, optimizer, and the training loop – to actually train an RNN LM.</p></li><li><p><strong>Key Steps in the Training Loop (per epoch, per batch):</strong></p><ol><li><code>model.train()</code>: Set the model to training mode.</li><li>Get <code>input_seq</code> and <code>target_seq</code> from the <code>DataLoader</code>.</li><li>Move data to the correct device (CPU/GPU).</li><li><code>optimizer.zero_grad()</code>: Clear old gradients.</li><li><code>outputs = model(input_seq)</code>: Forward pass to get logits.</li><li>Reshape <code>outputs</code> and <code>target_seq</code> so that the loss can be computed across all tokens in the batch efficiently. Typically, this means flattening them so that each row corresponds to a single token prediction:<ul><li><code>outputs</code> becomes <code>(batch_size * seq_len, vocab_size)</code></li><li><code>target_seq</code> becomes <code>(batch_size * seq_len)</code></li></ul></li><li><code>loss = criterion(outputs, target_seq)</code>: Calculate the cross-entropy loss. Remember <code>nn.CrossEntropyLoss</code> in PyTorch expects raw logits and handles the softmax internally. It also allows an <code>ignore_index</code> parameter, which is crucial for not calculating loss on padding tokens in the <code>target_seq</code>.</li><li><code>loss.backward()</code>: Backward pass to compute gradients.</li><li><code>optimizer.step()</code>: Update model parameters.</li></ol></li><li><p><strong>Reproducibility:</strong> Setting seeds (<code>random.seed()</code>, <code>torch.manual_seed()</code>, <code>torch.cuda.manual_seed_all()</code>) is important for consistent results, especially when debugging or comparing experiments.</p></li></ul><h2 id=37-dataset-and-dataloader-pytorch-utilities>3.7 Dataset and DataLoader (PyTorch Utilities)</h2><ul><li><strong>What they are ultimately trying to achieve:</strong>
To provide a standardized and efficient way to load, preprocess, and iterate over data in batches during training.<ul><li><strong><code>Dataset</code>:</strong> An abstract class representing your dataset. You need to implement:<ul><li><code>__init__(self, ...)</code>: Load/prepare your data (e.g., read from file, tokenize).</li><li><code>__len__(self)</code>: Return the total number of samples in the dataset.</li><li><code>__getitem__(self, idx)</code>: Return the <code>idx</code>-th sample (e.g., an input sequence and its corresponding target sequence, as tensors).</li></ul></li><li><strong><code>DataLoader</code>:</strong> Wraps a <code>Dataset</code> and provides an iterator to loop over the data in batches. It handles:<ul><li>Batching.</li><li>Shuffling (optional, but good for training).</li><li>Parallel data loading using multiple worker processes (optional, for speed).</li></ul></li></ul></li></ul><h2 id=38-training-data-and-loss-computation-for-language-modeling>3.8 Training Data and Loss Computation (for Language Modeling)</h2><ul><li><p><strong>What it&rsquo;s ultimately trying to achieve:</strong>
To clarify exactly how input and target sequences are structured for training an autoregressive language model, and how the loss is computed across all positions.</p></li><li><p><strong>The &ldquo;Shifted&rdquo; Target:</strong>
For an input sequence like <code>[T1, T2, T3, T4]</code>, the target sequence is <code>[T2, T3, T4, T5]</code>.</p><ul><li>When the model sees <code>T1</code>, it tries to predict <code>T2</code>.</li><li>When it sees <code>T1, T2</code>, it tries to predict <code>T3</code>.</li><li>And so on. The hidden state <code>h_t</code> carries context from <code>T1...T_t</code> to help predict <code>T_{t+1}</code>.</li></ul></li><li><p><strong>Loss Calculation:</strong>
The cross-entropy loss is calculated at <em>each position</em> where a prediction is made. The total loss for a sequence is typically the average of these per-position losses. When batching, it&rsquo;s the average loss over all predictable tokens in the batch.</p></li></ul></div></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script></body></html>