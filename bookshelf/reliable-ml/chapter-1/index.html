<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul></nav></header><main><article class=book-single><h1>Chapter 1: Introduction</h1><div class=book-details><div class=book-content><h1 id=chapter-1-introduction---reliable-ml-engineering-notes>Chapter 1: Introduction - Reliable ML Engineering Notes</h1><h2 id=core-concept-the-ml-loop>Core Concept: The ML Loop</h2><ul><li><strong>ML is Iterative, Not Linear:</strong> ML applications are rarely &ldquo;done.&rdquo; They exist in a continuous cycle of development, deployment, evaluation, and improvement.</li><li><strong>Why a Loop?</strong><ul><li><strong>If a model underperforms:</strong> Teams (DS, Business, MLE) collaborate to improve it (features, data, architecture).</li><li><strong>If a model performs well:</strong> Organizations want more sophistication and broader application, leading to further development.</li></ul></li><li><strong>The first model is just a starting point.</strong></li></ul><h2 id=the-ml-lifecycle-stages-the-pit-stops-in-the-loop>The ML Lifecycle Stages (The &ldquo;Pit Stops&rdquo; in the Loop)</h2><p><strong>(See Figure 1-1 for visual representation)</strong><figure><img src=/bookshelf/reliable-ml/image.png alt=image width=750></figure></p><h3 id=1-data-collection-and-analysis>1. Data Collection and Analysis</h3><ul><li><strong>Goal:</strong> Understand available data, identify needs, prioritize uses, collect, and process.</li><li><strong>Involves:</strong> Business/Product (for priorities), Data Engineers (pipelines), SREs (reliability of pipelines), DS/MLEs (data utility).</li><li><strong>Key:</strong> Proper data management is foundational. Business context drives data needs.</li></ul><h3 id=2-ml-training-pipelines>2. ML Training Pipelines</h3><ul><li><strong>Goal:</strong> Consume processed data and produce trained models using ML algorithms.</li><li><strong>Involves:</strong> Data Engineers, DS, MLEs, SREs.</li><li><strong>Tech:</strong> TensorFlow, PyTorch, XGBoost, etc.</li><li><strong>Critical:</strong> <strong>Training pipelines are production systems.</strong> Treat with rigor.</li><li><strong>Common Failures:</strong> Data issues (lack, format), bugs, misconfigurations, resource shortages, hardware/distributed system failures.</li><li><strong>Silent Failures:</strong> ML models can also fail due to subtle issues like data distribution shifts.</li></ul><h3 id=3-build-and-validate-applications>3. Build and Validate Applications</h3><ul><li><strong>Goal:</strong> Integrate the model into a customer-facing system to deliver value.</li><li><strong>Involves:</strong> Product/Business (specs), MLEs/SDEs (implementation), QA (oversight).</li><li><strong>Action:</strong> Log user interactions and model outputs for future improvement.</li></ul><h3 id=4-quality-and-performance-evaluation>4. Quality and Performance Evaluation</h3><ul><li><strong>Goal:</strong> Assess if the model &ldquo;works&rdquo; and how well, <em>before and during initial launch</em>.</li><li><strong>Methods:</strong><ul><li><strong>Offline Evaluation:</strong> Test on historical/curated data.</li><li><strong>Live Launch:</strong> Model sees live traffic (monitor closely).</li><li><strong>Dark Launch:</strong> Model sees live traffic, logs predictions, but doesn&rsquo;t affect user experience (tests integration).</li><li><strong>Fractional Launch (Canary/A/B):</strong> Model exposed to a subset of users (tests quality and integration).</li></ul></li><li><strong>Purpose:</strong> Gain confidence for wider rollout, establish baselines. It&rsquo;s a <strong>validation checkpoint.</strong></li></ul><h3 id=5-defining-and-measuring-slos-service-level-objectives>5. Defining and Measuring SLOs (Service-Level Objectives)</h3><ul><li><strong>Goal:</strong> Define and track thresholds (SLIs - Indicators) for system performance according to requirements.</li><li><strong>Involves:</strong> SREs, PMs, DS, MLEs, SDEs.</li><li><strong>Challenge in ML:</strong> Subtle data/world changes can degrade ML performance significantly.</li><li><strong>Types of SLOs:</strong><ul><li><strong>System:</strong> Latency, error rates, throughput (serving & training).</li><li><strong>Application:</strong> # of recommendations, successful model calls.</li><li><strong>ML Performance/Business:</strong> Click-through rates, revenue from model (often sliced by user segments).</li></ul></li><li><strong>Key:</strong> Business must define tolerable SLOs.</li></ul><h3 id=6-launch>6. Launch</h3><ul><li><strong>Goal:</strong> Ship the ML-enhanced application to users.</li><li><strong>Involves:</strong> Product SDEs, MLEs, SREs.</li><li><strong>ML-Specific Concerns:</strong><ul><li><strong>Models as Code:</strong> New models can break systems like bad code.</li><li><strong>Launch Slowly:</strong> Progressive rollouts to limit damage.</li><li><strong>Isolate Rollouts at Data Layer:</strong> Critical to avoid data format incompatibilities during rollbacks (see &ldquo;Progressive Rollouts in a Stateful System&rdquo; story).</li><li><strong>Release, Not Refactor:</strong> Minimize changes during a release.</li><li><strong>Measure SLOs During Launch:</strong> Monitor dashboards.</li><li><strong>Review the Rollout:</strong> Manual or automated oversight.</li></ul></li></ul><h3 id=7-monitoring-and-feedback-loops>7. Monitoring and Feedback Loops</h3><ul><li><strong>Goal:</strong> <em>Continuously observe</em> system health and effectiveness <em>post-launch</em>, and gather data for future improvements.</li><li><strong>Signals to Monitor:</strong><ul><li><strong>System Health (Golden Signals):</strong> Latency, traffic, errors, saturation.</li><li><strong>Basic Model Health:</strong> Model size, load errors (context-free checks).</li><li><strong>Model Quality (Domain-Specific):</strong> Business metrics (CTR, conversion), performance drift over time. Hardest but most crucial.</li></ul></li><li><strong>Feedback Collection:</strong> Log user interactions, predictions, and context to fuel the <em>next cycle</em>.</li><li><strong>Purpose:</strong> Ensure sustained performance, detect issues, provide input for the <strong>next iteration.</strong></li></ul><h2 id=key-differences-evaluation-vs-monitoring>Key Differences: Evaluation vs. Monitoring</h2><ul><li><strong>Quality & Performance Evaluation:</strong> A <em>checkpoint</em> before/during initial launch to validate and decide on wider rollout.</li><li><strong>Monitoring & Feedback Loops:</strong> <em>Continuous vigilance</em> post-launch to maintain health, detect drift, and gather data for <em>future</em> iterations.</li></ul><h2 id=overall-lessons-from-the-loop>Overall Lessons from the Loop</h2><ul><li><strong>Data is King:</strong> ML begins and ends with data.</li><li><strong>Cyclical Process:</strong> No single order; stages are revisited.</li><li><strong>Holistic View Required:</strong> Understand the entire loop and organization.</li><li><strong>Risk & Experimentation:</strong> Not all ML ideas work. Approach as continual experimentation.</li><li><strong>Organizational Readiness Matters.</strong></li></ul></div></div></article></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer></body></html>