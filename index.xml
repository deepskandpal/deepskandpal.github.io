<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>404EngineerNotFound</title>
    <link>http://localhost:65022/</link>
    <description>Recent content on 404EngineerNotFound</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 Jun 2025 14:01:29 +0530</lastBuildDate>
    <atom:link href="http://localhost:65022/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Chapter 1: Introduction</title>
      <link>http://localhost:65022/bookshelf/reliable-ml/chapter-1/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/reliable-ml/chapter-1/</guid>
      <description>&lt;h1 id=&#34;chapter-1-introduction---reliable-ml-engineering-notes&#34;&gt;Chapter 1: Introduction - Reliable ML Engineering Notes&lt;/h1&gt;&#xA;&lt;h2 id=&#34;core-concept-the-ml-loop&#34;&gt;Core Concept: The ML Loop&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ML is Iterative, Not Linear:&lt;/strong&gt; ML applications are rarely &amp;ldquo;done.&amp;rdquo; They exist in a continuous cycle of development, deployment, evaluation, and improvement.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Why a Loop?&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;If a model underperforms:&lt;/strong&gt; Teams (DS, Business, MLE) collaborate to improve it (features, data, architecture).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;If a model performs well:&lt;/strong&gt; Organizations want more sophistication and broader application, leading to further development.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;The first model is just a starting point.&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;the-ml-lifecycle-stages-the-pit-stops-in-the-loop&#34;&gt;The ML Lifecycle Stages (The &amp;ldquo;Pit Stops&amp;rdquo; in the Loop)&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;(See Figure 1-1 for visual representation)&lt;/strong&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;http://localhost:65022/bookshelf/reliable-ml/image.png&#34;&#xA;    alt=&#34;image&#34; width=&#34;750&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 1: Overview of Machine Learning Systems</title>
      <link>http://localhost:65022/bookshelf/design-ml-system/chapter-1/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/design-ml-system/chapter-1/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://aistudio.google.com/prompts/19Kbu6lIbeuASE0ER7Hc3puDMRZYY6_Z4&#34;&gt;Prompt&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The chapter opens with a great example: Google Translate&amp;rsquo;s multilingual neural machine translation system back in 2016. This was a landmark – deep learning making a massive, tangible impact at scale. It showed the world the power of modern ML.&lt;/p&gt;&#xA;&lt;p&gt;Since then, as the book says, ML has &amp;ldquo;found its way into almost every aspect of our lives.&amp;rdquo; This is true. At FAANG, we see this daily – from the recommendations you get, to how your photos are enhanced, to how we optimize our data centers.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 1: The Machine Learning Landscape</title>
      <link>http://localhost:65022/bookshelf/hands-on-ml/chapter-1/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hands-on-ml/chapter-1/</guid>
      <description>&lt;h1 id=&#34;notes-for-chapter-1&#34;&gt;Notes for Chapter 1&lt;/h1&gt;&#xA;&lt;p&gt;When you hear &amp;ldquo;Machine Learning,&amp;rdquo; what pops into your head? Robots? Terminators? Maybe a friendly butler? The book nails it – it&amp;rsquo;s not just sci-fi; it&amp;rsquo;s already here. Think about the &lt;strong&gt;spam filter&lt;/strong&gt;. That was one of the first really big ML applications that touched millions. It learned, from examples of spam and non-spam (or &amp;ldquo;ham,&amp;rdquo; as we call it), to tell the difference. And it got so good that we barely notice it anymore. That&amp;rsquo;s the hallmark of good ML – it just works.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 5: Arrays</title>
      <link>http://localhost:65022/bookshelf/elements-of-programming/chapter-1/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/elements-of-programming/chapter-1/</guid>
      <description>&lt;h1 id=&#34;chapter-5-overview&#34;&gt;Chapter 5 Overview:&lt;/h1&gt;&#xA;&lt;p&gt;This chapter covers the basic properties of arrays (specifically Python lists), common manipulation techniques, and classic array-based problems. It heavily emphasizes thinking about space complexity and optimizing operations.&lt;/p&gt;&#xA;&lt;h2 id=&#34;think-of-arrays-like-this&#34;&gt;Think of arrays like this:&lt;/h2&gt;&#xA;&lt;p&gt;Imagine a row of numbered boxes, right next to each other in memory.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Contiguous&lt;/strong&gt;: The boxes are physically adjacent. This is crucial for performance.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Indexed&lt;/strong&gt;: You can instantly jump to any box if you know its number (index). This is O(1) access.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Homogeneous (usually)&lt;/strong&gt;: Typically, all boxes hold the same type of item (like all integers or all strings), though Python lists offer more flexibility.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;why-arrays-are-important-especially-for-interviews&#34;&gt;Why Arrays Are Important (Especially for Interviews):&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: Direct access by index is super fast (O(1)).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Memory Locality&lt;/strong&gt;: Because elements are stored together, accessing sequential elements is often cache-friendly, leading to good practical performance.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Building Blocks&lt;/strong&gt;: Many other data structures (like hash maps, heaps, stacks, queues) are often &lt;em&gt;implemented&lt;/em&gt; using arrays underneath.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;In-Place Operations&lt;/strong&gt;: Interviewers love problems where you modify the array directly without using significant extra memory (O(1) space). This often involves clever use of pointers or swapping elements.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;array-boot-camp-reordering-array-entries-page-1&#34;&gt;Array Boot Camp: Reordering Array Entries (Page 1)&lt;/h2&gt;&#xA;&lt;p&gt;The boot camp problem is a fantastic introduction to &lt;strong&gt;in-place array manipulation using multiple pointers&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 2: End-to-End Machine Learning Project</title>
      <link>http://localhost:65022/bookshelf/hands-on-ml/chapter-2/</link>
      <pubDate>Thu, 22 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hands-on-ml/chapter-2/</guid>
      <description>&lt;h1 id=&#34;notes-for-chapter-2&#34;&gt;Notes for Chapter 2&lt;/h1&gt;&#xA;&lt;p&gt;Alright class, buckle up! Last time, we got a bird&amp;rsquo;s-eye view of the Machine Learning landscape – the different continents, climates, and major landmarks. Today, with Chapter 2, &amp;ldquo;End-to-End Machine Learning Project,&amp;rdquo; we&amp;rsquo;re grabbing our hiking boots and actually trekking through one of these landscapes. This is where the rubber meets the road!&lt;/p&gt;&#xA;&lt;p&gt;The book says we&amp;rsquo;re pretending to be a recently hired data scientist at a real estate company. Our mission? To predict median housing prices in California. This chapter is &lt;em&gt;fantastic&lt;/em&gt; because it walks you through the practical steps. It’s less about the theory of one specific algorithm and more about the &lt;em&gt;process&lt;/em&gt; of doing ML in the real world.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 2: Introduction to Machine Learning Systems Design</title>
      <link>http://localhost:65022/bookshelf/design-ml-system/chapter-2/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/design-ml-system/chapter-2/</guid>
      <description>&lt;p&gt;This chapter will guide us through:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Objectives&lt;/strong&gt;: Why are we building this system in the first place?&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: What qualities must our system possess? (Think reliability, scalability).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Iterative Process&lt;/strong&gt;: Spoiler: building ML systems is rarely a straight line.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Problem Framing&lt;/strong&gt;: How do we turn a vague business problem into a concrete ML task? This is crucial.&lt;/li&gt;&#xA;&lt;li&gt;And finally, a bit of a philosophical discussion: &lt;strong&gt;Data vs. Algorithms&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;You&amp;rsquo;ll see a recurring theme: &lt;em&gt;&amp;ldquo;Not so soon!&amp;rdquo;&lt;/em&gt; Before you jump to coding a model, there&amp;rsquo;s a lot of critical thinking and planning. This upfront work is what separates a successful, production-ready ML system from a science project. This chapter is packed with concepts that will come up in ML system design interviews.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 3: Classification</title>
      <link>http://localhost:65022/bookshelf/hands-on-ml/chapter-3/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hands-on-ml/chapter-3/</guid>
      <description>&lt;p&gt;This chapter is all about systems that predict a &lt;em&gt;category&lt;/em&gt; or a &lt;em&gt;class&lt;/em&gt; – Is this email spam or not? Is this image a cat or a dog? Is this handwritten digit a &amp;lsquo;5&amp;rsquo; or a &amp;lsquo;3&amp;rsquo;?&lt;/p&gt;&#xA;&lt;p&gt;And speaking of handwritten digits, we&amp;rsquo;re going to be working with a very famous dataset: &lt;strong&gt;MNIST&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It&amp;rsquo;s a set of 70,000 small images of digits (0-9) handwritten by high school students and US Census Bureau employees.&lt;/li&gt;&#xA;&lt;li&gt;Each image is labeled with the digit it represents.&lt;/li&gt;&#xA;&lt;li&gt;The book calls it the &lt;strong&gt;&amp;ldquo;hello world&amp;rdquo; of Machine Learning&lt;/strong&gt; because it&amp;rsquo;s a go-to dataset for testing new classification algorithms. Everyone who learns ML eventually plays with MNIST. It’s like a rite of passage!&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Scikit-Learn makes it super easy to fetch popular datasets like MNIST. The code shows:&#xA;&lt;code&gt;from sklearn.datasets import fetch_openml&lt;/code&gt;&#xA;&lt;code&gt;mnist = fetch_openml(&#39;mnist_784&#39;, version=1)&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 3: Data Engineering Fundamentals</title>
      <link>http://localhost:65022/bookshelf/design-ml-system/chapter-3/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/design-ml-system/chapter-3/</guid>
      <description>&lt;p&gt;As the book says, &amp;ldquo;The rise of ML in recent years is tightly coupled with the rise of big data.&amp;rdquo; At FAANG, this is our daily reality. Our ML systems, from recommendation engines to fraud detection, are built on vast, complex data landscapes. If you don&amp;rsquo;t have a solid grasp of data engineering, you&amp;rsquo;re going to struggle, no matter how fancy your model architecture is.&lt;/p&gt;&#xA;&lt;p&gt;This chapter is dense with terminology and concepts that might seem overwhelming if you&amp;rsquo;re new to large-scale data systems. But don&amp;rsquo;t worry, we&amp;rsquo;ll break it down. The goal here is to give you a &amp;ldquo;steady piece of land to stand on.&amp;rdquo;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 4: Training Data</title>
      <link>http://localhost:65022/bookshelf/design-ml-system/chapter-4/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/design-ml-system/chapter-4/</guid>
      <description>&lt;p&gt;Okay, class, let&amp;rsquo;s settle in. We&amp;rsquo;ve journeyed through the high-level overview of ML systems, the design process, and the nitty-gritty of data engineering fundamentals. Now, we arrive at a topic that is, in many ways, the heart of supervised machine learning: &lt;strong&gt;Chapter 4: Training Data.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;The author makes a critical point right at the start: ML curricula often skew heavily towards modeling – the &amp;ldquo;fun&amp;rdquo; part. But as anyone who&amp;rsquo;s worked in production ML at FAANG or elsewhere knows, &amp;ldquo;spending days wrangling with a massive amount of malformatted data that doesn’t even fit into your machine’s memory is frustrating&amp;rdquo; but &lt;em&gt;essential&lt;/em&gt;. Bad data can sink your entire ML operation, no matter how brilliant your model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 4: Training Models</title>
      <link>http://localhost:65022/bookshelf/hands-on-ml/chapter-4/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hands-on-ml/chapter-4/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Chapter 4: Training Models&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(Page 111: Introduction - Beyond Black Boxes)&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Up until now, as the book says, we&amp;rsquo;ve treated ML models and their training algorithms mostly like black boxes. We fed them data, they gave us results, and we learned to evaluate those results. You&amp;rsquo;ve optimized regression, improved classifiers, even built a spam filter, often without peeking under the hood. And that&amp;rsquo;s okay for many practical purposes!&lt;/p&gt;&#xA;&lt;p&gt;However, understanding &lt;em&gt;how&lt;/em&gt; these things work internally is incredibly powerful. It helps you:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 5: Support Vector Machines</title>
      <link>http://localhost:65022/bookshelf/hands-on-ml/chapter-5/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hands-on-ml/chapter-5/</guid>
      <description>&lt;p&gt;Alright class, let&amp;rsquo;s gear up for &lt;strong&gt;Chapter 5: Support Vector Machines (SVMs)&lt;/strong&gt;. This is a big one! SVMs are incredibly powerful and versatile models. You&amp;rsquo;ll find them used for linear or nonlinear classification, regression, and even outlier detection. They are a cornerstone of classical machine learning, and definitely a tool everyone interested in ML should have in their arsenal.&lt;/p&gt;&#xA;&lt;p&gt;The book mentions they&amp;rsquo;re particularly good for &lt;strong&gt;complex small- or medium-sized datasets.&lt;/strong&gt; This chapter will walk us through their core concepts, how to use them, and, importantly for our &amp;ldquo;what it&amp;rsquo;s ultimately trying to achieve&amp;rdquo; philosophy, how they actually work.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 6: Decision Trees</title>
      <link>http://localhost:65022/bookshelf/hands-on-ml/chapter-6/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hands-on-ml/chapter-6/</guid>
      <description>&lt;h1 id=&#34;introduction-to-decision-trees&#34;&gt;Introduction to Decision Trees&lt;/h1&gt;&#xA;&lt;p&gt;Alright class, settle in! After exploring the world of Support Vector Machines, we&amp;rsquo;re now turning our attention to another incredibly versatile and powerful family of algorithms: &lt;strong&gt;Decision Trees&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Versatility:&lt;/strong&gt; Like SVMs, Decision Trees can perform both &lt;strong&gt;classification&lt;/strong&gt; and &lt;strong&gt;regression&lt;/strong&gt; tasks. They can even handle &lt;strong&gt;multioutput tasks&lt;/strong&gt; (where each instance can have multiple output labels or values).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Power:&lt;/strong&gt; They are capable of fitting complex datasets. You might recall from Chapter 2, when we looked at the California housing data, a &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; was able to fit the training data perfectly (though, as we noted, it was actually overfitting).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Fundamental Building Blocks:&lt;/strong&gt; Decision Trees are also the core components of &lt;strong&gt;Random Forests&lt;/strong&gt; (which we&amp;rsquo;ll see in Chapter 7). Random Forests are among the most powerful and widely used ML algorithms today. So, understanding Decision Trees is crucial for understanding Random Forests.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;What this chapter will cover:&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 6: Statistics</title>
      <link>http://localhost:65022/bookshelf/201-ds-interview/chapter-6/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/201-ds-interview/chapter-6/</guid>
      <description>&lt;p&gt;Statistics is a core component of any data scientist&amp;rsquo;s toolkit. Since many commercial layers of a data science pipeline are built from statistical foundations (for example, A/B testing), knowing foundational topics of statistics is essential.&lt;/p&gt;&#xA;&lt;p&gt;Interviewers love to test a candidate&amp;rsquo;s knowledge about the statistic basics, starting with topics like the Central Limit Theorem and the Law of Large Numbers, and then progressing on to the concepts underlying hypothesis-testing, particularly p-values and confidence intervals, as well as Type 1 and Type II error, and their interpretations. All of those topics play an important role in the statistical underpinning of A/B testing. Additionally, derivations and manipulations involving random variables of various probability distributions are also common, particularly in finance interviews. Lastly, a common topic in more technical interviews will involve utilizing MLE and/or MAP.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 10: Introduction to Artificial Neural Networks</title>
      <link>http://localhost:65022/bookshelf/hands-on-ml/chapter-10/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hands-on-ml/chapter-10/</guid>
      <description>&lt;h1 id=&#34;introduction---inspiration-from-nature&#34;&gt;Introduction - Inspiration from Nature&lt;/h1&gt;&#xA;&lt;p&gt;The chapter beautifully starts by reminding us how nature has often inspired human inventions: birds inspired planes, burdock plants inspired Velcro. So, it&amp;rsquo;s logical to look at the brain&amp;rsquo;s architecture for inspiration on building intelligent machines. This is the core idea that sparked ANNs.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;ANNs vs. Biological Neurons:&lt;/strong&gt; While ANNs were inspired by the networks of biological neurons in our brains, they have evolved to be quite different. Just like planes don&amp;rsquo;t flap their wings, ANNs don&amp;rsquo;t need to be biologically perfectly realistic to be effective. The footnote mentions a good philosophy: be open to biological inspiration but don&amp;rsquo;t be afraid to create biologically &lt;em&gt;unrealistic&lt;/em&gt; models if they work well. Some researchers even prefer calling the components &amp;ldquo;units&amp;rdquo; rather than &amp;ldquo;neurons&amp;rdquo; to avoid this restrictive analogy.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 11: Training Deep Neural Networks</title>
      <link>http://localhost:65022/bookshelf/hands-on-ml/chapter-11/</link>
      <pubDate>Wed, 21 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hands-on-ml/chapter-11/</guid>
      <description>&lt;p&gt;Okay, great! We&amp;rsquo;ve built a solid conceptual understanding of Artificial Neural Networks and how to approach their design and hyperparameter tuning, without getting bogged down in the specific Keras/TensorFlow code from the later parts of Chapter 10.&lt;/p&gt;&#xA;&lt;p&gt;Now, we&amp;rsquo;re ready to tackle &lt;strong&gt;Chapter 11: Training Deep Neural Networks&lt;/strong&gt;. This chapter addresses the practical challenges and advanced techniques that arise when you start working with &lt;em&gt;deep&lt;/em&gt; networks – those with many layers.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapter 15: Recursion</title>
      <link>http://localhost:65022/bookshelf/elements-of-programming/chapter-15/</link>
      <pubDate>Fri, 23 Feb 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/elements-of-programming/chapter-15/</guid>
      <description>&lt;h1 id=&#34;recursion&#34;&gt;Recursion&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Core Idea of Recursion&lt;/strong&gt;: At its simplest, recursion means a function calls itself to solve a smaller version of the same problem.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://aistudio.google.com/prompts/1iT6e8TzouWLkMJqnlyvEmF3SBLGpkJJr&#34;&gt;Prompt Link&lt;/a&gt;(my private discussion for this chapter using ai studio and this chapters main system prompt along with the pdf of this chapter)&lt;/p&gt;&#xA;&lt;p&gt;Two Key Ingredients :&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Base Cases&lt;/strong&gt;: These are the simplest instances of the problem that the function can solve directly, without further recursion.&#xA;Intuition: &amp;ldquo;When do I know the answer without asking for more help?&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Progress (Recursive Call with different arguments):&lt;/strong&gt; The function must call itself with arguments that move it closer to a base case. If it doesn&amp;rsquo;t make progress, it&amp;rsquo;ll loop forever.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Recursion: The &amp;ldquo;Ask a Mini-Me&amp;rdquo; Approach:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hard work vs Risk</title>
      <link>http://localhost:65022/thoughts/hardwork-risk/</link>
      <pubDate>Sun, 08 Jun 2025 14:01:29 +0530</pubDate>
      <guid>http://localhost:65022/thoughts/hardwork-risk/</guid>
      <description></description>
    </item>
    <item>
      <title>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title>
      <link>http://localhost:65022/papershelf/illusion-of-thinking/</link>
      <pubDate>Sun, 08 Jun 2025 01:01:23 +0530</pubDate>
      <guid>http://localhost:65022/papershelf/illusion-of-thinking/</guid>
      <description>&lt;p&gt;At its heart, &amp;ldquo;The Illusion of Thinking,&amp;rdquo; is about trying to genuinely understand how well these new &amp;ldquo;Large Reasoning Models&amp;rdquo; (LRMs) actually &lt;em&gt;reason&lt;/em&gt;. You&amp;rsquo;ve probably heard about models that show their &amp;ldquo;thinking steps&amp;rdquo; before giving an answer, like with Chain-of-Thought. They often do better on benchmarks, which is exciting. But we felt that just looking at the final answer on standard math or coding tests wasn&amp;rsquo;t telling the whole story.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bollywood needs a hard reset</title>
      <link>http://localhost:65022/thoughts/bollywood/</link>
      <pubDate>Sat, 07 Jun 2025 15:39:28 +0530</pubDate>
      <guid>http://localhost:65022/thoughts/bollywood/</guid>
      <description>&lt;p&gt;So I watched or rather tried to watch this weeks movie releases. I had 2 options one was House full 5 and the other was a few weeks old but new to OTT bhul chuk maaf. Now I personally never found any housefull movie funny not even the first one. Except that one dialogue by Ritesh Deshmukh regarding &amp;ldquo;Biwi&amp;rdquo; and &amp;ldquo;Behen&amp;rdquo; in the first one I have mostly cringed. Pardon me if my sense of humour is not keeping up with the times but I grew up watching comedies of Priya Darshan and Akshay kumar. Raju hirani, and too some extent I find Rohit shetty older films also funny with Chennai Express being my favorite from his Filmography. So I would think of myself as someone who under stands what &amp;ldquo;Brain rot comedy&amp;rdquo; is in Hindi Cinema context. I don&amp;rsquo;t really understand why we have moved on to accept absolutely mediocre films with sub par production/acting/writing/production-design/music/dance in the name of &amp;ldquo;oH bUt iTs tImE pASS dON&amp;rsquo;t tHinK tOO mUch&amp;rdquo; . If i pay money for something I am liable for getting what is expected in return. Let me draw a parallel. If you go to a restaurant to have creamy shahi paneer and the waiter serves you the Now known to everyone &amp;ldquo;Analogue&amp;rdquo; Paneer. Which we all know is not real paneer and also bad for your health but it tastes almost like an original paneer. Would you accept my argument  &amp;ldquo;But bro itne paise mai aisa hi paneer milega restaurant ko bhi toh paisa banane hai&amp;rdquo;. Won&amp;rsquo;t you feel cheated that in the name of paneer garbage was fed to you? ( a seemingly paneer looking garbage but still gargbage)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding the difficulty of training deep feedforward neural networks</title>
      <link>http://localhost:65022/papershelf/understand-training-diff-deep-nn/</link>
      <pubDate>Fri, 06 Jun 2025 16:11:01 +0530</pubDate>
      <guid>http://localhost:65022/papershelf/understand-training-diff-deep-nn/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;&#xA;&lt;p&gt;(Your summary and notes about the paper go here)&lt;/p&gt;</description>
    </item>
    <item>
      <title>A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 – LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY</title>
      <link>http://localhost:65022/papershelf/disciplined-nn/</link>
      <pubDate>Fri, 06 Jun 2025 15:57:54 +0530</pubDate>
      <guid>http://localhost:65022/papershelf/disciplined-nn/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;&#xA;&lt;p&gt;(Your summary and notes about the paper go here)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Don&#39;T Lose Your Mind, Lose Your Weight</title>
      <link>http://localhost:65022/bookshelf/lose-weight/</link>
      <pubDate>Thu, 05 Jun 2025 15:24:47 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/lose-weight/</guid>
      <description>&lt;h1 id=&#34;notes--summary-for-the-book&#34;&gt;Notes / Summary for the Book&lt;/h1&gt;&#xA;&lt;p&gt;(Your general notes or summary about the book go here.&#xA;For chapter-specific notes, create separate .md files within this book&amp;rsquo;s folder, e.g., chapter-1.md, introduction.md etc.&#xA;These will be automatically listed on the book&amp;rsquo;s page.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multi-Head Attention: Collaborate Instead of Concatenate</title>
      <link>http://localhost:65022/papershelf/multihead-attention/</link>
      <pubDate>Wed, 04 Jun 2025 23:24:48 +0530</pubDate>
      <guid>http://localhost:65022/papershelf/multihead-attention/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;&#xA;&lt;p&gt;(Your summary and notes about the paper go here)&lt;/p&gt;</description>
    </item>
    <item>
      <title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title>
      <link>http://localhost:65022/papershelf/flash-attention/</link>
      <pubDate>Wed, 04 Jun 2025 23:24:37 +0530</pubDate>
      <guid>http://localhost:65022/papershelf/flash-attention/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;&#xA;&lt;p&gt;(Your summary and notes about the paper go here)&lt;/p&gt;</description>
    </item>
    <item>
      <title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
      <link>http://localhost:65022/papershelf/bert/</link>
      <pubDate>Wed, 04 Jun 2025 23:24:31 +0530</pubDate>
      <guid>http://localhost:65022/papershelf/bert/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;&#xA;&lt;p&gt;(Your summary and notes about the paper go here)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Improving Language Understanding by Generative Pre-Training</title>
      <link>http://localhost:65022/papershelf/gpt-1/</link>
      <pubDate>Tue, 03 Jun 2025 11:01:51 +0530</pubDate>
      <guid>http://localhost:65022/papershelf/gpt-1/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;&#xA;&lt;p&gt;(Your summary and notes about the paper go here)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fabric of Reality</title>
      <link>http://localhost:65022/bookshelf/fabric-of-reality/</link>
      <pubDate>Sun, 01 Jun 2025 21:59:55 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/fabric-of-reality/</guid>
      <description>&lt;h1 id=&#34;notes--summary-for-the-book&#34;&gt;Notes / Summary for the Book&lt;/h1&gt;&#xA;&lt;p&gt;(Your general notes or summary about the book go here.&#xA;For chapter-specific notes, create separate .md files within this book&amp;rsquo;s folder, e.g., chapter-1.md, introduction.md etc.&#xA;These will be automatically listed on the book&amp;rsquo;s page.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Thinking in Systems</title>
      <link>http://localhost:65022/bookshelf/thinking-systems/</link>
      <pubDate>Sun, 01 Jun 2025 21:56:09 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/thinking-systems/</guid>
      <description>&lt;h1 id=&#34;notes--summary-for-the-book&#34;&gt;Notes / Summary for the Book&lt;/h1&gt;&#xA;&lt;p&gt;(Your general notes or summary about the book go here.&#xA;For chapter-specific notes, create separate .md files within this book&amp;rsquo;s folder, e.g., chapter-1.md, introduction.md etc.&#xA;These will be automatically listed on the book&amp;rsquo;s page.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Creativity Inc</title>
      <link>http://localhost:65022/bookshelf/creativity-inc/</link>
      <pubDate>Sun, 01 Jun 2025 21:47:14 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/creativity-inc/</guid>
      <description>&lt;h1 id=&#34;notes--summary-for-the-book&#34;&gt;Notes / Summary for the Book&lt;/h1&gt;&#xA;&lt;p&gt;(Your general notes or summary about the book go here.&#xA;For chapter-specific notes, create separate .md files within this book&amp;rsquo;s folder, e.g., chapter-1.md, introduction.md etc.&#xA;These will be automatically listed on the book&amp;rsquo;s page.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Elements of Statistical Learning</title>
      <link>http://localhost:65022/bookshelf/elements/</link>
      <pubDate>Sun, 01 Jun 2025 16:53:23 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/elements/</guid>
      <description>&lt;h1 id=&#34;notes--summary-for-the-book&#34;&gt;Notes / Summary for the Book&lt;/h1&gt;&#xA;&lt;p&gt;(Your general notes or summary about the book go here.&#xA;For chapter-specific notes, create separate .md files within this book&amp;rsquo;s folder, e.g., chapter-1.md, introduction.md etc.&#xA;These will be automatically listed on the book&amp;rsquo;s page.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Everything is connected</title>
      <link>http://localhost:65022/thoughts/everything/</link>
      <pubDate>Sat, 31 May 2025 18:57:30 +0530</pubDate>
      <guid>http://localhost:65022/thoughts/everything/</guid>
      <description>&lt;p&gt;Coming from india we are privy of a very uncommon notion that science and arts are interlinked. They have always have been and they always will be. Now I am not saying that its an india only phenomenon the western world struggled with this too. For a very long time science had a hard tussle with religion which was the bearer of Arts back then. But if you look closely almost all western scientists had&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Done Right</title>
      <link>http://localhost:65022/bookshelf/linear-done-right/</link>
      <pubDate>Thu, 29 May 2025 21:45:19 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/linear-done-right/</guid>
      <description>&lt;h1 id=&#34;to-be-read&#34;&gt;TO BE READ&lt;/h1&gt;</description>
    </item>
    <item>
      <title>Machine Learning: A Probabilistic Perspective</title>
      <link>http://localhost:65022/bookshelf/machine-learning-prob-approach/</link>
      <pubDate>Thu, 29 May 2025 21:45:19 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/machine-learning-prob-approach/</guid>
      <description>&lt;p&gt;To be Read&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Algorithms, fourth edition </title>
      <link>http://localhost:65022/bookshelf/lsr/</link>
      <pubDate>Wed, 28 May 2025 13:05:07 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/lsr/</guid>
      <description>&lt;h1 id=&#34;notes--summary-for-the-book&#34;&gt;Notes / Summary for the Book&lt;/h1&gt;&#xA;&lt;p&gt;(Your general notes or summary about the book go here.&#xA;For chapter-specific notes, create separate .md files within this book&amp;rsquo;s folder, e.g., chapter-1.md, introduction.md etc.&#xA;These will be automatically listed on the book&amp;rsquo;s page.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Design Data Intensive</title>
      <link>http://localhost:65022/bookshelf/design-data-intensive/</link>
      <pubDate>Tue, 27 May 2025 11:18:35 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/design-data-intensive/</guid>
      <description>&lt;h1 id=&#34;notes--summary-for-the-book&#34;&gt;Notes / Summary for the Book&lt;/h1&gt;&#xA;&lt;p&gt;(Your general notes or summary about the book go here.&#xA;For chapter-specific notes, create separate .md files within this book&amp;rsquo;s folder, e.g., chapter-1.md, introduction.md etc.&#xA;These will be automatically listed on the book&amp;rsquo;s page.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Applied Predictive</title>
      <link>http://localhost:65022/bookshelf/applied-predictive/</link>
      <pubDate>Tue, 27 May 2025 10:05:28 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/applied-predictive/</guid>
      <description>&lt;h1 id=&#34;notes--summary-for-the-book&#34;&gt;Notes / Summary for the Book&lt;/h1&gt;&#xA;&lt;p&gt;(Your general notes or summary about the book go here.&#xA;For chapter-specific notes, create separate .md files within this book&amp;rsquo;s folder, e.g., chapter-1.md, introduction.md etc.&#xA;These will be automatically listed on the book&amp;rsquo;s page.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical Test</title>
      <link>http://localhost:65022/bookshelf/statistical-test/</link>
      <pubDate>Mon, 26 May 2025 21:55:39 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/statistical-test/</guid>
      <description>&lt;h1 id=&#34;notes--summary-for-the-book&#34;&gt;Notes / Summary for the Book&lt;/h1&gt;&#xA;&lt;p&gt;(Your general notes or summary about the book go here.&#xA;For chapter-specific notes, create separate .md files within this book&amp;rsquo;s folder, e.g., chapter-1.md, introduction.md etc.&#xA;These will be automatically listed on the book&amp;rsquo;s page.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Marketing Analytics</title>
      <link>http://localhost:65022/bookshelf/marketing-analytics/</link>
      <pubDate>Mon, 26 May 2025 21:41:54 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/marketing-analytics/</guid>
      <description>&lt;h1 id=&#34;notes--summary-for-the-book&#34;&gt;Notes / Summary for the Book&lt;/h1&gt;&#xA;&lt;p&gt;(Your general notes or summary about the book go here.&#xA;For chapter-specific notes, create separate .md files within this book&amp;rsquo;s folder, e.g., chapter-1.md, introduction.md etc.&#xA;These will be automatically listed on the book&amp;rsquo;s page.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Copyright: right to intellectual property or Tool for Extortation?</title>
      <link>http://localhost:65022/thoughts/copyright/</link>
      <pubDate>Mon, 26 May 2025 15:56:18 +0530</pubDate>
      <guid>http://localhost:65022/thoughts/copyright/</guid>
      <description>&lt;p&gt;In Indian Twitter (currently known as X) lingo, any kind of verbal skirmish is termed as e-lafda (fight). The latest one is between YouTuber Mohan Mangal and ANI over the use of copyrighted clips. Mohak claims that he has used 5-10 seconds of clips in a few of his recent videos from ANI coverage regarding recent events such as Operation Sindoor in his videos covering the same, and ANI has applied a copyright strike on his YouTube channel because of this. His claim is that when he contacted ANI for the reason and the removal of the copyright strike, he was informed that it would only happen if he pays somewhere in the range of 40-50 lakhs + GST to ANI for the use of those clips in his videos (they claim they have applied for 8 copyright strikes, so there are more to come). Failure to do so will result in getting 3 strikes (2 of which he has received already) that will lead to an automatic channel deletion as per YouTube policy.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Jitter Bug - How a Little Randomness Makes Your Recommendations Smarter (and More Fun!)</title>
      <link>http://localhost:65022/tech-writings/jitter-bug/</link>
      <pubDate>Sun, 25 May 2025 19:02:40 +0530</pubDate>
      <guid>http://localhost:65022/tech-writings/jitter-bug/</guid>
      <description>&lt;h2 id=&#34;the-tyranny-of-the-top-rank&#34;&gt;The Tyranny of the Top Rank&lt;/h2&gt;&#xA;&lt;p&gt;Ever feel like your favorite music app is playing &lt;em&gt;DJ Déjà Vu&lt;/em&gt;, stuck on an endless loop of your top-played genres or artists you already know and love? Or, if you&amp;rsquo;re an artist, have you wondered how your fresh, amazing track can cut through the noise and find new ears? This isn&amp;rsquo;t just a feeling; it&amp;rsquo;s a common challenge in any system that ranks and recommends items – whether it&amp;rsquo;s songs, movies, products, or even internal company resources.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Polyster Prince</title>
      <link>http://localhost:65022/bookshelf/polyster-prince/</link>
      <pubDate>Fri, 23 May 2025 13:35:05 +0530</pubDate>
      <guid>http://localhost:65022/bookshelf/polyster-prince/</guid>
      <description>&lt;h1 id=&#34;notes--summary-for-the-book&#34;&gt;Notes / Summary for the Book&lt;/h1&gt;</description>
    </item>
    <item>
      <title>Strings</title>
      <link>http://localhost:65022/dsa-concepts/strings/</link>
      <pubDate>Thu, 15 May 2025 20:30:18 +0530</pubDate>
      <guid>http://localhost:65022/dsa-concepts/strings/</guid>
      <description></description>
    </item>
    <item>
      <title>Dutch National Flag</title>
      <link>http://localhost:65022/dsa-log/dutch-national-flag/</link>
      <pubDate>Wed, 14 May 2025 20:04:51 +0530</pubDate>
      <guid>http://localhost:65022/dsa-log/dutch-national-flag/</guid>
      <description>&lt;h2 id=&#34;problem-description&#34;&gt;Problem Description&lt;/h2&gt;&#xA;&lt;p&gt;Given an array A and an index pivot_idx, rearrange A such that all elements less than &lt;code&gt;A[pivot_idx]&lt;/code&gt; (the pivot value) come first, followed by all elements equal to the pivot, and finally all elements greater than the pivot. This must be done in-place.&lt;/p&gt;&#xA;&lt;h2 id=&#34;solution-approach&#34;&gt;Solution Approach&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Let &#xA;A  = [0, 1, 2, 0, 2, 1, 1]&#xA;pivot_idx = 1&#xA;pivot_value = A[pivot_idx] = 1&#xA;We maintain three pointers (our “fingers”):&#xA;&#xA;smaller: end of the “LESS THAN pivot” zone.&#xA;&#xA;Anything to its left is &amp;lt; pivot_value.&#xA;&#xA;Starts at 0.&#xA;&#xA;equal: the current “inspector” pointer.&#xA;&#xA;Points to the element under examination.&#xA;&#xA;Starts at 0.&#xA;&#xA;larger: start of the “GREATER THAN pivot” zone (from the right end).&#xA;&#xA;Anything at or to its right is &amp;gt; pivot_value.&#xA;&#xA;Starts at len(A) (one past the end).&#xA;&#xA;&#xA;A[0...smaller-1]   = LESS zone      (empty)&#xA;A[smaller...equal-1] = EQUAL zone    (empty)&#xA;A[equal...larger-1]  = UNCLASSIFIED  (the whole array)&#xA;A[larger...end]    = GREATER zone    (empty)&#xA;&#xA;We loop while equal &amp;lt; larger:&#xA;&#xA;while equal &amp;lt; larger:&#xA;    if A[equal] &amp;lt; pivot_value:&#xA;        swap(A[smaller], A[equal])&#xA;        smaller += 1&#xA;        equal += 1&#xA;&#xA;    elif A[equal] == pivot_value:&#xA;        # Already in the EQUAL zone&#xA;        equal += 1&#xA;&#xA;    else:  # A[equal] &amp;gt; pivot_value&#xA;        larger -= 1&#xA;        swap(A[equal], A[larger])&#xA;        # do not increment `equal`!&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Step-by-Step Walkthrough&lt;/p&gt;</description>
    </item>
    <item>
      <title>Operation Sindoor</title>
      <link>http://localhost:65022/thoughts/operation-sindoor/</link>
      <pubDate>Wed, 14 May 2025 12:01:26 +0530</pubDate>
      <guid>http://localhost:65022/thoughts/operation-sindoor/</guid>
      <description></description>
    </item>
    <item>
      <title>System Prompts that I found usefull</title>
      <link>http://localhost:65022/tech-writings/prompts/</link>
      <pubDate>Sat, 03 May 2025 22:38:57 +0530</pubDate>
      <guid>http://localhost:65022/tech-writings/prompts/</guid>
      <description>&lt;p&gt;Below is the list of system prompts i use for different tasks with gemini 2.5 pro model&lt;/p&gt;&#xA;&lt;h1 id=&#34;for-learning-dsa&#34;&gt;For learning DSA&lt;/h1&gt;&#xA;&lt;p&gt;&lt;code&gt;you are an expert DSA expert who specializes in teaching how to track coding problems you have written books and specializes in coaching those students who run away from DSA. Your ability to boil down even complex problems and concepts into very simple intuitive first principals based explanation makes you the best in the trade . You are starting a new course based on the book elements of programming interview in python. You will cover each chapters core topics and also cover each section and the core problems in that chapter. &lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sequential Test and Adaptive Experimental Design</title>
      <link>http://localhost:65022/papershelf/msprt/</link>
      <pubDate>Sun, 27 Apr 2025 19:30:00 +0000</pubDate>
      <guid>http://localhost:65022/papershelf/msprt/</guid>
      <description></description>
    </item>
    <item>
      <title>The Matrix Calculus You Need For Deep Learning</title>
      <link>http://localhost:65022/papershelf/maths/</link>
      <pubDate>Sun, 27 Apr 2025 19:30:00 +0000</pubDate>
      <guid>http://localhost:65022/papershelf/maths/</guid>
      <description>&lt;h1 id=&#34;the-big-picture-goal&#34;&gt;&lt;strong&gt;The Big Picture Goal:&lt;/strong&gt;&lt;/h1&gt;&#xA;&lt;p&gt;We want our machine learning models to make good predictions. To do this, we define a &lt;strong&gt;loss function&lt;/strong&gt; (or cost function) that measures how &amp;ldquo;bad&amp;rdquo; our model&amp;rsquo;s predictions are compared to the true values.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Training = Minimizing Loss:&lt;/strong&gt; Training a model means finding the model parameters (like weights &lt;code&gt;w&lt;/code&gt; and biases &lt;code&gt;b&lt;/code&gt;) that make this loss function as small as possible.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Calculus as a Tool for Minimization:&lt;/strong&gt; Derivatives (and their extensions like gradients) tell us the &lt;strong&gt;rate of change&lt;/strong&gt; or the &lt;strong&gt;slope&lt;/strong&gt; of a function.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If we know the slope of our loss function with respect to our model parameters, we know which &amp;ldquo;direction&amp;rdquo; to tweak those parameters to &lt;em&gt;decrease&lt;/em&gt; the loss. This is the essence of &lt;strong&gt;Gradient Descent&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;So, all this calculus is ultimately about finding an efficient way to &amp;ldquo;walk downhill&amp;rdquo; on our loss function surface to find the parameter values that give the lowest error.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:65022/stories/ek-kahani-aisibhi/</link>
      <pubDate>Fri, 25 Apr 2025 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/stories/ek-kahani-aisibhi/</guid>
      <description></description>
    </item>
    <item>
      <title>Multi Processing , Multi Threading, AsyncIO: A Guide to Python Concurrency for Data Scientists</title>
      <link>http://localhost:65022/tech-writings/concurrency-ds/</link>
      <pubDate>Fri, 25 Apr 2025 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/tech-writings/concurrency-ds/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The problem : Help! My Python SDXL Script Isn&amp;rsquo;t Faster with Asyncio/Threading/Multiprocessing. Why?&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;You&amp;rsquo;ve built a cool script, maybe generating image variations with SDXL (&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#34;&gt;Stable Diffusion XL model&lt;/a&gt;) like one of our engineers. It works, but it&amp;rsquo;s slow. You think, &amp;ldquo;I know! Parallelism!&amp;rdquo; You try asyncio, then multithreading, maybe even multiprocessing. But&amp;hellip; nothing speeds up significantly, or you just hit weird errors, especially in your Jupyter Notebook. Sounds familiar?&lt;/p&gt;&#xA;&lt;p&gt;This is a common hurdle when data science tasks meet heavier computation. Let&amp;rsquo;s demystify Python&amp;rsquo;s asyncio, multithreading, and multiprocessing, touching on the underlying Operating System (OS) ideas and Python&amp;rsquo;s infamous GIL.&lt;/p&gt;</description>
    </item>
    <item>
      <title>About</title>
      <link>http://localhost:65022/about/</link>
      <pubDate>Mon, 14 Apr 2025 22:29:36 +0530</pubDate>
      <guid>http://localhost:65022/about/</guid>
      <description>&lt;p&gt;Hi—I’m Deepanshu Kandpal. Over the past eight years I’ve jumped between e‑commerce, telecom, consulting and cybersecurity, always chasing that moment when clever code becomes real‑world impact. Right now at KnowBe4 I’m building LLM‑powered agents that sniff out threats and automate responses at scale. Before that at Farfetch I helped grow an A/B testing platform to handle 100K+ queries a day across 300 TB of data—learning firsthand how to coordinate agile teams across continents. On the side, I tinker with LangChef, my open‑source framework for LLM experimentation. I actually got my start as an iOS developer at Jio, crafting AI assistants that now handle over 3 million queries daily and take a huge load off call‑center teams.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Current Diet Plan</title>
      <link>http://localhost:65022/fitness-log/diet-plan/</link>
      <pubDate>Thu, 01 Aug 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/fitness-log/diet-plan/</guid>
      <description>&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Day&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Meal Time&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Meal Description&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Notes / Katori Guide&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Day 1 (e.g., Monday)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Early Morning (Pre-Workout)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 Banana OR Small handful of almonds (5-6)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Breakfast&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;3-4 Egg Omelette (with onions, tomatoes, capsicum, spinach) + 1 Whole Wheat Roti OR 1 slice Whole Wheat Toast&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Mid-Morning Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 MK Sprouts Salad (moong, chana) with lemon juice &amp;amp; chaat masala&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Lunch&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 LK Chicken Curry (focus on chicken pieces) OR 1 LK Paneer Bhurji/Curry + 1 LK Mixed Vegetable Sabzi (e.g., Gobi, Carrot, Peas) + 1 MK Cooked Brown Rice OR 2 Whole Wheat Rotis + Generous Green Salad&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml, MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Evening Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 Apple OR Pear + Small handful of Roasted Chana (unsalted)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Dinner&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 LK Baked/Grilled Fish OR 1 LK Tofu Stir-fry with plenty of vegetables + 1 MK Dal (e.g., Moong Dal) + 1 LK Lauki/Bhindi Sabzi + 1 Whole Wheat Roti (optional)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml, MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Day 2 (e.g., Tuesday)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Early Morning (Pre-Workout)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Small sweet potato (boiled) OR 1 small apple&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Breakfast&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;2 Moong Dal Chillas OR Besan Chillas (medium size) stuffed with 1 SK grated paneer/vegetables + 1 SK Mint-coriander chutney (no sugar)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;SK: ~110ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Mid-Morning Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 Orange OR 1 MK Papaya&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Lunch&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 LK Rajma Curry OR Chana Masala + 1 LK Palak Sabzi (spinach preparation) + 1 MK Quinoa OR 2 Millets (Bajra/Jowar) Rotis + Generous Green Salad&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml, MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Evening Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1-2 Boiled Eggs (if you eat eggs) OR Handful of peanuts (approx. 20-25)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Dinner&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 LK Paneer Tikka (grilled/baked) OR 1 LK Soy Chunk Curry + 1 MK Vegetable Raita (plain yogurt) OR Large mixed bean salad + 1 LK Baingan Bharta OR Cabbage Sabzi&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml, MK: ~150ml. &lt;em&gt;Avoid sugary/mayo-based Raita.&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Day 3 (e.g., Wednesday)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Breakfast&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 MK Vegetable Poha (with peas, carrots, onions) + 1 SK roasted peanuts sprinkled on top + 1 Glass Unsweetened Lassi OR Buttermilk (Chaas)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MK: ~150ml, SK: ~110ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Mid-Morning Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 Guava OR 1 Pomegranate&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Lunch&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 LK Mixed Dal (e.g., Toor + Masoor) + 1 LK Methi Aloo Sabzi (more methi, less aloo) + 1 MK Cooked Rice OR 2 Whole Wheat Rotis + Generous Green Salad&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml, MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Evening Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 MK Fruit Salad (mixed fruits)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Dinner&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Vegetable Clear Soup (large bowl) + 1 LK Mushroom &amp;amp; Capsicum Sabzi OR Stir-fried Mixed Vegetables + 1-2 Besan or Moong Dal Chillas (plain or minimal veggie stuffing)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Day 4 (e.g., Thursday)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Early Morning (Pre-Workout)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Black coffee + 4-5 Almonds &amp;amp; 1-2 Walnuts&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Breakfast&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;3-Egg Scramble with 1 SK chopped chicken/turkey ham (if non-veg) or tofu/paneer cubes + 1 Whole Wheat Roti OR 1 slice Whole Wheat Toast&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;SK: ~110ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Mid-Morning Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 MK Steamed Edamame (if available) OR 1 MK Sprouts&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Lunch&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 LK Fish Curry OR 1 LK Matar Paneer (Peas &amp;amp; Paneer) + 1 LK Thoran/Poriyal (South Indian dry veg) + 1 MK Brown Rice OR 2 Rotis + Generous Salad&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml, MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Evening Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 Apple with 1 tablespoon Peanut Butter (unsweetened)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Dinner&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 LK Chicken Stir-fry with lots of colorful vegetables OR 1 LK Paneer &amp;amp; Bell Pepper Stir-fry + Small portion (1 SK) of Quinoa or Brown Rice (optional) + Large bowl Clear Broth&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml, SK: ~110ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Day 5 (e.g., Friday)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Early Morning (Pre-Workout)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 small Banana + Black Coffee&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Breakfast&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 LK Paneer Bhurji + 2 slices Whole Wheat Toast OR 2 small Uthappams (with lots of onion/tomato topping)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Mid-Morning Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 Pear OR handful of Grapes&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Lunch&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 LK Mutton Curry (lean pieces, less oil) OR 1 LK Lobia (Black-eyed peas) Curry + 1 LK Bhindi (Okra) Sabzi + 1 MK Rice OR 2 Rotis + Generous Salad&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml, MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Evening Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Handful of Mixed Nuts &amp;amp; Seeds (almonds, walnuts, pumpkin seeds)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Dinner&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Grilled Chicken Breast (1-2 pieces) OR Large portion (1 LK) of Grilled Paneer/Tofu steaks + 1 LK Steamed or Sautéed Mixed Vegetables + Salad with lemon-herb dressing&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Day 6 (e.g., Saturday)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Breakfast&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Vegetable Idli (3-4) with 1 MK Sambhar (focus on dal &amp;amp; veggies) + Minimal coconut chutney&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Mid-Morning Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 MK Watermelon or Muskmelon&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Lunch&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;em&gt;Mindful Meal Out/Relaxed Meal:&lt;/em&gt; E.g., Tandoori Chicken/Fish (2-3 pieces) with Salad OR Veggie &amp;amp; Paneer Kathi Roll (whole wheat, less sauce)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;em&gt;Avoid creamy curries, large biryanis, naan.&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Evening Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 MK Roasted Makhana (Fox Nuts)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Dinner&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;1 LK Dal Tadka (Yellow Dal) + 1 LK Gajar Matar Sabzi (Carrot &amp;amp; Peas) + 2 Whole Wheat Rotis + Side Salad&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Day 7 (e.g., Sunday)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Breakfast&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Paneer Paratha (1-2, less oil, more filling) OR Aloo Paratha (1-2) with 1 SK plain low-fat yogurt (optional) or pickle.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;SK: ~110ml. &lt;em&gt;If Aloo Paratha, add protein like leftover dal/egg.&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Mid-Morning Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Your choice of 1 Fruit&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Lunch&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Home-style Chicken Biryani (1 LK, focus on chicken, less rice) with 1 MK Raita (if using yogurt) OR Large Salad. Veg: Veg Biryani/Pulao (1 LK) with 1 MK Dal or Mixed Veg Raita.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LK: ~240ml, MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Evening Snack&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Sweet Potato Chaat (1 MK, boiled sweet potato, chaat masala, lemon)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;MK: ~150ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Dinner&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Keep it light: Large Bowl of Mixed Vegetable &amp;amp; Lentil Soup + 1-2 slices Whole Wheat Toast with 1 SK hummus (optional)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;SK: ~110ml&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;&lt;strong&gt;General Reminders for the Diet Plan:&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Current Workout Plan</title>
      <link>http://localhost:65022/fitness-log/workout-plan/</link>
      <pubDate>Thu, 01 Aug 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/fitness-log/workout-plan/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Important Notes Before You Start:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Warm-up:&lt;/strong&gt; Always perform a 5-10 minute warm-up before each workout (light cardio, dynamic stretches).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Cool-down:&lt;/strong&gt; Always perform a 5-10 minute cool-down after each workout (static stretching).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Weight Column:&lt;/strong&gt; This column is for &lt;em&gt;you&lt;/em&gt; to fill in each week. Start with a weight that challenges you to complete the target reps with &lt;em&gt;good form&lt;/em&gt;. If you can easily complete the highest number of reps listed (e.g., 12 reps) for all sets, increase the weight slightly the following week. &lt;em&gt;Form is always more important than the amount of weight.&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Progression (Weeks 1-8):&lt;/strong&gt; Focus on &lt;strong&gt;Progressive Overload&lt;/strong&gt;. Gradually increase weight, reps, sets, or intensity, or decrease rest time.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Listen to Your Body:&lt;/strong&gt; Stop if you feel pain. Take extra rest days if needed.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;&lt;strong&gt;Workout Plan: Weeks 1-8&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Two Sum</title>
      <link>http://localhost:65022/dsa-log/two-sum/</link>
      <pubDate>Wed, 01 May 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/dsa-log/two-sum/</guid>
      <description>&lt;h2 id=&#34;problem-description-optional-summary&#34;&gt;Problem Description (Optional Summary)&lt;/h2&gt;&#xA;&lt;p&gt;Given an array of integers &lt;code&gt;nums&lt;/code&gt; and an integer &lt;code&gt;target&lt;/code&gt;, return indices of the two numbers such that they add up to &lt;code&gt;target&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;solution-approach&#34;&gt;Solution Approach&lt;/h2&gt;&#xA;&lt;p&gt;Use a hash map (dictionary in Python) to store numbers encountered so far and their indices. For each number, check if &lt;code&gt;target - current_number&lt;/code&gt; exists in the hash map. If it does, we found the pair. Otherwise, add the current number and its index to the map.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Arrays</title>
      <link>http://localhost:65022/dsa-concepts/arrays/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:65022/dsa-concepts/arrays/</guid>
      <description>&lt;h2 id=&#34;introduction-to-arrays&#34;&gt;Introduction to Arrays&lt;/h2&gt;&#xA;&lt;p&gt;An array is a fundamental data structure used to store a collection of elements, typically of the same data type, in contiguous memory locations. Each element is identified by an index or a key.&lt;/p&gt;&#xA;&lt;h2 id=&#34;key-characteristics&#34;&gt;Key Characteristics&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Fixed Size (Static Arrays):&lt;/strong&gt; In many languages, traditional arrays have a fixed size defined at creation.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Dynamic Size (Dynamic Arrays/Lists):&lt;/strong&gt; Languages like Python offer dynamic arrays (lists) that can grow or shrink.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Contiguous Memory:&lt;/strong&gt; Elements are stored next to each other, allowing for efficient index-based access.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;O(1) Access:&lt;/strong&gt; Accessing an element by its index is typically a constant time operation.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;O(n) Insertion/Deletion (Worst Case):&lt;/strong&gt; Inserting or deleting elements in the middle may require shifting subsequent elements.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;common-operations--complexity&#34;&gt;Common Operations &amp;amp; Complexity&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Access (by index): O(1)&lt;/li&gt;&#xA;&lt;li&gt;Search (linear): O(n)&lt;/li&gt;&#xA;&lt;li&gt;Insertion (at end, amortized for dynamic): O(1)&lt;/li&gt;&#xA;&lt;li&gt;Insertion (at beginning/middle): O(n)&lt;/li&gt;&#xA;&lt;li&gt;Deletion (at end): O(1)&lt;/li&gt;&#xA;&lt;li&gt;Deletion (at beginning/middle): O(n)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;use-cases&#34;&gt;Use Cases&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Storing lists of items.&lt;/li&gt;&#xA;&lt;li&gt;Implementing other data structures (stacks, queues).&lt;/li&gt;&#xA;&lt;li&gt;Lookup tables (when used with indices).&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Hashing</title>
      <link>http://localhost:65022/dsa-concepts/hashing/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:65022/dsa-concepts/hashing/</guid>
      <description>&lt;h2 id=&#34;introduction-to-hashing&#34;&gt;Introduction to Hashing&lt;/h2&gt;&#xA;&lt;p&gt;Hashing is the process of converting an input item (key) into a fixed-size value, typically an integer index, using a hash function. This index is then used to place or locate the item in a data structure, most commonly a hash table (hash map or dictionary).&lt;/p&gt;&#xA;&lt;h2 id=&#34;key-concepts&#34;&gt;Key Concepts&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Hash Function:&lt;/strong&gt; A function that maps keys to indices. A good hash function should be fast to compute and distribute keys uniformly across the available indices.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Hash Table:&lt;/strong&gt; A data structure that uses a hash function to map keys to values for efficient lookups.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Collisions:&lt;/strong&gt; Occur when two different keys map to the same index.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Collision Resolution:&lt;/strong&gt; Strategies to handle collisions, such as:&#xA;*   &lt;strong&gt;Separate Chaining:&lt;/strong&gt; Each index points to a linked list (or other structure) containing all keys that hash to that index.&#xA;*   &lt;strong&gt;Open Addressing (Probing):&lt;/strong&gt; If an index is occupied, probe for the next available slot (linear probing, quadratic probing, double hashing).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;common-operations--complexity-average-case-for-hash-tables&#34;&gt;Common Operations &amp;amp; Complexity (Average Case for Hash Tables)&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Insertion: O(1)&lt;/li&gt;&#xA;&lt;li&gt;Deletion: O(1)&lt;/li&gt;&#xA;&lt;li&gt;Search: O(1)&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;(Worst Case for all can be O(n) if collisions are poorly handled or hash function is bad)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;use-cases&#34;&gt;Use Cases&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Implementing dictionaries/hash maps.&lt;/li&gt;&#xA;&lt;li&gt;Database indexing.&lt;/li&gt;&#xA;&lt;li&gt;Caching.&lt;/li&gt;&#xA;&lt;li&gt;Checking for duplicates.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Linked Lists</title>
      <link>http://localhost:65022/dsa-concepts/linked-lists/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:65022/dsa-concepts/linked-lists/</guid>
      <description></description>
    </item>
    <item>
      <title>Attention Is All You Need</title>
      <link>http://localhost:65022/papershelf/attention-all-u-need/</link>
      <pubDate>Sat, 27 Apr 2024 19:30:00 +0000</pubDate>
      <guid>http://localhost:65022/papershelf/attention-all-u-need/</guid>
      <description></description>
    </item>
    <item>
      <title>Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing</title>
      <link>http://localhost:65022/papershelf/infinit-retrevial-attention/</link>
      <pubDate>Sat, 27 Apr 2024 19:30:00 +0000</pubDate>
      <guid>http://localhost:65022/papershelf/infinit-retrevial-attention/</guid>
      <description></description>
    </item>
    <item>
      <title>One-Minute Video Generation with Test-Time Training</title>
      <link>http://localhost:65022/papershelf/ttt-layer-for-video-generation/</link>
      <pubDate>Mon, 15 Apr 2024 19:30:00 +0000</pubDate>
      <guid>http://localhost:65022/papershelf/ttt-layer-for-video-generation/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://test-time-training.github.io/video-dit/&#34;&gt;Website&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;tldr&#34;&gt;TLDR;&lt;/h1&gt;&#xA;&lt;p&gt;They tackled long video generation by replacing expensive global attention with efficient local attention, and bridging the gaps between local segments using novel TTT layers. These TTT layers act like RNNs but have a much smarter, adaptive hidden state (a neural network that learns on-the-fly during generation). This allows them to capture long-range dependencies and complex dynamics better than traditional RNNs, leading to more coherent minute-long videos, albeit with some remaining artifacts and efficiency challenges.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Animal GPT</title>
      <link>http://localhost:65022/creations/animal-gpt/</link>
      <pubDate>Wed, 20 Mar 2024 14:00:00 +0000</pubDate>
      <guid>http://localhost:65022/creations/animal-gpt/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;&#xA;&lt;p&gt;This is a brief summary of the example project, highlighting its main purpose or key feature. It&amp;rsquo;s used on the card display.&lt;/p&gt;&#xA;&lt;h2 id=&#34;details&#34;&gt;Details&lt;/h2&gt;&#xA;&lt;p&gt;Here you can write a full description of the project.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Explain the goals.&lt;/li&gt;&#xA;&lt;li&gt;Describe the technologies used.&lt;/li&gt;&#xA;&lt;li&gt;Discuss challenges faced and solutions implemented.&lt;/li&gt;&#xA;&lt;li&gt;Include code snippets or further images if relevant using Markdown.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-go-html&#34; data-lang=&#34;go-html&#34;&gt;{{/* Example code snippet */}}&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Hands-On Large Language Models: Language Understanding and Generation </title>
      <link>http://localhost:65022/bookshelf/hands-on-llm/</link>
      <pubDate>Sun, 10 Mar 2024 12:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hands-on-llm/</guid>
      <description></description>
    </item>
    <item>
      <title>The Hundred-Page Language Models Book</title>
      <link>http://localhost:65022/bookshelf/the-lm-book/</link>
      <pubDate>Sun, 10 Mar 2024 12:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/the-lm-book/</guid>
      <description></description>
    </item>
    <item>
      <title>Harry Potter and the Chamber of Secrets</title>
      <link>http://localhost:65022/bookshelf/hp-2/</link>
      <pubDate>Thu, 22 Feb 2024 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hp-2/</guid>
      <description></description>
    </item>
    <item>
      <title>Harry Potter and the Deathly Hallows</title>
      <link>http://localhost:65022/bookshelf/hp-7/</link>
      <pubDate>Thu, 22 Feb 2024 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hp-7/</guid>
      <description></description>
    </item>
    <item>
      <title>Harry Potter and the Half-Blood Prince</title>
      <link>http://localhost:65022/bookshelf/hp-6/</link>
      <pubDate>Thu, 22 Feb 2024 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hp-6/</guid>
      <description></description>
    </item>
    <item>
      <title>Harry Potter and the Order of the Phoenix</title>
      <link>http://localhost:65022/bookshelf/hp-5/</link>
      <pubDate>Thu, 22 Feb 2024 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hp-5/</guid>
      <description></description>
    </item>
    <item>
      <title>Harry Potter and the Prisoner of Azkaban</title>
      <link>http://localhost:65022/bookshelf/hp-3/</link>
      <pubDate>Thu, 22 Feb 2024 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hp-3/</guid>
      <description></description>
    </item>
    <item>
      <title>Harry Potter and the Prisoner of Azkaban</title>
      <link>http://localhost:65022/bookshelf/hp-4/</link>
      <pubDate>Thu, 22 Feb 2024 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hp-4/</guid>
      <description></description>
    </item>
    <item>
      <title>Elon Musk: Tesla, SpaceX, and the Quest for a Fantastic Future</title>
      <link>http://localhost:65022/bookshelf/elon-musk/</link>
      <pubDate>Tue, 20 Feb 2024 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/elon-musk/</guid>
      <description></description>
    </item>
    <item>
      <title>Harry Potter and the Philosopher&#39;s StoneS</title>
      <link>http://localhost:65022/bookshelf/hp-1/</link>
      <pubDate>Tue, 20 Feb 2024 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/hp-1/</guid>
      <description></description>
    </item>
    <item>
      <title>Shoe Dog</title>
      <link>http://localhost:65022/bookshelf/shoe-dog/</link>
      <pubDate>Tue, 20 Feb 2024 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/shoe-dog/</guid>
      <description></description>
    </item>
    <item>
      <title>STEVE JOBS</title>
      <link>http://localhost:65022/bookshelf/steve-jobs/</link>
      <pubDate>Tue, 20 Feb 2024 11:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/steve-jobs/</guid>
      <description></description>
    </item>
    <item>
      <title>Accelerating India&#39;s Development: A State-Led Roadmap for Effective Governance</title>
      <link>http://localhost:65022/bookshelf/accelerating-india/</link>
      <pubDate>Mon, 15 Jan 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/accelerating-india/</guid>
      <description></description>
    </item>
    <item>
      <title>Good To Great</title>
      <link>http://localhost:65022/bookshelf/good-to-great/</link>
      <pubDate>Mon, 15 Jan 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/good-to-great/</guid>
      <description></description>
    </item>
    <item>
      <title>Interpretable Machine Learning</title>
      <link>http://localhost:65022/bookshelf/interpretable-machine-learning/</link>
      <pubDate>Mon, 15 Jan 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/interpretable-machine-learning/</guid>
      <description></description>
    </item>
    <item>
      <title>Priceless: the myth of fair value (and how to take advantage of it)</title>
      <link>http://localhost:65022/bookshelf/priceless/</link>
      <pubDate>Mon, 15 Jan 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/priceless/</guid>
      <description></description>
    </item>
    <item>
      <title>The Beginning of Infinity</title>
      <link>http://localhost:65022/bookshelf/begin-infinity/</link>
      <pubDate>Mon, 15 Jan 2024 10:00:00 +0000</pubDate>
      <guid>http://localhost:65022/bookshelf/begin-infinity/</guid>
      <description></description>
    </item>
  </channel>
</rss>
