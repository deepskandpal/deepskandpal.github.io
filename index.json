[{"categories":null,"content":"Chapter 1: Introduction - Reliable ML Engineering Notes Core Concept: The ML Loop ML is Iterative, Not Linear: ML applications are rarely \u0026ldquo;done.\u0026rdquo; They exist in a continuous cycle of development, deployment, evaluation, and improvement. Why a Loop? If a model underperforms: Teams (DS, Business, MLE) collaborate to improve it (features, data, architecture). If a model performs well: Organizations want more sophistication and broader application, leading to further development. The first model is just a starting point. The ML Lifecycle Stages (The \u0026ldquo;Pit Stops\u0026rdquo; in the Loop) (See Figure 1-1 for visual representation) 1. Data Collection and Analysis Goal: Understand available data, identify needs, prioritize uses, collect, and process. Involves: Business/Product (for priorities), Data Engineers (pipelines), SREs (reliability of pipelines), DS/MLEs (data utility). Key: Proper data management is foundational. Business context drives data needs. 2. ML Training Pipelines Goal: Consume processed data and produce trained models using ML algorithms. Involves: Data Engineers, DS, MLEs, SREs. Tech: TensorFlow, PyTorch, XGBoost, etc. Critical: Training pipelines are production systems. Treat with rigor. Common Failures: Data issues (lack, format), bugs, misconfigurations, resource shortages, hardware/distributed system failures. Silent Failures: ML models can also fail due to subtle issues like data distribution shifts. 3. Build and Validate Applications Goal: Integrate the model into a customer-facing system to deliver value. Involves: Product/Business (specs), MLEs/SDEs (implementation), QA (oversight). Action: Log user interactions and model outputs for future improvement. 4. Quality and Performance Evaluation Goal: Assess if the model \u0026ldquo;works\u0026rdquo; and how well, before and during initial launch. Methods: Offline Evaluation: Test on historical/curated data. Live Launch: Model sees live traffic (monitor closely). Dark Launch: Model sees live traffic, logs predictions, but doesn\u0026rsquo;t affect user experience (tests integration). Fractional Launch (Canary/A/B): Model exposed to a subset of users (tests quality and integration). Purpose: Gain confidence for wider rollout, establish baselines. It\u0026rsquo;s a validation checkpoint. 5. Defining and Measuring SLOs (Service-Level Objectives) Goal: Define and track thresholds (SLIs - Indicators) for system performance according to requirements. Involves: SREs, PMs, DS, MLEs, SDEs. Challenge in ML: Subtle data/world changes can degrade ML performance significantly. Types of SLOs: System: Latency, error rates, throughput (serving \u0026amp; training). Application: # of recommendations, successful model calls. ML Performance/Business: Click-through rates, revenue from model (often sliced by user segments). Key: Business must define tolerable SLOs. 6. Launch Goal: Ship the ML-enhanced application to users. Involves: Product SDEs, MLEs, SREs. ML-Specific Concerns: Models as Code: New models can break systems like bad code. Launch Slowly: Progressive rollouts to limit damage. Isolate Rollouts at Data Layer: Critical to avoid data format incompatibilities during rollbacks (see \u0026ldquo;Progressive Rollouts in a Stateful System\u0026rdquo; story). Release, Not Refactor: Minimize changes during a release. Measure SLOs During Launch: Monitor dashboards. Review the Rollout: Manual or automated oversight. 7. Monitoring and Feedback Loops Goal: Continuously observe system health and effectiveness post-launch, and gather data for future improvements. Signals to Monitor: System Health (Golden Signals): Latency, traffic, errors, saturation. Basic Model Health: Model size, load errors (context-free checks). Model Quality (Domain-Specific): Business metrics (CTR, conversion), performance drift over time. Hardest but most crucial. Feedback Collection: Log user interactions, predictions, and context to fuel the next cycle. Purpose: Ensure sustained performance, detect issues, provide input for the next iteration. Key Differences: Evaluation vs. Monitoring Quality \u0026amp; Performance Evaluation: A checkpoint before/during initial launch to validate and decide on wider rollout. Monitoring \u0026amp; Feedback Loops: Continuous vigilance post-launch to maintain health, detect drift, and gather data for future iterations. Overall Lessons from the Loop Data is King: ML begins and ends with data. Cyclical Process: No single order; stages are revisited. Holistic View Required: Understand the entire loop and organization. Risk \u0026amp; Experimentation: Not all ML ideas work. Approach as continual experimentation. Organizational Readiness Matters. ","permalink":"https://deepskandpal.github.io/bookshelf/reliable-ml/chapter-1/","tags":null,"title":"Chapter 1: Introduction"},{"categories":null,"content":"Prompt\nThe chapter opens with a great example: Google Translate\u0026rsquo;s multilingual neural machine translation system back in 2016. This was a landmark – deep learning making a massive, tangible impact at scale. It showed the world the power of modern ML.\nSince then, as the book says, ML has \u0026ldquo;found its way into almost every aspect of our lives.\u0026rdquo; This is true. At FAANG, we see this daily – from the recommendations you get, to how your photos are enhanced, to how we optimize our data centers.\nBut here\u0026rsquo;s the first crucial takeaway for any ML design interview, and indeed, for your day-to-day work:\n\u0026ldquo;Many people, when they hear \u0026lsquo;machine learning system,\u0026rsquo; think of just the ML algorithms\u0026hellip; However, the algorithm is only a small part of an ML system in production.\u0026rdquo;\nThis is HUGE. I can\u0026rsquo;t stress this enough. In interviews, if you only talk about model architectures and training loops, you\u0026rsquo;re missing 90% of the picture. The system includes:\nBusiness Requirements: Why are we even building this? What problem does it solve? User Interface: How do users interact with it? How do developers interact with it? Data Stack: Ingestion, storage, processing, versioning – the lifeblood. Development, Monitoring, Updating Logic: This is MLOps territory. Infrastructure: The compute, storage, and networking that makes it all run. Figure 1-1 (which we\u0026rsquo;ll see on the next page) visually breaks this down and shows how the book will cover these components. We\u0026rsquo;ll keep coming back to this idea of the holistic system.\nSelf-Correction/Interview Tip: When an interviewer asks you to design an ML system, your first thoughts should be about the business problem, the users, and the data, not immediately about whether to use a Transformer or a ResNet.\nPage 2: MLOps vs. ML Systems Design \u0026amp; The Book\u0026rsquo;s Philosophy This page introduces two key terms:\nMLOps: This comes from DevOps. It\u0026rsquo;s about operationalizing ML – deploying, monitoring, maintaining. It\u0026rsquo;s a set of tools and best practices. Think CI/CD for ML, model registries, monitoring dashboards. ML Systems Design: This is the approach the book (and this cohort) takes. It\u0026rsquo;s a holistic view of MLOps, considering all components and stakeholders to meet objectives. It\u0026rsquo;s the \u0026ldquo;architecture\u0026rdquo; level thinking. Figure 1-1 is really important here. You see ML system users and business requirements feeding in (Chapters 1 \u0026amp; 2). You see ML system developers and the entire book dedicated to them. The \u0026ldquo;ML system\u0026rdquo; box itself has:\nData (Chapters 3 \u0026amp; 4) Feature Engineering (Chapter 5) ML Algorithms (Chapter 6 – notice it\u0026rsquo;s just one chapter!) Evaluation (also Chapter 6) Deployment, monitoring, updating of logics (Chapters 7, 8 \u0026amp; 9) Infrastructure (Chapter 10) And Chapter 11 covers the overarching aspects like stakeholders, ethics, etc. (though ethics is woven throughout).\nThe book explicitly states it won\u0026rsquo;t cover specific algorithms in detail. Why?\nBecause algorithms change, they get outdated. The framework for building robust, scalable, maintainable ML systems – that\u0026rsquo;s timeless.\nThis is exactly what FAANG companies look for: people who can think in terms of systems and principles, not just the latest hot model.\nFAANG Perspective: At places like Google, Meta, Amazon, we have dedicated MLOps platform teams building tools, but product-focused ML engineers are the ones using these tools within a systems design framework to solve specific business problems. They need to understand the whole lifecycle.\nPages 3-8: When to Use Machine Learning (The Litmus Test) This is arguably the most critical section for any ML project\u0026rsquo;s inception and a very common starting point for ML design interviews. Before you design anything, you must ask: Is ML necessary or cost-effective?\nThe book offers a fantastic five-part definition: \u0026ldquo;Machine learning is an approach to:\n(1) Learn: The system must have the capacity to learn. A relational database isn\u0026rsquo;t an ML system because you explicitly define relationships. ML systems infer them, usually from data. For supervised learning (e.g., predicting Airbnb prices), you need input-output pairs.\nInterview Relevance: \u0026ldquo;How does your system learn from new data?\u0026rdquo; (2) Complex patterns: ML shines when patterns are too complex to hand-code. Predicting if a fair die roll is a 6? No pattern. Predicting stock prices? Complex patterns (hopefully!). Sorting listings by state based on zip code? Lookup table. Predicting Airbnb rental price from many features? ML! This is Andrej Karpathy\u0026rsquo;s \u0026ldquo;Software 2.0\u0026rdquo; idea – you provide data, the system writes the \u0026ldquo;code\u0026rdquo; (the learned patterns).\nFAANG Nuance: We often deal with problems where the rules are unknown or too numerous to list, like ranking search results or newsfeed items. This is prime ML territory. (3) Existing data: ML needs data to learn from. No data, no ML (usually). The book mentions predicting taxes without tax data – impossible.\nZero-shot learning is an interesting case: it makes predictions for tasks it wasn\u0026rsquo;t directly trained on, but it was trained on related tasks/data. So, it still needs some data. Fake-it-til-you-make-it: Launch with humans making predictions, collect that data, then train an ML model. This is a common bootstrapping strategy for new products. Interview Relevance: \u0026ldquo;What data would you use? How would you collect it? What if you don\u0026rsquo;t have labeled data initially?\u0026rdquo; (4) Predictions: ML solves problems requiring predictive answers. \u0026ldquo;What will the weather be?\u0026rdquo; \u0026ldquo;What movie will a user watch next?\u0026rdquo; Even compute-intensive problems can be reframed: \u0026ldquo;What would the outcome of this complex rendering process look like?\u0026rdquo; (approximate it with ML).\n(5) Unseen data: The patterns learned must generalize to new, unseen data. If your model trained on 2008 app download data (Koi Pond!) tries to predict 2020 downloads, it\u0026rsquo;ll fail. Technically, training and unseen data should come from similar distributions. If not, your model will perform poorly (hello, monitoring and retraining, Chapters 8 \u0026amp; 9!).\nInterview Relevance: This leads directly to discussions of data drift, concept drift, and the importance of robust evaluation and monitoring. The book then lists additional characteristics where ML solutions shine:\nIt\u0026rsquo;s repetitive: ML algorithms (especially deep learning) often need many examples. Repetitive tasks provide these. The cost of wrong predictions is cheap (usually): A bad movie recommendation? User ignores it. A self-driving car makes a wrong turn? Catastrophic. This influences your choice of problem and acceptable error rates. However, even for high-stakes, if ML on average outperforms humans (like statistically safer self-driving cars), it can be viable. It\u0026rsquo;s at scale: ML often needs significant upfront investment (data, compute, talent). This is justified if you\u0026rsquo;re making many predictions (e.g., sorting millions of emails, routing thousands of support tickets). \u0026ldquo;At scale\u0026rdquo; also implies lots of data for training. The patterns are constantly changing: Spam emails evolve. Fashion trends change. ML models can be updated with new data to adapt, unlike hardcoded rules. This is where \u0026ldquo;Continual Learning\u0026rdquo; (page 264, Chapter 9) comes in. When NOT to use ML:\nIt\u0026rsquo;s unethical: Automated grader biases (page 341) – we\u0026rsquo;ll cover ethics in detail. Simpler solutions do the trick: (Chapter 6) ALWAYS start with a non-ML baseline. If a heuristic works, great! FAANG Mantra: \u0026ldquo;Start simple.\u0026rdquo; A common failure mode for junior engineers is over-engineering. It\u0026rsquo;s not cost-effective. A crucial caution: Don\u0026rsquo;t dismiss new tech just because it\u0026rsquo;s not cost-effective now. Early adoption can be a competitive advantage later. And sometimes, you can break a big problem into smaller pieces, using ML for just one part.\nInterview Gold: The \u0026ldquo;When to use ML\u0026rdquo; checklist is your first filter for any ML system design question. Always discuss these trade-offs. If you propose an ML solution, be ready to defend why ML is appropriate over a simpler, non-ML approach.\nPages 9-11: Machine Learning Use Cases This section is a tour of where ML is making an impact. It\u0026rsquo;s good for broadening your understanding of the applications.\nConsumer Applications:\nSearch engines (Google) Recommender systems (Netflix, Amazon) Predictive typing (your phone\u0026rsquo;s keyboard) Photo enhancement Biometric authentication (fingerprint, face ID) Machine translation (the author\u0026rsquo;s personal anecdote about parents using Google Translate is lovely) Smart assistants (Alexa, Google Assistant) Smart security cameras (pet detection, uninvited guests) At-home health monitoring (fall detection) Enterprise Applications: The book rightly states these are the majority of ML use cases.\nOften have stricter accuracy requirements but can be more forgiving on latency compared to consumer apps (e.g., 0.1% improvement in resource allocation for Google can save millions, even if the system takes a few seconds to run). Figure 1-3 (Algorithmia 2020 survey): Shows diverse enterprise uses: Internal: Reducing costs (38%), generating customer insights (37%), internal processing automation (30%). External: Improving customer experience (34%), retaining customers (29%), interacting with customers (28%). Specific examples: Fraud detection: Anomaly detection on transactions. (Very common, one of the oldest ML uses in enterprise). Price optimization: Dynamic pricing for ads, flights, ride-sharing. Maximize margin/revenue. Demand forecasting: For inventory, resource allocation. Customer acquisition: Identifying potential customers, targeted ads, optimizing discounts. Churn prediction: Predicting when customers (or employees!) might leave. Self-Correction/Interview Tip: Having a few diverse use cases in your back pocket is useful. If an interviewer asks, \u0026ldquo;Give me an example of an ML system you find interesting,\u0026rdquo; you can draw from these. It also helps you think about the types of problems ML can solve (classification, regression, anomaly detection, etc.).\nPage 12: More Enterprise Use Cases \u0026amp; Intro to Understanding ML Systems More enterprise examples:\nAutomated support ticket classification: Route tickets to the right department faster. Brand monitoring: Sentiment analysis on brand mentions. Healthcare: Skin cancer detection, diabetes diagnosis (often through a provider, due to accuracy/privacy needs). This page then transitions to the next major section: Understanding Machine Learning Systems. The key motivation here is that ML systems are different from:\nML in research (or academia). Traditional software. And these differences necessitate the kind of thinking this book promotes.\nPages 12-21: Machine Learning in Research Versus in Production This is a CRITICAL section, especially for those coming from academia or who have mostly worked on Kaggle-like projects. ML in the wild is a different beast. Table 1-1 summarizes key differences: Requirements Research: State-of-the-art model performance on benchmarks Production: Different stakeholders, often conflicting requirements Computational Priority Research: Fast training, high throughput Production: Fast inference, low latency Data Research: Static, benchmark datasets Production: Constantly shifting, messy, real-world data Fairness Research: Often not a focus Production: Must be considered Interpretability Research: Often not a focus Production: Must be considered (often a requirement) Let\u0026rsquo;s break these down:\nDifferent stakeholders and requirements (Page 13-14): Research: Usually a single objective – e.g., SOTA on a benchmark. Researchers might use complex techniques for marginal gains. Production: Multiple stakeholders with conflicting needs. The restaurant recommender example is classic: ML Engineers: Want best model for user clicks (maybe complex, needs more data). Sales Team: Want model recommending expensive restaurants (more service fees). Product Team: Want low latency (\u0026lt;100ms), as latency drops orders. ML Platform Team: Worried about scaling, want to pause updates to improve platform. Manager: Wants to maximize margin (maybe cut the ML team if costs are too high!). The book mentions decoupling objectives (page 41) – we\u0026rsquo;ll get there. For now, understand that production ML is about trade-offs. Is a 100ms latency a must-have or a nice-to-have? This determines if Model A or B is even viable.\nEnsembling: Popular in competitions (Netflix Prize), but often too complex, slow, or hard to interpret for production. A small performance lift might not justify the operational overhead.\nCriticism of ML Leaderboards (Page 15): Hard steps (data collection, problem formulation) are often done for you. Multiple hypothesis testing: With many teams, some might get good results by chance. Misaligned incentives: Drive for accuracy at the expense of compactness, fairness, energy efficiency (Ethayarajh \u0026amp; Jurafsky). FAANG Perspective: This stakeholder wrangling is daily life. Product Managers, Engineering Managers, ML Engineers, and sometimes legal/policy teams all have a say. Clear communication and defining priorities are key.\nComputational priorities (Page 15-18): Research: Focus on fast training and high throughput (samples/sec during training). Production: Focus on fast inference and low latency (time from query to result). Terminology Clash (Page 16): The book uses \u0026ldquo;latency\u0026rdquo; to mean \u0026ldquo;response time\u0026rdquo; (what the client sees, including network/queueing delays). This is common ML community usage.\nLatency vs. Throughput (Figure 1-4, Page 17):\nSingle query processing: Higher latency = lower throughput. Batched query processing: Can increase throughput, but also individual query latency (waiting for a batch to fill). This is a common trade-off. Impact of Latency: Real-world examples (Akamai: 100ms delay = 7% conversion drop; Booking.com: 30% latency increase = 0.5% conversion cost; Google: \u0026gt;3s load time, users leave). Users are impatient!\nLatency is a distribution (Page 18): Don\u0026rsquo;t just use average! Averages hide outliers. Use percentiles:\np50 (median): 50% of requests are faster/slower. p90, p95, p99: Show tail latencies. These outliers might affect your most valuable customers (e.g., Amazon customers with large purchase histories). Product requirements are often \u0026ldquo;p99 latency \u0026lt; X ms.\u0026rdquo; Interview Relevance: Be ready to discuss latency/throughput trade-offs, batching strategies, and how you\u0026rsquo;d measure and set SLOs for latency (e.g., \u0026ldquo;p95 latency for recommendations must be under 200ms\u0026rdquo;).\nData (Page 18-19): Research: Often clean, well-formatted, static benchmark datasets. Known quirks, public scripts for processing. Production: Data is MESSY! Noisy, unstructured, constantly shifting. Biased (and you might not know how). Labels can be sparse, imbalanced, incorrect. Changing requirements mean updating labels. Privacy and regulatory concerns (GDPR, CCPA). Data is constantly generated by users, systems, third parties. Figure 1-5 (Karpathy\u0026rsquo;s \u0026ldquo;Amount of sleep lost over\u0026hellip;\u0026rdquo; graphic): In PhD, sleep lost over models/algorithms. At Tesla (production), sleep lost over datasets. This is profoundly true.\nFAANG Perspective: Data engineering, data quality, data governance, and data privacy are massive efforts. Often, more engineering time is spent on the data pipeline than on the model itself.\nFairness (Page 19-20): Research: Often an afterthought. Production: CRITICAL. ML models encode past biases from data. Deployed at scale, they can discriminate at scale. Examples: Loan applications biased by zip code, resume ranking biased by name spelling, mortgage rates based on biased credit scores. Cathy O\u0026rsquo;Neil\u0026rsquo;s \u0026ldquo;Weapons of Math Destruction\u0026rdquo; is a must-read. Misclassifying minority groups might have a small impact on overall accuracy metrics but a huge impact on those individuals/groups. McKinsey study (2019): Only 13% of large companies mitigating algorithmic bias. This is changing, but slowly. We\u0026rsquo;ll cover Responsible AI in Chapter 11. Interview Relevance: Expect questions on fairness. \u0026ldquo;How would you detect bias in your model? How would you mitigate it?\u0026rdquo; This is table stakes now.\nInterpretability (Page 20-21): Geoffrey Hinton\u0026rsquo;s AI surgeon dilemma: 90% cure rate AI surgeon (black box) vs. 80% human surgeon (explainable). Who do you choose? (When the author asked execs, it was 50/50). Research: Often not incentivized if focus is pure performance. Production: Often a requirement. For users/business leaders: To trust the model, detect biases. For developers: To debug and improve the model. Legal: \u0026ldquo;Right to explanation\u0026rdquo; in some jurisdictions (e.g., GDPR). 2019 Stanford HAI report: Only 19% of large companies working on explainability. Again, this is improving. FAANG Nuance: For critical systems (e.g., fraud, content moderation, medical), interpretability is non-negotiable. Techniques like SHAP, LIME are used, but simpler, inherently interpretable models are often preferred if performance is comparable.\nDiscussion (Page 21): Why production focus matters? Most companies can\u0026rsquo;t afford pure research without business application. The \u0026ldquo;bigger, better\u0026rdquo; ML models (e.g., large language models) require massive data and compute, often tens of millions of dollars. As ML becomes more accessible, demand for productionizing ML grows. The vast majority of ML-related jobs are in productionizing ML. This is why you\u0026rsquo;re here! Pages 22-23: Machine Learning Systems Versus Traditional Software If ML is part of software engineering (SWE), why not just use existing SWE best practices? Good idea! ML production would be better if ML experts were also strong software engineers. Many SWE tools are useful.\nHowever, ML has unique challenges:\nCode + Data + Artifacts: SWE: Assumes code and data are separate. Focus on modularity. ML: Systems are tightly coupled: code (training scripts, inference logic), data (features, labels), and artifacts (trained models). Trend: \u0026ldquo;Best data wins\u0026rdquo; over \u0026ldquo;best algorithm.\u0026rdquo; So, focus shifts to improving data. Data changes quickly =\u0026gt; ML apps need to adapt quickly =\u0026gt; faster dev/deploy cycles. Testing and Versioning: SWE: Test and version code. ML: Must test and version code and data. Versioning large datasets is hard. How to know if a data sample is \u0026ldquo;good\u0026rdquo; or \u0026ldquo;bad\u0026rdquo;? Not all data samples are equal: A scan of a cancerous lung is more valuable if you have 1M normal lung scans and only 1k cancerous ones. Indiscriminate data acceptance can hurt performance or lead to data poisoning attacks (footnote 31). Model Size: As of 2022, models with billions of parameters are common (e.g., LLMs). Require GBs of RAM. This might seem quaint in the future (like the 32MB RAM of the Apollo moon computer). Deploying large models, especially on edge devices (Chapter 7), is a massive engineering challenge. Speed (Inference Latency): How to run these large models fast enough? An autocompletion model slower than typing is useless. Monitoring and Debugging: Non-trivial for complex, black-box models. Hard to know what went wrong or get alerted quickly. The good news (Page 23): These challenges are being tackled. BERT (2018) was initially seen as too big/slow (340M params, 1.35GB). By 2020, it was in \u0026ldquo;almost every English search on Google.\u0026rdquo; Progress is rapid.\nInterview Relevance: Understanding these unique challenges of ML (data-centricity, versioning data+model, model size/latency, monitoring complexity) distinguishes a candidate who has thought about production issues from one who has only trained models in a lab.\nPage 23: Summary This opening chapter gives you the lay of the land:\nML is widely used, especially in enterprise. Knowing when and when not to use ML is crucial. ML in production is very different from ML in research (stakeholders, compute, data, fairness, interpretability). ML systems have unique challenges compared to traditional software (data entanglement, versioning, model size, monitoring). The key theme of the book, and this cohort, is a holistic system approach. We don\u0026rsquo;t just look at algorithms; we look at all components working together.\n","permalink":"https://deepskandpal.github.io/bookshelf/design-ml-system/chapter-1/","tags":null,"title":"Chapter 1: Overview of Machine Learning Systems"},{"categories":null,"content":"Notes for Chapter 1 When you hear \u0026ldquo;Machine Learning,\u0026rdquo; what pops into your head? Robots? Terminators? Maybe a friendly butler? The book nails it – it\u0026rsquo;s not just sci-fi; it\u0026rsquo;s already here. Think about the spam filter. That was one of the first really big ML applications that touched millions. It learned, from examples of spam and non-spam (or \u0026ldquo;ham,\u0026rdquo; as we call it), to tell the difference. And it got so good that we barely notice it anymore. That\u0026rsquo;s the hallmark of good ML – it just works.\nThis chapter aims to clarify what ML is, why it\u0026rsquo;s useful, and give you a \u0026ldquo;map\u0026rdquo; of the ML continent: supervised vs. unsupervised, online vs. batch, instance-based vs. model-based. We\u0026rsquo;ll also touch on the typical project workflow and some common challenges.\n(Page 2: What Is Machine Learning?) Alright, so what is it? The book gives a great, simple definition:\n\u0026ldquo;Machine Learning is the science (and art) of programming computers so they can learn from data.\u0026rdquo;\nThe key here is \u0026ldquo;learn from data.\u0026rdquo; Instead of you, the programmer, writing explicit rules for every single scenario, you show the computer a bunch of examples, and it figures out the patterns itself.\nArthur Samuel, a pioneer back in 1959, said it\u0026rsquo;s the \u0026ldquo;field of study that gives computers the ability to learn without being explicitly programmed.\u0026rdquo; Think about that – without explicit programming. That\u0026rsquo;s the magic.\nThen there\u0026rsquo;s Tom Mitchell\u0026rsquo;s more engineering-focused definition from 1997, which is super useful for grounding this:\n\u0026ldquo;A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\u0026rdquo;\nLet\u0026rsquo;s break that down with our spam filter:\nTask T: Flagging spam emails. Experience E: The training data – thousands of example emails, each labeled as \u0026ldquo;spam\u0026rdquo; or \u0026ldquo;ham.\u0026rdquo; Performance Measure P: How well does it do the task? Maybe it\u0026rsquo;s accuracy – the percentage of emails it correctly classifies. So, if our spam filter gets better at correctly identifying spam (higher accuracy P) after being shown more examples of emails (more experience E), then it\u0026rsquo;s learning!\nAnd the book rightly points out: downloading all of Wikipedia doesn\u0026rsquo;t make your computer \u0026ldquo;learn\u0026rdquo; in the ML sense. It has more data, sure, but it\u0026rsquo;s not suddenly better at, say, translating languages or identifying cats in pictures, unless you use that data to train it for a specific task.\n(Page 3-4: Why Use Machine Learning?) So, why bother? Why not just write the rules, like we\u0026rsquo;ve always done in traditional programming? The book uses the spam filter example (Figure 1-1 vs 1-2), and it\u0026rsquo;s perfect.\nProblems too complex for traditional rules: Imagine trying to write rules for spam. \u0026ldquo;If email contains \u0026lsquo;4U\u0026rsquo;, \u0026lsquo;credit card\u0026rsquo;, \u0026lsquo;free\u0026rsquo;, \u0026lsquo;amazing\u0026rsquo;\u0026hellip;\u0026rdquo; Okay, a start. But spammers get smart. They start writing \u0026ldquo;For U\u0026rdquo; or using images. Your list of rules would become a monster – thousands, maybe millions of lines long, and a nightmare to maintain (Figure 1-1). An ML spam filter, on the other hand, learns which words and phrases are good predictors by looking at frequencies in spam vs. ham (Figure 1-2). It\u0026rsquo;s often shorter, easier to maintain, and more accurate. Adapting to changing environments: When spammers change tactics (\u0026ldquo;For U\u0026rdquo; instead of \u0026ldquo;4U\u0026rdquo;), a traditional filter needs you to manually update the rules. An ML system, especially an online learning one (we\u0026rsquo;ll get to that), can see these new patterns emerging in user-flagged spam and automatically adapt (Figure 1-3). It keeps learning! No known algorithm: Think about speech recognition. How would you even begin to write rules to distinguish \u0026ldquo;one\u0026rdquo; from \u0026ldquo;two\u0026rdquo; for every voice, accent, in noisy environments, across dozens of languages? It\u0026rsquo;s incredibly hard. But give an ML algorithm enough recordings of people saying \u0026ldquo;one\u0026rdquo; and \u0026ldquo;two,\u0026rdquo; and it can learn to distinguish them. Helping humans learn (Data Mining): This is a fascinating one (Figure 1-4). Sometimes, we train an ML model, and then we can peek inside (though it\u0026rsquo;s tricky for some complex models) to see what it learned. A spam filter might reveal surprising combinations of words that are highly predictive of spam. This can give us new insights into complex problems by finding patterns we wouldn\u0026rsquo;t have spotted. (Page 5-6: Examples of Applications) The book lists a ton, and this really shows the breadth of ML:\nImage Classification (CNNs): Identifying products on a production line, detecting tumors in brain scans (this is more semantic segmentation – classifying every pixel). Natural Language Processing (NLP): Classifying news articles, flagging offensive comments, summarizing documents, chatbots (NLU, question-answering). These often use RNNs, CNNs, or more recently, Transformers. Regression (predicting values): Forecasting company revenue. This can use Linear Regression, SVMs, Random Forests, Neural Networks. Speech Recognition: Making your app react to voice commands. Anomaly Detection: Detecting credit card fraud. Clustering (Unsupervised): Segmenting customers based on purchases for targeted marketing. Data Visualization/Dimensionality Reduction: Taking high-dimensional data and making it understandable in 2D or 3D. Recommender Systems: Suggesting products you might like. Reinforcement Learning (RL): Building intelligent bots for games, like AlphaGo that beat the world Go champion. This isn\u0026rsquo;t exhaustive, but it gives you a taste of the sheer power and versatility.\n(Page 7-9: Types of Machine Learning Systems - The Big Picture) Okay, now for the \u0026ldquo;map of the ML continent.\u0026rdquo; We can categorize ML systems based on a few key criteria. These aren\u0026rsquo;t mutually exclusive; a system can be a mix.\n1. Based on Human Supervision during Training: (Page 8) Supervised Learning: This is probably the most common. The \u0026ldquo;supervision\u0026rdquo; comes from the fact that your training data includes the desired solutions, called labels (Figure 1-5). You show the system an email AND tell it \u0026ldquo;this is spam.\u0026rdquo; You show it a picture of a cat AND tell it \u0026ldquo;this is a cat.\u0026rdquo;\nClassification: The task is to predict a category. Spam or ham? Cat or dog? Figure 1-5 (spam classification) is a classic example. Regression: The task is to predict a numerical value. What\u0026rsquo;s the price of this car given its mileage, age, brand (these are features or predictors)? (Figure 1-6). The \u0026ldquo;label\u0026rdquo; here is the actual price. A quick note on terminology (page 8): An attribute is a data type (e.g., \u0026ldquo;mileage\u0026rdquo;). A feature is often an attribute plus its value (e.g., \u0026ldquo;mileage = 15,000\u0026rdquo;). People often use them interchangeably, but it\u0026rsquo;s good to know the nuance. The book lists some key supervised algorithms we\u0026rsquo;ll cover: k-Nearest Neighbors, Linear Regression, Logistic Regression (often used for classification despite \u0026ldquo;regression\u0026rdquo; in its name!), SVMs, Decision Trees, Random Forests, and Neural Networks. (Page 9-12) Unsupervised Learning: Here, the training data is unlabeled (Figure 1-7). There\u0026rsquo;s no \u0026ldquo;teacher\u0026rdquo; providing answers. The system tries to find patterns and structure in the data on its own.\n(Page 10) Clustering: Trying to find natural groupings in the data. For example, grouping your blog visitors into different segments based on their behavior (Figure 1-8). You don\u0026rsquo;t tell it the groups beforehand; it discovers them. Algorithms include K-Means, DBSCAN, Hierarchical Cluster Analysis (HCA). (Page 11) Visualization \u0026amp; Dimensionality Reduction: Taking complex, high-dimensional data and creating a 2D or 3D plot (Figure 1-9, t-SNE example). This helps us understand the data. Dimensionality reduction aims to simplify the data by merging correlated features (e.g., car mileage and age into \u0026ldquo;wear and tear\u0026rdquo;) or dropping less important ones, without losing too much information. This is called feature extraction. It can make subsequent learning faster and sometimes better. PCA, Kernel PCA, LLE are examples. (Page 12) Anomaly Detection / Novelty Detection: Spotting unusual instances. Anomaly detection is about finding things that look different from most of the data (e.g., fraud detection, Figure 1-10). Novelty detection is similar but assumes your training data is \u0026ldquo;clean\u0026rdquo; and you want to find things different from anything seen in training. (Page 13) Association Rule Learning: Discovering relationships between attributes in large datasets. E.g., people who buy barbecue sauce and potato chips also tend to buy steak. (Page 13) Semisupervised Learning: This is a middle ground. You have a lot of unlabeled data and a little bit of labeled data (Figure 1-11). The system uses both. Think of Google Photos: it clusters faces (unsupervised), then you label a few faces (\u0026ldquo;That\u0026rsquo;s Aunt May\u0026rdquo;), and it can then label Aunt May in many other photos (supervised part). Deep Belief Networks (DBNs) using Restricted Boltzmann Machines (RBMs) are an example.\n(Page 14) Reinforcement Learning (RL): This is a different beast altogether! The learning system, called an agent, observes an environment, selects and performs actions, and gets rewards (or penalties) in return (Figure 1-12). It learns the best strategy, called a policy, to maximize its cumulative reward over time. Think training a robot to walk, or AlphaGo learning to play Go. It learns by trial and error, essentially.\n2. Based on Incremental Learning (On-the-fly): (Page 15) Batch Learning (Offline Learning): The system is trained using all available data at once. It takes time and resources. Once trained, it\u0026rsquo;s launched and doesn\u0026rsquo;t learn anymore; it just applies what it learned. If you want it to learn about new data (e.g., new types of spam), you have to retrain it from scratch on the full dataset (old + new). This can be automated (as in Figure 1-3), but it\u0026rsquo;s still a full retrain. This is fine for many things, but not if you need to adapt rapidly or have massive datasets.\n(Page 15-16) Online Learning (Incremental Learning): The system is trained incrementally by feeding it data instances sequentially, either individually or in small groups called mini-batches (Figure 1-13). Each learning step is fast and cheap. This is great for:\nSystems needing rapid adaptation (e.g., stock price prediction). Systems with limited computing resources (once it learns from a data instance, it might not need to store it anymore). Handling huge datasets that can\u0026rsquo;t fit in memory (called out-of-core learning, Figure 1-14). It loads a chunk, trains, loads the next chunk, trains, etc. The book notes, \u0026ldquo;Think of it as incremental learning,\u0026rdquo; which is a good way to avoid confusion, as out-of-core is often done offline. A key parameter here is the learning rate: how quickly it adapts. Too high, and it forgets old patterns quickly. Too low, and it learns slowly and might be less sensitive to noise or outliers. 3. Based on How They Generalize: This is about how systems make predictions on new, unseen data.\n(Page 17-18) Instance-Based Learning: The system learns the training examples by heart. Then, when it sees a new instance, it compares it to the stored examples using a similarity measure and makes a prediction based on the most similar known instances. For example, in Figure 1-15, the new instance (cross) is classified as a triangle because most of its nearest neighbors are triangles. k-Nearest Neighbors is a classic example.\n(Page 18-22) Model-Based Learning: The system builds a model from the training examples and then uses that model to make predictions. This is like a scientist observing data and building a theory. The book uses a great example: predicting life satisfaction based on GDP per capita (Table 1-1, Figure 1-17).\nYou select a type of model – say, a linear model (Equation 1-1: life_satisfaction = θ₀ + θ₁ × GDP_per_capita). This is model selection. This model has parameters (θ₀ and θ₁ – theta-zero and theta-one). By tweaking these, you get different lines (Figure 1-18). How do you find the best parameters? You need a performance measure. For linear regression, it\u0026rsquo;s often a cost function that measures how far the model\u0026rsquo;s predictions are from the training examples. The goal is to minimize this cost. The learning algorithm (e.g., Linear Regression algorithm) takes your training data and finds the parameter values (θ₀, θ₁) that make the model best fit the data. This is called training the model (Figure 1-19). Once trained (e.g., θ₀ = 4.85, θ₁ = 4.91 × 10⁻⁵), you can use the model to make predictions on new data (e.g., Cyprus\u0026rsquo;s life satisfaction, page 21). The code snippet on page 21-22 shows how you\u0026rsquo;d do this with Scikit-Learn. And then it shows how simple it is to swap in an instance-based algorithm like k-Nearest Neighbors! (Page 23-29: Main Challenges of Machine Learning) So, you select an algorithm and train it. What can go wrong? The book says: \u0026ldquo;bad algorithm\u0026rdquo; and \u0026ldquo;bad data.\u0026rdquo;\nLet\u0026rsquo;s start with \u0026ldquo;Bad Data\u0026rdquo;:\n(Page 23) Insufficient Quantity of Training Data: Most ML algorithms need a lot of data to work well. Thousands for simple problems, millions for complex ones like image recognition. The \u0026ldquo;Unreasonable Effectiveness of Data\u0026rdquo; paper (page 24, Figure 1-20) showed that even simple algorithms can perform incredibly well if given enough data. Data often trumps a fancy algorithm, but getting more data isn\u0026rsquo;t always cheap or easy. (Page 25) Nonrepresentative Training Data: Your training data must be representative of the new cases you want to generalize to. If you train a model on life satisfaction vs. GDP using only rich countries, it won\u0026rsquo;t predict well for poor countries (Figure 1-21). This can happen due to sampling noise (if your sample is too small and just happens to be unrepresentative by chance) or sampling bias (if your sampling method is flawed). The Literary Digest poll example (page 26) is a classic case of sampling bias – they polled wealthier people, who leaned Landon, but Roosevelt won. And nonresponse bias – only certain types of people responded. Want to build a funk music video recognizer by searching YouTube? Your results will be biased towards popular artists. (Page 26) Poor-Quality Data: Errors, outliers, noise. \u0026ldquo;Garbage in, garbage out.\u0026rdquo; Data cleaning is a huge part of a data scientist\u0026rsquo;s job! You might discard outliers, fix errors, or decide how to handle missing feature values (ignore the feature, ignore the instance, fill it in – e.g., with the median). (Page 27) Irrelevant Features: If your training data has too many irrelevant features (and not enough relevant ones), the system will struggle. Feature engineering is critical: Feature selection: Choosing the most useful features. Feature extraction: Combining existing features into a more useful one (like we saw with dimensionality reduction). Creating new features: Sometimes you need to gather new data or derive new features. Now for \u0026ldquo;Bad Algorithm\u0026rdquo; (or more accurately, issues with the model itself):\n(Page 27-29) Overfitting the Training Data: This is a HUGE one. The model performs great on the training data but poorly on new, unseen data. It\u0026rsquo;s like it memorized the training data, including its noise and quirks, instead of learning the underlying general pattern (Figure 1-22).\nImagine your life satisfaction model learns that countries with a \u0026lsquo;W\u0026rsquo; in their name are happier based on your training data (New Zealand, Norway, Sweden). This is just a chance pattern in your data, not a real rule! Solutions (page 28): Simplify the model: Choose one with fewer parameters (e.g., linear instead of high-degree polynomial), reduce features, or constrain the model. Gather more training data. Reduce noise in the training data (fix errors, remove outliers). Regularization (page 28-29): Constraining a model to make it simpler and reduce overfitting. For our linear model, if we force the slope (θ₁) to be small, it makes the line flatter and less likely to chase noise (Figure 1-23). The amount of regularization is controlled by a hyperparameter. Crucial distinction (page 29): A model parameter (like θ₀, θ₁) is something the learning algorithm tunes. A hyperparameter is a parameter of the learning algorithm itself (e.g., the amount of regularization to apply). You set hyperparameters before training. (Page 29) Underfitting the Training Data: The opposite of overfitting. Your model is too simple to learn the underlying structure of the data. A linear model for life satisfaction might underfit because reality is more complex (Figure 1-21 showed this too – the linear model wasn\u0026rsquo;t great for very rich or very poor countries).\nSolutions: Select a more powerful model (more parameters). Feed better features to the algorithm (feature engineering). Reduce constraints on the model (e.g., reduce the regularization hyperparameter). (Page 30-33: Stepping Back, Testing and Validating) Phew! That was a lot. The book summarizes it well on page 30:\nML is about machines learning from data, not explicit rules. Many types: supervised/unsupervised, batch/online, instance/model-based. Typical project: Gather data, feed to algorithm. Model-based learns parameters to fit. Instance-based learns by heart. Pitfalls: Bad data (too little, non-representative, noisy, irrelevant features) or bad model (overfitting/underfitting). So, you\u0026rsquo;ve trained a model. How do you know if it will generalize to new cases? You can\u0026rsquo;t just \u0026ldquo;hope\u0026rdquo;!\nTraining Set and Test Set (Page 30): Split your data. You train on the training set. You evaluate on the test set (data the model has never seen). The error rate on the test set is called the generalization error (or out-of-sample error). This tells you how well it will likely do in the real world.\nIf training error is low but generalization error is high, you\u0026rsquo;re overfitting! Common split: 80% train, 20% test (but depends on dataset size, as page 31 notes). (Page 31) Hyperparameter Tuning and Model Selection:\nWhat if you\u0026rsquo;re choosing between a linear model and a polynomial model? Or trying to find the best regularization hyperparameter? You can\u0026rsquo;t just try them all on the test set and pick the best. Why? Because then you\u0026rsquo;ve tuned your model and hyperparameters to that specific test set. It might not perform well on other new data. You\u0026rsquo;ve essentially \u0026ldquo;used up\u0026rdquo; your test set. Holdout Validation: The solution! Split your original training data further. Keep some aside as a validation set (or dev set). Train various models (with different hyperparameters, or different model types) on the reduced training set (full training set - validation set). Evaluate them on the validation set. Pick the best one. Now, train your best model (with its best hyperparameters) on the full original training set (including the validation set). This is your final model. Finally, evaluate this final model on the test set to get an estimate of its true generalization error. Cross-validation is mentioned as a way to deal with small validation sets by using many small validation sets. More robust, but takes longer. (Page 32) Data Mismatch: What if your training data (e.g., flower pictures from the web) isn\u0026rsquo;t perfectly representative of your production data (e.g., flower pictures taken by your mobile app)?\nCrucial rule: Your validation set and test set must be as representative of the production data as possible. So, they should come from the app pictures. If performance on the validation set is bad after training on web pictures, is it overfitting or data mismatch? Andrew Ng suggests a train-dev set: a subset of the web (training) pictures held out. If model does poorly on train-dev: it overfit the training web pictures. Simplify/regularize. If model does well on train-dev but poorly on (app-based) validation set: it\u0026rsquo;s data mismatch. Try to make web images look more like app images (preprocessing). (Page 33) No Free Lunch (NFL) Theorem: A humbling but important concept. There\u0026rsquo;s no single model that is a priori guaranteed to work best on all problems. If you make no assumptions about your data, any model is as good as any other.\nA linear model might be best for one dataset, a neural network for another. The only way to know for sure is to try them all (impossible!). In practice, we make reasonable assumptions about the data and try a few suitable models. (Page 33-34: Exercises) The chapter ends with a great set of exercises. I strongly encourage you to go through them. If you can answer these, you\u0026rsquo;ve got a solid grasp of this foundational material.\nOkay! We\u0026rsquo;ve covered a massive amount of ground – the entire landscape of Machine Learning, really. We\u0026rsquo;ve defined what it is, why it\u0026rsquo;s a game-changer, explored the main types of systems, the process of building models, and the pitfalls to watch out for.\nThe key takeaways:\nML is about learning patterns from data. The type of learning (supervised, unsupervised, etc.) depends on your data and your goal. Data is king, but it needs to be good quality and representative. Overfitting and underfitting are constant battles. Always test your model on unseen data! This chapter sets the stage. From here on out, we\u0026rsquo;ll be diving deeper into these concepts, with more math, more code, and more hands-on examples from the book.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-1/","tags":null,"title":"Chapter 1: The Machine Learning Landscape"},{"categories":null,"content":"Chapter 5 Overview: This chapter covers the basic properties of arrays (specifically Python lists), common manipulation techniques, and classic array-based problems. It heavily emphasizes thinking about space complexity and optimizing operations.\nThink of arrays like this: Imagine a row of numbered boxes, right next to each other in memory.\nContiguous: The boxes are physically adjacent. This is crucial for performance. Indexed: You can instantly jump to any box if you know its number (index). This is O(1) access. Homogeneous (usually): Typically, all boxes hold the same type of item (like all integers or all strings), though Python lists offer more flexibility. Why Arrays Are Important (Especially for Interviews): Efficiency: Direct access by index is super fast (O(1)). Memory Locality: Because elements are stored together, accessing sequential elements is often cache-friendly, leading to good practical performance. Building Blocks: Many other data structures (like hash maps, heaps, stacks, queues) are often implemented using arrays underneath. In-Place Operations: Interviewers love problems where you modify the array directly without using significant extra memory (O(1) space). This often involves clever use of pointers or swapping elements. Array Boot Camp: Reordering Array Entries (Page 1) The boot camp problem is a fantastic introduction to in-place array manipulation using multiple pointers.\nThe Problem: Given an array of integers, rearrange it so that all the even numbers appear before all the odd numbers.\nThe order within the evens or odds doesn\u0026rsquo;t matter.\nCrucially, do this in-place, meaning using only O(1) extra space (you can\u0026rsquo;t just create two new lists and combine them).\nExample: Input [3, 1, 2, 4, 5, 6] could become [2, 4, 6, 1, 5, 3] or [6, 2, 4, 3, 1, 5], etc.\nThe Boot Camp Code Explained (even_odd): def even_odd(A): next_even, next_odd = 0, len(A) - 1 # Pointers start at opposite ends while next_even \u0026lt; next_odd: # Stop when pointers meet or cross if A[next_even] % 2 == 0: # A[next_even] is even. It\u0026#39;s in the correct partition. # Leave it there and advance the \u0026#39;even\u0026#39; boundary. next_even += 1 else: # A[next_even] is odd. It needs to go to the \u0026#39;odd\u0026#39; section. # Swap it with the element at A[next_odd]. A[next_even], A[next_odd] = A[next_odd], A[next_even] # The element now at A[next_odd] is confirmed odd (or hasn\u0026#39;t been checked), # so shrink the \u0026#39;odd\u0026#39; boundary. next_odd -= 1 # No return needed as A is modified in-place Intuition (Partitioning): Think of the array being divided into three sections as you process it:\nA[0 ... next_even - 1]: Even Numbers (Correctly placed) A[next_even ... next_odd]: Unclassified Numbers (Yet to be examined) A[next_odd + 1 ... len(A) - 1]: Odd Numbers (Correctly placed) The while loop processes the Unclassified section.\nThe next_even pointer expands the Even section. The next_odd pointer shrinks the Unclassified section from the right (effectively expanding the Odd section). When next_even finds an odd number, it swaps it with A[next_odd], placing the odd number in the correct final partition, and then shrinks the next_odd boundary. The element that gets swapped into A[next_even] still needs to be checked in the next iteration of the loop, which is why next_even doesn\u0026rsquo;t advance in the else block.\nComplexity: Time: O(n). Each element is examined at most once by next_even, and involved in at most one swap. Space: O(1). We only use a couple of pointer variables and potentially one temporary variable for the swap (though Python handles the tuple swap elegantly without explicit temp). Key Takeaway: This boot camp demonstrates a powerful pattern: using two (or sometimes more) pointers to partition an array in-place based on some property.\nTop Tips for Arrays (Page 2) This section lists crucial strategies for array problems:\nUse the array itself for space: Cleverly reuse parts of the array or use indices/values to store information, avoiding extra lists/dicts. Fill from the back: When merging or writing results, starting from the end of the allocated space can avoid overwriting needed data and reduce shifting (e.g., merging two sorted arrays into one). Overwrite, don\u0026rsquo;t delete: If you need to remove elements and order doesn\u0026rsquo;t matter for the \u0026ldquo;removed\u0026rdquo; part, just overwrite them with elements you want to keep. Much faster than shifting everything left. Process integer digits: For arrays representing large numbers, process from the LSB (rightmost/last element) for arithmetic. Or reverse the array first if processing LSB-first simplifies logic. Subarrays: Be comfortable working with slices or pointer ranges (A[start:end]). Off-by-one errors: Double-check loop conditions (\u0026lt; vs \u0026lt;=), index calculations (i+1, i-1), and empty array edge cases. Don\u0026rsquo;t preserve properties prematurely: It\u0026rsquo;s often fine to temporarily violate sortedness or other properties during intermediate steps if you restore them by the end. Arrays as direct maps: If values are in a known, limited range (like 0-99 or ASCII), use a boolean or count array indexed by the value itself (e.g., seen[value] = True). Parallel logic for 2D arrays: Often, the logic for rows (iterating i) and columns (iterating j) is very similar; exploit this symmetry. Simulation vs. Formula: For generating sequences (like spiral order), sometimes writing code that directly simulates the generation process step-by-step is easier and less error-prone than deriving a complex mathematical formula for the k-th element. Know Your Array Libraries (Python list) (Page 2-3) Python\u0026rsquo;s list is your go-to array. It\u0026rsquo;s dynamic (grows/shrinks) and versatile.\nInstantiation: my_list = [] nums = [1, 6, 3] zeros = [0] * 10 copy_list = list(original_list) List comprehensions are powerful: squares = [x*x for x in range(10)] Basic Ops: len(A) A.append(val) A.insert(idx, val) A.remove(val) (removes first occurrence) val in A (O(n) search) 2D Arrays: matrix = [[0] * cols for _ in range(rows)] BEWARE: matrix = [[0] * cols] * rows creates shallow copies of the inner list - modifying one row modifies all! Copying: B = A: B is just another reference to A. Changes to B affect A. B = list(A) or B = A[:] or B = copy.copy(A): Shallow copy. Creates a new list B, but elements inside B are still references to the same objects as in A. If A contains mutable objects (like other lists), changing them via A will reflect in B, and vice-versa. B = copy.deepcopy(A): Deep copy. Creates a fully independent copy, including copies of any nested objects. Use this when you need total separation. Key Methods: min(A), max(A) bisect module (bisect_left, bisect_right, insort): For efficient searching/insertion in sorted lists (O(log n) search, O(n) insertion). A.reverse(): Reverses in-place. reversed(A): Returns an iterator for reverse order (doesn\u0026rsquo;t modify A, doesn\u0026rsquo;t create a full new list immediately). A.sort(): Sorts in-place. sorted(A): Returns a new sorted list (doesn\u0026rsquo;t modify A). del A[i]: Deletes element at index i. del A[i:j]: Deletes slice from i up to (not including) j. Slicing (A[start:stop:step]): Super powerful! A[i:j] gives subarray. A[:j] from start up to j. A[i:] from i to end. A[:] shallow copy. A[::-1] reverses the list (returns a new reversed list). A[k:] + A[:k] rotates left by k. List Comprehensions: [expression for item in iterable if condition] Concise way to build lists. Often clearer than map/filter. Can nest: [val for row in matrix for val in row] flattens 2D list. Avoid excessive nesting (more than 2 levels often becomes hard to read). Section 5.1: The Dutch National Flag Problem (Pages 3-6) This is a classic partitioning problem, essential for understanding quicksort and in-place array manipulation.\nThe Problem: Given an array A and an index i (whose value A[i] is the \u0026ldquo;pivot\u0026rdquo;), rearrange A such that:\nAll elements less than the pivot come first. Then, all elements equal to the pivot. Finally, all elements greater than the pivot. Do this in-place (O(1) space). The relative order within the \u0026lt;, ==, \u0026gt; groups doesn\u0026rsquo;t matter.\nExample: A = [0, 1, 2, 0, 2, 1, 1], pivot_index = 3 (pivot value = 0). A valid result: [0, 0, 1, 2, 2, 1, 1] or [0, 0, 1, 1, 1, 2, 2] (after sorting groups). But the simplest valid result might be [0, 0, 1, 2, 2, 1, 1].\nApproach 1: Two Passes (Less efficient, conceptually simpler) First Pass (Less Than): Iterate through A. Keep a pointer smaller (initially 0). If A[current_index] \u0026lt; pivot, swap A[current_index] with A[smaller] and increment smaller. This brings all smaller elements to the front. Second Pass (Greater Than): Iterate backwards through A. Keep a pointer larger (initially n-1). If A[current_index] \u0026gt; pivot, swap A[current_index] with A[larger] and decrement larger. This pushes all larger elements to the end. After these two passes, elements equal to the pivot will naturally be left in the middle.\nTime: O(n) + O(n) = O(n). Space: O(1). Approach 2: Single Pass (The Standard, Efficient Solution) This is the implementation shown in the book, often preferred in interviews.\nIntuition: Divide the array into four regions using three pointers:\nA[0 ... smaller - 1]: Bottom (Known to be \u0026lt; pivot) A[smaller ... equal - 1]: Middle (Known to be == pivot) A[equal ... larger - 1]: Unclassified (Elements we haven\u0026rsquo;t processed yet) A[larger ... n - 1]: Top (Known to be \u0026gt; pivot) Pointers:\nsmaller: Boundary between Bottom and Middle. equal: Boundary between Middle and Unclassified (the current element being processed). larger: Boundary between Unclassified and Top. Algorithm:\nInitialize smaller = 0, equal = 0, larger = len(A). Get pivot = A[pivot_index]. Loop while equal \u0026lt; larger: (While there are unclassified elements) Examine A[equal]. Case 1: A[equal] \u0026lt; pivot Swap A[equal] with A[smaller]. Increment smaller. Increment equal. Case 2: A[equal] == pivot Increment equal. (Element is in the right place relative to smaller). Case 3: A[equal] \u0026gt; pivot Decrement larger. (Shrink unclassified region from the right). Swap A[equal] with A[larger]. Crucially: Do NOT increment equal here. The element just swapped into A[equal] came from the unprocessed larger region and still needs to be classified in the next loop iteration. Let\u0026rsquo;s revisit the requirements:\nInput: An array A (e.g., [0, 1, 2, 0, 2, 1, 1]) and an index i (e.g., pivot_index = 3). Pivot Value: The value at that index, pivot = A[pivot_index] (so pivot = 0 in our example). Goal: Rearrange A so it looks like this: [ \u0026lt;pivot | ==pivot | \u0026gt;pivot ] Specifically: [ All elements LESS THAN pivot ] - These come first. [ All elements EQUAL TO pivot ] - These come next. [ All elements GREATER THAN pivot ] - These come last. Why three categories? This is extremely useful as a subroutine in Quicksort. If an array has many duplicate elements equal to the pivot, standard Quicksort (which partitions into just \u0026lt; pivot and \u0026gt;= pivot) can perform poorly (O(n^2)). By putting all elements equal to the pivot into their own group in the middle, you exclude them from the recursive calls on the \u0026lt; pivot and \u0026gt; pivot subarrays. This makes Quicksort much more efficient (O(n log n) on average, O(n) if all elements are equal) when duplicates are present.\n\u0026ldquo;Do this in-place (O(1) space)\u0026rdquo;: This means you cannot create new helper arrays to store the \u0026lt;, ==, and \u0026gt; elements temporarily. You must achieve the final arrangement by only swapping elements within the original array A, using only a constant amount of extra memory (like variables for the pivot value and your pointers smaller, equal, larger). This is the main challenge and why the pointer techniques are important.\n\u0026ldquo;The relative order within the \u0026lt;, ==, \u0026gt; groups doesn\u0026rsquo;t matter\u0026rdquo;: This is a relaxation that makes the problem easier. Look at the example A = [0, 1, 2, 0, 2, 1, 1] with pivot = 0.\nThe \u0026lt; pivot group is empty. The == pivot group contains [0, 0]. The \u0026gt; pivot group contains [1, 2, 2, 1, 1]. A valid final arrangement could be [0, 0, | 1, 2, 2, 1, 1]. Another could be [0, 0, | 2, 1, 1, 2, 1]. As long as all the 0s come first, followed by all the numbers greater than 0, it\u0026rsquo;s correct. We don\u0026rsquo;t need to keep the original relative order of the 1s and 2s within the \u0026gt; pivot group. This freedom allows us to perform swaps more easily. If we had to maintain relative order (a \u0026ldquo;stable partition\u0026rdquo;), the O(1) space solution would be much harder.\nIn essence: The Dutch Flag problem is just a more detailed partitioning than even_odd. Instead of two bins, you have three (\u0026lt;, ==, \u0026gt;). The challenge lies in achieving this three-way split efficiently using only the existing array.\nSection 5.2: Increment an Arbitrary-Precision Integer (Page 7 / PDF Page 43) The Concept: Standard integer types in many languages (like int in C++ or Java) have a fixed size (e.g., 32 or 64 bits) and thus a maximum value. Python handles this automatically with its arbitrary-precision integers, but interview problems often ask you to simulate this behavior, typically by storing the digits of a very large number in an array (or list in Python).\nRepresentation: An array A represents a non-negative integer D. Each element A[i] holds a single digit. The most significant digit is usually at A[0].\nExample: D = 129 is represented as A = [1, 2, 9]. Example: D = 99 is represented as A = [9, 9]. The Problem: Write a function that takes such an array A representing integer D and updates it in-place to represent D + 1. Handle potential carries, including the case where the number of digits increases (like 99 + 1 = 100).\nBrute-Force (and why it\u0026rsquo;s often not allowed/intended):\nConvert the array [1, 2, 9] into the integer 129. Add 1: 129 + 1 = 130. Convert 130 back into an array [1, 3, 0]. Limitation: This fails if the integer D is larger than the maximum value the language\u0026rsquo;s built-in integer type can hold (this isn\u0026rsquo;t an issue for Python\u0026rsquo;s runtime integers, but the problem setup often simulates fixed-precision constraints or asks you to avoid this conversion). It also doesn\u0026rsquo;t modify the array in-place directly.\nThe Grade-School Algorithm Approach: Think about how you add 1 to a number on paper, like 129 + 1:\nStart from the rightmost digit (Least Significant Digit - LSB). Add 1 to the last digit: 9 + 1 = 10. Write down the 0, carry-over the 1. Move to the next digit to the left (2). Add the carry: 2 + 1 = 3. Write down the 3. No carry-over this time (carry is 0). Move to the next digit to the left (1). Add the carry: 1 + 0 = 1. Write down the 1. No carry-over. No more digits, result is 130. Now consider 99 + 1:\nStart rightmost: 9 + 1 = 10. Write 0, carry 1. Next digit: 9 + 1 (carry) = 10. Write 0, carry 1. No more digits, but we still have a carry. This means the result needs an extra digit. The result is 100. Simulating on the Array:\nWe operate directly on the array A. Increment the last element: A[n-1] += 1. Iterate from right-to-left (from n-1 down to 1). If A[i] == 10 (meaning we had a carry into this position): Set A[i] = 0. Increment the digit to the left: A[i-1] += 1. If A[i] is not 10 after adding the potential carry, then the carry propagation stops, and we can break the loop early. Special Case: After the loop, check if the first digit A[0] became 10. If it did, it means we had a carry out of the most significant digit. Set A[0] = 1. Append a 0 to the end of the array to accommodate the new least significant digit. (The book uses a slightly different but equivalent way: set A[0]=1, A.append(0)). Section 5.3: Multiply Two Arbitrary-Precision Integers (Page 7-8 / PDF Pages 43-44) The Problem: Now, instead of adding 1, multiply two non-negative integers num1 and num2, represented as arrays of digits. Return their product, also as an array of digits. Handle potential negative numbers indicated by a negative sign in the first element (e.g., [-7, 6, 1] represents -761).\nExample: [1, 2, 3] (123) * [9, 8, 7] (987) = [1, 2, 1, 4, 0, 1] (121401)\nAgain, Brute-Force Conversion Fails: Converting to built-in integers and multiplying won\u0026rsquo;t work for numbers exceeding the language\u0026rsquo;s fixed precision limits.\nGrade-School Multiplication Algorithm: Think about 123 * 987:\n123 x 987 ------ 861 (= 7 * 123) 984 (= 8 * 123, shifted left 1 place) 1107 (= 9 * 123, shifted left 2 places) ------ 121401 (Sum of the above) Simulating on Arrays:\nHandle Sign:\nDetermine the sign of the result (-1 if one input is negative, 1 otherwise). Make both input arrays positive for the multiplication step (e.g., using abs() on the first element). Result Array Size:\nThe product of an n-digit number and an m-digit number can have at most n + m digits. Create a result array of size n + m, initialized to zeros. Core Multiplication Loop: Iterate through num1 from right-to-left (index i) and num2 from right-to-left (index j).\nFor each pair num1[i] and num2[j], calculate the product num1[i] * num2[j]. This product contributes to the position result[i + j + 1] (think about the place values: 10a * 10b = 10(a+b)). Add this product to result[i + j + 1]. Handle the carry: The carry from this addition goes into result[i + j]. result[i + j] += result[i + j + 1] // 10. Keep the digit: result[i + j + 1] %= 10. This directly adds the partial products into the correct positions in the result array, simulating the addition step of grade-school multiplication implicitly. Remove Leading Zeros:\nThe result array might have leading zeros (e.g., if multiplying small numbers like [1]*[1]). Find the first non-zero digit and return the slice from that point onwards. Handle the case where the result is 0 (return [0]). Apply Sign:\nIf the overall sign calculated in step 1 was negative, negate the first element of the final result array. Complexity: Time: O(n * m), where n and m are the number of digits in the two input numbers. This comes from the nested loops iterating through all pairs of digits. Space: O(n + m) for the result array. Section 5.4: Advancing Through an Array (Page 8 / PDF Page 44) The Problem: Imagine a board game where you\u0026rsquo;re on a sequence of positions represented by an array A. Each A[i] contains a non-negative integer telling you the maximum number of steps you can take forward from position i. You start at index 0. Can you reach the last index of the array?\nExample 1: A = [3, 3, 1, 0, 2, 0, 1]\nStart at index 0 (A[0]=3). Can move 1, 2, or 3 steps. Option 1: Move 1 step to index 1 (A[1]=3). From index 1, can move 1, 2, or 3 steps. Let\u0026rsquo;s move 3 steps to index 4 (A[4]=2). From index 4, can move 1 or 2 steps. Let\u0026rsquo;s move 2 steps to index 6 (A[6]=1). Index 6 is the last index. Yes, we can reach the end. Example 2: A = [3, 2, 0, 0, 2, 0, 1]\nStart at index 0 (A[0]=3). Can move 1, 2, or 3 steps. If we move 1 step -\u0026gt; index 1 (A[1]=2). Can reach index 2 or 3. If we move 2 steps -\u0026gt; index 2 (A[2]=0). Stuck. If we move 3 steps -\u0026gt; index 3 (A[3]=0). Stuck. From index 1, max reach is 1+2=3. Both indices 2 and 3 have 0s. We cannot get past index 3. No, we cannot reach the end. Initial Thoughts \u0026amp; Why Simple Greedy Fails: You might think: \u0026ldquo;Always take the maximum jump possible from the current position.\u0026rdquo; This doesn\u0026rsquo;t work. Consider A = [2, 4, 1, 1, 0, 2, 3]. Max jump from index 0 takes you to index 2 (A[2]=1). From there you get stuck. But taking just 1 step from index 0 to index 1 (A[1]=4) lets you jump much further and potentially reach the end.\nThe Efficient Approach: Track Maximum Reach The key idea is to iterate through the array and keep track of the furthest index we know we can possibly reach so far.\nAlgorithm:\nInitialize max_reach = 0. This variable will store the furthest index reachable from the positions we\u0026rsquo;ve visited. Iterate through the array with an index i from 0 up to len(A) - 1. Crucial Check: In each iteration i, first check if i \u0026gt; max_reach. If i is greater than max_reach, it means we could never have reached the current index i from any previous position. Therefore, we definitely cannot reach the end. Return False. Update the maximum reach: Calculate the furthest we could jump from the current position i. This is i + A[i]. Update max_reach = max(max_reach, i + A[i]). Check for early success: If at any point max_reach becomes greater than or equal to the last index (len(A) - 1), we know we can reach the end. Return True. If the loop completes without returning False (meaning we could always reach the current i) and without having reached the end yet (which implies max_reach might have landed exactly on the last index in the final iteration), we still need a final check after the loop, but the logic shown below incorporates this cleanly. Note: The book\u0026rsquo;s code slightly reformulates the loop condition and final check, but the core logic of tracking max_reach is the same.\nIntuition Walkthrough (Example 1: A = [3, 3, 1, 0, 2, 0, 1]) i=0: max_reach=0. i \u0026lt;= max_reach is true. max_reach = max(0, 0+3) = 3. i becomes 1. i=1: max_reach=3. i \u0026lt;= max_reach is true. max_reach = max(3, 1+3) = 4. i becomes 2. i=2: max_reach=4. i \u0026lt;= max_reach is true. max_reach = max(4, 2+1) = 4. i becomes 3. i=3: max_reach=4. i \u0026lt;= max_reach is true. max_reach = max(4, 3+0) = 4. i becomes 4. i=4: max_reach=4. i \u0026lt;= max_reach is true. max_reach = max(4, 4+2) = 6. i becomes 5. Loop condition: i \u0026lt;= max_reach (5 \u0026lt;= 6) is true, but max_reach \u0026lt; last_index (6 \u0026lt; 6) is false. Loop terminates. Return max_reach \u0026gt;= last_index (6 \u0026gt;= 6) -\u0026gt; True. Intuition Walkthrough (Example 2: A = [3, 2, 0, 0, 2, 0, 1]) i=0: max_reach=0. i \u0026lt;= max_reach is true. max_reach = max(0, 0+3) = 3. i becomes 1. i=1: max_reach=3. i \u0026lt;= max_reach is true. max_reach = max(3, 1+2) = 3. i becomes 2. i=2: max_reach=3. i \u0026lt;= max_reach is true. max_reach = max(3, 2+0) = 3. i becomes 3. i=3: max_reach=3. i \u0026lt;= max_reach is true. max_reach = max(3, 3+0) = 3. i becomes 4. i=4: max_reach=3. i \u0026lt;= max_reach (4 \u0026lt;= 3) is false. Loop terminates earlier. Return max_reach \u0026gt;= last_index (3 \u0026gt;= 6) -\u0026gt; False. Analogy: Hopping Stones Across a River Imagine the array indices 0, 1, 2, ... n-1 are stones lined up across a river.\nYou start at stone 0. The number A[i] on each stone tells you the maximum length of a single jump you can take from that stone. So, from stone i, you can land on any stone j where i \u0026lt; j \u0026lt;= i + A[i]. Goal: Can you reach the last stone (stone n-1)? Why simple jumping doesn\u0026rsquo;t work: If you just stand on stone i and decide which jump to take next, you might make a bad choice. A shorter jump might land you on a stone with a much bigger A[j] value that lets you cross the river, while the longest jump from i might land you somewhere useless.\nThe max_reach Idea: Tracking Your Potential Why this \u0026ldquo;tracking the potential\u0026rdquo; works:\nIt\u0026rsquo;s safe: We only advance our current position i if it\u0026rsquo;s within the known max_reach. If we ever encounter an i that\u0026rsquo;s beyond max_reach, it means no combination of previous jumps could have gotten us there. It\u0026rsquo;s sufficient: We don\u0026rsquo;t need to know the exact path. We only need to know if the potential to reach the end exists. By always updating max_reach to the maximum possible distance (max(current_max_reach, i + A[i])), we are essentially keeping track of the \u0026ldquo;frontier\u0026rdquo; of reachable stones. If that frontier ever touches or crosses the last stone, we know a path must exist. Think of it like this: max_reach is your \u0026ldquo;optimistic outlook\u0026rdquo;. At each step i you can actually reach, you check if the jump from i gives you a more optimistic outlook than you already had. If your current position i ever falls behind your optimistic outlook (i \u0026gt; max_reach), then your optimism was unfounded, and you\u0026rsquo;re stuck.\nInstead of deciding which specific jump to take, we focus on a simpler question: \u0026ldquo;What is the absolute furthest stone I could possibly reach, given the stones I\u0026rsquo;ve visited so far?\u0026rdquo;\nComplexity: Time: O(n). We iterate through the array at most once. Space: O(1). We only use a few variables (max_reach, i, last_index). Variant: Minimum Steps The variant asks for the minimum number of steps to reach the end. This smells like a Breadth-First Search (BFS).\nThink of array indices as nodes in a graph. There\u0026rsquo;s an edge from i to j if j is reachable from i (i.e., i \u0026lt; j \u0026lt;= i + A[i]). You want the shortest path from node 0 to node n-1.\nA standard BFS exploring reachable indices level by level will find the minimum number of jumps. You\u0026rsquo;d need a way to keep track of visited indices to avoid cycles/redundant work and a queue to manage the BFS levels. The state in the queue could be (index, num_jumps). This would likely take O(n) time and O(n) space (for the queue and visited set/array).\nSection 5.5: Delete Duplicates from a Sorted Array (Page 9 / PDF Page 45) The Problem: You are given a sorted array A. Your task is to remove duplicate elements so that each unique element appears only once. You should do this in-place, modifying the original array A. The elements that are kept should be shifted to the beginning of the array. You need to return the number of valid, unique elements remaining in the array. What\u0026rsquo;s stored in the array beyond the last valid element doesn\u0026rsquo;t matter.\nExample:\nInput: A = [2, 3, 5, 5, 7, 11, 11, 11, 13] Output: The function should return 6. After the function runs, the array A should be modified so its beginning looks like: [2, 3, 5, 7, 11, 13, ... ]. The values in the positions after index 5 are irrelevant (e.g., [2, 3, 5, 7, 11, 13, 11, 11, 13] would be a valid state for A). Why \u0026ldquo;Sorted\u0026rdquo; is Key: Because the array is sorted, all duplicate elements are guaranteed to be grouped together consecutively. This makes identifying them much easier than if the array were unsorted.\nCommon Pitfalls:\nUsing O(n) space: It\u0026rsquo;s easy to iterate through A, store unique elements in a set or a new list, and then copy them back. But the problem often requires O(1) extra space (in-place modification). Shifting elements repeatedly: You could find a duplicate A[i] == A[i-1], and then shift all elements from A[i+1] onwards one step to the left. This works, but if you have many duplicates (e.g., [2, 2, 2, 2, 3]), you end up doing a lot of shifting, leading to O(n^2) time complexity in the worst case. The Efficient In-Place Approach: Two Pointers (Read \u0026amp; Write) This is a very common and useful pattern for modifying arrays in-place when you want to keep only certain elements.\nThe Strategy:\nUse one pointer (read_index or just the loop index i in the code below) to iterate through the original array from the second element onwards. Use a second pointer (write_index) to keep track of the next position in the array where a unique element should be written. Initialize write_index = 1 (since the very first element A[0] is always unique by definition, assuming the array isn\u0026rsquo;t empty). Compare the element being read (A[i]) with the last unique element written (A[write_index - 1]). If A[i] is different from A[write_index - 1], it means we\u0026rsquo;ve found a new unique element. Copy this unique element A[i] to the next available write position: A[write_index] = A[i]. Advance the write pointer: write_index += 1. If A[i] is the same as A[write_index - 1], it\u0026rsquo;s a duplicate. We simply ignore it and move the read pointer (i) forward, without advancing write_index. This effectively overwrites duplicates later when a new unique element is found. After iterating through the entire array, write_index will hold the count of unique elements, and the subarray A[0 ... write_index - 1] will contain those unique elements. Section 5.6: Buy and Sell a Stock Once (Page 10 / PDF Page 46) The Problem: You\u0026rsquo;re given an array prices where prices[i] is the price of a given stock on day i. You want to find the maximum profit you can achieve by buying the stock on one day and selling it on a later day. If no profit can be made (prices always decrease), the maximum profit is 0. You must buy before you sell.\nExample: prices = [310, 315, 275, 295, 260, 270, 290, 230, 255, 250]\nBuy at 260 (day 4, index 4), Sell at 290 (day 6, index 6). Profit = 290 - 260 = 30. This is the maximum possible profit. Note: minimum price is 230, max is 315, but 315 occurs before 230, so you can\u0026rsquo;t buy at 230 and sell at 315. Brute-Force: Try every possible pair of (buy_day, sell_day) where sell_day \u0026gt; buy_day. Calculate the profit for each pair and keep track of the maximum.\nUse nested loops: outer loop for buy_day from 0 to n-2, inner loop for sell_day from buy_day + 1 to n-1. Time Complexity: O(n^2). Space Complexity: O(1). (Too slow for large inputs). Divide and Conquer (Mentioned in Book Introduction, not here): Split the array in half, find the max profit in the left half, find the max profit in the right half, and find the max profit that crosses the midpoint (buy in left, sell in right). The crossing part requires finding the min price in the left and max price in the right. This leads to O(n log n) time. Better, but we can do even better.\nThe Efficient Linear Scan Approach:\nKey Idea: As you iterate through the prices day by day, what information do you need to know from the past to decide the best potential profit if you sell today? You need to know the minimum price encountered before today.\nAlgorithm:\nInitialize min_price_so_far to a very large value (or prices[0] if the array is not empty). Initialize max_profit to 0.0. Iterate through the prices array, starting from the first price. Let the current price be price. Calculate potential profit if selling today: potential_profit = price - min_price_so_far. Update maximum profit: max_profit = max(max_profit, potential_profit). Update minimum price seen: min_price_so_far = min(min_price_so_far, price). After iterating through all prices, max_profit will hold the maximum profit achievable. Intuition Walkthrough (prices = [310, 315, 275, 295, 260, 270, 290, 230, 255, 250]): price min_price_so_far (Before Update) profit_sell_today max_profit (After Update) min_price_so_far (After Update) 310 inf -inf 0 310 315 310 5 5 310 275 310 -35 5 275 295 275 20 20 275 260 275 -15 20 260 270 260 10 20 260 290 260 30 30 260 230 260 -30 30 230 255 230 25 30 230 250 230 20 30 230 Final max_profit = 30.\nComplexity: Time: O(n). We iterate through the array exactly once. Space: O(1). We only use two variables (min_price_so_far, max_profit). Variant: Longest Equal Subarray Find the length of the longest contiguous subarray where all entries are equal.\nIterate through the array, keeping track of the current_length of the equal sequence and the max_length found so far.\nIf A[i] == A[i-1], increment current_length. If A[i] != A[i-1], reset current_length to 1. Update max_length = max(max_length, current_length) in each iteration. O(n) time, O(1) space.\nIntuition Walkthrough (prices = [310, 315, 275, 295, 260, 270, 290, 230, 255, 250]): price min_price_so_far (Before Update) profit_sell_today max_profit (After Update) min_price_so_far (After Update) 310 inf -inf 0 310 315 310 5 5 310 275 310 -35 5 275 295 275 20 20 275 260 275 -15 20 260 270 260 10 20 260 290 260 30 30 260 230 260 -30 30 230 255 230 25 30 230 250 230 20 30 230 Final max_profit = 30.\nComplexity: Time: O(n). We iterate through the array exactly once. Space: O(1). We only use two variables (min_price_so_far, max_profit). Variant: Longest Equal Subarray Find the length of the longest contiguous subarray where all entries are equal.\nIterate through the array, keeping track of the current_length of the equal sequence and the max_length found so far.\nIf A[i] == A[i-1], increment current_length. If A[i] != A[i-1], reset current_length to 1. Update max_length = max(max_length, current_length) in each iteration. O(n) time, O(1) space.\nSection 5.7: Buy and Sell a Stock Twice (Page 11 / PDF Page 47) The Problem: Same setup as before, but now you can buy and sell at most twice. The second buy must happen after the first sell. Find the maximum profit.\nExample: prices = [12, 11, 13, 9, 12, 8, 14, 13, 15]\nFirst Tx: Buy at 9, Sell at 12 (Profit 3) Second Tx: Buy at 8, Sell at 15 (Profit 7) Total Profit = 3 + 7 = 10. This is the maximum. Brute-Force: Try all combinations of two disjoint buy-sell intervals. O(n4) (choose 4 days). Too slow.\nImproved Brute-Force: Iterate through all possible split points i (day the first sale happens or second buy happens). Find the max profit for one transaction in prices[0...i] and the max profit for one transaction in prices[i...n-1]. Add them up. Keep track of the maximum sum.\nUsing the O(n) algorithm from Section 5.6 for each subarray takes O(n) time. Doing this for n possible split points gives O(n2) total time. Space is O(1). Better, but still not optimal. The Efficient Dynamic Programming Approach:\nThe Core Difficulty: How do you decide the four days (buy1, sell1, buy2, sell2) optimally? Trying all combinations is too slow (O(n4)).\nThe Key Insight (Decomposition): Imagine you complete your first transaction by selling on day i.\nThis means you bought on some day b1 \u0026lt;= i and sold on day i. The best you could have done for this first transaction ending on day i is MaxProfit(prices[0...i]). Let\u0026rsquo;s call this Profit1_ending_at_i. Now, you need to make your second transaction using only the days after day i.\nThis means you buy on some day b2 \u0026gt; i and sell on some day s2 \u0026gt;= b2. The best you could do for this second transaction happening after day i is MaxProfit(prices[i+1...n-1]). Let\u0026rsquo;s call this Profit2_starting_after_i. If we split the entire process at day i (meaning the first transaction finishes by day i, and the second starts after day i), the total profit for this specific split point i would be: TotalProfit(split at i) = Profit1_ending_at_i + Profit2_starting_after_i\nThe Problem: We don\u0026rsquo;t know the best day i to split the timeline. So, we need to calculate this TotalProfit(split at i) for every possible split day i (from i=0 to i=n-1) and take the maximum among them.\nHow to Calculate Efficiently? Calculating Profit1_ending_at_i and Profit2_starting_after_i from scratch for every i is slow (O(n2)). We need to precompute or compute iteratively.\nStep 1: Calculate all Profit1_ending_at_i values.\nNotice that Profit1_ending_at_i is exactly the maximum profit you can make with a single transaction within the range prices[0...i]. We already know how to calculate this efficiently! It\u0026rsquo;s the logic from the \u0026ldquo;Buy and Sell Once\u0026rdquo; problem (Section 5.6). We can run that algorithm once, and as we iterate through the prices array, we store the maximum profit found up to that point in a new array. Let\u0026rsquo;s call this array F (for Forward). F[i] = Maximum profit from one transaction using prices from day 0 to day i. This takes one pass, O(n) time, and O(n) space for the F array. Step 2: Calculate all Profit2_starting_after_i values.\nThis is the maximum profit from one transaction using only prices from day i+1 to day n-1. How can we calculate this efficiently for all i? We can run the \u0026ldquo;Buy and Sell Once\u0026rdquo; logic backwards! Imagine iterating from the end of the prices array (n-1) down to 0. Keep track of the max_price_seen_so_far (during the backward scan). The best profit if you buy on day j (during this backward scan) is max_price_seen_so_far - prices[j]. Let B[j] be the maximum profit from one transaction using prices from day j to day n-1. We can compute all B[j] values in a single backward pass (O(n) time). We could store this in another array B. Step 3: Combine the Results.\nNow we have F[i] (best profit ending by day i) and B[j] (best profit starting from day j). The total profit for a split right after day i is F[i] + B[i+1]. We need to calculate F[i] + B[i+1] for all i from 0 to n-2. The final answer is the maximum value found in these sums. We also need to consider the possibility that the best profit comes from only one transaction (e.g., if prices always go down after the first transaction). The best single-transaction profit over the whole period is simply F[n-1]. So the overall maximum is max(F[n-1], max(F[i] + B[i+1] for i in 0..n-2)). The Book\u0026rsquo;s Space Optimization: The book realizes we don\u0026rsquo;t actually need to store the entire B array from Step 2. We can combine Step 2 and Step 3.\nCalculate and store the F array (forward pass) as described in Step 1. Perform the backward pass (like Step 2). As you iterate backwards with index i (from n-1 down to 1): Calculate the maximum profit for a single transaction starting at or after day i. Let\u0026rsquo;s call this profit_second_tx. (This is conceptually B[i]). You do this by keeping track of max_price_so_far encountered during the backward scan. profit_second_tx = max(profit_if_buy_at_i, profit_if_buy_later). Immediately combine: Now that you know the best profit starting after day i-1 (which is profit_second_tx you just calculated for index i), you can add it to the best profit ending before day i (which is F[i-1], already calculated in the forward pass). total_profit_for_split_at_i-1 = F[i-1] + profit_second_tx. Keep track of the overall maximum total profit seen across all split points considered so far. Let\u0026rsquo;s retry the Intuition Walkthrough (prices = [12, 11, 13, 9, 12, 8, 14, 13, 15]): Forward Pass: Compute F = [0, 0, 2, 2, 3, 3, 6, 6, 7].\nmax_total_profit initially is the best single transaction profit, so max_total_profit = F[8] = 7. Backward Pass \u0026amp; Combine:\ni = 8 (price prices[8]=15): max_price (for range [8..8]) = 15. profit_second_tx (for range [8..8]) is max(0, 15-15) = 0. total = F[7](6) + 0 = 6. max_total_profit = max(7, 6) = 7. i = 7 (price prices[7]=13): max_price (for range [7..8]) = max(15, 13) = 15. profit_second_tx (for range [7..8]) is max(profit_starting_at_8=0, profit_buy_at_7=15-13=2) = 2. total = F[6](6) + 2 = 8. max_total_profit = max(7, 8) = 8. i = 6 (price prices[6]=14): max_price (for range [6..8]) = max(15, 14) = 15. profit_second_tx (for range [6..8]) is max(profit_starting_at_7=2, profit_buy_at_6=15-14=1) = 2. total = F[5](3) + 2 = 5. max_total_profit = max(8, 5) = 8. i = 5 (price prices[5]=8): max_price (for range [5..8]) = max(15, 8) = 15. profit_second_tx (for range [5..8]) is max(profit_starting_at_6=2, profit_buy_at_5=15-8=7) = 7. total = F[4](3) + 7 = 10. max_total_profit = max(8, 10) = 10. \u0026hellip; and so on. The backward pass calculates the best profit for the second transaction on the fly and immediately combines it with the pre-calculated best profit for the first transaction ending just before it.\nThink of the backward loop index i as defining the start day for the potential second transaction. profit_second_tx calculated at step i represents the best possible profit if your second transaction happens entirely within prices[i...n-1]. We add this to F[i-1] (best profit from first transaction ending before day i) to get the total profit for that specific way of splitting the two transactions. We do this for all possible start days i for the second transaction and find the overall maximum.\nSection 5.8: Computing an Alternation (Page 12 / PDF Page 48) The Problem: You are given an array A of numbers. Rearrange its elements in-place to create a new array (or modify A directly) such that it follows an alternating pattern: A[0] \u0026lt;= A[1] \u0026gt;= A[2] \u0026lt;= A[3] \u0026gt;= A[4] \u0026lt;= A[5] ... (Less-than-or-equal followed by greater-than-or-equal, repeating).\nExample:\nInput: A = [3, 1, 4, 1, 5, 9, 2, 6] A valid output: [1, 4, 1, 5, 2, 9, 3, 6] (Check: 1\u0026lt;=4\u0026gt;=1\u0026lt;=5\u0026gt;=2\u0026lt;=9\u0026gt;=3\u0026lt;=6) - Note: 3\u0026lt;=6 is satisfied at the end. Another valid output: [1, 3, 1, 4, 2, 6, 5, 9] Approach 1: Sorting (Simple but Suboptimal) Sort the array A. Example: [1, 1, 2, 3, 4, 5, 6, 9] Swap adjacent pairs starting from the second element. Swap A[1] and A[2], then A[3] and A[4], then A[5] and A[6], etc. Swap A[1](1) and A[2](2) -\u0026gt; [1, 2, 1, 3, 4, 5, 6, 9] Swap A[3](3) and A[4](4) -\u0026gt; [1, 2, 1, 4, 3, 5, 6, 9] Swap A[5](5) and A[6](6) -\u0026gt; [1, 2, 1, 4, 3, 6, 5, 9] Check the result: 1\u0026lt;=2\u0026gt;=1\u0026lt;=4\u0026gt;=3\u0026lt;=6\u0026gt;=5\u0026lt;=9. It works! Complexity: Dominated by the sort step. Time: O(n log n). Space: O(1) if using an in-place sort, or O(n) otherwise. Approach 2: Median Finding (Mentioned, but not fully explored) Find the median element of the array A (using an O(n) median-of-medians algorithm, like in Problem 11.8). Partition the array around the median (like one step of Quicksort or using Dutch Flag) so elements smaller than the median are first, then elements equal to the median, then elements larger. This takes O(n) time. Interleave elements from the lower half and the upper half, similar to the swapping pattern in Approach 1. Complexity: Dominated by median finding and partitioning. Time: O(n). Space: O(1). This is theoretically optimal time, but the median-of-medians algorithm is complex to implement. Approach 3: The Clever Local Swap (Efficient and Simple) Key Insight: The required property (A[i-1] \u0026lt;= A[i] \u0026gt;= A[i+1] or A[i-1] \u0026gt;= A[i] \u0026lt;= A[i+1]) is very local. It only involves adjacent elements. We don\u0026rsquo;t necessarily need the global order provided by sorting or median finding.\nThe Goal: Arrange the array so it goes \u0026ldquo;up, down, up, down\u0026hellip;\u0026rdquo; like this: Small \u0026lt;= Big \u0026gt;= Small \u0026lt;= Big \u0026gt;= Small ...\nThe Rule We Need:\nAt every even index i, we need A[i] \u0026lt;= A[i+1]. At every odd index i, we need A[i] \u0026gt;= A[i+1]. The Local Swap Algorithm: We walk through the array, looking at pairs (A[i], A[i+1]). We force the local rule to be true for that pair.\nLook at (A[0], A[1]) (i=0, which is even):\nRule: We need A[0] \u0026lt;= A[1]. If it\u0026rsquo;s already true, great, do nothing. If A[0] \u0026gt; A[1], swap them. Now A[0] \u0026lt;= A[1] is true for this pair. Look at (A[1], A[2]) (i=1, which is odd):\nRule: We need A[1] \u0026gt;= A[2]. If it\u0026rsquo;s true, do nothing. If A[1] \u0026lt; A[2], swap them. Now A[1] \u0026gt;= A[2] is true for this pair. Look at (A[2], A[3]) (i=2, which is even):\nRule: We need A[2] \u0026lt;= A[3]. If A[2] \u0026gt; A[3], swap them. Now A[2] \u0026lt;= A[3] is true. \u0026hellip;and so on. Why Doesn\u0026rsquo;t Enforcing the Rule at Step i Break the Rule We Just Fixed at Step i-1? This is the core confusion. Let\u0026rsquo;s trace carefully. We need to check the condition involving A[i] in both the step i-1 comparison and the step i comparison.\nConsider index i. It\u0026rsquo;s involved in two checks:\nCheck at step i-1: Compares (A[i-1], A[i]). Check at step i: Compares (A[i], A[i+1]). Let\u0026rsquo;s analyze the transition at index i when we process step i.\nCase 1: i is EVEN.\nThe rule we enforce at step i: We ensure A[i] \u0026lt;= A[i+1] (by swapping if needed). What about the rule from step i-1? Step i-1 was ODD. The rule enforced there was A[i-1] \u0026gt;= A[i]. Does enforcing A[i] \u0026lt;= A[i+1] break A[i-1] \u0026gt;= A[i]? If we didn\u0026rsquo;t swap at step i (because A[i] \u0026lt;= A[i+1] was already true): No problem, the previous condition A[i-1] \u0026gt;= A[i] still holds (we didn\u0026rsquo;t change A[i] or A[i-1]). If we did swap at step i (because A[i] \u0026gt; A[i+1] initially): Let the original values be Old_A[i] and Old_A[i+1]. We know Old_A[i] \u0026gt; Old_A[i+1]. The new value at A[i] is New_A[i] = Old_A[i+1]. The condition from step i-1 was A[i-1] \u0026gt;= Old_A[i]. Since Old_A[i] \u0026gt; Old_A[i+1] = New_A[i], it follows that A[i-1] \u0026gt; New_A[i]. So, the required condition A[i-1] \u0026gt;= New_A[i] still holds! Swapping in a smaller value at A[i] cannot break the previous A[i-1] \u0026gt;= A[i] condition. Case 2: i is ODD.\nThe rule we enforce at step i: We ensure A[i] \u0026gt;= A[i+1] (by swapping if needed). What about the rule from step i-1? Step i-1 was EVEN. The rule enforced there was A[i-1] \u0026lt;= A[i]. Does enforcing A[i] \u0026gt;= A[i+1] break A[i-1] \u0026lt;= A[i]? If we didn\u0026rsquo;t swap at step i (because A[i] \u0026gt;= A[i+1] was already true): No problem. If we did swap at step i (because A[i] \u0026lt; A[i+1] initially): Let the original values be Old_A[i] and Old_A[i+1]. We know Old_A[i] \u0026lt; Old_A[i+1]. The new value at A[i] is New_A[i] = Old_A[i+1]. The condition from step i-1 was A[i-1] \u0026lt;= Old_A[i]. Since Old_A[i] \u0026lt; Old_A[i+1] = New_A[i], it follows that A[i-1] \u0026lt; New_A[i]. So, the required condition A[i-1] \u0026lt;= New_A[i] still holds! Swapping in a larger value at A[i] cannot break the previous A[i-1] \u0026lt;= A[i] condition. Conclusion: Enforcing the rule for the pair (A[i], A[i+1]) preserves the rule that was already established for the pair (A[i-1], A[i]) in the previous step. Since we process the array from left to right, by the time we reach the end, all adjacent pairs satisfy their respective conditions (\u0026lt;= for even i, \u0026gt;= for odd i), fulfilling the overall alternation requirement.\nAnalogy: Smoothing Out Bumps Imagine the numbers are heights. You want a profile like low \u0026lt;= high \u0026gt;= low \u0026lt;= high ...\nYou walk along the profile, looking at two adjacent points i and i+1.\nIf i should be a \u0026ldquo;low\u0026rdquo; point (i is even), but A[i] is higher than A[i+1], you swap them to push the high point to i+1. This makes A[i] lower, which doesn\u0026rsquo;t violate any preceding \u0026ldquo;high \u0026gt;= low\u0026rdquo; condition. If i should be a \u0026ldquo;high\u0026rdquo; point (i is odd), but A[i] is lower than A[i+1], you swap them to pull the high point to i. This makes A[i] higher, which doesn\u0026rsquo;t violate any preceding \u0026ldquo;low \u0026lt;= high\u0026rdquo; condition. Section 5.9: Enumerate All Primes to n (Page 13 / PDF Page 49) The Problem: Given an integer n, return a list of all prime numbers between 1 and n (inclusive).\nPrime Number Definition: A natural number p is prime if it is greater than 1 and has no divisors other than 1 and itself. (2, 3, 5, 7, 11, 13, 17, 19\u0026hellip; are primes). 1 is not prime.\nExample:\nInput: n = 18 Output: [2, 3, 5, 7, 11, 13, 17] Approach 1: Trial Division (Brute-Force) Iterate through each number i from 2 up to n. For each i, check if it\u0026rsquo;s prime. How to check if i is prime? Try dividing i by every number d from 2 up to sqrt(i). If you find any d that divides i evenly (i % d == 0), then i is not prime. Stop checking for this i and move to the next i. If you check all d up to sqrt(i) and find no divisors, then i is prime. If i is found to be prime, add it to your results list. Why only check up to sqrt(i)? If i has a divisor d larger than sqrt(i), then i = d * k. For this equation to hold, k must be smaller than sqrt(i). So, if i has any divisor other than 1 and itself, it must have at least one divisor less than or equal to its square root.\nComplexity: Checking if a single number i is prime takes roughly O(sqrt(i)) time. Doing this for all numbers from 2 to n gives a total time complexity around O(n * sqrt(n)) or O(n1.5). Space is O(p) where p is the number of primes found, roughly O(n/log n).\nApproach 2: Sieve of Eratosthenes (Much More Efficient) This is the classic algorithm for finding all primes up to n. It avoids redundant checks by eliminating multiples.\nThe Idea:\nStart with a list (or boolean array) representing all numbers from 0 up to n. Initially, mark all numbers from 2 to n as potentially prime (e.g., True). Mark 0 and 1 as not prime (False). Find the first number p in the list (starting from 2) that is still marked as potentially prime. This number p must be prime (because if it had a smaller divisor, that divisor would have been found earlier, and p would have already been marked as not prime). Add p to your list of primes. Sieve: Mark all multiples of p (i.e., 2*p, 3*p, 4*p, \u0026hellip;) up to n as not prime (False) in your boolean array. They cannot be prime because they have p as a divisor. Repeat from step 2: Find the next number in the list greater than p that is still marked as potentially prime, and repeat the sieving process. Continue until you have checked numbers up to n. Sieve Optimization 1: When sieving multiples of prime p, you can start marking from p*p (p2). Why? Any smaller multiple k*p where k \u0026lt; p would have already been marked as not prime when you processed the prime factors of k.\nSieve Optimization 2: You only need to find primes p up to sqrt(n) to perform the sieving. Any composite number c \u0026lt;= n must have a prime factor less than or equal to sqrt(n). If c hasn\u0026rsquo;t been marked off by the time you reach sqrt(n), it must be prime itself.\nThe Old Way (Trial Division): Is 2 prime? Yes. Is 3 prime? Yes. Is 4 prime? No (divisible by 2). Is 5 prime? Yes. Is 6 prime? No (divisible by 2). Is 7 prime? Yes. Is 8 prime? No (divisible by 2). Is 9 prime? No (divisible by 3). \u0026hellip; This is slow because we check 9 for divisibility by 3 even though we already knew 3 was prime and could have somehow used that information. The Sieve Idea: Be Proactive! Instead of checking each number individually, when we find a prime number, let\u0026rsquo;s immediately cross out all of its multiples because we know they cannot be prime.\nSteps for n = 20:\nList all numbers: Write down all numbers from 2 up to 20. Imagine we have little flags (or boolean values) next to them, initially all set to \u0026ldquo;potentially prime\u0026rdquo; (True). [ (2,T), (3,T), (4,T), (5,T), (6,T), (7,T), (8,T), (9,T), (10,T), (11,T), (12,T), (13,T), (14,T), (15,T), (16,T), (17,T), (18,T), (19,T), (20,T) ]\nFind the first number p that is marked True: Start from the beginning. The first number marked True is p = 2.\nAction: We know 2 is prime (it\u0026rsquo;s the first one we found). Add 2 to our prime list: Primes = [2]. Sieve Action: Now, cross out (mark as False) all multiples of 2 in our list (4, 6, 8, 10, 12, 14, 16, 18, 20). List becomes: [ (2,T), (3,T), (4,F), (5,T), (6,F), (7,T), (8,F), (9,T), (10,F), (11,T), (12,F), (13,T), (14,F), (15,T), (16,F), (17,T), (18,F), (19,T), (20,F) ] Find the next number p (after the previous p=2) that is marked True: Looking through the list, the next True is at p = 3.\nAction: We know 3 must be prime (because if it had a smaller prime factor, it would have been crossed out in a previous step - the only smaller prime was 2, and 3 isn\u0026rsquo;t a multiple of 2). Add 3 to our prime list: Primes = [2, 3]. Sieve Action: Cross out all multiples of 3 (starting from 3*2=6, though 6 is already False. Next is 3*3=9, then 3*4=12(F), 3*5=15, 3*6=18(F)). List becomes: [ (2,T), (3,T), (4,F), (5,T), (6,F), (7,T), (8,F), (9,F), (10,F), (11,T), (12,F), (13,T), (14,F), (15,F), (16,F), (17,T), (18,F), (19,T), (20,F) ] Find the next number p (after p=3) that is marked True: The next True is at p = 5.\nAction: 5 must be prime. Add 5 to list: Primes = [2, 3, 5]. Sieve Action: Cross out multiples of 5 (5*2=10(F), 5*3=15(F), 5*4=20(F)). No changes needed as they were already False. List remains the same. Find the next number p (after p=5) that is marked True: The next True is at p = 7.\nAction: 7 must be prime. Add 7 to list: Primes = [2, 3, 5, 7]. Sieve Action: Cross out multiples of 7 (7*2=14(F)). No changes. List remains the same. Find the next number p (after p=7) that is marked True: The next True is at p = 11.\nAction: 11 must be prime. Add 11 to list: Primes = [2, 3, 5, 7, 11]. Sieve Action: Cross out multiples of 11 (11*2=22, which is \u0026gt; 20). No changes. List remains the same. Find the next number p (after p=11) that is marked True: The next True is at p = 13.\nAction: 13 must be prime. Add 13: Primes = [2, 3, 5, 7, 11, 13]. Sieve Action: Multiples are \u0026gt; 20. Find the next number p (after p=13) that is marked True: The next True is at p = 17.\nAction: 17 must be prime. Add 17: Primes = [2, 3, 5, 7, 11, 13, 17]. Sieve Action: Multiples are \u0026gt; 20. Find the next number p (after p=17) that is marked True: The next True is at p = 19.\nAction: 19 must be prime. Add 19: Primes = [2, 3, 5, 7, 11, 13, 17, 19]. Sieve Action: Multiples are \u0026gt; 20. Find the next number p (after p=19) that is marked True: There are none left up to n=20. We are done.\nThe Final Prime List: [2, 3, 5, 7, 11, 13, 17, 19].\nWhy is this efficient?\nWe only \u0026ldquo;process\u0026rdquo; (find multiples of) the actual prime numbers (2, 3, 5, 7, etc.). We touch each composite number (like 4, 6, 8, 9, 10, 12\u0026hellip;) only when we cross it out as a multiple of one of its prime factors. We don\u0026rsquo;t waste time trying to see if 12 is divisible by 4, 5, 6, etc., because it was already crossed out when we processed prime 2 (and again for prime 3). The Optimization (Sieving from p*p) Let\u0026rsquo;s look at step 3 again, when p=3. We crossed out 6, 9, 12, 15, 18. Notice that 6 was already crossed out when we processed p=2. Why? Because 6 = 2 * 3. The factor k=2 is less than p=3. In general, when processing prime p, any multiple k * p where k \u0026lt; p has already been crossed out by the prime factors of k (which are all smaller than p). So, the first multiple of p that we really need to worry about crossing out is p * p.\nFor p=3, we start crossing out from 3*3 = 9. For p=5, we start crossing out from 5*5 = 25 (which is \u0026gt; 20, so we do nothing). For p=7, we start from 7*7=49 (\u0026gt; 20). This optimization saves some redundant work. The Boolean Array (is_prime) In the code, instead of a list of pairs like (2, T), we use a simple boolean array (a list of True/False values). The index of the array represents the number. is_prime = [False, False, True, True, True, ... True] (up to index n)\nWhen we process p=2, we loop for i in range(p*p, n+1, p): is_prime[i] = False. When we process p=3, we loop for i in range(p*p, n+1, p): is_prime[i] = False. And so on. Section 5.10: Permute the Elements of an Array (Page 14 / PDF Page 50) The Problem: You are given an array A of n elements, and a permutation array P of length n. The permutation array P describes how to rearrange A: for each index i, the element currently at A[i] should be moved to position P[i]. You need to apply this permutation to array A in-place (O(1) extra space).\nExample:\nA = [a, b, c, d] P = [2, 0, 1, 3] Interpretation of P:\nElement at index 0 (a) moves to index P[0]=2. Element at index 1 (b) moves to index P[1]=0. Element at index 2 (c) moves to index P[2]=1. Element at index 3 (d) moves to index P[3]=3. Desired Result: A should become [b, c, a, d].\nApproach 1: Using Extra Space (Simple but Not In-Place) Create a new array B of the same size as A. Iterate through A from i = 0 to n-1. For each element A[i], place it in its target position in B: B[P[i]] = A[i]. Copy the contents of B back into A. Complexity: Time: O(n) (for the iteration and copy). Space: O(n) (for the auxiliary array B). Issue: Doesn\u0026rsquo;t meet the O(1) space requirement. Approach 2: In-Place Permutation using Cycles Key Insight: Any permutation can be broken down into one or more disjoint cycles.\nA cycle means that element at index i0 moves to i1, element at i1 moves to i2, \u0026hellip;, element at ik-1 moves to ik, and element at ik moves back to i0. In our example P = [2, 0, 1, 3]:\nStart at index 0: Element a moves to index 2 (P[0]=2). Look at index 2: Element c moves to index 1 (P[2]=1). Look at index 1: Element b moves to index 0 (P[1]=0). We\u0026rsquo;re back at index 0. This forms the cycle 0 -\u0026gt; 2 -\u0026gt; 1 -\u0026gt; 0. Start at index 3 (the next unvisited index): Element d moves to index 3 (P[3]=3). This is a cycle of length 1: 3 -\u0026gt; 3. The permutation P consists of two cycles: (0 2 1) and (3). Algorithm Idea: If we can process one cycle at a time, we can do it in place.\nIterate through the array A from i = 0 to n-1. For each index i, check if the element at this position has already been moved as part of a previous cycle. If it has, skip it. If the element at i hasn\u0026rsquo;t been moved, we\u0026rsquo;ve found the start of a new cycle. Process this cycle: Remember the starting index start = i and the value temp = A[start]. Easier cycle traversal: Store the element you\u0026rsquo;re about to overwrite: value_to_move = A[i]. Follow the cycle: curr = i. Loop: next_idx = P[curr]. Save the element at the destination: next_value = A[next_idx]. Move the value_to_move to the destination: A[next_idx] = value_to_move. Update curr = next_idx. Update value_to_move = next_value. Mark the position curr (or maybe P[curr]) as visited so we don\u0026rsquo;t process this cycle again. Stop when curr gets back to the starting index i. How to Mark Visited Cycles without O(n) Space? This is the clever trick. We can modify the permutation array P itself to mark elements that have been processed. The book suggests subtracting n (or using the sign bit if possible) from P[i] after its corresponding element A[i] has been placed correctly.\nThe Cycle Idea: Follow One Element All the Way Let\u0026rsquo;s pick an element and follow its journey until it lands in its final spot, and see where the element it displaced needs to go, and so on.\nLet\u0026rsquo;s trace starting with index i = 0:\nElement: A[0] is \u0026lsquo;a\u0026rsquo;. Destination: Where should \u0026lsquo;a\u0026rsquo; go? P[0] tells us it goes to index 2. Problem: Index 2 currently holds \u0026lsquo;c\u0026rsquo;. We can\u0026rsquo;t just overwrite \u0026lsquo;c\u0026rsquo;. Solution: Before moving \u0026lsquo;a\u0026rsquo; to index 2, let\u0026rsquo;s save \u0026lsquo;c\u0026rsquo; somewhere temporarily. Let temp = 'c'. Move \u0026lsquo;a\u0026rsquo;: Now, put \u0026lsquo;a\u0026rsquo; into A[2]. The array A is now ['a', 'b', 'a', 'd']. (We still have \u0026lsquo;c\u0026rsquo; saved in temp). Where does \u0026lsquo;c\u0026rsquo; go? \u0026lsquo;c\u0026rsquo; originally came from index 2. According to P, the element from index 2 should go to index P[2] = 1. Problem: Index 1 currently holds \u0026lsquo;b\u0026rsquo;. Solution: Save \u0026lsquo;b\u0026rsquo;. Let temp = 'b'. Move \u0026lsquo;c\u0026rsquo;: Put \u0026lsquo;c\u0026rsquo; (which we saved way back in step 4) into A[1]. Array A is now ['a', 'c', 'a', 'd']. (We have \u0026lsquo;b\u0026rsquo; saved in temp). Where does \u0026lsquo;b\u0026rsquo; go? \u0026lsquo;b\u0026rsquo; originally came from index 1. According to P, the element from index 1 should go to index P[1] = 0. Problem: Index 0 currently holds \u0026lsquo;a\u0026rsquo;. Solution: Save \u0026lsquo;a\u0026rsquo;. Let temp = 'a'. Move \u0026lsquo;b\u0026rsquo;: Put \u0026lsquo;b\u0026rsquo; (saved in step 8) into A[0]. Array A is now ['b', 'c', 'a', 'd']. (We have \u0026lsquo;a\u0026rsquo; saved in temp). Where does \u0026lsquo;a\u0026rsquo; go? This \u0026lsquo;a\u0026rsquo; originally came from index 0. We followed the path 0 -\u0026gt; 2 -\u0026gt; 1 -\u0026gt; 0. We are back where we started! This \u0026lsquo;a\u0026rsquo; we saved in step 12 is the one we started with. Since we\u0026rsquo;re back at the start, the cycle is complete. We don\u0026rsquo;t need to place this saved \u0026lsquo;a\u0026rsquo;; it\u0026rsquo;s already implicitly handled because the element now at index 0 (\u0026lsquo;b\u0026rsquo;) is correct. We\u0026rsquo;ve just processed the cycle 0 -\u0026gt; 2 -\u0026gt; 1 -\u0026gt; 0! The array A is now ['b', 'c', 'a', 'd'].\nHow do we know we are done? We need to check other starting points.\nLet\u0026rsquo;s trace starting with index i = 1: We need a way to know if index 1 was already handled. If we followed the cycle correctly above, index 1 was involved. We need a mechanism to remember this. Let\u0026rsquo;s trace starting with index i = 3: Assume index 3 hasn\u0026rsquo;t been handled yet. Element: A[3] is \u0026rsquo;d\u0026rsquo;. Destination: P[3] tells us it goes to index 3. It\u0026rsquo;s already there! This is a cycle of length 1: 3 -\u0026gt; 3. We don\u0026rsquo;t need to do anything. Putting it Together \u0026amp; The \u0026ldquo;Marking\u0026rdquo; Trick: The process above shows how a cycle works with temporary variables. The challenge is doing it without explicitly using lots of temp variables (just one is needed at a time) and knowing which cycles have been completed. This is where modifying the perm array comes in (or using a visited array). We mark perm[k] by making it negative after we\u0026rsquo;ve moved the element originally at A[k] to its correct spot.\nMy previous manual trace using the book\u0026rsquo;s code swap logic was slightly off. The core idea of the book\u0026rsquo;s code is to complete a cycle correctly.\nCorrected Trace (Using Book\u0026rsquo;s Code Logic - A[next_idx], A[target_idx] = A[target_idx], A[next_idx]):\nInitial State: A = ['a', 'b', 'c', 'd'], perm = [2, 0, 1, 3]\nOuter loop: i = 0\nnext_idx = 0. perm[0] (2) \u0026gt;= 0. Enter while. target_idx = perm[0] = 2. Swap A[0] (\u0026lsquo;a\u0026rsquo;) and A[target_idx] (A[2] which is \u0026lsquo;c\u0026rsquo;). A becomes ['c', 'b', 'a', 'd']. temp = perm[0] (which is 2). Mark perm[0]: perm[0] = 2 - 4 = -2. perm is now [-2, 0, 1, 3]. next_idx = temp = 2. perm[2] (1) \u0026gt;= 0. Continue while. target_idx = perm[2] = 1. Swap A[next_idx] (A[2] which is \u0026lsquo;a\u0026rsquo;) and A[target_idx] (A[1] which is \u0026lsquo;b\u0026rsquo;). A becomes ['c', 'a', 'b', 'd']. temp = perm[2] (which is 1). Mark perm[2]: perm[2] = 1 - 4 = -3. perm is now [-2, 0, -3, 3]. next_idx = temp = 1. perm[1] (0) \u0026gt;= 0. Continue while. target_idx = perm[1] = 0. Swap A[next_idx] (A[1] which is \u0026lsquo;a\u0026rsquo;) and A[target_idx] (A[0] which is \u0026lsquo;c\u0026rsquo;). A becomes ['a', 'c', 'b', 'd']. temp = perm[1] (which is 0). Mark perm[1]: perm[1] = 0 - 4 = -4. perm is now [-2, -4, -3, 3]. next_idx = temp = 0. perm[0] (-2) \u0026lt; 0. Exit while loop. Outer loop: i = 1\nperm[1] (-4) is negative. Skip while loop. (This index was part of the cycle we just processed). Outer loop: i = 2\nperm[2] (-3) is negative. Skip while loop. Outer loop: i = 3\nnext_idx = 3. perm[3] (3) \u0026gt;= 0. Enter while. target_idx = perm[3] = 3. Swap A[3] (\u0026rsquo;d\u0026rsquo;) and A[3] (\u0026rsquo;d\u0026rsquo;). A is unchanged: ['a', 'c', 'b', 'd']. temp = perm[3] (which is 3). Mark perm[3]: perm[3] = 3 - 4 = -1. perm is now [-2, -4, -3, -1]. next_idx = temp = 3. perm[3] (-1) \u0026lt; 0. Exit while loop. Outer loop finishes.\nFinal state for A is ['a', 'c', 'b', 'd']. This trace still gives the incorrect result.\nThe confusion arises from how the swaps in the book\u0026rsquo;s code achieve the \u0026ldquo;following an element\u0026rdquo; logic. Let\u0026rsquo;s use the \u0026ldquo;pick up and carry\u0026rdquo; mental model which is more direct.\nCycle Following - Simpler View (Corrected to match desired outcome):\nThis approach correctly gets the desired result by focusing on placing the element currently \u0026ldquo;held\u0026rdquo; (value) to its destination, and then picking up the element that was at that destination.\nOuter loop i = 0 to n-1: If perm[i] is negative, this element\u0026rsquo;s cycle is already processed. Skip. Start a new cycle: current_cycle_idx = i value_to_place = A[i] (the element we are \u0026ldquo;holding\u0026rdquo;) Inner do-while loop (or while True with a break): next_target_pos = perm[current_cycle_idx] (where value_to_place should go). value_at_target = A[next_target_pos] (save the element currently at the target). A[next_target_pos] = value_to_place (place our \u0026ldquo;held\u0026rdquo; value). Mark perm[current_cycle_idx] as processed (e.g., perm[current_cycle_idx] -= len(perm)). current_cycle_idx = next_target_pos (this is where value_at_target came from). value_to_place = value_at_target (this is the new value we are \u0026ldquo;holding\u0026rdquo;). If current_cycle_idx == i (we\u0026rsquo;ve returned to the start of the cycle), break the inner loop. Let\u0026rsquo;s try this view on A = ['a', 'b', 'c', 'd'], P = [2, 0, 1, 3]\nOuter loop i = 0:\nperm[0] (2) is not negative. Start cycle. current_cycle_idx = 0. value_to_place = A[0] = 'a'. Inner Loop 1: next_target_pos = perm[0] = 2. value_at_target = A[2] = 'c'. A[2] = 'a'. A is ['a', 'b', 'a', 'd']. perm[0] -= 4 \\implies -2. perm is [-2, 0, 1, 3]. current_cycle_idx = 2. value_to_place = 'c'. current_cycle_idx (2) != i (0). Continue. Inner Loop 2: next_target_pos = perm[2] = 1. value_at_target = A[1] = 'b'. A[1] = 'c'. A is ['a', 'c', 'a', 'd']. perm[2] -= 4 \\implies -3. perm is [-2, 0, -3, 3]. current_cycle_idx = 1. value_to_place = 'b'. current_cycle_idx (1) != i (0). Continue. Inner Loop 3: next_target_pos = perm[1] = 0. value_at_target = A[0] = 'a'. A[0] = 'b'. A is ['b', 'c', 'a', 'd']. perm[1] -= 4 \\implies -4. perm is [-2, -4, -3, 3]. current_cycle_idx = 0. value_to_place = 'a'. current_cycle_idx (0) == i (0). Break inner loop. Cycle for i=0 is complete. Outer loop i = 1:\nperm[1] (-4) is negative. Skip. Outer loop i = 2:\nperm[2] (-3) is negative. Skip. Outer loop i = 3:\nperm[3] (3) is not negative. Start cycle. current_cycle_idx = 3. value_to_place = A[3] = 'd'. Inner Loop 1: next_target_pos = perm[3] = 3. value_at_target = A[3] = 'd'. A[3] = 'd'. A is ['b', 'c', 'a', 'd']. perm[3] -= 4 \\implies -1. perm is [-2, -4, -3, -1]. current_cycle_idx = 3. value_to_place = 'd'. current_cycle_idx (3) == i (3). Break inner loop. Cycle for i=3 is complete. Done. Final A is ['b', 'c', 'a', 'd']. This matches the desired result! The key is correctly following the element you \u0026ldquo;picked up\u0026rdquo; and placing it, then picking up the displaced element and continuing, while marking the origin index in P as complete for that step of placing its element. The book\u0026rsquo;s code\u0026rsquo;s swap A[next_idx], A[target_idx] = A[target_idx], A[next_idx] is actually performing this \u0026ldquo;pick up and carry\u0026rdquo; implicitly. The element at target_idx is moved to next_idx, and the old A[next_idx] is moved to target_idx. The next_idx for the next iteration then becomes the original target_idx. This is a common way to implement cycle sort.\nSection 5.11: Compute the Next Permutation (Page 16 / PDF Page 52) The Problem: Given an array perm representing a permutation of numbers (e.g., [1, 0, 3, 2]), find the next permutation in dictionary (lexicographical) order. If the input is already the largest possible permutation (i.e., sorted in descending order, like [3, 2, 1, 0]), return an empty array or list.\nDictionary Order: Think of the permutations as words. [0, 1, 2] comes before [0, 2, 1], which comes before [1, 0, 2], etc. We want the permutation that comes immediately after the input one in this sorted list of all permutations.\nExample:\nInput: [1, 0, 3, 2] -\u0026gt; Output: [1, 2, 0, 3] Input: [1, 2, 0, 3] -\u0026gt; Output: [1, 2, 3, 0] Input: [3, 2, 1, 0] -\u0026gt; Output: [] (or indication of none) The Key Insight: Minimizing the Change We want to find the next permutation, which means we want to increase the sequence by the smallest possible amount while still making it larger.\nWhere to Make the Change? To make the smallest possible increase, we want to change the sequence as far to the right as possible.\nConsider [1, 0, 3, 2]:\nIf we only change [3, 2]? The only other arrangement is [2, 3]. This gives [1, 0, 2, 3]. Is this larger than the original [1, 0, 3, 2]? Yes (because at index 2, 2 \u0026lt; 3). If we only change [0, 3, 2]? We could rearrange these to [2, 0, 3], [2, 3, 0], [3, 0, 2], [3, 2, 0]. Combined with the 1 at the start, these give [1, 2, 0, 3], [1, 2, 3, 0], [1, 3, 0, 2], [1, 3, 2, 0]. All of these are larger than the original. Which one is the smallest? [1, 2, 0, 3]. Notice that [1, 2, 0, 3] seems to be the smallest possible increase we found. It involved changing the 0 at index 1.\nFinding the \u0026ldquo;Pivot\u0026rdquo; - Where the Increase Must Happen Let\u0026rsquo;s look at the sequence from right to left. perm = [1, 0, 3, 2]\n2: Nothing to compare. 3, 2: 3 \u0026gt; 2. The sequence is decreasing here. 0, 3, 2: 0 \u0026lt; 3. The decreasing trend is broken! This 0 at index k=1 is important. Why? Because any rearrangement only involving the suffix [3, 2] will keep the [1, 0] prefix the same. To get a permutation larger than [1, 0, 3, 2] but potentially smaller than [1, 0, 2, 3] (which we found by rearranging only the suffix), we must change something at or before index 1. To make the smallest change, we should change the rightmost possible element, which is perm[1]=0. This leads to:\nAlgorithm Steps:\nFind the \u0026ldquo;pivot\u0026rdquo; (k):\nScan the permutation perm from right to left. Find the first index k such that perm[k] \u0026lt; perm[k+1]. This perm[k] is the rightmost element we can increase while potentially making the resulting permutation larger than the original but as small as possible. If no such k exists (the entire array is in descending order), it\u0026rsquo;s the last permutation. Return an empty list or signal no next permutation. For [1, 0, 3, 2], k=1 (where perm[k]=0). The suffix is [3, 2]. Find the \u0026ldquo;successor\u0026rdquo; (l) to swap with the pivot:\nWe need to replace perm[k] with something larger than it. To make the overall permutation increase by the least amount, we should replace perm[k] with the smallest possible value from the suffix perm[k+1:] that is still greater than perm[k]. Scan the suffix perm[k+1:] from right to left. Find the first element perm[l] that is greater than perm[k]. For [1, 0, 3, 2], perm[k]=0. Suffix is [3, 2]. Scan from right: perm[3] = 2. Is 2 \u0026gt; 0? Yes. Found it! l=3. The successor value is 2. (Why scan from the right for the successor? Because the suffix perm[k+1:] was identified because k was the first break in a descending sequence from the right. This means the suffix perm[k+1:] itself must be in descending order. Scanning a descending sequence from the right guarantees that the first element we find that\u0026rsquo;s greater than the pivot is also the smallest element in that suffix that\u0026rsquo;s greater than the pivot.) Swap pivot and successor:\nSwap perm[k] and perm[l]. Swap perm[1](0) and perm[3](2). perm becomes [1, 2, 3, 0]. Make the rest of the sequence (suffix after k) as small as possible:\nWe\u0026rsquo;ve now made the prefix up to index k (e.g., [1, 2]) the smallest possible larger prefix. Now, we need to make the suffix (the part after index k, which is now [3, 0]) as small as possible in dictionary order to ensure this is the very next permutation. How do you make a sequence of numbers as small as possible in dictionary order? You sort it in ascending order. The suffix after index k (perm[k+1:]) was originally in descending order (or became so after the swap, as perm[l] was replaced by perm[k], which is smaller than other elements in the original suffix). To sort a descending sequence into ascending order efficiently, just reverse it! The suffix after k=1 is [3, 0]. Reversing it gives [0, 3]. Attach this to the prefix: [1, 2] + [0, 3] = [1, 2, 0, 3]. This is the final answer. Let\u0026rsquo;s try perm = [6, 2, 1, 5, 4, 3, 0] again:\nFind k: Scan right-to-left.\n0 3 \u0026gt; 0 4 \u0026gt; 3 5 \u0026gt; 4 1 \u0026lt; 5. Break! k=2 (pivot perm[k]=1). Suffix is [5, 4, 3, 0]. Find l: Scan suffix [5, 4, 3, 0] right-to-left for first element \u0026gt; perm[k](1).\n0 \u0026gt; 1? No. 3 \u0026gt; 1? Yes. Found it! l=5 (index in original array, successor perm[l]=3). Swap: Swap perm[k](1) and perm[l](3).\nperm becomes [6, 2, 3, 5, 4, 1, 0]. Reverse suffix: Reverse the part after k=2, which is [5, 4, 1, 0].\nReversing gives [0, 1, 4, 5]. Final Result: [6, 2, 3] + [0, 1, 4, 5] = [6, 2, 3, 0, 1, 4, 5].\nThis algorithm correctly finds the immediate next permutation by making the smallest necessary change to the prefix and then making the suffix as small as possible.\nSampling Algorithms and their implementation Problem: You have a bag (array A) with n distinct items. You want to pull out exactly k items such that every possible group of k items has the same chance of being selected. You do this by modifying the original array A so the chosen k items end up in the first k positions (A[0] to A[k-1]).\nAnalogy: Picking Players for a Team You have n players lined up (A[0] to A[n-1]). You need to form a team of k players. You want the selection process to be completely fair – any group of k players is equally likely. A Naive (But Fair) Way - Drawing Names from a Hat:\nWrite each of the n player names on a slip of paper. Put all n slips into a hat. Shake the hat well. Draw out k slips one by one without putting them back. This is perfectly fair. Every group of k names has an equal chance. How can we simulate this \u0026ldquo;drawing from a hat\u0026rdquo; using the array A directly? This is what the Offline Sampling algorithm (random_sampling) does. Let\u0026rsquo;s trace it with A = ['A', 'B', 'C', 'D', 'E'] and k = 3. We want to pick 3 players fairly and put them in A[0], A[1], A[2].\nThe Algorithm Step-by-Step: Goal: Fill A[0], then A[1], then A[2] with fairly chosen players.\nStep 1: Fill A[0] (i=0)\nIdea: We need to pick one player randomly out of the entire group (A[0] to A[4]) to put into the first team slot (A[0]). Action: Choose a random index r between 0 and 4 (inclusive). Let\u0026rsquo;s say random.randrange(0, 5) gives r = 3. Swap the player currently at A[0] (which is \u0026lsquo;A\u0026rsquo;) with the player at the random index A[r] (which is A[3], \u0026lsquo;D\u0026rsquo;). A becomes ['D', 'B', 'C', 'A', 'E']. Result: A[0] now contains \u0026lsquo;D\u0026rsquo;. \u0026lsquo;D\u0026rsquo; was chosen uniformly from all 5 original players. We have fairly selected the first team member and placed them in the first slot. The remaining players (['B', 'C', 'A', 'E']) are now in slots A[1] to A[4]. Step 2: Fill A[1] (i=1)\nIdea: We need to pick the second team member. This member must be chosen fairly from the players who haven\u0026rsquo;t been picked yet. Where are the unpicked players? They are currently in the array from index i=1 to the end (A[1] to A[4]). There are n-i = 5-1 = 4 players remaining (\u0026lsquo;B\u0026rsquo;, \u0026lsquo;C\u0026rsquo;, \u0026lsquo;A\u0026rsquo;, \u0026lsquo;E\u0026rsquo;). Action: Choose a random index r between 1 and 4 (inclusive). Let\u0026rsquo;s say random.randrange(1, 5) gives r = 1. Swap the player currently at A[i] (which is A[1], \u0026lsquo;B\u0026rsquo;) with the player at the random index A[r] (which is A[1], \u0026lsquo;B\u0026rsquo;). A remains ['D', 'B', 'C', 'A', 'E']. (Swapping with itself). Result: A[1] now contains \u0026lsquo;B\u0026rsquo;. \u0026lsquo;B\u0026rsquo; was chosen uniformly from the 4 players available (B, C, A, E) after \u0026lsquo;D\u0026rsquo; was picked. We have fairly selected the second team member. The remaining 3 players (['C', 'A', 'E']) are now in slots A[2] to A[4]. Step 3: Fill A[2] (i=2)\nIdea: Pick the third team member fairly from the remaining players. The available players are at indices i=2 to the end (A[2] to A[4]). There are n-i = 5-2 = 3 players remaining (\u0026lsquo;C\u0026rsquo;, \u0026lsquo;A\u0026rsquo;, \u0026lsquo;E\u0026rsquo;). Action: Choose a random index r between 2 and 4 (inclusive). Let\u0026rsquo;s say random.randrange(2, 5) gives r = 4. Swap the player currently at A[i] (which is A[2], \u0026lsquo;C\u0026rsquo;) with the player at the random index A[r] (which is A[4], \u0026lsquo;E\u0026rsquo;). A becomes ['D', 'B', 'E', 'A', 'C']. Result: A[2] now contains \u0026lsquo;E\u0026rsquo;. \u0026lsquo;E\u0026rsquo; was chosen uniformly from the 3 players available (C, A, E) after \u0026lsquo;D\u0026rsquo; and \u0026lsquo;B\u0026rsquo; were picked. We have fairly selected the third team member. Stop: We needed k=3 players, and we have filled A[0], A[1], A[2].\nFinal State: The array is ['D', 'B', 'E', 'A', 'C']. The Sample: The first k=3 elements are ['D', 'B', 'E']. This is our uniformly random sample.\nWhy is this like drawing from a hat? At step i, choosing a random index r from i to n-1 is like drawing one name randomly from the n-i names still left in the hat. Swapping A[i] with A[r] is like taking the drawn name (A[r]) and putting it aside as the i-th selected player (by placing it at A[i]), while the name that was at A[i] is effectively put back into the \u0026ldquo;remaining players\u0026rdquo; part of the array (at index r) to potentially be drawn later. This process ensures that at each step i, every available element has an equal chance of being selected for position i. Over k steps, this builds up a sample where every possible group of k had an equal shot.\nLet\u0026rsquo;s try a different random sequence for k=2, A=['a','b','c','d']:\ni=0: n=4. Pick r from [0, 1, 2, 3]. Say r=2. Swap A[0](\u0026lsquo;a\u0026rsquo;) and A[2](\u0026lsquo;c\u0026rsquo;). A = ['c', 'b', 'a', 'd']. A[0] is fairly chosen. i=1: n=4. Pick r from [1, 2, 3]. Say r=3. Swap A[1](\u0026lsquo;b\u0026rsquo;) and A[3](\u0026rsquo;d\u0026rsquo;). A = ['c', 'd', 'a', 'b']. A[1] is fairly chosen from the rest. Stop. k=2. Sample is A[0..1] = ['c', 'd']. Think about filling slots 0 to k-1. For slot i, you randomly pick one element from all elements currently at or after index i and swap it into place.\nSection 5.12: Sample Offline Data (Page 18 / PDF Page 54) The Scenario: Imagine you have a large dataset (like all your customers, or all sensor readings from a day) stored in an array A. You want to select a smaller, random sample of size k from this dataset (e.g., to send a survey to, or to analyze more closely). The key requirement is that every possible subset of size k must have an equal chance of being selected. This is called uniform random sampling. \u0026ldquo;Offline\u0026rdquo; means you have the entire dataset A available before you start sampling.\nThe Problem: Implement an algorithm that takes an array A of n distinct elements and an integer k (where k \u0026lt;= n). It should rearrange A such that the first k elements (A[0] to A[k-1]) hold a uniformly random subset of the original elements of size k.\nExample:\nInput: A = [3, 7, 5, 11], k = 3 Possible outputs (in the first k=3 slots): [3, 7, 5], [3, 5, 11], [7, 3, 11], [11, 5, 7], etc. Each combination of 3 elements should appear with probability 1/(4 choose 3) = 1/4. The order within the first k elements doesn\u0026rsquo;t strictly matter for the subset definition, but the algorithm naturally produces a random permutation within the subset. Why Not Just Pick Random Indices? If you just pick k random indices between 0 and n-1, you might pick the same index multiple times. If you discard duplicates and retry, the process becomes complex and potentially slow (related to the Coupon Collector\u0026rsquo;s problem), especially if k is close to n.\nThe Efficient In-Place Algorithm (Fisher-Yates Shuffle Adaptation): This algorithm is elegant and efficient. It builds the random subset in the first k positions of the array iteratively.\nAlgorithm:\nIterate with an index i from 0 up to k-1. This loop runs exactly k times. In each iteration i: We need to select a random element from the portion of the array that hasn\u0026rsquo;t been considered for the sample yet. This \u0026ldquo;unseen\u0026rdquo; portion is from index i to n-1 (inclusive). Generate a random integer r such that i \u0026lt;= r \u0026lt; n. (Pick a random index from the current position i to the end). Swap the element at the current index i with the element at the randomly chosen index r: A[i], A[r] = A[r], A[i]. After the loop finishes (after k swaps), the subarray A[0...k-1] contains the uniformly random sample of size k. Why does this work? (Intuition)\nIteration i=0: We randomly pick an index r from 0 to n-1. We swap A[0] with A[r]. Now A[0] holds a uniformly random element chosen from the entire original array. Iteration i=1: We randomly pick an index r from 1 to n-1. This picks uniformly from all elements except the one we already placed at A[0]. We swap A[1] with A[r]. Now A[1] holds a uniformly random element chosen from the remaining n-1 elements. A[0] and A[1] together now form a random pair. Iteration i: We pick a random index r from i to n-1. This selects uniformly from the n-i elements that haven\u0026rsquo;t yet been fixed into the sample positions 0 to i-1. We swap A[i] with A[r]. This places a uniformly chosen element (from the remaining available ones) into position i. By induction, after k steps, A[0...k-1] holds a uniformly chosen subset of size k. Each element had an equal chance of ending up in each position of the sample.\nSection 5.13: Sample Online Data (Page 19 / PDF Page 55) The Scenario: Now imagine data isn\u0026rsquo;t available all at once (\u0026ldquo;offline\u0026rdquo;). Instead, you\u0026rsquo;re receiving a stream of data items (like packets in a network session, or log entries as they happen). You don\u0026rsquo;t know how many items will be in the stream in total beforehand. You want to maintain a random sample of size k from the items seen so far. As each new item arrives, you update your sample. The sample must always be a uniformly random subset of the items processed up to that point.\nThe Problem: Design an algorithm that takes an input size k and reads a sequence of data items (an iterator or stream). It should continuously maintain a subset of size k of the items read so far. This subset must be a uniformly random sample.\nWhy this is Tricky:\nYou don\u0026rsquo;t know n (the total number of items) in advance. When the (m+1)-th item arrives, you need to decide if it should be in your sample of size k (replacing one of the existing k items) or if it should be discarded. This decision must ensure that after processing m+1 items, any k items from the m+1 seen so far have an equal chance of being in your sample. The Algorithm: Reservoir Sampling This is a classic algorithm for this problem.\nAlgorithm:\nInitialization: Read the first k items from the stream and store them directly into your sample (let\u0026rsquo;s call it sample_array of size k). At this point, sample_array is trivially a uniform random sample of the first k items. Process Subsequent Items: For each subsequent item x that arrives (from item k+1 onwards): Let num_items_seen_so_far be the total number of items processed so far (including the current item x). So, when processing the (k+1)-th item, num_items_seen_so_far = k+1. When processing the (k+2)-th, num_items_seen_so_far = k+2, and so on. Generate a random integer r between 0 and num_items_seen_so_far - 1 (inclusive). (Or 1 to num_items_seen_so_far and adjust). Decision: If r \u0026lt; k (i.e., if the random number falls within the range 0 to k-1), then replace the element sample_array[r] with the current item x. If r \u0026gt;= k, do nothing; discard the current item x. After processing all items in the stream, sample_array will hold a uniformly random sample of size k from the entire stream. Why does this work? (Proof by Induction - High Level)\nBase Case: After seeing k items, the sample_array contains all k items. This is a uniform random sample (the only possible sample of size k from k items).\nInductive Hypothesis: Assume that after seeing m items (where m \u0026gt;= k), the sample_array (of size k) is a uniformly random sample of size k from those m items. This means any specific item y from the first m items has a probability of k/m of being in the sample_array.\nInductive Step: Now, the (m+1)-th item, let\u0026rsquo;s call it x_m+1, arrives.\nWhat is the probability that x_m+1 enters the sample? It enters if we pick a random number r (from 0 to m) and r \u0026lt; k. The probability of this is k / (m+1). What is the probability that an old item y (which was among the first m items and was in the sample) remains in the sample? Prob(y was in sample after m items) = k/m. Prob(y is not kicked out when x_m+1 arrives) = Prob(x_m+1 is not chosen to replace anything) + Prob(x_m+1 is chosen, but y is not the one picked for replacement). This is (1 - k/(m+1)) (if x_m+1 doesn\u0026rsquo;t replace anything) + (k/(m+1)) * ((k-1)/k) (if x_m+1 replaces, but it replaces one of the other k-1 items). = (m+1-k)/(m+1) + (k-1)/(m+1) = (m+1-k+k-1)/(m+1) = m/(m+1). So, Prob(y remains in sample) = (Prob(y was in sample)) * (Prob(y not kicked out)) = (k/m) * (m/(m+1)) = k/(m+1). Conclusion: After m+1 items, both the new item x_m+1 and any old item y (that was previously in the sample) now have a probability of k/(m+1) of being in the sample. This holds true for all items, thus maintaining the uniform random property.\nsampling Algorithms Recap Recall the Offline Sampling (Fisher-Yates) Logic: At step i (filling the i-th slot of our sample, 0-indexed), we do:\nr = random.randrange(i, n) A[i], A[r] = A[r], A[i] The key principle ensuring fairness here is: At step i, we choose uniformly from all items not yet placed in the sample (those in positions i through n-1) and place the chosen item into position i.\nNow, the Online/Reservoir Sampling Challenge: When the m-th item arrives (m \u0026gt; k), we don\u0026rsquo;t know n. We cannot simply pick a random index r from i to n-1. We only know the items seen so far (m items). We have a reservoir R of size k which (by induction) is a fair sample of the first m-1 items.\nGoal: Update the reservoir R using the m-th item (x_m) such that R becomes a fair sample of all m items. Fairness means every item x_1, ..., x_m must have a k/m probability of being in R.\nReservoir Sampling Step for item m:\nGenerate r = random.randrange(m) (an index from 0 to m-1). if r \u0026lt; k: R[r] = x_m (replace element at index r in the reservoir with the new item). How does this achieve the k/m probability goal?\nLet\u0026rsquo;s connect this to the \u0026ldquo;drawing from a hat\u0026rdquo; idea.\nImagine a Hypothetical Hat: Conceptually, after m-1 items, our reservoir R holds k items drawn fairly from a hat containing the first m-1 items. Item m Arrives: It\u0026rsquo;s like adding the m-th item\u0026rsquo;s name (x_m) to the hat. Now the hat contains m items. Fair Draw Needed: To get a fair sample of size k from this hat of m items, each item in the hat should have a k/m chance of being selected if we were to redraw completely. Can we achieve this without redrawing?\nLet\u0026rsquo;s analyze the chances for the new item x_m:\nThe algorithm gives x_m a chance to get into the reservoir if r \u0026lt; k, where r is chosen from 0 to m-1. There are k \u0026ldquo;winning\u0026rdquo; values for r (0 to k-1) out of m total possibilities. So, Prob(x_m enters the reservoir) = k/m. This is exactly the fairness probability we need for the new item! Now, let\u0026rsquo;s analyze the chances for an old item Y that was already in the reservoir R before x_m arrived.\nBy our assumption (inductive hypothesis), Y was in the reservoir with probability k / (m-1) after m-1 items.\nFor Y to still be in the reservoir after item m is processed, one of two things must happen:\nThe random number r was not less than k (i.e., r \u0026gt;= k). This happens with probability (m-k)/m. In this case, x_m is ignored, and Y definitely stays. The random number r was less than k (prob k/m), AND the slot chosen for replacement was not Y\u0026rsquo;s slot. Since r is chosen uniformly from 0 to k-1 in this case to be the replacement index, the chance that Y\u0026rsquo;s specific slot is not picked is (k-1)/k. So, the probability of this combined event is (k/m) * (k-1)/k = (k-1)/m. Total probability Y stays in the reservoir = Prob(Case 1) + Prob(Case 2) = (m-k)/m + (k-1)/m = (m-1)/m.\nNow, the final probability that Y is in the reservoir after processing item m is: Prob(Y was in after m-1) * Prob(Y stays when m arrives) = [k / (m-1)] * [(m-1) / m] = k/m.\nThe Connection: Both algorithms achieve fairness, but through different mechanisms constrained by what information is available:\nOffline (Fisher-Yates): Knows n. Can directly simulate picking from the remaining available items at each step i by choosing a random index r in the future part of the array (i to n-1) and swapping into position i. It ensures fairness by giving every available item an equal chance (1/(n-i)) to be put into slot i. Online (Reservoir): Doesn\u0026rsquo;t know n. Cannot pick from future indices. Instead, it uses probabilities based on the count m seen so far. It gives the new item x_m the exact target probability k/m of being included. If included, the random replacement maintains the correct probabilities for the older items, ensuring they also end up with probability k/m. It achieves the same final probability distribution as Offline Sampling, but through an incremental, probabilistic update rather than direct selection and placement. Think of Reservoir Sampling as cleverly calculating how often the new item should replace an old one to maintain the correct k/m probability for everyone in the long run, even without knowing the total number of items m will eventually reach.\nSection 5.15: Compute a Random Subset (Page 21 / PDF Page 57) The Problem: Given a positive integer n and a size k \u0026lt;= n, return a size-k subset of the set {0, 1, 2, ..., n-1}. The subset should be represented as an array. All (n choose k) possible subsets should be equally likely. Additionally, all permutations of elements within the returned array should be equally likely (though this is often a natural byproduct of algorithms that produce uniform subsets).\nHow is this different from Offline Sampling (Section 5.12)?\nOffline Sampling (5.12): Input is an existing array A. Output is a modification of A where A[0...k-1] holds the sample. It operates on given elements. This Problem (5.15): Input is just integers n and k. The \u0026ldquo;elements\u0026rdquo; are implicitly the integers from 0 to n-1. We need to construct an array containing a random subset of these integers. Solution Approach 1: Mimic Offline Sampling (Fisher-Yates) Create an array elements = [0, 1, ..., n-1]. Apply the Offline Sampling algorithm (from Section 5.12, also known as Fisher-Yates shuffle) to this elements array to pick k items. Return the first k elements of the shuffled elements array (elements[0...k-1]). Complexity: Time: O(n) to create the initial elements array + O(k) for sampling = O(n). Space: O(n) for the elements array. Can we do better if k is much smaller than n? (e.g., sample 3 items from 1 million) Creating an array of 1 million elements just to pick 3 seems wasteful.\nSolution Approach 2: Using a Hash Table (for k \u0026laquo; n) This approach avoids creating the full array of n elements if k is small. It directly simulates the Fisher-Yates logic but only keeps track of the elements that would have been moved from their original positions.\nThe Idea: Imagine you\u0026rsquo;re running the Fisher-Yates shuffle on the conceptual array [0, 1, ..., n-1].\nThe array initially has A[j] = j. When you iterate for the i-th element of your sample (from i=0 to k-1): You pick a random index rand_idx from i to n-1. You swap A[i] and A[rand_idx]. If k is small, most of the conceptual array A will remain A[j]=j. Only a few elements get swapped. We can use a hash table (dictionary in Python) to store only those elements whose values are not equal to their indices (i.e., the elements that have been moved). Algorithm with Hash Table:\nInitialize an empty dictionary changed_elements. This dictionary will store index: value pairs for any position where the value is different from what it would be in the identity permutation [0, 1, ..., n-1] after our simulated swaps. Iterate i from 0 to k-1 (to select k elements for our subset). Generate a random index rand_idx in the range [i, n-1]. Get current values: value_at_i = changed_elements.get(i, i) (If i is in the dictionary, use its stored value; otherwise, its value is i). value_at_rand_idx = changed_elements.get(rand_idx, rand_idx) (Similarly for rand_idx). Perform the swap (conceptually, updating the dictionary): Store value_at_i at rand_idx: changed_elements[rand_idx] = value_at_i. Store value_at_rand_idx at i: changed_elements[i] = value_at_rand_idx. After the loop, the desired subset consists of the values that are conceptually in A[0]...A[k-1]. We retrieve these from our changed_elements dictionary (defaulting to i if i is not a key). result_subset = [changed_elements.get(j, j) for j in range(k)]. When is the Hash Table approach better?\nWhen k is significantly smaller than n.\nFisher-Yates on full array (Approach 1): Time: O(n) Space: O(n) (for the initial array [0, ..., n-1]) Hash Table method (Approach 2): Time: O(k) (k iterations, each with dictionary operations which are O(1) on average) Space: O(k) (at most 2k entries in the dictionary if i and rand_idx are always different and neither was previously in the dictionary) So, if k \u0026lt;\u0026lt; n (k is much much less than n), the hash table method wins on both time and space by avoiding the creation and full scan of the n-element array.\nIf k is close to n (e.g., k = n/2 or k = n-1), the O(n) approach (Approach 1) is fine and might even be slightly faster in practice due to less overhead compared to dictionary get/set operations, even though their asymptotic complexities might suggest otherwise for k ~ O(n). The constant factors matter.\nSection 5.16: Generate Nonuniform Random Numbers (Page 22 / PDF Page 58) The Problem: You are given:\nA list of n numbers (values): T = [t_0, t_1, ..., t_{n-1}]. A list of n corresponding probabilities: P = [p_0, p_1, ..., p_{n-1}]. These probabilities sum up to 1. You need to write a function that generates one of the numbers from T according to its specified probability p_i. You have access to a standard uniform random number generator that produces values in the range [0.0, 1.0) (i.e., 0 inclusive, 1 exclusive).\nExample:\nNumbers T = [3, 5, 7, 11] Probabilities P = [9/18, 6/18, 2/18, 1/18] (which are [0.5, 0.333..., 0.111..., 0.055...]) If you call your function many times:\nAbout 50% of the time, it should return 3. About 33.3% of the time, it should return 5. About 11.1% of the time, it should return 7. About 5.5% of the time, it should return 11. The Core Idea: Mapping the [0,1) Interval The uniform random number generator gives us a value r in [0.0, 1.0). We need to divide this [0.0, 1.0) interval into n segments, where the length of each segment corresponds to the probability of picking the associated number.\nSegment 0: Length p_0. Corresponds to t_0. Range [0, p_0). Segment 1: Length p_1. Corresponds to t_1. Range [p_0, p_0 + p_1). Segment 2: Length p_2. Corresponds to t_2. Range [p_0 + p_1, p_0 + p_1 + p_2). \u0026hellip;and so on. Segment n-1: Length p_{n-1}. Corresponds to t_{n-1}. Range [sum(p_0 to p_{n-2}), sum(p_0 to p_{n-1}) = 1.0) Analogy: A Dartboard with Unequal Slices Imagine you have a circular dartboard. Instead of all slices being equal (like in a standard dartboard), the slices have different sizes.\nYou have n numbers you want to pick from: t_0, t_1, t_2, ..., t_{n-1}. Each number t_i has a probability p_i of being picked. The slice on the dartboard for t_0 takes up p_0 fraction of the dartboard\u0026rsquo;s area. The slice for t_1 takes up p_1 fraction of the area. And so on. All p_i fractions add up to 1 (the whole dartboard). Example:\nNumbers T = [Red, Green, Blue] Probabilities P = [0.5 (for Red), 0.3 (for Green), 0.2 (for Blue)] Red slice covers 50% of the dartboard. Green slice covers 30%. Blue slice covers 20%. How to Pick Fairly?\nThrow a dart completely randomly at the dartboard. \u0026ldquo;Completely randomly\u0026rdquo; means any point on the board is equally likely to be hit. See which slice your dart lands in. That\u0026rsquo;s the number you pick. Since the Red slice is bigger, you\u0026rsquo;re more likely to hit Red. This simulates the probabilities. Connecting to random.random() which gives a number in [0.0, 1.0) Instead of a circular dartboard, let\u0026rsquo;s imagine a straight line segment of length 1. Think of it like a ruler from 0.0 to 1.0. We\u0026rsquo;re going to divide this ruler into sections, where the length of each section is proportional to the probability.\nFor T = [Red, Green, Blue] and P = [0.5, 0.3, 0.2]:\nSection for Red: Length 0.5. It goes from 0.0 up to 0.5. (Range: [0.0, 0.5)) Section for Green: Length 0.3. It starts where Red left off, so it goes from 0.5 up to 0.5 + 0.3 = 0.8. (Range: [0.5, 0.8)) Section for Blue: Length 0.2. It starts where Green left off, so it goes from 0.8 up to 0.8 + 0.2 = 1.0. (Range: [0.8, 1.0)) Visually:\n0.0 0.5 0.8 1.0 \u0026lt;\u0026mdash; Red \u0026mdash;\u0026gt; \u0026lt;\u0026ndash; Green \u0026ndash;\u0026gt; \u0026lt;\u0026ndash; Blue \u0026ndash;\u0026gt; (length 0.5) (length 0.3) (length 0.2) The Algorithm using this \u0026ldquo;Ruler\u0026rdquo;: Prepare the Ruler Divisions (these are the \u0026ldquo;cumulative probabilities\u0026rdquo;):\nRed ends at: 0.5 Green ends at: 0.5 + 0.3 = 0.8 Blue ends at: 0.5 + 0.3 + 0.2 = 1.0 Let\u0026rsquo;s call these end-points: Endpoints = [0.5, 0.8, 1.0] \u0026ldquo;Throw a Dart\u0026rdquo;: Get a random number from your computer, rand_val = random.random(). This rand_val will be somewhere between 0.0 and 1.0 (but not including 1.0).\nSee Which Section it Lands In:\nIf 0.0 \u0026lt;= rand_val \u0026lt; Endpoints[0] (i.e., rand_val \u0026lt; 0.5), it landed in Red\u0026rsquo;s section. Output: Red. Else if Endpoints[0] \u0026lt;= rand_val \u0026lt; Endpoints[1] (i.e., 0.5 \u0026lt;= rand_val \u0026lt; 0.8), it landed in Green\u0026rsquo;s section. Output: Green. Else if Endpoints[1] \u0026lt;= rand_val \u0026lt; Endpoints[2] (i.e., 0.8 \u0026lt;= rand_val \u0026lt; 1.0), it landed in Blue\u0026rsquo;s section. Output: Blue. Why does this work? Because the length of each section on our \u0026ldquo;ruler\u0026rdquo; is exactly its probability, a uniformly random rand_val is proportionally more likely to fall into longer sections.\nExample from the Book:\nNumbers T = [3, 5, 7, 11] Probabilities P = [9/18, 6/18, 2/18, 1/18] P_decimal = [0.5, 0.333..., 0.111..., 0.055...] Ruler Divisions (Cumulative Probabilities):\ndiv[0] (for number 3): 0.5 div[1] (for number 5): 0.5 + 0.333... = 0.833... div[2] (for number 7): 0.833... + 0.111... = 0.944... div[3] (for number 11): 0.944... + 0.055... = 1.0 So, Endpoints = [0.5, 0.833..., 0.944..., 1.0] Throw a Dart: rand_val = random.random(). Let\u0026rsquo;s say rand_val = 0.75.\nSee Where it Lands:\nIs 0.75 \u0026lt; Endpoints[0] (0.5)? No. Is 0.75 \u0026lt; Endpoints[1] (0.833\u0026hellip;)? Yes. Aha! It landed between Endpoints[0] and Endpoints[1]. This corresponds to the second number in our original list T, which is 5. So, we output 5. What if rand_val = 0.2?\nIs 0.2 \u0026lt; Endpoints[0] (0.5)? Yes. It landed before the first endpoint. This corresponds to the first number, 3. Output 3. The \u0026ldquo;Binary Search\u0026rdquo; Part (e.g., bisect_left) When we have many sections (many numbers t_i), checking rand_val against each endpoint one by one (if rand_val \u0026lt; E[0]\u0026hellip; else if rand_val \u0026lt; E[1]\u0026hellip;) is slow (O(n)). Since the Endpoints array [0.5, 0.833, 0.944, 1.0] is sorted, we can use binary search to quickly find which \u0026ldquo;bucket\u0026rdquo; rand_val falls into. bisect_left(Endpoints, rand_val) efficiently finds the index idx such that all Endpoints[j] for j \u0026lt; idx are less than rand_val, and Endpoints[idx] is the first endpoint that is greater than or equal to rand_val. This idx is precisely the index of the chosen t_i in our original list T.\nSummary in Simple Terms: Line up all your probabilities end-to-end on a ruler of length 1. Mark where each probability \u0026ldquo;ends\u0026rdquo;. Generate a random point on that ruler (a random number between 0 and 1). See which probability\u0026rsquo;s segment your random point landed in. Pick the number associated with that segment. (Binary search is just a fast way to do step 3 if you have lots of segments). Section 5.17: The Sudoku Checker Problem (Page 24 / PDF Page 60) The Problem: You are given a 9x9 2D array (a grid) representing a partially or fully completed Sudoku puzzle. The grid contains integers from 0 to 9. A value of 0 indicates an empty cell. Values from 1 to 9 are filled-in digits.\nYour task is to determine if the current state of the Sudoku grid is valid according to Sudoku rules:\nEach row must contain unique digits from 1 to 9 (ignoring 0s). Each column must contain unique digits from 1 to 9 (ignoring 0s). Each of the nine 3x3 subgrids (also called \u0026ldquo;boxes\u0026rdquo; or \u0026ldquo;regions\u0026rdquo;) must contain unique digits from 1 to 9 (ignoring 0s). The problem asks only to check for validity (no duplicate 1-9 digits in any row, column, or 3x3 box). It does not ask you to solve the Sudoku.\nExample:\nFigure 5.2(a) on page 60 is a partially filled valid Sudoku. Figure 5.2(b) is a complete, valid solution. An invalid example: A row [5, 3, 4, 6, 7, 8, 9, 1, 5] is invalid because 5 appears twice. Core Implementation Idea (Directly Testing Constraints): The most straightforward approach is to check each of the three types of constraints one by one. For checking uniqueness within a row, column, or 3x3 box, a common technique is to use a helper data structure (like a hash set or a boolean array) to keep track of the digits seen so far within that unit.\nAlgorithm Outline:\nFunction to Check a Unit (Row, Column, or Box):\nCreate a helper function, say has_duplicate(unit_array), that takes a 1D array (representing a row, column, or flattened 3x3 box) and returns True if it contains duplicate digits from 1-9, False otherwise. Inside has_duplicate(unit_array): Initialize a seen_digits = set() (or a boolean array seen = [False]*10). Iterate through each digit in unit_array: If digit == 0, ignore it (empty cell). If digit != 0: If digit is already in seen_digits (or seen[digit] is True), then a duplicate is found. Return True immediately. Otherwise, add digit to seen_digits (or set seen[digit] = True). If the loop finishes without finding duplicates, return False. Check Row Constraints:\nIterate i from 0 to 8 (for each row). Extract row i from the Sudoku grid: row_data = grid[i]. If has_duplicate(row_data) is True, then the Sudoku is invalid. Return False. Check Column Constraints:\nIterate j from 0 to 8 (for each column). Extract column j: col_data = [grid[i][j] for i in range(9)]. If has_duplicate(col_data) is True, then the Sudoku is invalid. Return False. Check 3x3 Subgrid Constraints:\nIterate through the 9 subgrids. A common way is to use nested loops for the top-left corner of each subgrid: Iterate box_row_start from 0 to 6, with a step of 3 (i.e., 0, 3, 6). Iterate box_col_start from 0 to 6, with a step of 3 (i.e., 0, 3, 6). For each (box_row_start, box_col_start): (The next step would be to extract the 3x3 subgrid data and call has_duplicate on it. If it returns True, the Sudoku is invalid and we return False from the main function.) All Checks Passed: If all the above checks pass without returning False, then the Sudoku grid is valid. Return True.\nSimplified Implementation Steps:\nHelper function is_unit_valid(list_of_9_cells):\nTakes a list (a row, a column, or the 9 cells of a box). Uses a seen_numbers = set(). For each cell in list_of_9_cells: If cell == 0, continue. If cell is in seen_numbers, return False (duplicate found!). Add cell to seen_numbers. If loop finishes, return True (no duplicates). Main function is_sudoku_valid(grid):\nRows: For each row r from 0 to 8: If not is_unit_valid(grid[r]), then return False. Columns: For each column c from 0 to 8: Create column_list = [grid[r][c] for r in range(9)]. If not is_unit_valid(column_list), then return False. Boxes: For each box (e.g., box_row_start from 0,3,6 and box_col_start from 0,3,6): Create box_list by picking the 9 cells. If not is_unit_valid(box_list), then return False. If you get through all of this, return True. Section 5.18: Compute the Spiral Ordering of a 2D Array (Page 25 / PDF Page 61) What: Given an n x n 2D array, return a 1D list of its elements in \u0026ldquo;spiral order\u0026rdquo; (outside-in, clockwise).\nWhy (Interview): This is a common 2D array traversal problem. It tests:\nAbility to manage 2D array indices carefully. Handling boundary conditions. Devising a systematic way to cover all elements without repetition. State management (current position, current direction, visited cells). Underlying Pattern: Systematic Traversal with State Management. The two main patterns are \u0026ldquo;Layer Peeling\u0026rdquo; or \u0026ldquo;Path Simulation.\u0026rdquo;\nSimplified Implementation Intuition (Simulation Approach - \u0026ldquo;Robot Walk\u0026rdquo;): Imagine a robot starting at matrix[0][0]. It needs to walk in a spiral and report the numbers it steps on.\nSetup:\nresult_list = [] (to store the spiral numbers). current_row = 0, current_col = 0. directions = [[0, 1], [1, 0], [0, -1], [-1, 0]] (These are [row_change, col_change] for Right, Down, Left, Up). current_direction_idx = 0 (Start by going Right). num_elements = n * n. A way to mark visited cells: Let\u0026rsquo;s assume we can modify the input matrix. We\u0026rsquo;ll set matrix[row][col] = None (or some other special marker) after visiting. (If not, use a separate visited[n][n] boolean array). The Walk (Loop num_elements times):\nCollect: Add matrix[current_row][current_col] to result_list. Mark: matrix[current_row][current_col] = None (or visited[current_row][current_col] = True). Try to move in current direction: next_row_change, next_col_change = directions[current_direction_idx]. potential_next_row = current_row + next_row_change. potential_next_col = current_col + next_col_change. Check if turn is needed: Is (potential_next_row, potential_next_col) OFF the grid (e.g., potential_next_row \u0026lt; 0 or \u0026gt;= n, or potential_next_col \u0026lt; 0 or \u0026gt;= n)? OR, is matrix[potential_next_row][potential_next_col] ALREADY visited (e.g., == None)? If YES (turn needed): Change direction: current_direction_idx = (current_direction_idx + 1) % 4 (turn clockwise). Recalculate actual next step with new direction: next_row_change, next_col_change = directions[current_direction_idx]. current_row += next_row_change. current_col += next_col_change. If NO (can continue in current direction): current_row = potential_next_row. current_col = potential_next_col. Return result_list. Example Trace Snippet (3x3): [[1,2,3],[4,5,6],[7,8,9]] res=[], (r,c)=(0,0), dir=0 (Right)\nAdd 1. res=[1]. matrix[0][0]=None. Try (0,1) (Right). Valid. (r,c)=(0,1). Add 2. res=[1,2]. matrix[0][1]=None. Try (0,2) (Right). Valid. (r,c)=(0,2). Add 3. res=[1,2,3]. matrix[0][2]=None. Try (0,3) (Right). INVALID (off grid). Turn! dir=(0+1)%4 = 1 (Down). New try from (0,2) with Down: (1,2). Valid. (r,c)=(1,2). Add 6. res=[1,2,3,6]. matrix[1][2]=None. Try (2,2) (Down). Valid. (r,c)=(2,2). Add 9. res=[1,2,3,6,9]. matrix[2][2]=None. Try (3,2) (Down). INVALID (off grid). Turn! dir=(1+1)%4 = 2 (Left). New try from (2,2) with Left: (2,1). Valid. (r,c)=(2,1). \u0026hellip;and so on. Managing Direction: We need two things:\nA way to represent the current direction. A way to change the direction systematically (usually clockwise). 1. Representing Direction: We can assign numbers to directions:\n0: Right 1: Down 2: Left 3: Up We store the robot\u0026rsquo;s current direction in a variable, say current_direction_idx (initialized to 0 for Right). 2. Changing Direction (Turning Clockwise): When the robot hits a wall or an already visited cell, it needs to turn right (clockwise).\nIf current direction is Right (0), next is Down (1). If current direction is Down (1), next is Left (2). If current direction is Left (2), next is Up (3). If current direction is Up (3), next is Right (0). This pattern can be achieved with a simple calculation: current_direction_idx = (current_direction_idx + 1) % 4 (% 4 (modulo 4) ensures that if current_direction_idx becomes 4, it wraps around back to 0). (0 + 1) % 4 = 1 (1 + 1) % 4 = 2 (2 + 1) % 4 = 3 (3 + 1) % 4 = 0 3. How Direction Affects Movement (Delta Row, Delta Column): Once we know the current_direction_idx, how do we know how to change the row and col to move one step? We can use a lookup table (an array or list of lists/tuples) that stores the change in row (dr) and change in column (dc) for each direction index.\nLet SHIFT = [[0, 1], # Direction 0 (Right): row doesn't change, col increases by 1 [1, 0], # Direction 1 (Down): row increases by 1, col doesn't change [0, -1], # Direction 2 (Left): row doesn't change, col decreases by 1 [-1, 0]] # Direction 3 (Up): row decreases by 1, col doesn't change\nWhen the robot wants to move:\nGet the changes for the current direction: dr = SHIFT[current_direction_idx][0] dc = SHIFT[current_direction_idx][1] Calculate the potential next position: potential_next_row = current_row + dr potential_next_col = current_col + dc Section 5.19: Rotate a 2D Array (Page 28 / PDF Page 64) The Problem: You are given an n x n 2D array (a square matrix). You need to rotate this matrix by 90 degrees clockwise, in-place (O(1) additional space).\nExample (from Figure 5.4):\nInput Matrix A:\n[ [ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [13, 14, 15, 16] ] Output Matrix A (after rotation):\n[ [13, 9, 5, 1], [14, 10, 6, 2], [15, 11, 7, 3], [16, 12, 8, 4] ] Observations / How Elements Move:\nThe first row of the original ([1,2,3,4]) becomes the last column of the rotated, read from top to bottom ([1,2,3,4]). The last column of the original ([4,8,12,16]) becomes the first row of the rotated, but reversed (or the last row of the transposed, reversed matrix - it\u0026rsquo;s easier to think in terms of layers). More precisely: A[row][col] moves to A[col][n-1-row]. Approach 1: Using Extra Space (Not In-Place, for understanding) Create a new n x n matrix, say rotated_matrix. For each element A[row][col] in the original matrix: The new position in rotated_matrix will be rotated_matrix[col][n - 1 - row]. rotated_matrix[col][n - 1 - row] = A[row][col] Copy rotated_matrix back to A. Time: O(N2) Space: O(N2) for rotated_matrix. Fails the in-place requirement. Approach 2: In-Place Rotation by Layers This is the standard and efficient way to do it in-place.\nKey Idea: The rotation can be performed layer by layer, from the outermost layer inwards. Within each layer, elements move in groups of four.\nConsider the outermost layer of the 4x4 example:\nA[0][0] (1) moves to where A[0][3] (4) was. A[0][3] (4) moves to where A[3][3] (16) was. A[3][3] (16) moves to where A[3][0] (13) was. A[3][0] (13) moves to where A[0][0] (1) was. This is a 4-way swap for the corners: (1) -\u0026gt; (4) -\u0026gt; (16) -\u0026gt; (13) -\u0026gt; (1) Original positions: A[0][0], A[0][3], A[3][3], A[3][0] After 90-deg clockwise rotation, their new conceptual positions are:\nThe value originally at A[3][0] (13) moves to A[0][0]. The value originally at A[0][0] (1) moves to A[0][3]. The value originally at A[0][3] (4) moves to A[3][3]. The value originally at A[3][3] (16) moves to A[3][0]. So, the values move like this in a cycle (let TL=TopLeft, TR=TopRight, BR=BottomRight, BL=BottomLeft for a specific group of 4):\ntemp = value_at_TL value_at_TL = value_at_BL value_at_BL = value_at_BR value_at_BR = value_at_TR value_at_TR = temp Algorithm for In-Place Layer Rotation:\nThe number of layers is n // 2. Iterate through layers, from layer = 0 to n // 2 - 1. For each layer: Define first = layer (first row/col index of this layer). Define last = n - 1 - layer (last row/col index of this layer). Now, iterate through the elements within this layer\u0026rsquo;s top row (excluding the last one which is handled as a corner of another 4-element cycle). Iterate i from first to last - 1. An offset = i - first. This offset helps identify the corresponding elements in the other three sides of the current square within the layer. For each i (or offset), perform a 4-way swap: Save top-left element: top = matrix[first][i] (which is matrix[first][first + offset]) Left element moves to Top-Left: matrix[first][i] = matrix[last - offset][first] Bottom element moves to Left: matrix[last - offset][first] = matrix[last][last - offset] Right element moves to Bottom: matrix[last][last - offset] = matrix[i][last] (which is matrix[first + offset][last]) Saved Top element moves to Right: matrix[i][last] = top Section 5.20: Compute Rows in Pascal\u0026rsquo;s Triangle (Page 29-30 / PDF Page 65-66 in your full book)\nThe Problem: You are asked to generate the first n rows of Pascal\u0026rsquo;s Triangle. Pascal\u0026rsquo;s Triangle has a specific structure:\nThe first row (Row 0 conceptually) is just [1]. Each subsequent row has one more element than the previous row. The first and last element of every row is 1. Every other element in a row is the sum of the two elements directly above it (to its left and right) in the previous row. Figure 5.5 Example (First 5 rows):\nRow 0: 1 Row 1: 1 1 Row 2: 1 2 1 Row 3: 1 3 3 1 Row 4: 1 4 6 4 1 Input: A non-negative integer n (number of rows to generate). Output: A list of lists, where each inner list represents a row of Pascal\u0026rsquo;s Triangle.\nExample: If n = 5, output should be:\n[ [1], [1, 1], [1, 2, 1], [1, 3, 3, 1], [1, 4, 6, 4, 1] ] The Core Rule (from the problem description): \u0026ldquo;the j-th entry in the i-th row is 1 if j = 0 or j = i, otherwise it is the sum of the (j - 1)-th and j-th entries in the (i - 1)-th row.\u0026rdquo; (Assuming 0-indexed rows i and 0-indexed entries j within a row).\nLet Triangle[i][j] be the element at row i, column j.\nTriangle[i][0] = 1 Triangle[i][i] = 1 (since row i has i+1 elements, so index i is the last one) Triangle[i][j] = Triangle[i-1][j-1] + Triangle[i-1][j] for 0 \u0026lt; j \u0026lt; i. Implementation Idea (Building Row by Row):\nInitialize an empty list pascal_triangle_result = []. Iterate i from 0 to n-1 (to generate n rows, which will be Row 0 to Row n-1). For each i, create a current_row list. Row i will have i+1 elements. The first element of current_row (current_row[0]) is always 1. The last element of current_row (current_row[i]) is always 1 (if i \u0026gt; 0). For the elements in between (from j=1 to j=i-1): current_row[j] = pascal_triangle_result[i-1][j-1] + pascal_triangle_result[i-1][j]. This means we need access to the previous row (pascal_triangle_result[i-1]) that we already computed and stored. Add current_row to pascal_triangle_result. Return pascal_triangle_result. The Code Explained (generate_pascal_triangle - from the book):\ndef generate_pascal_triangle(n): if n == 0: return [] # Initialize the result list. # The book\u0026#39;s list comprehension initializes with all 1s, which is clever # because the first and last elements of each row are always 1. # Triangle[i] will have i+1 elements. result = [[1] * (i + 1) for i in range(n)] # Example for n=3: result will be [[1], [1,1], [1,1,1]] # Iterate from the third row (index 2) onwards, because Row 0 and Row 1 # are already correctly filled with all 1s by the initialization. for i in range(n): # Iterate through rows # Iterate through elements in the current row \u0026#39;i\u0026#39; # We need to calculate elements from index 1 up to index i-1 # (since index 0 and index i are already 1). for j in range(1, i): # This loop only runs if i \u0026gt;= 2 # Sum of the two elements directly above in the previous row result[i][j] = result[i - 1][j - 1] + result[i - 1][j] return result Let\u0026rsquo;s trace for n=4:\nresult = [[1] * (i + 1) for i in range(4)] result = [[1], [1,1], [1,1,1], [1,1,1,1]]\nOuter loop for i in range(4):\ni = 0: Inner loop for j in range(1, 0) does not run. result[0] is [1]. Correct. i = 1: Inner loop for j in range(1, 1) does not run. result[1] is [1,1]. Correct. i = 2: (Generating Row 2: [1,2,1]) Inner loop for j in range(1, 2): j = 1: result[2][1] = result[2-1][1-1] + result[2-1][1] result[2][1] = result[1][0] + result[1][1] result[2][1] = 1 + 1 = 2. result[2] is now [1, 2, 1]. Correct. (It was [1,1,1], element at index 1 got updated). i = 3: (Generating Row 3: [1,3,3,1]) Inner loop for j in range(1, 3): j = 1: result[3][1] = result[3-1][1-1] + result[3-1][1] result[3][1] = result[2][0] + result[2][1] result[3][1] = 1 + 2 = 3. j = 2: result[3][2] = result[3-1][2-1] + result[3-1][2] result[3][2] = result[2][1] + result[2][2] result[3][2] = 2 + 1 = 3. result[3] is now [1, 3, 3, 1]. Correct. Return result.\nComplexity:\nThe number of elements in Pascal\u0026rsquo;s triangle up to row n-1 is 1 + 2 + ... + n = n*(n+1)/2. Each element is computed in O(1) time (one addition). Time Complexity: O(n^2). Space Complexity: O(n^2) to store the triangle. Variant: Compute only the n-th row of Pascal\u0026rsquo;s triangle using O(n) space. (Here n would mean Row n, e.g., if input is 3, compute Row 3 [1,3,3,1]).\nIdea: To compute Row i, you only need Row i-1. You don\u0026rsquo;t need Row i-2, Row i-3, etc. So, you can keep track of just two rows: previous_row and current_row. Initialize previous_row = [1]. Loop k from 1 up to n (to compute up to Row n): current_row will have k+1 elements. current_row[0] = 1, current_row[k] = 1. For j from 1 to k-1: current_row[j] = previous_row[j-1] + previous_row[j]. Set previous_row = current_row for the next iteration. The final current_row (or previous_row after the loop) is the answer. This takes O(n) iterations to get to the n-th row. Each iteration does O(k) work (where k goes up to n). Total time: O(1+2+...+n) = O(n^2). Space: O(n) because we only store two rows at a time, each of length at most n+1. Further Optimization for O(n) space, O(n^2) time (generating k-th row): You can even do it with a single array of size k+1 by computing the values for the next row in reverse order within that array. If row stores row i-1, to compute row i in place: row[j] = row[j] + row[j-1] for j from i-1 down to 1. The row[0] remains 1. You\u0026rsquo;d need to append a 1 at the end for row[i]. Interview Takeaway:\nTests basic 2D array/list of lists manipulation. Understanding the recursive definition of Pascal\u0026rsquo;s triangle is key. The O(n^2) time and space for generating all n rows is standard. The variant tests if you can optimize space when only the last result is needed, a common DP optimization technique (reducing space by noticing limited dependencies). ","permalink":"https://deepskandpal.github.io/bookshelf/elements-of-programming/chapter-1/","tags":null,"title":"Chapter 5: Arrays"},{"categories":null,"content":"Notes for Chapter 2 Alright class, buckle up! Last time, we got a bird\u0026rsquo;s-eye view of the Machine Learning landscape – the different continents, climates, and major landmarks. Today, with Chapter 2, \u0026ldquo;End-to-End Machine Learning Project,\u0026rdquo; we\u0026rsquo;re grabbing our hiking boots and actually trekking through one of these landscapes. This is where the rubber meets the road!\nThe book says we\u0026rsquo;re pretending to be a recently hired data scientist at a real estate company. Our mission? To predict median housing prices in California. This chapter is fantastic because it walks you through the practical steps. It’s less about the theory of one specific algorithm and more about the process of doing ML in the real world.\n(Page 35: The 8 Steps \u0026amp; Working with Real Data)\nThe chapter lays out 8 key steps, and we\u0026rsquo;ll follow them closely:\nLook at the big picture. Get the data. Discover and visualize the data to gain insights. Prepare the data for Machine Learning algorithms. Select a model and train it. Fine-tune your model. Present your solution. Launch, monitor, and maintain your system. It also rightly emphasizes using real-world data. Artificial datasets are fine for understanding a specific algorithm, but real data? That\u0026rsquo;s where you learn about the messiness, the missing values, the quirks – the stuff that makes this field challenging and fun! The book lists some great places to find datasets: UC Irvine, Kaggle, AWS datasets, etc. (Page 36).\nFor our project, we\u0026rsquo;ll use the California Housing Prices dataset (Figure 2-1, page 36). It\u0026rsquo;s from the 1990 census – a time when, as the book cheekily notes, \u0026ldquo;a nice house in the Bay Area was still affordable.\u0026rdquo; Ha! The map shows housing prices across California, with population density. It\u0026rsquo;s a good, rich dataset for learning.\n(Page 37-41: Look at the Big Picture)\nThis is step one, and it\u0026rsquo;s absolutely crucial. Before you write a single line of code, you need to understand the why.\nFrame the Problem:\nYour boss wants a model of housing prices. But why? What’s the business objective? Is it to decide where to invest? Is it to advise clients? Knowing this shapes everything: how you frame the problem, what algorithms you pick, how you measure success, and how much effort you put into tweaking. The book says our model\u0026rsquo;s output (median housing price for a district) will be fed into another ML system (Figure 2-2, page 38). This downstream system decides if investing in an area is worthwhile. So, getting our price prediction right directly impacts revenue. That makes it critical! The book also introduces the idea of Pipelines (page 38). ML systems are often sequences of data processing components. Data flows from one to the next. This modularity is great for organization and robustness but needs monitoring. What\u0026rsquo;s the current solution (if any)? The boss says experts currently estimate prices manually – costly, time-consuming, and often off by \u0026gt;20%. This gives us a baseline performance to beat and justifies building an ML model. Now, we frame it technically (page 39): Supervised, Unsupervised, or Reinforcement? We have labeled examples (districts with their median housing prices). So, it’s supervised learning. Classification or Regression? We\u0026rsquo;re predicting a value (price). So, it’s a regression task. More specifically, it\u0026rsquo;s multiple regression (using multiple features like population, median income to predict the price) and univariate regression (predicting a single value – the median housing price – per district). If we were predicting multiple values (e.g., price and rental yield), it\u0026rsquo;d be multivariate regression. Batch or Online Learning? There\u0026rsquo;s no continuous flow of new data, and the dataset is small enough. So, batch learning is fine. Select a Performance Measure (Page 39):\nHow do we know if our model is good? For regression, a common measure is Root Mean Square Error (RMSE). Equation 2-1 shows the formula. What it\u0026rsquo;s ultimately trying to achieve: RMSE gives us a sense of how much error our system typically makes in its predictions, in the same units as the thing we\u0026rsquo;re predicting (dollars, in this case). It penalizes larger errors more heavily because of the squaring. The book then introduces some common Notations (page 40), which are standard in ML:\nm: Number of instances. x⁽ⁱ⁾: Feature vector for the i-th instance (all input attributes like longitude, population, etc., for one district). y⁽ⁱ⁾: Label for the i-th instance (the actual median house price for that district). X (capital X): Matrix containing all feature vectors for all instances (one row per district). h: The prediction function (our model), also called the hypothesis. ŷ⁽ⁱ⁾ (y-hat): The predicted value for the i-th instance, i.e., h(x⁽ⁱ⁾). What if RMSE isn\u0026rsquo;t ideal? If you have many outliers, you might prefer Mean Absolute Error (MAE) (Equation 2-2, page 41). What it\u0026rsquo;s trying to achieve: MAE measures the average absolute difference. It\u0026rsquo;s less sensitive to outliers than RMSE because it doesn\u0026rsquo;t square the errors. The book connects these to different norms: RMSE is related to the Euclidean norm (l₂ norm – your standard straight-line distance), and MAE to the Manhattan norm (l₁ norm – like distance in a city grid). Generally, higher norm indices focus more on large values. RMSE (l₂) is usually preferred when outliers are rare (like in a bell-shaped distribution).(more details at the end of the chapter glossary)\nCheck the Assumptions (Page 42):\nSuper important sanity check! We assume the downstream system needs actual price values. What if it actually converts prices into categories like \u0026ldquo;cheap,\u0026rdquo; \u0026ldquo;medium,\u0026rdquo; \u0026ldquo;expensive\u0026rdquo;? Then our regression task is overkill; it should have been a classification task! Fortunately, the book says our colleagues confirm they need actual prices. Phew! (Page 42-50: Get the Data)\nTime to get our hands dirty!\nCreate the Workspace (Page 42-45):\nThis is about setting up your Python environment. The book walks through installing Python, Jupyter, NumPy, pandas, Matplotlib, and Scikit-Learn. It strongly recommends using an isolated environment (like virtualenv or conda env). This is crucial in real projects to avoid library version conflicts. Think of it as a clean, dedicated lab space for each experiment. Then, it shows how to start Jupyter and create a new notebook (Figure 2-3, 2-4). Download the Data (Page 46):\nIn real life, data might be in a database, spread across files, etc. Here, it\u0026rsquo;s simpler: a single compressed file. The book provides a nifty Python function fetch_housing_data() to download and extract the data. Why a function? Automation! If data changes regularly, you can re-run the script. Take a Quick Look at the Data Structure (Page 47-50):\nFirst, a function load_housing_data() using pandas to load the CSV into a DataFrame. .head() (Figure 2-5): Shows the top 5 rows. We see attributes like longitude, latitude, housing_median_age, total_rooms, etc. 10 attributes in total. .info() (Figure 2-6): Super useful! 20,640 instances (rows) – fairly small for ML, but good for learning. total_bedrooms has only 20,433 non-null values. This means 207 districts are missing this data. We\u0026rsquo;ll need to handle this! Most attributes are float64 (numerical), but ocean_proximity is an object. Since it\u0026rsquo;s from a CSV, it\u0026rsquo;s likely text, probably categorical. .value_counts() on ocean_proximity (page 48) confirms it\u0026rsquo;s categorical: \u0026lt;1H OCEAN, INLAND, etc., with counts for each. .describe() (Figure 2-7, page 49): Shows summary statistics for numerical attributes (count, mean, std, min, 25th/50th/75th percentiles, max). std (standard deviation) tells us how spread out the values are. Percentiles (quartiles) give us a sense of the distribution. 25% of districts have housing_median_age \u0026lt; 18. Histograms (page 49, Figure 2-8 on page 50): A histogram for each numerical attribute. This helps us see: Median Income: Doesn\u0026rsquo;t look like USD. The book clarifies it\u0026rsquo;s scaled (roughly tens of thousands of dollars) and capped at 15 (for high incomes) and 0.5 (for low incomes). Preprocessing is common. Housing Median Age \u0026amp; Median House Value: Also capped. The median_house_value cap (our target variable!) could be a problem. If the model learns prices never go above $500k, it can\u0026rsquo;t predict higher. We might need to get uncapped data or remove these districts. Different Scales: Attributes have very different ranges (e.g., total_rooms vs. median_income). Many ML algorithms don\u0026rsquo;t like this. We\u0026rsquo;ll need feature scaling. Tail-Heavy Distributions: Many histograms are skewed (e.g., extend far to the right). This can make it harder for algorithms to detect patterns. We might try transforming them (e.g., log transform) later. (Page 51-55: Create a Test Set)\n\u0026ldquo;Wait!\u0026rdquo; says the scorpion icon (page 51). \u0026ldquo;Before you look at the data any further, you need to create a test set, put it aside, and never look at it.\u0026rdquo;\nWhy so early? Our brains are amazing pattern detectors. If we look too closely at the entire dataset, including what will be our test set, we might subconsciously pick up on patterns in the test data. This is data snooping bias. When we later evaluate our model on this \u0026ldquo;seen\u0026rdquo; test set, our performance estimate will be too optimistic. The model won\u0026rsquo;t generalize as well in the real world. How to create it? Theoretically, pick ~20% randomly and set aside. The book gives a simple split_train_test function (page 52). Problem: Running it again gives a different test set. Over time, your algorithm (or you!) might see the whole dataset. Solutions: Save the test set on the first run. Set a random seed (np.random.seed(42) – 42 is the \u0026ldquo;Answer to the Ultimate Question of Life, the Universe, and Everything,\u0026rdquo; a common tongue-in-cheek seed). But these break if you fetch an updated dataset. Better solution for stable splits: Use a unique, immutable instance identifier. Compute a hash of the ID, and if the hash is below a threshold (e.g., 20% of max hash value), it goes into the test set. This ensures consistency even if the dataset is refreshed (new instances go to train/test consistently, old instances stay put). The housing dataset doesn\u0026rsquo;t have an ID column. Simplest is to use row index (if data is only ever appended). Or create an ID from stable features (e.g., latitude + longitude, though page 53 notes this can cause issues if coarse). Scikit-Learn\u0026rsquo;s train_test_split (page 53): Simpler, has random_state for reproducibility. Can also split multiple datasets (e.g., features and labels) consistently. Stratified Sampling (Page 53-55): Random sampling is fine for large datasets. For smaller ones, you risk sampling bias. Imagine surveying 1000 people: random sampling might accidentally give you 60% men, not representative of the population. Stratified sampling ensures the test set is representative of important subgroups (strata). Experts say median_income is very important. We want our test set to have a similar income distribution to the full dataset. Since median_income is continuous, we first create an income category attribute (pd.cut()) with 5 categories (Figure 2-9, page 54). Not too many strata, each large enough. Then use Scikit-Learn\u0026rsquo;s StratifiedShuffleSplit to sample, ensuring similar proportions of each income category in train and test sets. Figure 2-10 (page 55) shows stratified sampling is much better than random sampling at preserving income category proportions. Finally, drop the temporary income_cat attribute. This attention to test set creation is critical but often overlooked! (Page 56-61: Discover and Visualize the Data to Gain Insights)\nNow, working only with the training set (or a copy, strat_train_set.copy()):\nVisualizing Geographical Data (Page 56-57):\nScatter plot of longitude vs. latitude (Figure 2-11). Looks like California, but hard to see patterns. Setting alpha=0.1 (Figure 2-12) reveals high-density areas (Bay Area, LA, San Diego, Central Valley). More advanced plot (Figure 2-13, page 57): Circle radius = population, color = median house price. Clearly shows prices are higher near coasts and in dense areas. Looking for Correlations (Page 58-60):\nStandard correlation coefficient (Pearson\u0026rsquo;s r): Ranges from -1 (strong negative correlation) to 1 (strong positive correlation). 0 means no linear correlation. Use .corr() method on the DataFrame. corr_matrix[\u0026quot;median_house_value\u0026quot;].sort_values(ascending=False) shows median_income has the strongest positive correlation (0.68) with median_house_value. Figure 2-14 (page 59) shows examples: correlation only captures linear relationships. Nonlinear relationships can have r=0. scatter_matrix function (pandas.plotting): Plots every numerical attribute against every other (Figure 2-15, page 60). Diagonal shows histograms. Focus on attributes promising for predicting median_house_value. Zoom in on median_income vs. median_house_value (Figure 2-16, page 61): Strong upward trend. Price cap at $500k clearly visible. Other less obvious horizontal lines (data quirks around $450k, $350k). Might want to remove these districts to prevent model learning these quirks. Experimenting with Attribute Combinations (Page 61):\nSometimes, combining attributes makes more sense. total_rooms isn\u0026rsquo;t useful without households. So, create rooms_per_household. Similarly, bedrooms_per_room and population_per_household. Check correlations again (page 62). bedrooms_per_room is more negatively correlated with price than total_bedrooms or total_rooms. Lower bedroom/room ratio (fewer bedrooms for a given house size, or larger rooms) often means more expensive. This exploration is iterative. Get a prototype, analyze its output, come back here.\n(Page 62-71: Prepare the Data for Machine Learning Algorithms)\nTime to write functions for data transformations. Why functions?\nReproducibility. Build a library of transformations. Use in live system for new data. First, revert to a clean training set and separate predictors (housing) from labels (housing_labels).\nData Cleaning (Missing Values - page 63):\nWe saw total_bedrooms has missing values. Options: Get rid of corresponding districts: housing.dropna(subset=[\u0026quot;total_bedrooms\u0026quot;]) Get rid of the whole attribute: housing.drop(\u0026quot;total_bedrooms\u0026quot;, axis=1) Set values to something (zero, mean, median): median = housing[\u0026quot;total_bedrooms\u0026quot;].median(), then housing[\u0026quot;total_bedrooms\u0026quot;].fillna(median, inplace=True) If using option 3, compute median on training set and save it to fill missing values in test set and live data. Scikit-Learn\u0026rsquo;s SimpleImputer (page 63): Create an imputer: imputer = SimpleImputer(strategy=\u0026quot;median\u0026quot;) It only works on numerical attributes, so drop ocean_proximity first: housing_num = housing.drop(\u0026quot;ocean_proximity\u0026quot;, axis=1) Fit the imputer to numerical training data: imputer.fit(housing_num) (computes medians). The medians are stored in imputer.statistics_. Transform the training set: X = imputer.transform(housing_num) (returns NumPy array). Convert back to DataFrame if needed. Scikit-Learn Design Principles (sidebar, page 64-65): Consistency: Estimators, Transformers, Predictors. Estimators: Any object that can estimate params from data (e.g., imputer). fit(data, [labels]) method. Hyperparameters set in constructor. Transformers: Estimators that can transform data (e.g., imputer). transform(data) method. Also fit_transform() (often optimized). Predictors: Estimators that can make predictions (e.g., LinearRegression). predict(data) method. score(data, labels) method. Inspection: Hyperparameters are public instance vars (e.g., imputer.strategy). Learned params end with _ (e.g., imputer.statistics_). Nonproliferation of classes: Uses NumPy arrays, basic Python types. Composition: Easy to combine (e.g., Pipeline). Sensible defaults. Handling Text and Categorical Attributes (Page 65-67):\nocean_proximity is categorical. OrdinalEncoder (Scikit-Learn \u0026gt;= 0.20): Converts categories to numbers (0, 1, 2\u0026hellip;). (Page 66). Problem: ML algorithms assume nearby numbers are more similar (e.g., 0 and 1 more similar than 0 and 4). This isn\u0026rsquo;t true for ocean_proximity. OneHotEncoder (Scikit-Learn): Common solution (page 67). Creates one binary attribute per category (dummy variables). Only one is \u0026ldquo;hot\u0026rdquo; (1), others \u0026ldquo;cold\u0026rdquo; (0). Output is a SciPy sparse matrix (efficient for many categories, as it only stores non-zero locations). Can convert to dense with .toarray(). If many categories, one-hot encoding creates many features. Alternatives: replace with numerical related features (e.g., distance to ocean) or use embeddings (learnable low-dimensional vectors, Ch 13 \u0026amp; 17). Custom Transformers (Page 68):\nFor custom cleanup or attribute combination. Create a class, implement fit(), transform(), fit_transform(). Add TransformerMixin (gets fit_transform() for free) and BaseEstimator (gets get_params(), set_params() for hyperparameter tuning, if no *args, **kwargs in constructor). Example: CombinedAttributesAdder class to add rooms_per_household etc. Has a hyperparameter add_bedrooms_per_room. Feature Scaling (Page 69):\nCrucial! ML algos often perform badly if numerical inputs have very different scales. Two common ways: Min-max scaling (Normalization): Values shifted/rescaled to range 0-1. Subtract min, divide by (max-min). Scikit-Learn: MinMaxScaler. Standardization: Subtract mean, divide by std dev. Resulting distribution has zero mean, unit variance. Not bounded to a specific range. Less affected by outliers. Scikit-Learn: StandardScaler. Important: Fit scalers on training data only! Then use them to transform train set, test set, and new data. Transformation Pipelines (Page 70-71):\nMany data prep steps need to be in order. Scikit-Learn\u0026rsquo;s Pipeline helps. Example num_pipeline for numerical attributes (page 70): SimpleImputer -\u0026gt; CombinedAttributesAdder -\u0026gt; StandardScaler. Call pipeline.fit_transform(housing_num). ColumnTransformer (Scikit-Learn \u0026gt;= 0.20): Applies different transformers to different columns. Super convenient! (Page 71) Takes list of (name, transformer, column_list) tuples. Applies num_pipeline to numerical columns, OneHotEncoder to categorical. Concatenates outputs. Handles sparse/dense matrix mix. This gives one preprocessing pipeline for all data! (Page 72-74: Select and Train a Model)\nFinally, the exciting part!\nTraining and Evaluating on the Training Set (Page 72):\nLinear Regression: lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) Try on a few instances: predictions aren\u0026rsquo;t great. RMSE on whole training set: $68,628. Not good (most house values are $120k-$265k). This is underfitting. Features might not be good enough, or model too simple. Decision Tree Regressor (more complex model, Ch 6): tree_reg = DecisionTreeRegressor() tree_reg.fit(housing_prepared, housing_labels) Evaluate on training set: RMSE = 0.0! Perfect? (Page 73) No, much more likely it badly overfit the data. Don\u0026rsquo;t touch test set yet! Use part of training set for validation. Better Evaluation Using Cross-Validation (Page 73-74):\nOption 1: train_test_split training set into smaller train/validation. Option 2 (better): K-fold cross-validation. Scikit-Learn\u0026rsquo;s cross_val_score. Splits training set into K folds (e.g., 10). Trains K times, each time using a different fold for evaluation and other K-1 folds for training. Returns K scores. Scoring function: cross_val_score expects utility (higher is better), not cost (lower is better). So use scoring=\u0026quot;neg_mean_squared_error\u0026quot;. Then take np.sqrt(-scores). Decision Tree CV scores (page 74): Mean RMSE ~71,407. Standard deviation ~2,439. Much worse than 0! And worse than Linear Regression. Linear Regression CV scores: Mean RMSE ~69,052. Std dev ~2,732. Decision Tree is overfitting badly. Random Forest Regressor (Ensemble Learning, Ch 7): Trains many Decision Trees on random subsets of features, averages predictions. Skip code details (similar to others). Training set RMSE (page 75, forest_rmse): ~18,603. CV scores: Mean RMSE ~50,182. Std dev ~2,097. Much better! But training score is still much lower than validation scores -\u0026gt; still overfitting. Solutions: simplify model, regularize, get more data. Goal: Shortlist 2-5 promising models (SVMs, neural nets, etc.) without too much hyperparameter tweaking yet. Save your models! Use joblib (better for large NumPy arrays) or pickle. Save hyperparameters, CV scores, predictions. (Page 75-78: Fine-Tune Your Model)\nNow, take your shortlisted models and fine-tune their hyperparameters.\nGrid Search (Page 76):\nTedious to do manually. Use Scikit-Learn\u0026rsquo;s GridSearchCV. Tell it which hyperparameters to try and what values. It uses cross-validation to evaluate all combinations. Example param_grid for RandomForestRegressor. It will explore (3x4 + 2x3) = 18 combinations, each trained 5 times (for cv=5). 90 rounds of training! grid_search.best_params_ gives the best combination. grid_search.best_estimator_ gives the best model retrained on the full training set (if refit=True, default). Evaluation scores for each combo are in grid_search.cv_results_. Example result (page 77): RMSE 49,682. Better than default 50,182. Can treat data prep steps as hyperparameters too! (e.g., add_bedrooms_per_room in your custom transformer). Randomized Search (Page 78):\nRandomizedSearchCV. Good for large hyperparameter search spaces. Instead of trying all combos, evaluates a given number of random combinations. Benefits: explores more values per hyperparameter if run long enough; more control over budget. Ensemble Methods (Page 78):\nCombine best models. Often performs better than any single model, especially if they make different types of errors. (More in Ch 7). Analyze the Best Models and Their Errors (Page 78-79):\nGain insights! RandomForestRegressor can show feature importances. List importances with attribute names (page 79). median_income is most important. INLAND (categorical feature) is surprisingly important. Might decide to drop less useful features. Look at specific errors your system makes. Why? How to fix? (Page 79-80: Evaluate Your System on the Test Set)\nThe moment of truth! After all tweaking, evaluate the final model on the test set.\nGet predictors (X_test) and labels (y_test) from strat_test_set. Run full_pipeline.transform(X_test) (NOT fit_transform – don\u0026rsquo;t fit to test set!). final_model = grid_search.best_estimator_ final_predictions = final_model.predict(X_test_prepared) final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse) -\u0026gt; e.g., 47,730. Point estimate might not be enough. Compute a 95% confidence interval using scipy.stats.t.interval(). If performance is worse than CV scores (often is due to tuning on validation set), resist temptation to tweak based on test set results! Those tweaks are unlikely to generalize. (Page 80-83: Launch, Monitor, and Maintain Your System)\nYou got approval to launch!\nPrelaunch: Present solution, highlight learnings, assumptions, limitations. Document. Create clear visualizations. Launch: Polish code, write docs, tests. Deploy model. Save trained pipeline (e.g., with joblib). Load in production environment. Maybe wrap in a web service that your app queries via REST API (Figure 2-17, page 81). Easier upgrades, scaling. Or deploy on cloud (e.g., Google Cloud AI Platform). Monitor: This is not the end! Check live performance regularly. Trigger alerts if it drops. Models \u0026ldquo;rot\u0026rdquo; over time as the world changes. (Cats and dogs don\u0026rsquo;t mutate, but camera tech and breed popularity do!) Infer performance from downstream metrics (e.g., sales of recommended products). Or use human raters / crowdsourcing (Amazon Mechanical Turk) for tasks needing human judgment. Monitor input data quality (malfunctioning sensors, stale data from other teams). Maintain: Automate as much as possible: Collect fresh data regularly and label it. Script to retrain model and fine-tune hyperparameters. Script to evaluate new vs. old model on updated test set, deploy if better. Keep backups of models and datasets. Easy rollback. Compare new models to old. Evaluate on specific subsets of test data for deeper insights (e.g., recent data, specific input types like inland vs. ocean districts). ML involves a lot of infrastructure! First project is a lot of effort. Once it\u0026rsquo;s in place, future projects are faster.\n(Page 83-84: Try It Out! \u0026amp; Exercises) The chapter ends by encouraging you to try this whole process on a dataset you\u0026rsquo;re interested in (e.g., from Kaggle). And then lists some great exercises based on the housing dataset. Definitely try these!\nGlossary RMSE , MAE a Deep dive Think about it this way: both RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) are trying to tell us, on average, how \u0026ldquo;far off\u0026rdquo; our model\u0026rsquo;s predictions are from the actual values. The \u0026ldquo;how far off\u0026rdquo; part is where the idea of \u0026ldquo;distance\u0026rdquo; comes in, and norms are just formal mathematical ways of defining distance.\nLet\u0026rsquo;s look at a single prediction error first. Suppose the actual house price (y) is $300,000 and our model predicts (ŷ) $250,000. The error is $50,000.\nMAE (Mean Absolute Error) and the l₁ norm (Manhattan Distance):\nMAE takes the absolute value of this error: |$250,000 - $300,000| = $50,000. Then it averages these absolute errors over all your predictions. The l₁ norm (read as \u0026ldquo;L-one norm\u0026rdquo;) of a vector (think of a vector of errors [e₁, e₂, ..., eₘ]) is the sum of the absolute values of its components: |e₁| + |e₂| + ... + |eₘ|. The MAE is just this sum divided by m (the number of errors). Why \u0026ldquo;Manhattan Distance\u0026rdquo;? Imagine you\u0026rsquo;re in a city like Manhattan where streets form a grid. To get from point A to point B, you can\u0026rsquo;t cut diagonally through buildings. You have to travel along the streets (say, 3 blocks east and 4 blocks north). The total distance is 3 + 4 = 7 blocks. This is the l₁ distance. You\u0026rsquo;re summing the absolute differences along each axis (the \u0026ldquo;east-west\u0026rdquo; error and the \u0026ldquo;north-south\u0026rdquo; error, if you will). In our error context, we only have one \u0026ldquo;axis\u0026rdquo; of error for each prediction, and MAE sums these absolute \u0026ldquo;street-block\u0026rdquo; errors. RMSE (Root Mean Square Error) and the l₂ norm (Euclidean Distance):\nRMSE takes the error ($50,000), squares it (($50,000)² = 2,500,000,000), then averages these squared errors, and finally takes the square root of that average. The l₂ norm (read as \u0026ldquo;L-two norm\u0026rdquo;) of a vector of errors [e₁, e₂, ..., eₘ] is √(e₁² + e₂² + ... + eₘ²). Why \u0026ldquo;Euclidean Distance\u0026rdquo;? This is the \u0026ldquo;as the crow flies\u0026rdquo; distance you learned in geometry using the Pythagorean theorem: distance = √(Δx² + Δy²). If you have a vector of errors, the l₂ norm is like calculating the length of that vector in a multi-dimensional space. It\u0026rsquo;s our everyday understanding of straight-line distance. When RMSE squares the errors, it gives much more weight to larger errors. An error of 10 becomes 100, but an error of 2 becomes 4. The error of 10 contributes 25 times more to the sum of squares than the error of 2 (100 vs 4), whereas for MAE, it would only contribute 5 times more (10 vs 2). The square root at the end brings the measurement back into the original units (e.g., dollars). So, what\u0026rsquo;s the ultimate goal of connecting them to norms?\nGeneralization: Norms are a general mathematical concept for measuring the \u0026ldquo;size\u0026rdquo; or \u0026ldquo;length\u0026rdquo; of vectors. By recognizing that RMSE and MAE are related to specific norms (l₂ and l₁ respectively), it places them within a broader mathematical framework. There are other norms too (l₀, l∞, lₚ in general), and each has different properties and might be useful in different contexts for measuring error or regularizing models.\nFor instance, the l₀ norm counts the number of non-zero elements in a vector. The l∞ (L-infinity) norm gives the maximum absolute value in a vector (it focuses only on the largest error). Understanding Properties: Knowing the underlying norm helps us understand the properties of the error metric.\nBecause RMSE uses the l₂ norm (squaring), it\u0026rsquo;s more sensitive to outliers. One very large error, when squared, can dominate the RMSE. Because MAE uses the l₁ norm (absolute value), it\u0026rsquo;s more robust to outliers. A large error is just a large error; it\u0026rsquo;s not disproportionately magnified. Consistency in Terminology: In more advanced ML literature, you\u0026rsquo;ll often see error functions or regularization terms described using norm notation (e.g., \u0026ldquo;l₁ regularization\u0026rdquo; or \u0026ldquo;l₂ regularization\u0026rdquo;). Understanding this connection early on helps you navigate that.\nLet\u0026rsquo;s try a simple analogy:\nImagine you have two friends, Alex (l₁) and Beth (l₂), and you ask them to measure how \u0026ldquo;messy\u0026rdquo; a room is based on how far items are from their correct places.\nAlex (l₁ / MAE): For each misplaced item, Alex measures how far it is from its spot (e.g., book is 2 feet off, sock is 3 feet off). Alex just adds up these distances (2+3=5 feet of \u0026ldquo;mess\u0026rdquo;). Alex treats every foot of displacement equally. Beth (l₂ / RMSE): For each misplaced item, Beth measures the distance, squares it (book 2²=4, sock 3²=9), adds them up (4+9=13), and then might take a square root. Beth gets really upset by items that are very far off, because squaring makes those large distances much bigger. A sock 10 feet away (10²=100) is much, much worse to Beth than 5 socks each 2 feet away (5 * 2² = 20). So, when the book says \u0026ldquo;RMSE corresponds to the Euclidean norm\u0026rdquo; and \u0026ldquo;MAE corresponds to the l₁ norm,\u0026rdquo; it\u0026rsquo;s essentially saying:\nRMSE measures error in a way that emphasizes large errors more (like a straight-line distance calculation that involves squaring). MAE measures error in a way that treats all magnitudes of error more evenly (like adding up block-by-block distances). Did that make it a bit clearer? The key is that they are both ways to quantify \u0026ldquo;how far off\u0026rdquo; you are, but they \u0026ldquo;feel\u0026rdquo; or \u0026ldquo;react\u0026rdquo; to large errors differently because of their mathematical structure, which is captured by these different types of norms.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-2/","tags":null,"title":"Chapter 2: End-to-End Machine Learning Project"},{"categories":null,"content":"This chapter will guide us through:\nObjectives: Why are we building this system in the first place? Requirements: What qualities must our system possess? (Think reliability, scalability). Iterative Process: Spoiler: building ML systems is rarely a straight line. Problem Framing: How do we turn a vague business problem into a concrete ML task? This is crucial. And finally, a bit of a philosophical discussion: Data vs. Algorithms. You\u0026rsquo;ll see a recurring theme: \u0026ldquo;Not so soon!\u0026rdquo; Before you jump to coding a model, there\u0026rsquo;s a lot of critical thinking and planning. This upfront work is what separates a successful, production-ready ML system from a science project. This chapter is packed with concepts that will come up in ML system design interviews.\nLet\u0026rsquo;s get to it!\nPage 1 (of Chapter 2 text): Reiteration and Roadmap This page just sets the stage, reminding us of the holistic view from Chapter 1. The key components are:\nBusiness requirements Data stack Infrastructure Deployment Monitoring And, importantly, the stakeholders involved. The chapter will flow from high-level objectives down to the specifics of framing the ML task. The author teases: \u0026ldquo;The difficulty of your job can change significantly depending on how you frame your problem.\u0026rdquo; This is an understatement! A good problem framing can save you months of effort.\nPages 2-4: Business and ML Objectives – The \u0026ldquo;Why\u0026rdquo; This is the absolute starting point for any ML project, especially in a business context like FAANG.\nML Objectives vs. Business Objectives: Data Scientists often focus on ML Objectives: These are metrics we can directly measure from our model – accuracy, precision, recall, F1-score, RMSE, inference latency. We get excited about tweaking a model to push accuracy from 94% to 94.2%. Companies care about Business Objectives: The truth is, most businesses don\u0026rsquo;t care about that 0.2% accuracy bump unless it translates into something meaningful for them. This means: Increasing revenue (e.g., more sales, higher ad click-through rates) Reducing costs (e.g., less fraud, more efficient operations) Improving user engagement (e.g., more time on site, higher monthly active users) Increasing customer satisfaction. The book quotes Milton Friedman: \u0026ldquo;The social responsibility of business is to increase its profits.\u0026rdquo; While this is a specific economic viewpoint, the underlying message for us is that ML projects within a business context must ultimately contribute to the business\u0026rsquo;s success.\nThe Pitfall: A very common failure pattern, as the book highlights (citing Eugene Yan\u0026rsquo;s excellent post), is when data science teams get lost in \u0026ldquo;hacking ML metrics\u0026rdquo; without connecting them to business impact. If your manager can\u0026rsquo;t see how your fancy model is helping the business, the project (and sometimes the team) might get cut.\nFAANG Perspective: This is a constant conversation. PMs will always ask, \u0026ldquo;How does this new model move our North Star metric?\u0026rdquo; If you propose a project, you need to have a hypothesis about its business impact.\nMapping ML Metrics to Business Metrics: This is key. Easy Mappings: For ad click-through rate (CTR) prediction, an increase in model AUC (an ML metric) often directly correlates with increased ad revenue (a business metric). For fraud detection, better precision/recall directly means less money lost. Harder Mappings / Custom Metrics: Netflix\u0026rsquo;s \u0026ldquo;take-rate\u0026rdquo;: (quality plays) / (recommendations shown). They found a higher take-rate correlated with more streaming hours and lower subscription cancellations (business metrics). This is a great example of a derived metric that bridges the ML and business worlds. Recommender Systems: An e-commerce site wants to move from batch to online recommendations. Hypothesis: online recommendations are more relevant -\u0026gt; higher purchase-through rate. Experiment: Show X% improvement in predictive accuracy (ML metric) from online. Historically, Y% accuracy improvement led to Z% purchase-through rate increase (business metric). A/B Testing is Crucial: Often, the only way to truly know if your ML model is improving business metrics is through rigorous A/B testing. You might have a model with slightly worse offline ML metrics (e.g., accuracy) that performs better in an A/B test on the actual business KPI. The A/B test is usually the ultimate decider.\nThe \u0026ldquo;AI-Powered\u0026rdquo; Hype vs. ROI (Return on Investment): Many companies want to say they\u0026rsquo;re \u0026ldquo;AI-powered\u0026rdquo; because it\u0026rsquo;s trendy. But ML isn\u0026rsquo;t magic. It doesn\u0026rsquo;t transform businesses overnight. Google\u0026rsquo;s success with ML came from decades of investment. Maturity Matters (Figure 2-1): This graph from Algorithmia is insightful. Companies sophisticated in ML (models in production \u0026gt; 5 years) can often deploy new models in \u0026lt; 30 days (almost 75% of them). Those just starting often take \u0026gt; 30 days (60% of them). This shows that mature MLOps pipelines, experienced teams, and established processes significantly speed up development and deployment, leading to better ROI. Interview Tip: When asked to design a system, implicitly consider the company\u0026rsquo;s maturity. A startup will have different constraints and capabilities than Google.\nAttribution in Complex Systems: Sometimes ML is just one small cog in a huge machine. For a cybersecurity threat detection system, an ML model might detect anomalies, which then go through rule-based filters, then human review, then an automated blocking process. If a threat gets through, was it the ML model\u0026rsquo;s fault? Hard to say. This makes direct attribution of business impact tricky.\nSelf-Correction/Interview Tip: In an ML system design interview, the first thing you should clarify is the business objective. \u0026ldquo;What are we trying to achieve for the business? What is the key metric we want to move?\u0026rdquo; This shows you\u0026rsquo;re thinking about the big picture, not just the tech.\nPages 5-7: Requirements for ML Systems – The \u0026ldquo;What\u0026rdquo; Once objectives are clear, we define system requirements. These are often non-functional requirements that dictate the quality and robustness of our system. The book focuses on four:\nReliability (Page 5): The system should perform its correct function at the desired level of performance, even in the face of adversity (hardware/software faults, human error).\n\u0026ldquo;Correctness\u0026rdquo; is tricky for ML: A traditional software system might crash or throw an error. ML systems can fail silently. Google Translate might give you a grammatically correct sentence that means the opposite of the original, and if you don\u0026rsquo;t know the target language, you\u0026rsquo;d never know! How do we know a prediction is wrong if we don\u0026rsquo;t have ground truth for new, unseen data in real-time? This leads to the need for robust monitoring (Chapter 8). FAANG Perspective: Silent failures are a nightmare. Imagine a recommendation system silently starts recommending inappropriate content. Robust monitoring and alerting are paramount. Scalability (Pages 6-7): The system\u0026rsquo;s ability to handle growth. Growth can occur in several dimensions:\nModel Complexity: From logistic regression (1GB RAM) to a 100M parameter neural net (16GB RAM). Traffic Volume: From 10k daily requests to 1-10M daily requests. Model Count: One model for one use case (e.g., trending hashtag detection). Adding more models for the same use case (e.g., NSFW filter, bot filter). One model per customer in enterprise scenarios (author mentions a startup with 8,000 models for 8,000 customers). This is common in B2B SaaS where models are fine-tuned on customer-specific data. Resource Scaling: Scaling up: Bigger machine. Scaling out: More machines (footnote 8). Autoscaling: Automatically adjusting resources (e.g., GPUs) based on demand. Peak might need 100 GPUs, off-peak only 10. Keeping 100 GPUs always on is expensive. Autoscaling is tricky! Amazon\u0026rsquo;s Prime Day autoscaling failure example (costing $72M-$99M in an hour) shows even the giants can struggle. Artifact Management: Managing 100 models is vastly different from 1. Manual monitoring/retraining is not feasible. Need automated MLOps pipelines, code generation for reproducibility. The book cross-references later chapters covering distributed training, model optimization, resource management, experiment tracking, and development environments, all related to scalability. Interview Tip: Always consider scalability. \u0026ldquo;What happens if traffic 10x-es? What if the number of items to recommend 10x-es?\u0026rdquo; Be prepared to discuss strategies like sharding, caching, load balancing, and autoscaling. Maintainability (Page 7): Making it easy for various teams (ML engineers, DevOps, Subject Matter Experts - SMEs) to operate, debug, and evolve the system.\nTeams have different backgrounds, tools, programming languages. Needs: Well-structured workloads and infrastructure. Good documentation (often overlooked but critical!). Versioning of code, data, and artifacts (models). Reproducibility: Can someone else reproduce your model and results if you leave? Collaborative debugging without finger-pointing. The book mentions \u0026ldquo;Team Structure\u0026rdquo; (page 334) in this context. FAANG Perspective: Maintainability is huge for long-term cost of ownership. Complex, poorly documented systems become \u0026ldquo;tribal knowledge\u0026rdquo; nightmares. We invest heavily in tools and processes for versioning, reproducibility (e.g., model registries, feature stores), and clear ownership. Adaptability (Pages 7-8): The system\u0026rsquo;s ability to evolve with:\nShifting data distributions (data drift, concept drift): What users search for changes, spam tactics evolve. (Covered in \u0026ldquo;Data Distribution Shifts\u0026rdquo; on page 237). Changing business requirements. The system needs capacity to discover opportunities for improvement and allow updates without service interruption. (Covered in \u0026ldquo;Continual Learning\u0026rdquo; on page 264). Tightly linked to maintainability. Interview Tip: \u0026ldquo;How will your system adapt to changing user behavior or new product features?\u0026rdquo; This leads to discussions of online learning, retraining frequency, and monitoring for drift. These four requirements are often interconnected and sometimes involve trade-offs. For instance, a highly scalable system might introduce complexity that challenges maintainability.\nPages 8-10: Iterative Process – The \u0026ldquo;How Often\u0026rdquo; Forget the idea of a linear, one-shot ML project: Collect data -\u0026gt; Train model -\u0026gt; Deploy -\u0026gt; Done. It\u0026rsquo;s a myth!\nML development is iterative and often never-ending. (Footnote 10: \u0026ldquo;a property of traditional software\u0026rdquo; – true, but often more pronounced in ML due to data dependency). Once in production, it needs continuous monitoring and updating.\nExample Workflow (Ad Prediction - Steps 1-13): This is a fantastic illustration of the cyclical nature: Choose metric (e.g., impressions). Collect data/labels. Feature engineering. Train models. Error analysis -\u0026gt; realize labels are wrong -\u0026gt; relabel. Train again. Error analysis -\u0026gt; model always predicts \u0026ldquo;no ad\u0026rdquo; (due to 99.99% negative labels - class imbalance!) -\u0026gt; collect more positive data. Train again. Model good on old test data, bad on recent data (stale!) -\u0026gt; update with recent data. Train again. Deploy. Business feedback: Revenue decreasing! Ads shown, but few clicks. Original metric (impressions) was wrong. Change to optimize for click-through rate. Go to step 1. (And as footnote 11 hilariously adds: \u0026ldquo;Praying and crying not featured, but present through the entire process.\u0026rdquo; So true.) Figure 2-2 (Simplified Iterative Cycle): Project Scoping: Goals, objectives, constraints, stakeholders, resources. (Covered in Ch1 and earlier in Ch2, also Ch11 for team organization). Data Engineering: Handling sources/formats (Ch3), curating training data (sampling, labeling - Ch4). ML Model Development: Feature extraction (Ch5), model selection, training, evaluation (Ch6). (This is the \u0026ldquo;sexy\u0026rdquo; part often overemphasized in courses). Deployment: Making model accessible to users (Ch7). \u0026ldquo;Like writing, you never reach done, but you reach a point where you have to put it out there.\u0026rdquo; Monitoring and Continual Learning: Performance decay, adapting to changes (Ch8 \u0026amp; Ch9). Business Analysis: Evaluate model against business goals, generate insights, decide to kill/scope new projects. (This closes the loop back to Step 1). Notice the dashed arrows in Figure 2-2: you can jump between these stages. Error analysis in model development (3) might send you back to data engineering (2). Business analysis (6) might redefine project scope (1).\nThe perspective of an ML platform engineer or DevOps engineer might differ; they\u0026rsquo;d focus more on infrastructure setup.\nSelf-Correction/Interview Tip: When outlining your approach in an ML system design interview, always frame it as an iterative process. Mention baselines, prototyping, feedback loops, and phased rollouts. This demonstrates a mature understanding of real-world ML development.\nPages 11-19: Framing ML Problems – The \u0026ldquo;What Exactly\u0026rdquo; This is where your expertise as an ML engineer truly shines. You take a business problem and translate it into something ML can solve.\nBusiness Problem vs. ML Problem: Boss\u0026rsquo;s request: \u0026ldquo;Rival bank uses ML to speed up customer service by 2x. Do that.\u0026rdquo; This is a business problem, not an ML problem. ML problem needs: Inputs, Outputs, Objective Function. Your job: Investigate. Bottleneck is routing requests to Accounting, Inventory, HR, IT. ML Framing: Predict which department a request should go to. Input: Customer request text. Output: Predicted department (one of four). Task Type: Classification. Objective function: Minimize difference between predicted and actual department. Types of ML Tasks (Page 12-14, Figure 2-3): The model\u0026rsquo;s output dictates the task type. Classification vs. Regression:\nClassification: Output is a category (e.g., spam/not-spam, cat/dog/mouse). Regression: Output is a continuous value (e.g., house price, temperature). They can be interchanged (Figure 2-4): House price regression -\u0026gt; classification by bucketing prices (e.g., \u0026lt;$100k, $100k-$200k). Email spam classification -\u0026gt; regression by outputting a spamminess score (0-1), then thresholding. FAANG Perspective: Choosing classification vs. regression can have subtle implications for user experience and error analysis. Sometimes a score is more nuanced than a hard class label. Within Classification:\nBinary Classification: Two classes (e.g., fraud/not-fraud, toxic/not-toxic). Simplest. F1, confusion matrices are intuitive. Multiclass Classification: More than two classes, an example belongs to exactly one (e.g., classifying an image as cat OR dog OR bird). High Cardinality Classification: Many classes (e.g., thousands of diseases, tens of thousands of product categories). Challenge 1: Data Collection. Often need many examples per class (author suggests ~100). For 1000 classes, that\u0026rsquo;s 100,000 examples. Difficult for rare classes. Strategy: Hierarchical Classification. First classify into broad categories (e.g., electronics, fashion), then a second model classifies into subcategories (e.g., shoes, shirts within fashion). This is common in product categorization at Amazon or e-commerce sites. Multilabel Classification: An example can belong to multiple classes simultaneously (e.g., an article can be about \u0026rsquo;tech\u0026rsquo; AND \u0026lsquo;finance\u0026rsquo;). This is often the trickiest! Labeling: Annotator disagreement is common (see Chapter 4). One says 2 labels, another says 1. Prediction: If model outputs probabilities [0.45 (tech), 0.2 (ent), 0.02 (fin), 0.33 (pol)], how many labels do you pick? The top one? Top two? This requires careful thresholding or learning the number of labels. Two Approaches: Treat as multiclass with a multi-hot encoded target vector (e.g., [0, 1, 1, 0] for entertainment and finance). Train multiple binary classifiers (one for each topic: \u0026ldquo;is it tech?\u0026rdquo;, \u0026ldquo;is it entertainment?\u0026rdquo;). This is often simpler to manage and interpret. Interview Tip: Clearly state your problem framing, including the task type. Be ready to justify why you chose that framing and discuss alternatives.\nMultiple Ways to Frame a Problem (Pages 15-16, Figures 2-5, 2-6): Next App Prediction Problem: Predict the app a user will most likely open next. Naive Framing (Multiclass Classification - Figure 2-5): Input: User features (demographics, past apps), Environment features (time, location). Output: A probability distribution vector of size N (for N apps on the phone). [P(App0), P(App1), \u0026hellip;, P(AppN-1)]. Bad because: If a new app is installed (N changes), the output layer of your neural network changes. You have to retrain from scratch or at least a significant part of the model. Better Framing (Regression - Figure 2-6): Input: User features, Environment features, AND App features (category of app, metadata). Output: A single score (0-1) indicating likelihood of opening that specific app given the context. To recommend, you make N predictions (one for each app, feeding its features into the model) and pick the app with the highest score. Good because: If a new app is installed, you just featurize it and score it with the existing model. No retraining needed immediately. This is much more scalable and practical for dynamic environments like app stores. FAANG Perspective: This \u0026ldquo;Siamese\u0026rdquo; network-like approach (where you compare user context to item features) is very common in large-scale recommendation and search ranking systems. Objective Functions (Page 16): AKA Loss Functions. The function the model tries to minimize during training.\nFor supervised learning, it compares model outputs to ground truth labels. Examples: RMSE or MAE for regression, Logistic Loss (Log Loss) for binary classification, Cross-Entropy for multiclass classification. The book gives a Python snippet for cross-entropy: p is ground truth (e.g., [0,0,0,1]), q is model prediction (e.g., [0.45,0.2,0.02,0.33]). Most ML engineers use standard loss functions; deriving novel ones requires deeper math. Important Note (footnote 12): These mathematical objective functions are different from the business/ML objectives we discussed earlier. The hope is that optimizing the mathematical loss function will improve the ML objectives, which in turn will improve business objectives. This chain isn\u0026rsquo;t always perfect! Decoupling Objectives (Pages 17-18): What if you have multiple, potentially conflicting, goals? Example: Newsfeed Ranking. Initial goal: Maximize user engagement (clicks). Objectives: Filter spam, filter NSFW, rank by click likelihood. Problem: Prioritizing engagement alone leads to extreme content (clickbait, outrage). (Footnote 13 cites Facebook/YouTube examples). New goal: Maximize engagement AND minimize extreme views/misinformation. New objectives: Filter spam, filter NSFW, filter misinformation, rank by quality, rank by engagement. Now, quality and engagement might conflict. A high-quality post might be boring; an engaging post might be low-quality. Approach 1: Combined Loss Function: loss = alpha * quality_loss + beta * engagement_loss Train one model to minimize this combined loss. Problem: Tuning alpha and beta is tricky. Every time you adjust them (e.g., business decides quality is more important), you have to retrain the entire model. Approach 2: Decouple - Train Separate Models: quality_model: Minimizes quality_loss, predicts quality_score. engagement_model: Minimizes engagement_loss, predicts engagement_score. Combine scores at inference time: final_score = alpha * quality_score + beta * engagement_score. Advantages: You can tweak alpha and beta without retraining models. Much more agile. Easier maintenance: Spam techniques evolve faster than quality perception. The spam filter (part of quality) might need frequent updates, while the core engagement model might be more stable. Decoupling allows different maintenance schedules. FAANG Perspective: Decoupling objectives into separate models or model components is a very common and powerful pattern in complex production systems like search ranking, ad ranking, and feed ranking. It allows for modularity, independent iteration on components, and easier tuning of business trade-offs. Pages 19-22: Mind Versus Data – The Big Debate This is a fascinating, ongoing discussion in the ML community. What\u0026rsquo;s more important for progress?\nThe Premise: \u0026ldquo;More Data Usually Beats Better Algorithms\u0026rdquo; (Anand Rajaraman, footnote 16). Many successes in the last decade (AlexNet, BERT, GPT) relied heavily on massive datasets. Companies increasingly focus on managing and improving their data. The Debate: \u0026ldquo;Mind\u0026rdquo; Camp: Emphasizes inductive biases, intelligent architectural designs, causal inference. Judea Pearl (Turing Award winner): \u0026ldquo;Mind over Data,\u0026rdquo; \u0026ldquo;Data is profoundly dumb.\u0026rdquo; He controversially tweeted that data-centric ML folks might be jobless in 3-5 years. Christopher Manning: Huge data + simple algorithm = \u0026ldquo;incredibly bad learners.\u0026rdquo; Structure helps learn from less data. \u0026ldquo;Data\u0026rdquo; (and Compute) Camp: Richard Sutton (\u0026ldquo;The Bitter Lesson\u0026rdquo;): General methods leveraging computation ultimately win by a large margin. Trying to bake in human domain knowledge gives short-term gains, but scaling computation is the long-term winner. Peter Norvig (on Google Search): \u0026ldquo;We don\u0026rsquo;t have better algorithms. We just have more data.\u0026rdquo; (This is from \u0026ldquo;The Unreasonable Effectiveness of Data,\u0026rdquo; Halevy, Norvig, Pereira). Monica Rogati\u0026rsquo;s \u0026ldquo;Data Science Hierarchy of Needs\u0026rdquo; (Figure 2-7): Data is foundational. You can\u0026rsquo;t do ML/AI (top of pyramid) without the lower levels: collect, move/store, explore/transform, aggregate/label. Self-Correction: This hierarchy is a fantastic mental model. Often, companies want to jump to \u0026ldquo;AI\u0026rdquo; without solid data foundations, which is a recipe for failure. The Reality: Data is essential (for now): Quality and quantity matter. Dataset sizes are exploding (Figure 2-8): Language model datasets grew from PTB (few million tokens) to Text8 (100M) to One Billion Word (0.8B tokens, 2013) to GPT-2 (10B tokens) to GPT-3 (500B tokens). Caveat: More data isn\u0026rsquo;t always better. Low-quality data (outdated, incorrect labels) can hurt performance. This emphasizes the \u0026ldquo;quality\u0026rdquo; aspect. The debate isn\u0026rsquo;t whether finite data is necessary (it is), but whether it\u0026rsquo;s sufficient, or if \u0026ldquo;mind\u0026rdquo; offers a more efficient path. If we had infinite data, we could just look up answers. A lot of data is different from infinite data. Interview Tip: While you won\u0026rsquo;t be asked to solve this philosophical debate, understanding it shows you\u0026rsquo;re aware of the field\u0026rsquo;s trends. You can articulate the practical importance of good data pipelines and also appreciate the research into more data-efficient or structured models.\nPages 22-23: Summary Chapter 2 provides a crucial introduction to the ML system design process:\nStart with \u0026ldquo;Why\u0026rdquo;: Business objectives must drive ML project selection and definition. Translate business needs into ML objectives. Define Requirements: Systems need to be reliable, scalable, maintainable, and adaptable. Embrace Iteration: ML system development is a continuous cycle, not a one-shot deal. The Role of Data is Undeniable: While the \u0026ldquo;mind vs. data\u0026rdquo; debate continues, practical ML today heavily relies on access to large amounts of high-quality data. The book will devote significant attention to data questions. Building Blocks: Complex ML systems are made of simpler components. The following chapters will zoom into these, starting with data engineering. If any of this feels abstract, the book promises concrete examples in later chapters. This chapter provides the mental framework you need to start thinking like an ML Systems Engineer. The principles here – tying to business value, defining non-functional requirements, iterative development, and careful problem framing – are universal.\nOkay, that\u0026rsquo;s Chapter 2! A lot to digest, but incredibly important. What are your initial thoughts? Any of these points particularly resonate with your experiences or spark questions? This stuff is the bread and butter of what we do in production ML.\n","permalink":"https://deepskandpal.github.io/bookshelf/design-ml-system/chapter-2/","tags":null,"title":"Chapter 2: Introduction to Machine Learning Systems Design"},{"categories":null,"content":"This chapter is all about systems that predict a category or a class – Is this email spam or not? Is this image a cat or a dog? Is this handwritten digit a \u0026lsquo;5\u0026rsquo; or a \u0026lsquo;3\u0026rsquo;?\nAnd speaking of handwritten digits, we\u0026rsquo;re going to be working with a very famous dataset: MNIST.\nIt\u0026rsquo;s a set of 70,000 small images of digits (0-9) handwritten by high school students and US Census Bureau employees. Each image is labeled with the digit it represents. The book calls it the \u0026ldquo;hello world\u0026rdquo; of Machine Learning because it\u0026rsquo;s a go-to dataset for testing new classification algorithms. Everyone who learns ML eventually plays with MNIST. It’s like a rite of passage! Scikit-Learn makes it super easy to fetch popular datasets like MNIST. The code shows: from sklearn.datasets import fetch_openml mnist = fetch_openml('mnist_784', version=1)\nThis mnist object is a dictionary-like structure (a Scikit-Learn \u0026ldquo;Bunch\u0026rdquo; object, actually). The mnist.keys() output shows it contains:\n'data': The features (the pixel values of the images). 'target': The labels (the digit each image represents). 'feature_names': Names of the features. 'DESCR': A description of the dataset. And a few others. This structure is common for datasets loaded with Scikit-Learn. (Page 86: Exploring the MNIST Data)\nLet\u0026rsquo;s look at the actual data arrays: X, y = mnist[\u0026quot;data\u0026quot;], mnist[\u0026quot;target\u0026quot;] X.shape gives (70000, 784) y.shape gives (70000,)\nWhat does this mean? We have 70,000 images (X has 70,000 rows). Each image has 784 features (X has 784 columns). Why 784? Because each image is 28x28 pixels, and 28 * 28 = 784. Each feature represents the intensity of one pixel, typically from 0 (white) to 255 (black). y contains the 70,000 labels, one for each image. Let\u0026rsquo;s visualize one digit:\nGrab an instance\u0026rsquo;s feature vector: some_digit = X[0] (this is a flat array of 784 pixel values). Reshape it to a 28x28 array: some_digit_image = some_digit.reshape(28, 28). Display it using Matplotlib\u0026rsquo;s imshow(): plt.imshow(some_digit_image, cmap=\u0026quot;binary\u0026quot;) (cmap=\u0026ldquo;binary\u0026rdquo; gives a black and white image). plt.axis(\u0026quot;off\u0026quot;) (to remove the axis ticks). The image on page 86 clearly looks like a \u0026lsquo;5\u0026rsquo;. Let\u0026rsquo;s check its label: y[0] gives '5'. Notice the label is a string. Most ML algorithms expect numbers, so we convert y to integers: y = y.astype(np.uint8) (np.uint8 is an unsigned 8-bit integer, good for values 0-255, perfect for digits 0-9).\n(Page 87: MNIST Dataset Structure \u0026amp; Test Set)\nFigure 3-1 shows a sample of the digits – you can see the variety and sometimes the messiness of handwriting!\nCrucially, just like in Chapter 2, we need a test set! The MNIST dataset as loaded by fetch_openml is often already split:\nFirst 60,000 images for training. Last 10,000 images for testing. So, we can do: X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] The training set (X_train, y_train) is also typically pre-shuffled. This is good because:\nIt ensures cross-validation folds will be similar (e.g., you don\u0026rsquo;t want one fold missing all images of the digit \u0026lsquo;7\u0026rsquo;). Some algorithms are sensitive to the order of training instances and perform poorly if they see many similar instances in a row. Shuffling prevents this. (The book notes shuffling isn\u0026rsquo;t always good, e.g., for time series data where order matters). (Page 88: Training a Binary Classifier)\nLet\u0026rsquo;s start simple. Instead of classifying all 10 digits, let\u0026rsquo;s try to identify just one digit – say, the number 5. This will be a binary classifier: it distinguishes between two classes, \u0026ldquo;5\u0026rdquo; and \u0026ldquo;not-5\u0026rdquo;.\nCreate target vectors for this binary task: y_train_5 = (y_train == 5) # This will be True for all 5s, False for others. y_test_5 = (y_test == 5)\nPick a classifier and train it: A good starting point is Scikit-Learn\u0026rsquo;s SGDClassifier (Stochastic Gradient Descent classifier).\nWhy SGD? It\u0026rsquo;s efficient and can handle very large datasets because it processes training instances one at a time (making it suitable for online learning, too). from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) Why random_state=42? SGD relies on randomness during training (hence \u0026ldquo;stochastic\u0026rdquo;). Setting random_state ensures reproducible results. If you run the code again, you get the same model. sgd_clf.fit(X_train, y_train_5) Make a prediction: Let\u0026rsquo;s use that some_digit (which was a \u0026lsquo;5\u0026rsquo;) we looked at earlier: sgd_clf.predict([some_digit]) returns array([True]). The classifier correctly guessed it\u0026rsquo;s a 5!\nBut one correct guess doesn\u0026rsquo;t mean much. We need to evaluate its overall performance.\n(Page 88-93: Performance Measures for Classifiers)\nEvaluating classifiers is often \u0026ldquo;significantly trickier than evaluating a regressor.\u0026rdquo; Get ready for new concepts!\nMeasuring Accuracy Using Cross-Validation (Page 89): Just like in Chapter 2, cross-validation is a good way to evaluate.\nImplementing Cross-Validation Manually: The book shows how you could implement cross-validation yourself using StratifiedKFold. This gives more control but is more work.\nStratifiedKFold ensures each fold has a representative ratio of each class (important for skewed datasets, where one class is much more frequent). The loop creates a clone of the classifier, trains on training folds, predicts on the test fold, and calculates accuracy for that fold. Using cross_val_score(): Much easier! from sklearn.model_selection import cross_val_score cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\u0026quot;accuracy\u0026quot;) This performs K-fold cross-validation (here, 3 folds) and returns the accuracy for each fold. The output is something like array([0.96355, 0.93795, 0.95615]). Wow! Over 93% accuracy on all folds! Seems amazing, right?\nThe Pitfall of Accuracy with Skewed Datasets: Before getting too excited, let\u0026rsquo;s consider a \u0026ldquo;dumb\u0026rdquo; classifier: class Never5Classifier(BaseEstimator): def fit(self, X, y=None): return self def predict(self, X): return np.zeros((len(X), 1), dtype=bool) # Always predicts False (not-5) If we run cross_val_score on this Never5Classifier (page 90), we get over 90% accuracy!\nWhy? Only about 10% of the MNIST digits are 5s. So, if you always guess \u0026ldquo;not a 5,\u0026rdquo; you\u0026rsquo;ll be right about 90% of the time. Key takeaway: Accuracy is generally not the preferred performance measure for classifiers, especially with skewed datasets (where some classes are much more frequent than others). Confusion Matrix (Page 90-91): A much better way to evaluate! It counts how many times instances of class A are classified as class B.\nGet \u0026ldquo;clean\u0026rdquo; predictions: Use cross_val_predict(): from sklearn.model_selection import cross_val_predict y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3) This performs K-fold CV but returns the predictions made on each test fold (so each prediction is \u0026ldquo;clean\u0026rdquo; – made by a model that hadn\u0026rsquo;t seen that instance during its training). Compute the confusion matrix: from sklearn.metrics import confusion_matrix confusion_matrix(y_train_5, y_train_pred) The output (page 91) is a 2x2 matrix for our binary \u0026ldquo;5\u0026rdquo; vs \u0026ldquo;not-5\u0026rdquo; classifier: Predicted: Not-5 Predicted: 5 Actual: Not-5 [[ TN, FP ]] Actual: 5 [[ FN, TP ]] TN (True Negatives): Correctly classified as not-5 (e.g., 53,057). FP (False Positives): Wrongly classified as 5 (they were not-5) (e.g., 1,522). Also called a Type I error. FN (False Negatives): Wrongly classified as not-5 (they were 5s) (e.g., 1,325). Also called a Type II error. TP (True Positives): Correctly classified as 5 (e.g., 4,096). A perfect classifier would have only TPs and TNs (non-zero values only on the main diagonal). Figure 3-2 (page 92) provides a nice illustration of this. Precision and Recall (Page 91-92): The confusion matrix is great, but sometimes we want more concise metrics.\nPrecision (Equation 3-1): Accuracy of the positive predictions. precision = TP / (TP + FP) What it\u0026rsquo;s ultimately trying to achieve: Of all the instances the classifier claimed were positive (e.g., said were \u0026lsquo;5\u0026rsquo;s), what proportion were actually positive? A trivial way to get 100% precision: make only one positive prediction and ensure it\u0026rsquo;s correct. Not very useful! Recall (Equation 3-2): Sensitivity or True Positive Rate (TPR). Ratio of positive instances that are correctly detected. recall = TP / (TP + FN) What it\u0026rsquo;s ultimately trying to achieve: Of all the instances that were actually positive (e.g., all actual \u0026lsquo;5\u0026rsquo;s), what proportion did the classifier correctly identify? Scikit-Learn provides functions: from sklearn.metrics import precision_score, recall_score precision_score(y_train_5, y_train_pred) gives ~72.9%. recall_score(y_train_5, y_train_pred) gives ~75.6%. So, when our sgd_clf claims an image is a \u0026lsquo;5\u0026rsquo;, it\u0026rsquo;s correct only 72.9% of the time. And it only detects 75.6% of all actual \u0026lsquo;5\u0026rsquo;s. Not as shiny as the 90%+ accuracy suggested!\nF₁ Score (Page 92): It\u0026rsquo;s often convenient to combine precision and recall into a single metric, especially for comparing classifiers. The F₁ score is the harmonic mean of precision and recall (Equation 3-3). F₁ = 2 * (precision * recall) / (precision + recall)\nWhat it\u0026rsquo;s ultimately trying to achieve: The F₁ score gives more weight to low values. So, a classifier only gets a high F₁ score if both precision and recall are high. from sklearn.metrics import f1_score f1_score(y_train_5, y_train_pred) gives ~74.2%. The F₁ score isn\u0026rsquo;t always what you want. Sometimes you care more about precision (e.g., kid-safe video filter – high precision, even if recall is low). Sometimes you care more about recall (e.g., shoplifter detection – high recall, even if precision is low and there are false alarms).\nPrecision/Recall Trade-off (Page 93-96): Unfortunately, you usually can\u0026rsquo;t have it both ways: increasing precision tends to reduce recall, and vice-versa. This is the precision/recall trade-off.\nHow it works: Classifiers like SGDClassifier compute a decision score for each instance. If the score \u0026gt; threshold, it\u0026rsquo;s positive; else, negative. (Figure 3-3 illustrates this).\nRaising the threshold: Fewer instances classified as positive. This usually increases precision (fewer false positives among the ones called positive) but decreases recall (more true positives get missed and become false negatives). Lowering the threshold: More instances classified as positive. This usually increases recall (fewer true positives missed) but decreases precision (more false positives creep in). Scikit-Learn lets you access these decision scores: y_scores = sgd_clf.decision_function([some_digit]) SGDClassifier uses a threshold of 0 by default. If we set threshold = 8000, (y_scores \u0026gt; threshold) might become False, meaning a \u0026lsquo;5\u0026rsquo; is now missed (recall drops).\nChoosing a threshold:\nGet decision scores for all training instances: y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\u0026quot;decision_function\u0026quot;) Compute precision and recall for all possible thresholds: from sklearn.metrics import precision_recall_curve precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores) Plot precision and recall vs. threshold (Figure 3-4, page 95). Plot precision vs. recall (Figure 3-5, page 96). From these plots, you can choose a threshold that gives a good balance for your project. E.g., if you want 90% precision, find the threshold for that (around 8000 in the example, page 96). At this threshold, recall might be lower (e.g., 43.7%). As the book wisely notes: \u0026ldquo;If someone says, \u0026lsquo;Let\u0026rsquo;s reach 99% precision,\u0026rsquo; you should ask, \u0026lsquo;At what recall?\u0026rsquo;\u0026rdquo;\nThe ROC Curve (Page 97-99): Another common tool for binary classifiers: Receiver Operating Characteristic (ROC) curve.\nPlots True Positive Rate (TPR, which is just recall) against False Positive Rate (FPR). FPR = FP / (FP + TN): Ratio of negative instances incorrectly classified as positive. FPR is also 1 - True Negative Rate (TNR). TNR is called specificity. So, ROC plots sensitivity (recall) vs. 1 - specificity. Plotting it: Compute TPR and FPR for various thresholds: from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(y_train_5, y_scores) (using the same y_scores from decision_function). Plot FPR vs. TPR using Matplotlib (Figure 3-6, page 98). Interpretation: Dotted diagonal line: Purely random classifier. Good classifier: Stays as far away from the diagonal as possible (toward the top-left corner – high TPR, low FPR). Trade-off: Higher TPR (recall) usually means more FPRs. Area Under the Curve (AUC) or ROC AUC: A single number to compare classifiers. Perfect classifier: ROC AUC = 1. Random classifier: ROC AUC = 0.5. from sklearn.metrics import roc_auc_score roc_auc_score(y_train_5, y_scores) gives ~0.96 for SGDClassifier. ROC vs. Precision-Recall (PR) Curve (Page 98 sidebar): When to use which? Prefer PR curve when positive class is rare OR you care more about false positives than false negatives. Otherwise, ROC is fine. MNIST \u0026lsquo;5\u0026rsquo;s are ~10% of data (positive class is somewhat rare). The ROC curve might look good because there are many true negatives. The PR curve might reveal more room for improvement. Comparing with RandomForestClassifier (Page 98-99): RandomForestClassifier doesn\u0026rsquo;t have decision_function(). It has predict_proba(). predict_proba() returns an array: one row per instance, one column per class, with the probability of that instance belonging to that class. Get probabilities for the positive class: y_probas_forest = cross_val_predict(forest_clf, ..., method=\u0026quot;predict_proba\u0026quot;) y_scores_forest = y_probas_forest[:, 1] (probabilities for the positive class, i.e., \u0026lsquo;5\u0026rsquo;). Plot ROC for RandomForest (Figure 3-7, page 99). It\u0026rsquo;s much closer to top-left. roc_auc_score(y_train_5, y_scores_forest) is ~0.998. Much better! RandomForest also has much better precision (~99.0%) and recall (~86.6%). (Page 100-102: Multiclass Classification)\nSo far, binary (5 vs. not-5). Now, distinguishing all 10 digits (0-9). This is multiclass classification (or multinomial).\nSome algorithms (SGD, RandomForest, Naive Bayes) can handle multiple classes natively.\nOthers (Logistic Regression, SVMs) are strictly binary.\nStrategies for using binary classifiers for multiclass:\nOne-versus-the-Rest (OvR) or One-versus-All (OvA): Train N binary classifiers (one for each class). E.g., a 0-detector, a 1-detector, \u0026hellip;, a 9-detector. To classify a new image: get decision score from each of the 10 classifiers. Select the class whose classifier outputs the highest score. One-versus-One (OvO): Train a binary classifier for every pair of digits. 0-vs-1, 0-vs-2, \u0026hellip;, 1-vs-2, \u0026hellip;, 8-vs-9. For N classes, this is N * (N-1) / 2 classifiers. For MNIST (N=10), it\u0026rsquo;s 45 classifiers! To classify: run image through all 45 classifiers. See which class \u0026ldquo;wins\u0026rdquo; the most duels. Main advantage: Each classifier is trained only on the subset of data for the two classes it distinguishes. Good for algorithms that scale poorly with training set size (like SVMs). For most others, OvR is preferred. Scikit-Learn\u0026rsquo;s behavior (page 101):\nIt detects if you use a binary classifier for a multiclass task and automatically runs OvR or OvO. Example with SVC (Support Vector Classifier): from sklearn.svm import SVC svm_clf = SVC() svm_clf.fit(X_train, y_train) (Note: y_train, not y_train_5) svm_clf.predict([some_digit]) correctly predicts [5]. Under the hood, SVC used OvO. It trained 45 binary classifiers. svm_clf.decision_function([some_digit]) returns 10 scores (one per class). The highest score corresponds to class \u0026lsquo;5\u0026rsquo;. svm_clf.classes_ shows the list of target classes. You can force OvO or OvR: from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier ovr_clf = OneVsRestClassifier(SVC()) SGDClassifier for multiclass (page 102):\nSGD can do multiclass natively. Scikit-Learn doesn\u0026rsquo;t need OvR/OvO. sgd_clf.fit(X_train, y_train) sgd_clf.decision_function([some_digit]) returns 10 scores. Class \u0026lsquo;5\u0026rsquo; has the highest score (2412.5), class \u0026lsquo;3\u0026rsquo; has a small positive score (573.5), others negative. Evaluate with cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\u0026quot;accuracy\u0026quot;) -\u0026gt; gets ~84-87%. Scaling inputs (as in Ch 2) with StandardScaler improves accuracy to \u0026gt;89%! scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train.astype(np.float64)) cross_val_score(sgd_clf, X_train_scaled, ...) (Page 103-105: Error Analysis)\nAssume you have a promising model. Now, analyze its errors to improve it.\nMulticlass Confusion Matrix: Get predictions: y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) conf_mx = confusion_matrix(y_train, y_train_pred) It\u0026rsquo;s a 10x10 matrix. Plot it with plt.matshow(conf_mx, cmap=plt.cm.gray) (image on page 103). Most images on main diagonal (correctly classified). The \u0026lsquo;5\u0026rsquo;s look slightly darker. Either fewer 5s, or classifier performs worse on 5s. (Book says both are true). Focus on errors: Divide each value in confusion matrix by number of images in the actual class (row sums) to get error rates. row_sums = conf_mx.sum(axis=1, keepdims=True) norm_conf_mx = conf_mx / row_sums Fill diagonal with zeros to keep only errors. Plot norm_conf_mx (image on page 104). Rows = actual classes, Columns = predicted classes. Column for class \u0026lsquo;8\u0026rsquo; is bright: many images get misclassified as 8s. Row for class \u0026lsquo;8\u0026rsquo; is not too bad: actual 8s are generally classified correctly. 3s and 5s often get confused (in both directions). What to do? Improve classification of digits that look like 8s (but aren\u0026rsquo;t). Gather more such training data. Engineer new features (e.g., count closed loops: 8 has two, 6 has one, 5 has none). Preprocess images to make patterns stand out (Scikit-Image, Pillow, OpenCV). Analyzing individual errors (page 104-105): Plot examples of 3s classified as 5s, 5s classified as 3s, etc. (Figure on page 105). Some errors are understandable (even humans would struggle). Many seem like obvious errors. Why does SGDClassifier (a linear model) make them? It assigns a weight per pixel per class and sums weighted intensities. 3s and 5s differ by only a few pixels. Sensitivity to shifting/rotation. Preprocessing (centering, de-skewing) could help. (Page 106-107: Multilabel Classification)\nSometimes, an instance can belong to multiple binary classes.\nExample: Face recognition – if Alice and Charlie are in a picture, output should be [Alice:yes, Bob:no, Charlie:yes] or [1, 0, 1]. Simpler example with MNIST: Label 1: Is the digit large (7, 8, or 9)? y_train_large = (y_train \u0026gt;= 7) Label 2: Is the digit odd? y_train_odd = (y_train % 2 == 1) Combine into y_multilabel = np.c_[y_train_large, y_train_odd] (each instance has two labels). Train a classifier (e.g., KNeighborsClassifier, which supports multilabel): knn_clf = KNeighborsClassifier() knn_clf.fit(X_train, y_multilabel) Prediction for some_digit (a \u0026lsquo;5\u0026rsquo;): knn_clf.predict([some_digit]) gives array([[False, True]]). Correct! (5 is not large, 5 is odd). Evaluation: Measure F₁ score for each label, then average. f1_score(y_multilabel, y_train_knn_pred, average=\u0026quot;macro\u0026quot;) (\u0026ldquo;macro\u0026rdquo; gives equal weight to each label). Can set average=\u0026quot;weighted\u0026quot; to weight by support (number of instances per label). (Page 107-108: Multioutput Classification)\nGeneralization of multilabel: each label can be multiclass (not just binary).\nExample: Image denoising. Input: Noisy digit image (each pixel is an input feature). Output: Clean digit image (each pixel is a label, and each label can have values 0-255). This is multilabel (one label per pixel) and each label is multiclass (0-255 intensity). Create noisy data: X_train_mod = X_train + noise. Target is clean image: y_train_mod = X_train. Train KNeighborsClassifier: knn_clf.fit(X_train_mod, y_train_mod). Predict on a noisy test image: clean_digit = knn_clf.predict([X_test_mod[some_index]]). The result (image on page 108) looks close to the target! The line between classification and regression can be blurry here (predicting pixel intensity). Multioutput systems can even mix class labels and value labels.\n(Page 108-109: Exercises) The chapter concludes with some excellent exercises:\nBuild a \u0026gt;97% accuracy classifier for MNIST (hint: KNeighborsClassifier with grid search). Data augmentation: shift images, add to training set, retrain. Tackle Titanic dataset (Kaggle). Build a spam classifier (challenging!). Glossary Q . why does ROC curve exist only for binary classifiier? That\u0026rsquo;s a sharp question! You\u0026rsquo;re right to notice that the ROC curve, as we\u0026rsquo;ve discussed it so far (plotting True Positive Rate vs. False Positive Rate), is fundamentally designed for and most intuitively understood in the context of binary classification.\nLet\u0026rsquo;s break down why:\nClear Definition of \u0026ldquo;Positive\u0026rdquo; and \u0026ldquo;Negative\u0026rdquo; Classes:\nIn binary classification, you have a clear distinction: one class is designated as \u0026ldquo;positive\u0026rdquo; (e.g., \u0026ldquo;is a 5,\u0026rdquo; \u0026ldquo;is spam,\u0026rdquo; \u0026ldquo;has disease\u0026rdquo;) and the other as \u0026ldquo;negative\u0026rdquo; (e.g., \u0026ldquo;is not a 5,\u0026rdquo; \u0026ldquo;is not spam,\u0026rdquo; \u0026ldquo;does not have disease\u0026rdquo;). This allows for unambiguous definitions of: True Positives (TP): Correctly identifying a positive instance. False Positives (FP): Incorrectly identifying a negative instance as positive. True Negatives (TN): Correctly identifying a negative instance. False Negatives (FN): Incorrectly identifying a positive instance as negative. TPR (Recall) = TP / (TP + FN) FPR = FP / (FP + TN) These metrics are the axes of the ROC curve. Single Decision Threshold:\nBinary classifiers often output a score or probability that an instance belongs to the positive class. A single threshold is then used on this score to make the binary decision. The ROC curve visualizes performance across all possible values of this single threshold. Why it\u0026rsquo;s tricky for multiclass classification (more than 2 classes):\nWhen you have, say, 3 classes (A, B, C), the concepts of TP, FP, TN, FN become less straightforward if you try to apply them in the same way.\nWhat is \u0026ldquo;positive\u0026rdquo;? If you\u0026rsquo;re trying to detect class A, then A is \u0026ldquo;positive,\u0026rdquo; and B and C combined are \u0026ldquo;negative.\u0026rdquo; You could make an ROC curve for this \u0026ldquo;A vs. Not-A\u0026rdquo; scenario. What about class B? You\u0026rsquo;d then need a separate \u0026ldquo;B vs. Not-B\u0026rdquo; (where Not-B is A and C) scenario and its own ROC curve. And another for \u0026ldquo;C vs. Not-C.\u0026rdquo; So, for a multiclass problem with N classes, you can\u0026rsquo;t draw a single, simple ROC curve in the same 2D TPR vs. FPR space that directly represents the performance across all classes simultaneously.\nHow ROC is often adapted or used in multiclass settings:\nEven though a single ROC curve isn\u0026rsquo;t a direct fit, the underlying principles can be extended, typically using a One-vs-Rest (OvR) or One-vs-One (OvO) approach:\nOne-vs-Rest (OvR) ROC Curves:\nYou can create N separate binary classification problems. For each class i, you treat class i as the positive class and all other classes (N-1 of them) as the negative class. You can then compute an ROC curve and its AUC for each of these N binary problems. This gives you N ROC curves and N AUC scores. You can then, for example, average these AUC scores (e.g., macro-average or weighted-average) to get an overall performance metric for the multiclass classifier. Some libraries might even plot all N curves on the same graph or an \u0026ldquo;averaged\u0026rdquo; ROC curve, but this average curve can be harder to interpret directly than a binary ROC. One-vs-One (OvO) ROC Curves:\nYou could, in theory, create an ROC curve for each pair of classes (N*(N-1)/2 curves). This becomes unwieldy very quickly and isn\u0026rsquo;t commonly done for overall visualization. The Key Idea:\nThe ROC curve\u0026rsquo;s power lies in visualizing the trade-off for a single decision boundary between two distinct outcomes as you vary the threshold. In multiclass, you inherently have multiple potential decision boundaries between multiple classes, or you\u0026rsquo;re reducing it to multiple binary problems.\nThink of it intuitively:\nBinary: \u0026ldquo;How good am I at separating the \u0026lsquo;good guys\u0026rsquo; from the \u0026lsquo;bad guys\u0026rsquo; as I change my definition of \u0026lsquo;suspicious activity\u0026rsquo; (the threshold)?\u0026rdquo; Multiclass: If you have apples, oranges, and bananas, how would you define a single TPR and FPR that captures the misclassification rates between all pairs (apples called oranges, oranges called bananas, apples called bananas, etc.) on a single 2D plot in the same way? It\u0026rsquo;s not directly possible with the original TPR/FPR definitions. In summary:\nThe standard ROC curve (TPR vs. FPR) is a tool specifically tailored for visualizing the performance of a binary classifier by showing its ability to distinguish between a positive and a negative class across various decision thresholds. While the concept can be extended to multiclass scenarios by breaking the problem down into multiple binary subproblems (like OvR), you won\u0026rsquo;t typically see a single, direct \u0026ldquo;multiclass ROC curve\u0026rdquo; in the same simple 2D form. Instead, you\u0026rsquo;d look at metrics derived from these multiple binary evaluations or other multiclass-specific evaluation tools (like a full confusion matrix for all classes).\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-3/","tags":null,"title":"Chapter 3: Classification"},{"categories":null,"content":"As the book says, \u0026ldquo;The rise of ML in recent years is tightly coupled with the rise of big data.\u0026rdquo; At FAANG, this is our daily reality. Our ML systems, from recommendation engines to fraud detection, are built on vast, complex data landscapes. If you don\u0026rsquo;t have a solid grasp of data engineering, you\u0026rsquo;re going to struggle, no matter how fancy your model architecture is.\nThis chapter is dense with terminology and concepts that might seem overwhelming if you\u0026rsquo;re new to large-scale data systems. But don\u0026rsquo;t worry, we\u0026rsquo;ll break it down. The goal here is to give you a \u0026ldquo;steady piece of land to stand on.\u0026rdquo;\nWe\u0026rsquo;ll cover:\nData Sources: Where does our data even come from? Data Formats: How is it stored? What are the trade-offs? Data Models: How is it structured and represented? Data Storage Engines \u0026amp; Processing: Databases, transactional vs. analytical workloads, ETL. Modes of Dataflow: How does data move between different parts of our systems? Batch vs. Stream Processing: Two fundamental paradigms for handling data. This chapter is critical for ML system design interviews. Understanding data is non-negotiable. Let\u0026rsquo;s get started!\nPage 49 (Chapter Introduction): The Data Deluge The intro page sets the scene:\nML\u0026rsquo;s growth is linked to big data. Large data systems are complex, even without ML. Full of acronyms, evolving standards, diverse tools. It can feel like every company does it differently. This chapter provides the basics of data engineering. The roadmap mentioned:\nSources of data. Formats for storage. Structure of data (data models). Databases (storage engines) for transactional and analytical processing. Passing data across multiple processes and services. This chapter really emphasizes that data engineering is a distinct discipline that ML practitioners must understand.\nPages 50-52: Data Sources – Where It All Begins An ML system ingests data from various sources. Understanding these sources helps use the data more efficiently and anticipate challenges.\nUser Input Data (Page 50) Data explicitly provided by users.\nExamples: Text typed into a search bar, images/videos uploaded, form submissions. Challenges: Malformatted: \u0026ldquo;If it\u0026rsquo;s even remotely possible for users to input wrong data, they are going to do it.\u0026rdquo; This is a golden rule! Text too long/short, text in numerical fields, wrong file formats. Requires heavy-duty validation and processing. User Patience is Low: Users expect immediate results from their input. This implies a need for fast processing. FAANG Perspective: Input validation is a huge deal. For example, search queries need sanitization, image uploads need format checks and virus scans. Latency is critical – a slow search box is a bad user experience.\nSystem-Generated Data (Pages 50-51) Data generated by your system\u0026rsquo;s components.\nExamples: Logs (memory usage, services called, job results), model predictions. Purpose: Visibility into system health, debugging, improving the application. Essential \u0026ldquo;when something is on fire.\u0026rdquo; Characteristics: Less likely to be malformatted than user input. Processing doesn\u0026rsquo;t always need to be immediate (e.g., hourly/daily log processing is often fine). However, you might want faster processing for \u0026ldquo;interesting\u0026rdquo; events (footnote 1: \u0026ldquo;interesting\u0026rdquo; often means \u0026ldquo;catastrophic\u0026rdquo; – like a system crash or a runaway cloud bill!). This is where real-time alerting on logs comes in. Challenges with Logs: Volume: \u0026ldquo;Log everything you can\u0026rdquo; is common practice for debugging ML systems, leading to massive log volumes. Signal vs. Noise: Hard to find useful information. Services like Logstash, Datadog, Logz.io (often using ML themselves) help process and analyze logs. Storage Cost: Store logs only as long as useful. Low-access, cheaper storage (e.g., AWS S3 Glacier vs. S3 Standard – footnote 2 notes a 5x cost difference for much higher retrieval latency) can be used for older logs. FAANG Perspective: We generate petabytes of logs daily. Effective log management, aggregation, and analysis are critical for SREs (Site Reliability Engineers) and ML engineers alike to debug production issues.\nUser Behavior Data (Page 51) System-generated data specifically recording user actions.\nExamples: Clicks, suggestions chosen, scrolling, zooming, ignoring pop-ups, time spent on page. Crucial for ML: This is the raw material for training many models (recommenders, personalization, engagement prediction). Privacy Concerns: Even if system-generated, it\u0026rsquo;s considered user data and subject to privacy regulations (e.g., GDPR, CCPA). Footnote 3 has a great anecdote: an ML engineer says his team only uses browsing/purchase history, not \u0026ldquo;personal data\u0026rdquo; like age/location. The author rightly points out that browsing/purchase history is extremely personal! FAANG Perspective: This is gold. But handling it ethically and in compliance with regulations is paramount. Data anonymization and aggregation are key, but true anonymization is very hard.\nInternal Databases (Page 52) Generated by various services and enterprise applications within the company.\nExamples: Inventory, customer relationship management (CRM), user accounts. Usage in ML: Directly by models (e.g., a model might need current user subscription status). By components of an ML system (e.g., Amazon search: ML model detects intent for \u0026ldquo;frozen,\u0026rdquo; then system checks internal inventory database for \u0026ldquo;Frozen\u0026rdquo; movie vs. \u0026ldquo;frozen foods\u0026rdquo; availability before ranking). FAANG Perspective: These are often the \u0026ldquo;source of truth\u0026rdquo; for many entities. Integrating them into ML pipelines reliably and efficiently is a common data engineering task.\nThird-Party Data (Page 52) The \u0026ldquo;wonderfully weird world.\u0026rdquo;\nFirst-party data: What your company collects about its own users/customers. (This is the best, you own it, you know its provenance). Second-party data: Data from another company on their customers, which they make available to you (usually for a fee). Third-party data: Companies collect data about the public (not their direct customers) and sell it. Collection Mechanisms: Historically, unique advertiser IDs on phones (IDFA on iOS, AAID on Android) made it easy to aggregate activity across apps, websites, check-ins. This data is (hopefully) anonymized. Types of Data: Social media activity, purchase history, web browsing, car rentals, political leaning, demographics (e.g., \u0026ldquo;men, age 25-34, tech workers, Bay Area\u0026rdquo;). Use Cases: Inferring correlations (people liking Brand A also like Brand B), helpful for recommenders. Usually sold pre-cleaned and processed. Privacy Pushback: Apple\u0026rsquo;s IDFA opt-in (early 2021) significantly reduced third-party data on iPhones, forcing companies to focus more on first-party data (footnote 4). Advertisers seek workarounds (e.g., CAID device fingerprinting in China - footnote 5). FAANG Perspective: While first-party data is king, third-party data can be useful for cold-start problems or enriching user profiles. However, the privacy and ethical implications are significant, and reliance on it is decreasing due to regulations and platform changes.\nUnderstanding your data sources tells you about its likely quality, freshness, volume, and any constraints (privacy, cost, processing needs).\nPages 53-57: Data Formats – How Data is Represented for Storage and Transmission Once you have data, you need to store it (\u0026ldquo;persist\u0026rdquo; it). The format matters for cost, access speed, and ease of use. Key questions to consider:\nStoring multimodal data (images + text)? Cheap and fast access? Storing complex models for cross-hardware compatibility? Data Serialization: Converting a data structure/object state into a format that can be stored/transmitted and later reconstructed.\nTable 3-1: Common Data Formats\nFormat Binary/Text Human-readable Example Use Cases JSON Text Yes Everywhere CSV Text Yes Everywhere Parquet Binary No Hadoop, Amazon Redshift Avro Binary primary No Hadoop Protobuf Binary primary No Google, TensorFlow (TFRecord) Pickle Binary No Python, PyTorch serialization (models) Key characteristics to consider: human readability, access patterns (how data is read/written - footnote 6), text vs. binary (impacts file size).\nLet\u0026rsquo;s look at a few in detail:\nJSON (JavaScript Object Notation) (Page 54) Ubiquitous, language-independent, human-readable. Key-value pair paradigm, handles different levels of structuredness. Structured: {\u0026quot;firstName\u0026quot;: \u0026quot;Boatie\u0026quot;, \u0026quot;address\u0026quot;: {\u0026quot;city\u0026quot;: \u0026quot;Port Royal\u0026quot;}} Unstructured blob: {\u0026quot;text\u0026quot;: \u0026quot;Boatie McBoatFace, aged 12, is vibing...\u0026quot;} Pain points: Schema evolution is painful once committed. Changing a schema in existing JSON files is hard. Text files = take up a lot of space (more on this later). FAANG Perspective: JSON is incredibly common for API responses, configs, and semi-structured data logging. Its human readability is a big plus for debugging.\nRow-Major Versus Column-Major Format (Pages 54-55, Figure 3-1) CSV (Comma-Separated Values): Row-major. Consecutive elements in a row are stored together in memory. Example1_Feat1, Example1_Feat2, ... Example1_FeatN Example2_Feat1, Example2_Feat2, ... Example2_FeatN Parquet: Column-major (Columnar). Consecutive elements in a column are stored together. Example1_Feat1, Example2_Feat1, ... ExampleM_Feat1 Example1_Feat2, Example2_Feat2, ... ExampleM_Feat2 Performance implications (due to sequential data access being efficient): Row-major (CSV): Better for accessing entire rows/examples (e.g., \u0026ldquo;get all examples from today\u0026rdquo;). Faster writes when adding new individual examples. Column-major (Parquet): Better for accessing specific columns/features (e.g., \u0026ldquo;get timestamps for all examples\u0026rdquo;). More flexible column-based reads, especially with many features. If you have 1000 features but only need 4 (time, location, distance, price), Parquet lets you read just those 4 columns directly. With CSV, you\u0026rsquo;d often read all 1000 and then filter. Overall: Row-major for write-heavy workloads or row-based reads. Column-major for read-heavy, analytical workloads needing specific columns. FAANG Perspective: Columnar formats like Parquet (and ORC) are the standard for data warehouses and data lakes (e.g., in S3, GCS) because analytical queries usually operate on subsets of columns. This leads to huge I/O savings and faster query performance. Compression is also more effective on columnar data.\nNumPy Versus pandas (Page 56, Figure 3-2): A Common Gotcha! Many don\u0026rsquo;t realize: pandas DataFrame is built around a columnar format (inspired by R\u0026rsquo;s data frame). NumPy ndarrays are row-major by default (though configurable). People coming from NumPy often treat DataFrames like ndarrays, accessing by row, and find it slow. Figure 3-2 Performance: Iterating pandas DataFrame by column: 0.07 seconds. Iterating pandas DataFrame by row (df.iloc[i]): 2.41 seconds (MUCH SLOWER!). Converting to NumPy array (df.to_numpy()) and iterating by row: 0.019 seconds (FAST!). Iterating NumPy array by column: 0.005 seconds (FASTEST, as expected for columnar data if NumPy array was C-contiguous/columnar, but it\u0026rsquo;s likely F-contiguous/row-major here, so this illustrates pandas\u0026rsquo; overhead). Self-Correction/Teaching Point: The code snippet df_np[:, j] in Figure 3-2 iterates through a NumPy array column by column. If df_np is row-major (NumPy default), this is non-contiguous access, which should be slower than row-wise access. The 0.005s vs 0.019s suggests the test DataFrame might have few rows and many columns, or there\u0026rsquo;s something subtle about to_numpy() memory layout or caching effects. The main takeaway is pandas row iteration is slow due to its internal structure and overhead. Use vectorized operations in pandas, or convert to NumPy for row-wise loops if absolutely necessary. The author\u0026rsquo;s \u0026ldquo;Just pandas Things\u0026rdquo; GitHub repo (footnote 7) is a good resource for these quirks.\nText Versus Binary Format (Page 57, Figure 3-3) CSV, JSON are text files (plain text, human-readable). Parquet is a binary file (0s and 1s, machine-readable). Binary files are more compact: Example: Storing the number 1000000. Text file: 7 characters, 7 bytes (if 1 byte/char). Binary file (int32): 32 bits = 4 bytes. (Significant saving!) Figure 3-3 Illustration (interviews.csv): CSV (text): 17,654 rows, 10 columns. File size: 14 MB. Parquet (binary): Same data. File size: 6 MB. (Over 2x smaller). AWS recommends Parquet: \u0026ldquo;up to 2x faster to unload and consumes up to 6x less storage in Amazon S3, compared to text formats\u0026rdquo; (footnote 8). This is due to efficient encoding and compression schemes that work well with columnar data. FAANG Perspective: For large datasets, binary columnar formats (Parquet, ORC) are almost always preferred over text formats like CSV/JSON for storage in data lakes due to space savings, query performance, and schema evolution support. Text formats are fine for smaller files, human inspection, or system interchange where readability is key.\nPages 58-66: Data Models – Structuring Your Data Data models describe how data is represented and the relationships between data elements. This choice affects system build and the problems you can solve.\nRelational Model (Pages 59-61) Invented by Edgar F. Codd (1970), still dominant. Data organized into relations (tables), each a set of tuples (rows). Key Property (Figure 3-4): Relations are unordered (rows and columns can be shuffled, it\u0026rsquo;s still the same relation). Stored in formats like CSV/Parquet. Normalization: Reducing redundancy, improving integrity. Example (Tables 3-2, 3-3, 3-4): Book data. Initial Book relation (Table 3-2): Title, Author, Format, Publisher, Country, Price. Duplicates publisher info (Banana Press, UK) for different books/formats. If \u0026ldquo;Banana Press\u0026rdquo; changes to \u0026ldquo;Pineapple Press\u0026rdquo;, multiple rows need updates. Normalized: Book relation (Table 3-3): Title, Author, Format, Publisher_ID, Price. Publisher relation (Table 3-4): Publisher_ID, Publisher, Country. Now, if publisher name changes, only one row in Publisher table needs update. Standardizes spelling, easier to translate values. Downside of Normalization: Data spread across tables. Retrieving full info requires joins, which can be expensive for large tables. Relational Databases \u0026amp; SQL (Structured Query Language): SQL is the most popular query language. Declarative: You specify what data you want (pattern, conditions, transformations like join, sort, group, aggregate), not how to get it. Imperative (like Python): You specify the steps. Database system has a query optimizer to figure out the execution plan (break query, methods for each part, order of execution). This is hard! ML is even being used to improve query optimizers (footnote 14, Neo). SQL is Turing-complete (with additions), but complex queries can be \u0026ldquo;nightmarish\u0026rdquo; (footnote 12, 700-line SQL query). FAANG Perspective: Relational databases (PostgreSQL, MySQL, Spanner, Aurora) are workhorses for many transactional systems and structured data stores. SQL is a fundamental skill. Understanding query plans (EXPLAIN) is key for performance tuning.\nAside: From Declarative Data Systems to Declarative ML Systems (Page 62) Inspired by SQL\u0026rsquo;s success, \u0026ldquo;Declarative ML\u0026rdquo; aims to abstract away model construction/tuning. User declares feature schema and task; system finds the best model. Examples: Ludwig (Uber), H2O AutoML. Ludwig: User can specify model structure (layers, units) on top of schema. H2O AutoML: No need to specify structure/hyperparameters; it experiments and picks best. Example code shows simple API: aml.train(x=x, y=y, training_frame=train). Limitation: Abstracts away model development (often the easier part now with commoditized models). Hard parts remain: feature engineering, data processing, evaluation, drift detection, continual learning. Self-Correction: Declarative ML is great for baselining and for users who aren\u0026rsquo;t ML experts, but for complex, high-stakes production systems at FAANG, engineers often need finer-grained control.\nNoSQL (Not Only SQL) (Pages 63-65) Movement against relational model\u0026rsquo;s restrictions (strict schema, schema management pain - #1 reason for Couchbase adoption, footnote 16). SQL can be hard for specialized apps. Many NoSQL systems now also support relational models/SQL. Two major types discussed: Document and Graph. Document Model (Pages 63-64) Built around \u0026ldquo;documents\u0026rdquo; (often JSON, XML, or binary like BSON). Each document has a unique key. Collection of documents ~ table, document ~ row. Flexibility: Documents in a collection can have different schemas (unlike rows in a relational table). \u0026ldquo;Schemaless\u0026rdquo; is Misleading: The reading application usually assumes some structure. Responsibility shifts from write-time schema enforcement (relational) to read-time schema interpretation (document). Example (Examples 3-1, 3-2, 3-3): Book data as JSON documents. All info for one book (Harry Potter) is in one document, including \u0026ldquo;Sold as\u0026rdquo; array for formats/prices. Better Locality: All info for a book is in one place, easier retrieval than joining multiple relational tables. Worse for Joins/Cross-Document Queries: Finding all books under $25 requires reading all documents, extracting prices, comparing. Less efficient than SQL WHERE price \u0026lt; 25. Many DBs (PostgreSQL, MySQL) now support both relational and document models. FAANG Perspective: Document databases (MongoDB, DynamoDB) are great for use cases with self-contained data items, flexible schemas, and high scalability needs (e.g., user profiles, product catalogs where attributes vary widely).\nGraph Model (Page 65) Data as a graph: nodes and edges (relationships). Prioritizes relationships between data items. Example (Figure 3-5): Social network. Nodes: person, city, country. Edges: lives_in, born_in, coworker, friend, within. Efficient for Relationship-Based Queries: \u0026ldquo;Find everyone born in USA.\u0026rdquo; Start at \u0026ldquo;USA\u0026rdquo; node, traverse within and born_in edges to find \u0026ldquo;person\u0026rdquo; nodes. Hard to do this easily in SQL or document model if #hops is unknown/variable (e.g., 3 hops from Zhenzhong Xu to USA, 2 from Chloe He). FAANG Perspective: Graph databases (Neo4j, Amazon Neptune) shine for use cases like social networks, knowledge graphs, fraud detection (rings of fraudsters), recommendation (users-who-bought-this-also-bought). Query languages like Cypher or Gremlin are used.\nPicking the Right Model: Crucial for simplifying development. Many queries easy in one model are hard in another.\nStructured Versus Unstructured Data (Page 66, Table 3-5) Structured Data: Follows a predefined data model/schema (e.g., name=string(50), age=int(0-200)). Easy to analyze (e.g., average age). Disadvantage: Schema changes require retrospective updates, can cause bugs (e.g., new \u0026rsquo;email\u0026rsquo; field; or null ages becoming 0, confusing ML model - footnote 18\u0026rsquo;s anecdote, solved by using -1). Unstructured Data: No predefined schema. Usually text, but can be numbers, dates, images, audio (e.g., log files). Advantage: Appealing when business reqs change, or data from many sources can\u0026rsquo;t conform to one schema. May still have intrinsic patterns (e.g., CSV-like log lines: Lisa,43). But no guarantee all lines follow it. Storage Options: Schema-enforced storage can only store conforming data. Schema-less storage can store any data (e.g., convert all to bytestrings). Data Warehouse: Repository for structured data (processed, ready to use). Data Lake: Repository for unstructured (or raw) data, often before processing. Table 3-5 Differences:\nFeature Structured Unstructured Schema Clearly defined Doesn\u0026rsquo;t have to follow a schema Search/Analyze Easy (Implied harder until structure is imposed) Fast arrival Data Handling Specific schema only Any source Schema Changes Lots of trouble Worry shifted to downstream apps Stored In Data warehouses Data lakes FAANG Perspective: The distinction is fluid. \u0026ldquo;Schema-on-read\u0026rdquo; (data lakes) vs. \u0026ldquo;schema-on-write\u0026rdquo; (warehouses). The trend is towards data lakehouses (e.g., Databricks, Snowflake) combining flexibility of lakes with management features of warehouses. Raw data lands in lake, then curated/structured versions are created.\nPages 67-71: Data Storage Engines and Processing Data formats/models = interface. Storage engines (databases) = implementation on machines. Two main workload types:\nTransactional and Analytical Processing (Pages 67-69) Transaction: Digital world: any action (tweet, ride order, model upload, YouTube watch). Inserted as generated, occasionally updated/deleted. Online Transaction Processing (OLTP): Needs to be fast (low latency) for users. High availability. If system can\u0026rsquo;t process, transaction fails. Transactional Databases: Designed for OLTP. Often associated with ACID properties (Atomicity, Consistency, Isolation, Durability - definitions on page 68 are standard). Atomicity: All steps succeed or all fail (e.g., payment fails, driver not assigned). Consistency: Transactions follow predefined rules (e.g., valid user). Isolation: Concurrent transactions appear isolated (e.g., two users don\u0026rsquo;t book same driver simultaneously). Durability: Committed transaction persists despite system failure (e.g., phone dies, ride still coming). Not all need ACID. Some find it too restrictive. BASE (Basically Available, Soft state, Eventual consistency) is an alternative, \u0026ldquo;even more vague\u0026rdquo; (Kleppmann, footnote 20). Often row-major (transactions processed as units). Online Analytical Processing (OLAP): For analytical questions (e.g., \u0026ldquo;average ride price in SF in Sept?\u0026rdquo;). Requires aggregating columns across many rows. Analytical Databases: Designed for this. Efficient with queries from different viewpoints. Often columnar. OLTP/OLAP are Outdated Terms? (Figure 3-6, Google Trends): Separation was due to tech limits (hard to do both well). This is closing. Transactional DBs handling analytical queries (e.g., CockroachDB). Analytical DBs handling transactional queries (e.g., Apache Iceberg, DuckDB). Traditional OLTP/OLAP: Storage and processing tightly coupled. Often meant same data stored multiple times for different query types. Modern Paradigm: Decouple Storage from Processing (Compute). Data in one place, different processing layers on top. (Google BigQuery, Snowflake, IBM, Teradata - footnote 21). \u0026ldquo;Online\u0026rdquo; is Overloaded: Used to mean \u0026ldquo;internet-connected,\u0026rdquo; then \u0026ldquo;in production.\u0026rdquo; Data world: speed of processing/availability (online, nearline, offline - footnote 22). FAANG Perspective: The decoupling of storage (e.g., S3, GCS) and compute (e.g., Spark, Presto, BigQuery) is a dominant architecture. It provides flexibility, scalability, and cost-efficiency. ACID is critical for financial transactions, eventual consistency is often fine for social media feeds.\nETL: Extract, Transform, and Load (Pages 70-71, Figure 3-7) Early days: relational data, mostly structured. ETL was data warehousing process. Still relevant for ML. General purpose processing/aggregating data into desired shape/format. Extract: Get data from sources. Validate, reject corrupted/malformatted data. Notify sources of rejected data. Crucial first step. Transform: Meaty part. Join, clean, standardize values (Male/Female vs M/F vs 1/2), transpose, deduplicate, sort, aggregate, derive new features, more validation. Load: Decide how/how often to load transformed data into target (file, DB, warehouse). Figure 3-7: Shows sources (DB, App, Flat files) -\u0026gt; ETL -\u0026gt; Targets (Data warehouse, Feature store, DB). Rise of ELT (Extract, Load, Transform): Internet/hardware boom -\u0026gt; easy to collect massive, evolving data. Schemas changed. Idea: Store all raw data in a data lake first (fast arrival, little pre-processing). Applications pull and process as needed. Problem with ELT as data grows: Inefficient to search massive raw data. (Footnote 23: storage cost is rarely a problem now, but processing cost/time is). Trend: Cloud/standardized infra -\u0026gt; committing to predefined schema becomes feasible again. Hybrid: Data Lakehouse (Databricks, Snowflake). Flexibility of lakes + management of warehouses. FAANG Perspective: ETL/ELT pipelines are the backbone of our data infrastructure. Building robust, scalable, and maintainable ETLs (often using Spark, Beam, Airflow) is a core data engineering function. Feature stores are becoming common \u0026ldquo;targets\u0026rdquo; for ML features.\nPages 72-77: Modes of Dataflow – How Data Moves In production, data isn\u0026rsquo;t in one process; it flows between many. How does it pass if processes don\u0026rsquo;t share memory?\nData Passing Through Databases (Page 72) Easiest way: Process A writes to DB, Process B reads from DB. Limitations: Both processes need access to same DB (infeasible if different companies). DB read/writes can be slow, unsuitable for low-latency apps (most consumer-facing ones). FAANG Perspective: Used for asynchronous tasks or when latency isn\u0026rsquo;t paramount. E.g., a batch job updates a model quality table, a dashboard service reads it.\nData Passing Through Services (Request-Driven) (Pages 73-74) Direct network communication. Process A requests data from Process B; B returns it. Service-Oriented Architecture (SOA) / Microservices: Process B is a \u0026ldquo;service\u0026rdquo; A can call. B can also call A if A is a service. Can be different companies (e.g., investment firm service calls stock exchange service for prices). Can be components of one app (microservices). Allows independent development, testing, maintenance. ML Example: Ride-Sharing Price Optimization (Lyft): Services: Driver Management (available drivers), Ride Management (requested rides), Price Optimization. Price Optimization service needs data from other two to predict optimal price (supply/demand). It requests this data. (Footnote 24: in practice, might use cached data, refresh periodically). Popular Styles: REST (Representational State Transfer) vs. RPC (Remote Procedure Call): REST: For network requests. Often public APIs. RPC: Make remote call look like local function call. Often internal services in same org/datacenter. (Kleppmann, footnote 25). RESTful = implements REST architecture. HTTP is an implementation, not same as REST (footnote 26). FAANG Perspective: Microservices are ubiquitous. REST for external/public APIs, gRPC (an RPC framework) very common for internal service-to-service communication due to efficiency and strong typing.\nData Passing Through Real-Time Transport (Event-Driven) (Pages 74-77) Motivation (Ride-sharing example, Figure 3-8): If Price Optimization, Driver Mgmt, Ride Mgmt all need data from each other via requests, it becomes a complex web. With hundreds/thousands of services, this is a bottleneck. Request-driven is synchronous: Target service must be listening. If Driver Mgmt is down, Price Opt. keeps resending, times out. Response lost if Price Opt. goes down. Cascading failures. Broker/Event Bus (Figure 3-9): Services communicate via a central broker. Driver Mgmt makes a prediction, broadcasts it (an event) to broker. Other services wanting this data get it from broker. Technically, a DB can be a broker. But slow for low-latency. So, use in-memory storage for brokering (real-time transports). Event-driven architecture: Better for data-heavy systems. Request-driven for logic-heavy. Common Types: Pub/Sub (Publish-Subscribe): Apache Kafka, Amazon Kinesis. Services publish events to topics. Services subscribe to topics to read events. Producers don\u0026rsquo;t care about consumers. Retention Policy (Figure 3-10): Events kept in in-memory transport for a period (e.g., 7 days), then deleted or moved to permanent storage (e.g., S3). This is key for Kafka\u0026rsquo;s design – it\u0026rsquo;s a durable commit log. Message Queue: Apache RocketMQ, RabbitMQ. Event (message) often has intended consumers. Queue gets message to right consumers. Both Kafka/RabbitMQ are very popular (Figure 3-11, Stackshare). (Footnote 27: Mitch Seymour\u0026rsquo;s Kafka/otters animation is great!) FAANG Perspective: Kafka is a cornerstone of many real-time data pipelines for logging, metrics, event sourcing, stream processing. It enables decoupling of services and resilience.\nPages 78-79: Batch Processing Versus Stream Processing Two paradigms for processing data based on its nature (historical vs. in-flight).\nBatch Processing Data in storage (DBs, lakes, warehouses) = historical data. Processed in batch jobs (kicked off periodically, e.g., daily job for average surge charge). Distributed systems like MapReduce, Spark process batch data efficiently. Use in ML: Compute features that change less often (static features). E.g., driver\u0026rsquo;s overall rating (if hundreds of rides, one more doesn\u0026rsquo;t change it much day-to-day). Stream Processing Data in real-time transports (Kafka, Kinesis) = streaming data. Computation on this data. Can be periodic (shorter periods, e.g., every 5 mins) or triggered (e.g., user requests ride -\u0026gt; process stream for available drivers). Low latency: Process data as generated, without writing to DB first. Efficiency: Myth: Less efficient than batch (can\u0026rsquo;t use Spark/MapReduce). Not always true. Streaming tech (Apache Flink) is scalable, distributed (parallel computation). Strength: Stateful computation. Example: 30-day user engagement trial. Batch: recompute over last 30 days daily. Stream: compute on new day\u0026rsquo;s data, join with older computation (state). Avoids redundancy. Use in ML: Compute features that change quickly (dynamic features / streaming features). E.g., drivers available right now, rides requested last minute, median price of last 10 rides in area. Essential for optimal real-time predictions. Both Batch and Stream Features Needed: Many problems need both. Need infra to process both and join them for ML models (preview of Chapter 7). Stream Computation Engines: Kafka\u0026rsquo;s built-in stream processing is limited (various data sources). ML streaming features often need complex queries (joins, aggregations). Need efficient stream processing engines: Apache Flink, KSQL (Kafka SQL), Spark Streaming. Flink, KSQL more recognized, nice SQL abstraction for data scientists. Stream processing is harder: Unbounded data, variable rates/speeds. Argument (Flink maintainers, footnote 28): Batch is a special case of streaming. (i.e., a bounded stream). Stream engines can unify both. FAANG Perspective: This unification is a powerful trend (e.g., Apache Beam model). Lambda architectures (separate batch/stream paths) are complex; Kappa architectures (all stream) are simpler if feasible. Choosing the right features (static, dynamic, or both) is key for model performance and system complexity.\nPages 79-80: Summary of Chapter 3 This chapter built on Chapter 2\u0026rsquo;s emphasis on data\u0026rsquo;s importance. Key takeaways:\nData Formats: Choose wisely for future use. Row-major vs. column-major, text vs. binary have pros/cons. Data Models: Relational (SQL), Document, Graph. All widely used, each suited for different tasks. Structured (writer assumes schema) vs. Unstructured (reader assumes schema) is fluid. Storage \u0026amp; Processing: Traditionally coupled (OLTP DBs for transactional, OLAP for analytical). Decoupling storage/compute is the trend. Hybrid DBs emerging. Modes of Dataflow: Databases (slow, simple), Services (request-driven, microservices), Real-time Transports (event-driven, Kafka/RabbitMQ for async, low latency). Batch vs. Stream Processing: Historical data -\u0026gt; batch jobs -\u0026gt; static features. Streaming data -\u0026gt; stream engines -\u0026gt; dynamic features. Stream engines can potentially unify both. With data systems figured out, next chapter is about collecting data and creating training data!\nInterview Questions \u0026amp; Page References (Chapter 3) As promised, here\u0026rsquo;s a list of potential interview questions related to Chapter 3, with page numbers for where you can find relevant concepts in the book:\nGeneral Data Understanding: \u0026ldquo;Describe the different types of data sources you might encounter in an ML project and their characteristics.\u0026rdquo; (p. 50-52) \u0026ldquo;What are the challenges associated with user-input data? How would you handle them?\u0026rdquo; (p. 50) \u0026ldquo;Why is system-generated log data important? What are the challenges in managing it?\u0026rdquo; (p. 50-51) \u0026ldquo;What are the privacy considerations for user behavior data?\u0026rdquo; (p. 51, esp. footnote 3) \u0026ldquo;Explain the difference between first-party, second-party, and third-party data. What are the trends affecting third-party data?\u0026rdquo; (p. 52) Data Formats: \u0026ldquo;What is data serialization? Name some common data formats and their use cases.\u0026rdquo; (p. 53, Table 3-1) \u0026ldquo;Compare and contrast row-major (e.g., CSV) and column-major (e.g., Parquet) data formats. When would you choose one over the other?\u0026rdquo; (p. 54-55, Figure 3-1) \u0026ldquo;Why might iterating over pandas DataFrame rows be slow? How does its internal storage format relate to this?\u0026rdquo; (p. 56, Figure 3-2) \u0026ldquo;Discuss the trade-offs between text formats (like JSON/CSV) and binary formats (like Parquet).\u0026rdquo; (p. 57, Figure 3-3) \u0026ldquo;How would you choose a data format for storing a large dataset intended for analytical queries?\u0026rdquo; (Implied: Parquet, p. 55, 57) Data Models: \u0026ldquo;What is the relational data model? Explain the concept of normalization and its pros/cons.\u0026rdquo; (p. 59-60, Tables 3-2 to 3-4) \u0026ldquo;What does it mean for SQL to be a declarative language?\u0026rdquo; (p. 61) \u0026ldquo;What is NoSQL? Describe the document model and its advantages/disadvantages compared to the relational model.\u0026rdquo; (p. 63-64, Examples 3-1 to 3-3) \u0026ldquo;When would a graph data model be appropriate? Give an example.\u0026rdquo; (p. 65, Figure 3-5) \u0026ldquo;Explain the difference between structured and unstructured data. What are data lakes and data warehouses?\u0026rdquo; (p. 66, Table 3-5) Data Storage Engines \u0026amp; Processing: \u0026ldquo;What are OLTP and OLAP? How do their requirements differ?\u0026rdquo; (p. 67-69) \u0026ldquo;What are ACID properties? Why are they important for transactional databases?\u0026rdquo; (p. 68) \u0026ldquo;Why are the terms OLTP/OLAP becoming outdated? What is the significance of decoupling storage and compute?\u0026rdquo; (p. 69, Figure 3-6) \u0026ldquo;Describe the ETL process. What are the key steps?\u0026rdquo; (p. 70-71, Figure 3-7) \u0026ldquo;What is ELT, and how does it relate to data lakes?\u0026rdquo; (p. 71) Modes of Dataflow: \u0026ldquo;Describe different ways data can be passed between processes in a production system.\u0026rdquo; (p. 72) \u0026ldquo;When is passing data through databases suitable/unsuitable?\u0026rdquo; (p. 72) \u0026ldquo;Explain request-driven data passing and its connection to microservices. What are REST and RPC?\u0026rdquo; (p. 73-74) \u0026ldquo;What is event-driven architecture for data passing? Describe real-time transports like pub/sub (Kafka) and message queues.\u0026rdquo; (p. 74-77, Figures 3-8 to 3-11) \u0026ldquo;Why might an event-driven architecture be preferred over a request-driven one for a system with many services?\u0026rdquo; (p. 75) Batch vs. Stream Processing: \u0026ldquo;Compare batch processing and stream processing. What types of data and features are typically associated with each?\u0026rdquo; (p. 78) \u0026ldquo;What are the advantages of stream processing, especially concerning latency and stateful computation?\u0026rdquo; (p. 78) \u0026ldquo;Name some stream processing engines. Why are they necessary for complex streaming ML features?\u0026rdquo; (p. 79) \u0026ldquo;How can batch and stream processing be combined in an ML system?\u0026rdquo; (p. 79, though more in Ch7) ","permalink":"https://deepskandpal.github.io/bookshelf/design-ml-system/chapter-3/","tags":null,"title":"Chapter 3: Data Engineering Fundamentals"},{"categories":null,"content":"Okay, class, let\u0026rsquo;s settle in. We\u0026rsquo;ve journeyed through the high-level overview of ML systems, the design process, and the nitty-gritty of data engineering fundamentals. Now, we arrive at a topic that is, in many ways, the heart of supervised machine learning: Chapter 4: Training Data.\nThe author makes a critical point right at the start: ML curricula often skew heavily towards modeling – the \u0026ldquo;fun\u0026rdquo; part. But as anyone who\u0026rsquo;s worked in production ML at FAANG or elsewhere knows, \u0026ldquo;spending days wrangling with a massive amount of malformatted data that doesn’t even fit into your machine’s memory is frustrating\u0026rdquo; but essential. Bad data can sink your entire ML operation, no matter how brilliant your model.\nThis chapter shifts focus from the systems perspective of data (Chapter 3) to the data science perspective. We\u0026rsquo;re going to cover:\nSampling techniques: How to select data for training. Labeling challenges: The pains of hand labels, label multiplicity, and the beauty of natural labels. Handling lack of labels: Weak supervision, semi-supervision, transfer learning, active learning. Class imbalance: Why it\u0026rsquo;s a problem and how to tackle it. Data augmentation: Creating more data from what you have. A key terminology point: The book uses \u0026ldquo;training data\u0026rdquo; instead of \u0026ldquo;training dataset\u0026rdquo; because \u0026ldquo;dataset\u0026rdquo; implies finite and stationary. Production data is rarely either (hello, \u0026ldquo;Data Distribution Shifts\u0026rdquo; on page 237, which we\u0026rsquo;ll cover much later). Creating training data, like everything else in ML systems, is an iterative process.\nLet\u0026rsquo;s begin!\nPage 81 (Chapter Introduction): The Importance and Pain of Data\nThe intro reiterates the core message:\nData is \u0026ldquo;messy, complex, unpredictable, and potentially treacherous.\u0026rdquo; Handling it well saves time and headaches. This chapter is about techniques to obtain/create good training data. \u0026ldquo;Training data\u0026rdquo; here encompasses all data for development: train, validation, and test splits. A Crucial Word of Caution (Page 82, top): Data is full of potential biases!\nBiases can creep in during collection, sampling, or labeling. Historical data can embed human biases. ML models trained on biased data will perpetuate those biases. \u0026ldquo;Use data but don’t trust it too much!\u0026rdquo; This is a mantra every ML practitioner should live by. Always be skeptical, always question your data. Pages 82-87: Sampling – Choosing Your Data Wisely\nSampling is often overlooked in coursework but is integral to the ML lifecycle. It happens when:\nCreating training data from all possible real-world data. Creating train/validation/test splits from a given dataset. Sampling events for monitoring. Why sample?\nNecessity: You rarely have access to all possible data. Your training data is inherently a sample. Feasibility: Processing all accessible data might be too time-consuming or resource-intensive. Efficiency: Quick experiments on small subsets can validate a new model\u0026rsquo;s promise before full-scale training (footnote 1: even for large models, experimenting with dataset sizes reveals its effect). Understanding sampling helps avoid bias and improve efficiency. Two families:\nNonprobability Sampling (Pages 83-84): Selection not based on probability criteria. Often driven by convenience, leading to selection biases (footnote 2, Heckman).\nConvenience sampling: Selected based on availability. Popular because it\u0026rsquo;s easy. Example: Language models often trained on easily collected Wikipedia, Common Crawl, Reddit, not necessarily representative of all text. Example: Sentiment analysis data from IMDB/Amazon reviews. Biased towards those willing to leave reviews, not representative of all users. Example: Self-driving car data initially from sunny Phoenix/Bay Area, less from rainy Kirkland (footnote 3). Model might be great in sun, poor in rain. Snowball sampling: Future samples selected based on existing ones (e.g., scrape Twitter accounts, then accounts they follow, etc.). Judgment sampling: Experts decide what to include. Quota sampling: Select based on quotas for slices (e.g., survey: 100 responses from \u0026lt;30yo, 100 from 30-60yo, etc., regardless of actual age distribution). Usefulness: Quick way to get initial data. For reliable models, probability-based sampling is preferred. FAANG Perspective: Convenience sampling is often how initial datasets are gathered for new problem domains, but we\u0026rsquo;re acutely aware of the biases and strive to get more representative data over time. Random Sampling (Probability-Based) (Pages 84-87):\nSimple Random Sampling (Page 84): All samples in the population (footnote 4: \u0026ldquo;statistical population\u0026rdquo; = potentially infinite set of all possible samples) have equal selection probability.\nAdvantage: Easy to implement. Drawback: Rare categories might be missed. If a class is 0.01% of data, a 1% random sample will likely miss it. Model might assume rare class doesn\u0026rsquo;t exist. Stratified Sampling (Page 84):\nDivide population into groups (strata) you care about. Sample from each group separately. Example: To sample 1% from data with classes A and B, sample 1% of class A and 1% of class B. Ensures rare classes are included. Drawback: Not always possible if groups are hard to define or samples belong to multiple groups (multilabel tasks - footnote 5). FAANG Perspective: Stratified sampling is crucial for creating representative validation/test sets, especially when dealing with imbalanced classes or ensuring coverage across different user segments (e.g., regions, demographics). Weighted Sampling (Page 85): Each sample given a weight determining its selection probability.\nExample: Samples A, B, C with desired probabilities 50%, 30%, 20% get weights 0.5, 0.3, 0.2. Leverages domain expertise (e.g., more recent data is more valuable, give it higher weight). Corrects for distribution mismatch (e.g., your data is 25% red, 75% blue; real world is 50/50 red/blue. Give red samples 3x weight of blue during sampling). Python: random.choices(population, weights, k). Related concept: Sample Weights (in training, not sampling to create dataset): Assigns \u0026ldquo;importance\u0026rdquo; to training samples. Higher weight samples affect the loss function more. Can significantly change decision boundaries (Figure 4-1 from scikit-learn). FAANG Perspective: Weighted sampling is used to upweight important/rare data. Sample weights in training are used for cost-sensitive learning or to prioritize certain types of errors. Reservoir Sampling (Page 86, Figure 4-2): For streaming data when you don\u0026rsquo;t know total N, can\u0026rsquo;t fit all in memory, but want to sample k items such that each has equal selection probability.\nAlgorithm: Fill reservoir (array of size k) with first k elements. For each incoming n-th element (where n \u0026gt; k): Generate random integer i from 1 to n. If 1 \u0026lt;= i \u0026lt;= k, replace reservoir element at index i with the n-th element. Else, do nothing. Ensures: Every tweet/element has an equal probability of being selected at any point in time. If stopped, current reservoir is a fair sample. n-th element has k/n probability of being in the reservoir. Each element already in reservoir has k/n probability of staying (or more precisely, 1 - (k/n)*(1/k) = (n-1)/n probability of one specific item in reservoir not being replaced by the nth item, which then combines with prior probabilities). The math ensures uniform probability for all seen items. Figure 4-2 visualizes this. FAANG Perspective: Essential for sampling from massive, unbounded streams (e.g., sampling search queries for analysis, sampling events for monitoring from a firehose). Importance Sampling (Page 87): Sample from a distribution Q(x) (proposal/importance distribution, easy to sample from) when you actually want to sample from P(x) (target distribution, hard to sample from). Weigh sample x from Q(x) by P(x)/Q(x).\nRequires Q(x) \u0026gt; 0 whenever P(x) != 0. Equation: E_P[x] = E_Q[x * P(x)/Q(x)]. Use in ML: Policy-based reinforcement learning. Estimate value of new policy (P) using rewards from old policy (Q) and reweight. FAANG Perspective: Used in some advanced modeling scenarios, off-policy evaluation in RL, and in areas like Bayesian inference (though less common in typical production ML pipelines). Pages 88-97: Labeling – The Quest for Ground Truth\nMost production ML is supervised, needing labeled data. Quality/quantity of labels heavily impacts performance.\nAndrej Karpathy anecdote: Recruiter asked how long he\u0026rsquo;d need a labeling team. His reply: \u0026ldquo;How long do we need an engineering team?\u0026rdquo; Labeling is now a core, ongoing function. Hand Labels (Pages 88-90): The classic approach.\nChallenges: Expensive: Especially with Subject Matter Expertise (SME). Spam classification: 20 crowdworkers, 15 mins training. Chest X-rays: board-certified radiologists (limited, expensive). Privacy Threat: Someone looks at your data. Can\u0026rsquo;t ship patient records or confidential financials to third-party labelers. Might need on-premise annotators. Slow: Phonetic speech transcription can take 400x utterance duration (footnote 7). 1 hour speech = 400 hours (3 months) labeling. Author\u0026rsquo;s colleagues waited almost a year for lung cancer X-ray labels. Slow Iteration: If task/data changes, relabeling is slow, model becomes less adaptive. E.g., sentiment model (NEG/POS) needs to add ANGRY class -\u0026gt; relabel, collect more ANGRY examples. Label Multiplicity / Ambiguity (Page 89, Table 4-1): Multiple annotators, different expertise/accuracy -\u0026gt; conflicting labels for same instance. Entity recognition example: \u0026ldquo;Darth Sidious, known simply as the Emperor, was a Dark Lord of the Sith who reigned over the galaxy as Galactic Emperor of the First Galactic Empire.\u0026rdquo; Annotator 1: 3 entities. Annotator 2: 6 entities (more granular, e.g., \u0026ldquo;Dark Lord\u0026rdquo; and \u0026ldquo;Sith\u0026rdquo; separate). Annotator 3: 4 entities. Which to train on? Models will perform differently. Disagreements common, especially with high SME needed (footnote 8: if obvious, no SME needed). What is \u0026ldquo;human-level performance\u0026rdquo; if experts disagree? Mitigation: Clear problem definition (e.g., \u0026ldquo;pick longest substring for entities\u0026rdquo;) and annotator training. Data Lineage (Page 90): Track origin of samples and labels. Critical if using multiple sources/annotators. Example: Model trained on 100k good samples. Add 1M crowdsourced samples (lower accuracy). Performance decreases. If data mixed, hard to debug. Helps flag bias, debug models. If model fails on recent data, investigate its acquisition. Often, it\u0026rsquo;s bad labels, not bad model. FAANG Perspective: We have large in-house and vendor labeling operations. Managing quality, cost, throughput, and privacy is a huge operational challenge. Data lineage is critical for regulatory compliance and debugging. Adjudication (resolving labeler disagreements) is a standard part of the process. Natural Labels (Behavioral Labels) (Pages 91-93): Labels inferred from system or user behavior, no human annotation needed.\nModel predictions automatically/partially evaluated by system. Examples: Google Maps ETA: At trip end, actual travel time is known. Stock price prediction: After 2 mins, actual price is known. Recommender Systems (canonical): User clicks (POSITIVE label) or doesn\u0026rsquo;t click after time (NEGATIVE label) on recommendation. Many tasks framed as recommendation (e.g., ad CTR prediction = recommend relevant ads). Can be set up: Google Translate allows community to submit alternative translations (becomes labels for next iteration, after review). Facebook \u0026ldquo;Like\u0026rdquo; button provides feedback for newsfeed ranking. Common in Industry (Figure 4-3): Survey of 86 companies: 63% use natural labels. (Percentages don\u0026rsquo;t sum to 1 as companies use multiple sources). Likely because it\u0026rsquo;s cheaper/easier to start with. Implicit vs. Explicit Labels (Page 92): Recommendation not clicked after time = implicit negative label. User downvotes recommendation = explicit negative label. Feedback Loop Length (Pages 92-93): Time from prediction to feedback. Short loops (minutes): Many recommenders (Amazon related products, Twitter follows). Longer loops (hours/days/weeks): Recommending blog posts, YouTube videos, Stitch Fix clothes (feedback after user tries them on). Different Types of User Feedback (sidebar, page 93): Ecommerce example: Clicking product (high volume, weaker signal, fast feedback). Adding to cart. Buying product (lower volume, stronger signal, business-correlated). Rating/reviewing. Returning product. Optimizing for clicks vs. purchases is a common trade-off. Depends on use case, discuss with stakeholders. Choosing Window Length: Speed vs. accuracy. Short window = faster labels, but might prematurely label as negative. Twitter Ads study (footnote 10): most clicks in 5 mins, but some hours later. Short window underestimates true CTR. Long feedback loops (weeks/months): Fraud detection (dispute window 1-3 months). Good for quarterly reports, bad for quick issue detection. A faulty fraud model can bankrupt a small business if issues take months to fix. FAANG Perspective: Natural labels are heavily used for large-scale systems (search, ads, recommendations). Designing for good feedback collection is key. Understanding feedback delay and its impact on evaluation is critical. Handling the Lack of Labels (Pages 94-101, Table 4-2): What to do when hand labels are too hard and natural labels are absent/insufficient.\nTable 4-2 Summary:\nMethod How Ground Truths Required? Weak Supervision (Noisy) heuristics to generate labels No, but small set recommended to guide heuristic dev. Semi-supervision Structural assumptions, seed labels Yes, small initial set as seeds Transfer Learning Pretrained models from another task No (zero-shot). Yes (fine-tuning, often fewer than scratch) Active Learning Label samples most useful to model Yes Weak Supervision (Pages 95-97): Use heuristics (SME rules) to label data.\nSnorkel (Stanford AI Lab, footnote 11): Popular open-source tool. Labeling Functions (LFs): Functions encoding heuristics. Example: if \u0026quot;pneumonia\u0026quot; in nurse_note: return \u0026quot;EMERGENT\u0026quot; Types of LFs: Keyword, regex, DB lookup, outputs of other models. LFs produce noisy labels. Multiple LFs can conflict. Need to combine, denoise, reweight LFs (Figure 4-4 shows this high-level process). Theoretically no hand labels needed, but small set recommended to guide LF development/accuracy check. Benefits: Privacy (write LFs on small, cleared subset; apply to all data without looking). SME expertise versioned, reused, shared. Adaptive (data/reqs change -\u0026gt; reapply LFs). Known as programmatic labeling. Table 4-3 compares with hand labeling (cost, privacy, speed, adaptivity). Case Study (Stanford Medicine, footnote 13, Figure 4-5): Weakly supervised X-ray models (1 radiologist, 8hrs writing LFs) comparable to fully supervised (almost a year hand labeling). Models improved with more unlabeled data (LFs applied). LFs reused across tasks (CXR, EXR). (Footnote 14: study used 18-20 LFs; author has seen hundreds). Why still need ML models if heuristics work? LFs might not cover all samples. Train ML on LF-labeled data to generalize to samples not covered by any LF. Powerful, but not perfect (labels can be too noisy). Good for bootstrapping. FAANG Perspective: Weak supervision is gaining traction for problems with limited labels or high SME cost. Combining heuristics with small amounts of gold data is common. Semi-Supervision (Pages 98-99): Leverages structural assumptions to generate new labels from a small initial set of labeled data.\nUsed since \u0026rsquo;90s (footnote 16, Blum \u0026amp; Mitchell co-training). Self-training (classic): Train model on existing labeled data. Predict on unlabeled data. Add high-confidence predictions (with their predicted labels) to training set. Retrain. Repeat. Similarity-based: Assume similar samples have similar labels. Obvious similarity: Twitter hashtags. Label #AI as CS. If #ML, #BigData in same MIT CSAIL profile (Figure 4-6), label them CS too. Complex similarity: Use clustering or k-NN. Perturbation-based (popular): Small perturbations to sample shouldn\u0026rsquo;t change label. Apply small noise to images or word embeddings. (More in \u0026ldquo;Perturbation\u0026rdquo; on page 114). Can reach fully supervised performance with fewer labels (footnote 17). Consideration with limited data: How much for evaluation vs. training? Small eval set -\u0026gt; overfitting to it. Large eval set -\u0026gt; less data for training boost. Common trade-off: use reasonably large eval set, then continue training champion model on that eval data too. FAANG Perspective: Used when labeled data is scarce. Self-training and consistency regularization (perturbation-based) are common techniques. Transfer Learning (Pages 99-100): Reuse model from one task (base task) as starting point for another (downstream task).\nBase task usually has abundant/cheap data (e.g., language modeling: predict next token from vast text corpora like books, Wikipedia - footnote 18 token definition). Usage: Zero-shot: Use base model directly on downstream task. Fine-tuning: Make small changes to base model (e.g., continue training on downstream data - footnote 19 Howard \u0026amp; Ruder ULMFiT). Prompting (footnote 20): Modify inputs with a template for base model. E.g., for QA with GPT-3: Q: When was US founded? A: July 4, 1776. Q: Who wrote Dec of Ind? A: Thomas Jefferson. Q: What year Alex Hamilton born? A: [GPT-3 outputs year] Appealing for tasks with few labels. Boosts performance even with many labels. Enabled many apps (ImageNet pretrained object detectors; BERT/GPT-3 for text - footnote 21). Lowers entry barrier. Trend: Larger pretrained base model -\u0026gt; better downstream performance. Training large models (GPT-3) costs tens of millions USD. Future: few companies train huge models, rest use/fine-tune them. FAANG Perspective: Transfer learning is THE dominant paradigm in NLP and increasingly in CV. We train massive foundation models and fine-tune them for various tasks. This saves immense labeling effort and compute. Active Learning (Query Learning) (Pages 101-102): Improve label efficiency. Model (active learner) chooses which unlabeled samples to send to annotators.\nLabel samples most helpful to model. Uncertainty measurement (most straightforward): Label examples model is least certain about (hoping to clarify decision boundary). E.g., classification: samples with lowest predicted class probability. (Figure 4-7 from Burr Settles, footnote 22: toy example, 30 random labels = 70% acc; 30 active learning labels = 90% acc). Query-by-committee (ensemble method, footnote 23 Ch6): Multiple models vote on samples. Label samples committee disagrees on most. Other heuristics: samples giving highest gradient updates or reducing loss most. (Settles 2010 survey). Data regimes for samples: Synthesized (model generates uncertain input points - footnote 24 Angluin). Stationary pool of unlabeled data. Real-world stream (production data). Author most excited about active learning with real-time, changing data (Chapter 1, Chapter 8). Adapts faster. FAANG Perspective: Active learning is used to prioritize labeling efforts, especially when budgets are constrained or labeling is a bottleneck. It\u0026rsquo;s often integrated with human-in-the-loop systems. Pages 102-113: Class Imbalance – The Uneven Playing Field\nA very common real-world problem.\nDefinition (Page 102): Classification: substantial difference in number of samples per class. (E.g., 99.99% normal lung X-rays, 0.01% cancerous). Regression: can also happen with skewed continuous labels. (Eugene Yan\u0026rsquo;s healthcare bills example, footnote 25: median bill low, 95th percentile astronomical. 100% error on $250 bill ($250 vs $500) okay; 100% error on $10k bill ($10k vs $20k) not. Might need to optimize for 95th percentile prediction even if overall metrics suffer). Challenges of Class Imbalance (Page 103-104, Figure 4-8): ML (esp. Deep Learning) works well with balanced data, struggles with imbalance.\nFigure 4-8 (Andrew Ng image): ML works well when distribution is like [Cat, Dog, Chair, Bike, Person] (balanced). Not so well when like [Effusion, Atelectasis, Mass, Consolidation, Hernia] (highly imbalanced medical findings). Reason 1: Insufficient signal for minority class. Becomes few-shot learning problem. If no instances, model assumes class doesn\u0026rsquo;t exist. Reason 2: Model exploits simple heuristic. E.g., lung cancer: always predict majority class (normal) -\u0026gt; 99.99% accuracy (footnote 27: why accuracy is bad for imbalance). Hard for gradient descent to beat this trivial solution. Reason 3: Asymmetric error costs. Misclassifying cancerous X-ray (rare) far more dangerous than misclassifying normal lung (common). Standard loss functions treat all samples equally. Imbalance is the norm in real world (Page 104): Author shocked after school (balanced datasets) to find this. Rare events are often more interesting/dangerous. Examples: Fraud detection (6.8c per $100 is fraud - footnote 29), churn prediction, disease screening, resume screening (98% eliminated initially - footnote 30), object detection (most generated bounding boxes are background). Other causes: Sampling bias (spam: 85% of all email is spam, but filtered before DB, so dataset has little spam - footnote 31), labeling errors. Always examine data to understand causes of imbalance. Handling Class Imbalance (Pages 105-113): Extensively studied (footnote 32). Sensitivity varies by task complexity, imbalance level (footnote 33). Binary easier than multiclass. Deep NNs (10+ layers in 2017) better on imbalanced data than shallower ones (footnote 34).\nSome argue: don\u0026rsquo;t \u0026ldquo;fix\u0026rdquo; imbalance if it\u0026rsquo;s real-world. Good model should learn it. Challenging.\nThree approaches:\nA. Using the Right Evaluation Metrics (Page 106-108): Most important first step!\nOverall accuracy/error rate: Insufficient. Dominated by majority class. Example: CANCER (positive, 10% of data) vs. NORMAL (negative, 90%). Model A (Table 4-4): Predicts 10/100 CANCER, 890/900 NORMAL. Overall Accuracy = (10+890)/1000 = 0.9. Model B (Table 4-5): Predicts 90/100 CANCER, 810/900 NORMAL. Overall Accuracy = (90+810)/1000 = 0.9. Both 90% accurate, but Model B much better for CANCER detection. Better choice: Per-class accuracy. Model A CANCER acc: 10%. Model B CANCER acc: 90%. Precision, Recall, F1 (sidebar, Table 4-6): For binary tasks, measure performance wrt positive class (footnote 35: scikit-learn pos_label). Asymmetric (values change if you swap positive/negative class). Precision = TP / (TP + FP) (Of those predicted positive, how many were actually positive?) Recall (Sensitivity, True Positive Rate) = TP / (TP + FN) (Of all actual positives, how many did we find?) F1 = 2 * (Precision * Recall) / (Precision + Recall) (Harmonic mean) Table 4-7: Model A (CANCER positive): P=0.5, R=0.1, F1=0.17. Model B: P=0.5, R=0.9, F1=0.64. Clearly shows B is better. ROC Curve (Receiver Operating Characteristics) (Page 108, Figure 4-9): Classification often outputs probability. Threshold (e.g., 0.5) converts to class. Plot True Positive Rate (Recall) vs. False Positive Rate (1 - Specificity) for different thresholds. Perfect model: Line at top (TPR=1). Random: Diagonal. Closer to top-left = better. AUC (Area Under Curve): Measures area under ROC. Larger = better. Precision-Recall Curve (Page 108): ROC focuses on positive class, doesn\u0026rsquo;t show negative class performance. Davis \u0026amp; Goadrich (footnote 36) argue PR curve is more informative for heavy imbalance. FAANG Perspective: For imbalanced problems, we never rely on overall accuracy. We look at Precision, Recall, F1 for the classes of interest, AUC-ROC, AUC-PR. Confusion matrices are essential. B. Data-Level Methods: Resampling (Pages 109-110): Modify training data distribution.\nUndersampling: Remove instances from majority class. Simplest: random removal. Oversampling: Add instances to minority class. Simplest: random replication. Figure 4-10 (Rafael Alencar, footnote 37) visualizes this. Tomek links (undersampling, footnote 38): Find close pairs from opposite classes, remove majority class sample. Clears decision boundary, but might make model less robust (loses subtlety of true boundary). For low-dim data. SMOTE (Synthetic Minority Oversampling TEchnique, footnote 39): Synthesize new minority samples by convex combinations of existing ones (footnote 40 linear). For low-dim data. Sophisticated methods (Near-Miss, one-sided selection - footnote 41) need distance calcs, expensive for high-dim data/features (e.g., NNs). CRITICAL: Never evaluate on resampled validation/test data! Model will overfit to resampled distribution. Evaluate on original, true distribution. Risks: Undersampling loses data. Oversampling (replication) overfits. Two-phase learning (footnote 42): Train on resampled data (e.g., undersample majority to N instances per class). Fine-tune on original data. Dynamic sampling (footnote 43 Pouyanfar): Oversample low-performing classes, undersample high-performing ones during training. Show model less of what it knows, more of what it doesn\u0026rsquo;t. FAANG Perspective: Resampling is common, but needs care. SMOTE is popular. Often combined with algorithm-level methods. Evaluation on original distribution is key. C. Algorithm-Level Methods (Pages 110-112): Keep data intact, alter algorithm (usually loss function) to be robust to imbalance.\nPrioritize learning instances we care about by giving them higher weight in loss. L(X; θ) = (1/N) * Σ_x L(x; θ) (standard average loss). Treats all instances equally. Cost-sensitive learning (Elkan 2001, footnote 44, Table 4-8): Misclassification costs vary. C_ij = cost if class i classified as j. C_ii = 0. If classifying POS as NEG is 2x costly as NEG as POS, C_10 = 2 * C_01. L(x; θ) = Σ_j C_ij * P(j|x; θ) (loss for instance x of class i is weighted average of costs for possible predicted classes j). Problem: Manually define cost matrix, task/scale dependent. Class-balanced loss (Page 112): Punish model for misclassifying minority classes. Vanilla: Weight class i by W_i = N_total / N_i (rarer class = higher weight). L(x; θ) = W_i * Σ_j P(j|x; θ) * Loss(x, j) (where x is instance of class i). Sophisticated: Consider overlap among samples (effective number of samples - footnote 45 Cui et al.). Focal Loss (Lin et al. 2017, footnote 46, Figure 4-11): Incentivize model to focus on hard-to-classify samples. Adjust loss: if sample has lower probability of being right, give it higher weight. Figure 4-11 shows Focal Loss (FL) vs. Cross Entropy (CE). FL reduces loss more for well-classified examples, focusing on hard ones. γ parameter controls focusing rate. Ensembles can help (footnote 47), but not their primary purpose. (Covered in Ch6). FAANG Perspective: Modifying loss functions (class weighting, focal loss) is very common for imbalanced problems, especially in deep learning. It\u0026rsquo;s often more effective and easier to implement than complex resampling if you have large data. Pages 113-117: Data Augmentation – Creating More from Less (or More from More!)\nIncrease amount of training data. Traditionally for limited data (medical imaging). Now useful even with lots of data (robustness to noise, adversarial attacks). Standard in CV, finding way into NLP. Format-dependent.\nSimple Label-Preserving Transformations (Page 114):\nComputer Vision: Randomly modify image, preserve label. Crop, flip, rotate, invert, erase part. Rotated dog is still a dog. PyTorch, TF, Keras support this. AlexNet (footnote 48): generated on CPU while GPU trains on previous batch (computationally \u0026ldquo;free\u0026rdquo;). NLP (Table 4-9): Randomly replace word with similar one (synonym dictionary, or close in embedding space), assume meaning/sentiment preserved. I'm so happy to see you. -\u0026gt; I'm so glad to see you. / ...see y'all. / I'm very happy... Quick way to double/triple training data. FAANG Perspective: Standard practice in CV. For NLP, synonym replacement, back-translation (translate Eng-\u0026gt;Fre-\u0026gt;Eng) are common. Need to be careful not to change meaning too much. Perturbation (Pages 114-116): Also label-preserving, but sometimes used to trick models, so gets own section.\nNNs sensitive to noise. CV: Small noise can cause misclassification. One-pixel attack (Su et al., footnote 49, Figure 4-12): Changing one pixel misclassifies many CIFAR-10/ImageNet images. Adversarial attacks: Using deceptive data to trick NNs. Adding noise is common. Adversarial augmentation/training (footnote 53): Add noisy samples to training data -\u0026gt; helps model recognize weak spots, improve performance (footnote 51 Goodfellow). Noise can be random or found by search (DeepFool, footnote 52, finds min noise for misclassification). NLP: Less common (random chars -\u0026gt; gibberish). But perturbation used for robustness. BERT (footnote 54): 15% tokens chosen; of these, 10% replaced with random words (1.5% total tokens become nonsensical, e.g., \u0026ldquo;My dog is apple\u0026rdquo;). Small performance boost. Chapter 6 covers perturbation for evaluation too. FAANG Perspective: Adversarial training is important for security-sensitive models (spam, fraud, face recognition) and for improving general robustness. Data Synthesis (Pages 116-117): Sidestep expensive/slow/private data collection by synthesizing it. Still far from synthesizing all data, but can boost performance.\nNLP Templates (Table 4-10): Bootstrap chatbot training data. Template: Find me a [CUISINE] restaurant within [NUMBER] miles of [LOCATION]. Fill with lists of cuisines, numbers, locations -\u0026gt; thousands of queries. CV Mixup (Zhang et al. ICLR 2018, footnote 55): Combine existing examples with discrete labels to make continuous labels. x' = γ*x1 + (1-γ)*x2 (e.g., x1=DOG (0), x2=CAT (1)). Label for x' = γ*0 + (1-γ)*1. Improves generalization, reduces memorization of corrupt labels, robust to adversarial examples, stabilizes GAN training. NNs to synthesize data (e.g., CycleGAN, footnote 56 Sandfort): Exciting research, not yet popular in production. Adding CycleGAN images to CT segmentation improved performance. CV Augmentation Survey (Shorten \u0026amp; Khoshgoftaar 2019). FAANG Perspective: Templating is used for bootstrapping NLU models. Mixup and related techniques (CutMix, CutOut) are standard in CV training. GAN-based synthesis is still mostly research but promising for rare data or privacy-preserving data generation. Page 118: Summary of Chapter 4\nTraining data is foundational. Bad data = bad models. Invest time/effort to curate/create it.\nSampling: Nonprobability (convenience) vs. Random (simple, stratified, weighted, reservoir, importance). Labeling: Most ML is supervised. Natural labels (delivery times, recommender clicks) are great, but often delayed (feedback loop length). Hand labels: Expensive, slow, privacy issues, label multiplicity. Data lineage is key. Lack of labels: Weak supervision (heuristics, Snorkel), semi-supervision (self-training, similarity, perturbation), transfer learning (pretrained models), active learning (querying for most useful labels). Class Imbalance: Norm in real world. Hard for ML. Handle by: right metrics (Precision/Recall/F1, ROC/PR AUC), resampling (over/under, SMOTE), algorithm changes (cost-sensitive loss, focal loss). Data Augmentation: Increase data (simple transforms, perturbation/adversarial, synthesis/mixup/templates). Improves performance, generalization, robustness. Next: Feature extraction (Chapter 5).\nInterview Questions \u0026amp; Page References (Chapter 4):\nGeneral Training Data Concepts:\n\u0026ldquo;Why is handling training data well so critical in ML projects?\u0026rdquo; (p. 81) \u0026ldquo;What are some potential sources of bias in training data, and why is it important to be aware of them?\u0026rdquo; (p. 82) \u0026ldquo;Why does the author prefer the term \u0026rsquo;training data\u0026rsquo; over \u0026rsquo;training dataset\u0026rsquo; in the context of production ML?\u0026rdquo; (p. 81) Sampling:\n\u0026ldquo;Why is sampling necessary or helpful in the ML workflow?\u0026rdquo; (p. 82) \u0026ldquo;Describe different nonprobability sampling methods and their potential biases. Give examples where they might be used.\u0026rdquo; (p. 83-84) \u0026ldquo;Compare simple random sampling with stratified sampling. When would you prefer stratified sampling?\u0026rdquo; (p. 84) \u0026ldquo;What is weighted sampling, and how can it be used to leverage domain expertise or correct for distribution mismatches?\u0026rdquo; (p. 85) \u0026ldquo;Explain reservoir sampling. When is it particularly useful?\u0026rdquo; (p. 86, Figure 4-2) \u0026ldquo;What is importance sampling and where might it be applied in ML?\u0026rdquo; (p. 87) Labeling:\n\u0026ldquo;What are the main challenges associated with acquiring hand labels?\u0026rdquo; (p. 88) \u0026ldquo;What is label multiplicity? How can disagreements among annotators be minimized?\u0026rdquo; (p. 89, Table 4-1) \u0026ldquo;Explain the concept of data lineage and its importance.\u0026rdquo; (p. 90) \u0026ldquo;What are natural labels (behavioral labels)? Give some examples. How do they compare to hand labels?\u0026rdquo; (p. 91) \u0026ldquo;Discuss the concept of feedback loop length for natural labels and its implications. Provide examples of short and long feedback loops.\u0026rdquo; (p. 92-93) \u0026ldquo;Explain the difference between implicit and explicit labels.\u0026rdquo; (p. 92) Handling Lack of Labels:\n\u0026ldquo;Describe weak supervision. How do Labeling Functions (LFs) work in tools like Snorkel?\u0026rdquo; (p. 95-97, Figure 4-4, Table 4-3) \u0026ldquo;What are the advantages of programmatic labeling (weak supervision) over hand labeling?\u0026rdquo; (p. 96, Table 4-3) \u0026ldquo;What is semi-supervised learning? Describe self-training and perturbation-based methods.\u0026rdquo; (p. 98-99, Figure 4-6) \u0026ldquo;Explain transfer learning. What are base models, downstream tasks, fine-tuning, and prompting?\u0026rdquo; (p. 99-100) \u0026ldquo;What is active learning? How can uncertainty measurement or query-by-committee be used to select samples for labeling?\u0026rdquo; (p. 101-102, Figure 4-7) Class Imbalance:\n\u0026ldquo;What is class imbalance, and why does it make learning difficult for ML models?\u0026rdquo; (p. 102-104, Figure 4-8) \u0026ldquo;Give some real-world examples of tasks with class imbalance.\u0026rdquo; (p. 104) \u0026ldquo;Why is overall accuracy an insufficient metric for tasks with class imbalance? What are better alternatives?\u0026rdquo; (p. 106-108, Tables 4-4, 4-5, 4-7, Figure 4-9) \u0026ldquo;Explain Precision, Recall, and F1-score. Why are they useful for imbalanced datasets?\u0026rdquo; (p. 107, Table 4-6) \u0026ldquo;Describe data-level methods for handling class imbalance, such as oversampling and undersampling (including SMOTE and Tomek links).\u0026rdquo; (p. 109-110, Figure 4-10) \u0026ldquo;What are algorithm-level methods for class imbalance? Explain cost-sensitive learning, class-balanced loss, and focal loss.\u0026rdquo; (p. 110-112, Table 4-8, Figure 4-11) \u0026ldquo;When resampling training data, what is a critical consideration for model evaluation?\u0026rdquo; (p. 110) Data Augmentation:\n\u0026ldquo;What is data augmentation, and why is it used?\u0026rdquo; (p. 113) \u0026ldquo;Describe simple label-preserving transformations for image and text data.\u0026rdquo; (p. 114, Table 4-9) \u0026ldquo;What is perturbation in the context of data augmentation? How does it relate to adversarial attacks and adversarial training?\u0026rdquo; (p. 114-116, Figure 4-12) \u0026ldquo;Explain some data synthesis techniques like using templates for NLP or mixup for CV.\u0026rdquo; (p. 116-117, Table 4-10) This chapter is packed with practical techniques essential for any ML engineer. Getting the training data right is often more than half the battle! Any questions on these topics?\n","permalink":"https://deepskandpal.github.io/bookshelf/design-ml-system/chapter-4/","tags":null,"title":"Chapter 4: Training Data"},{"categories":null,"content":"Chapter 4: Training Models.\n(Page 111: Introduction - Beyond Black Boxes)\nUp until now, as the book says, we\u0026rsquo;ve treated ML models and their training algorithms mostly like black boxes. We fed them data, they gave us results, and we learned to evaluate those results. You\u0026rsquo;ve optimized regression, improved classifiers, even built a spam filter, often without peeking under the hood. And that\u0026rsquo;s okay for many practical purposes!\nHowever, understanding how these things work internally is incredibly powerful. It helps you:\nChoose the right model and algorithm: Knowing the mechanics helps you match the tool to the task. Select good hyper parameters: Hyper parameters often control the learning process itself. Understanding that process helps you tune them effectively. Debug issues and perform error analysis: When things go wrong, or your model makes weird mistakes, knowing the \u0026ldquo;why\u0026rdquo; is crucial. Foundation for advanced topics: Especially for neural networks (Part II of the book), the concepts here are fundamental. This chapter focuses on Linear Regression as a starting point because it\u0026rsquo;s simple, yet we can train it in very different ways. We\u0026rsquo;ll explore two main approaches:\nA direct \u0026ldquo;closed-form\u0026rdquo; equation (The Normal Equation): This is like having a magic formula that directly spits out the best model parameters in one go. An iterative optimization approach (Gradient Descent): This is more like a trial-and-error process. We start with a guess for the parameters and gradually tweak them, step by step, to minimize the error, eventually (hopefully!) arriving at the same best parameters. We\u0026rsquo;ll see different \u0026ldquo;flavors\u0026rdquo; of Gradient Descent: Batch, Mini-batch, and Stochastic. Then, the chapter will touch on:\nPolynomial Regression: How to use linear models for non-linear data. Learning Curves: Tools to diagnose over fitting or under fitting. Regularization: Techniques to prevent over fitting. Logistic Regression and Softmax Regression: Models commonly used for classification tasks. The scorpion icon on page 112 gives a fair warning: there will be some math (linear algebra, calculus). If you\u0026rsquo;re \u0026ldquo;allergic,\u0026rdquo; the book suggests you can still get the concepts by focusing on the text. My job is to make sure you get those concepts, regardless of how comfortable you are with the equations. We\u0026rsquo;ll always ask: \u0026ldquo;What is this equation ultimately trying to achieve?\u0026rdquo;\n(Page 112-113: Linear Regression - The Model)\nRemember our life satisfaction model from Chapter 1? life_satisfaction = θ₀ + θ₁ × GDP_per_capita. That was a simple linear regression with one feature.\nMore generally, a linear model predicts a value by:\nTaking all the input features (like a house\u0026rsquo;s square footage, number of bedrooms, age, etc.). Multiplying each feature by a specific weight (a model parameter). Summing up these weighted features. Adding a constant bias term (another model parameter, also called the intercept). Equation 4-1 (page 112): Linear Regression model prediction ŷ = θ₀ + θ₁x₁ + θ₂x₂ + ⋯ + θₙxₙ\nŷ (y-hat): The predicted value. n: The number of features. xᵢ: The value of the i-th feature. θ₀: The bias term (theta-zero). What it\u0026rsquo;s ultimately trying to achieve: It\u0026rsquo;s the baseline prediction if all feature values were zero. It allows the line/plane to shift up or down. θ₁ to θₙ: The feature weights. θⱼ is the weight for the j-th feature. What they\u0026rsquo;re ultimately trying to achieve: They represent how much a one-unit change in that feature xⱼ affects the predicted value ŷ, holding other features constant. A positive weight means the feature positively influences the prediction; a negative weight means it negatively influences it. The magnitude shows the strength of the influence. Equation 4-2 (page 113): Vectorized form ŷ = h_θ(x) = θ · x\nThis is just a more compact way to write Equation 4-1 using vector notation.\nθ (theta): Is now a parameter vector [θ₀, θ₁, ..., θₙ]. x: Is the instance\u0026rsquo;s feature vector [x₀, x₁, ..., xₙ]. Here, we add a \u0026ldquo;dummy\u0026rdquo; feature x₀ which is always set to 1. This allows us to include the bias term θ₀ neatly into the dot product (because θ₀ * x₀ = θ₀ * 1 = θ₀). θ · x: This is the dot product of the two vectors. It\u0026rsquo;s exactly the sum θ₀x₀ + θ₁x₁ + ... + θₙxₙ. h_θ(x): This is our hypothesis function (our model), parameterized by θ. Given an input x, it predicts ŷ. The bird sidebar (page 113) explains that vectors are often column vectors (2D arrays with one column). So, if θ and x are column vectors, the dot product can be written as a matrix multiplication: ŷ = θᵀx (where θᵀ is the transpose of θ, making it a row vector). Don\u0026rsquo;t let this bog you down; it\u0026rsquo;s a notational convenience. The goal is the same: calculate a weighted sum of features plus a bias.\nHow do we train it? Training means finding the values for the parameters θ (the bias θ₀ and the weights θ₁ to θₙ) that make the model \u0026ldquo;best fit\u0026rdquo; the training data.\nTo do this, we need a way to measure how well (or poorly) the model fits. We learned in Chapter 2 that for regression, a common measure is RMSE (Root Mean Square Error).\nWhat it\u0026rsquo;s ultimately trying to achieve: Quantify the typical prediction error. However, for mathematical convenience during training, it\u0026rsquo;s often easier to minimize the MSE (Mean Squared Error) instead. Minimizing MSE will also minimize RMSE (since the square root function is monotonic). The footnote on page 113 is important: the function we optimize during training (the cost function, here MSE) might be different from the final performance metric we use to evaluate the model (e.g., RMSE). This is often because the cost function has nice mathematical properties (like being easily differentiable) that make optimization easier. (Page 114: MSE Cost Function \u0026amp; The Normal Equation)\nEquation 4-3 (page 114): MSE cost function for a Linear Regression model MSE(X, h_θ) = (1/m) * Σᵢ (θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾)² (summing from i=1 to m, where m is the number of instances)\nWhat it\u0026rsquo;s ultimately trying to achieve: For each training instance i: θᵀx⁽ⁱ⁾ is the model\u0026rsquo;s prediction for that instance. y⁽ⁱ⁾ is the actual target value. (θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾) is the error for that instance. We square this error: (error)². (Why square? It makes all errors positive, and it penalizes larger errors more heavily). We sum these squared errors over all m training instances: Σᵢ (error)². We divide by m to get the mean of the squared errors. This function tells us, on average, how \u0026ldquo;bad\u0026rdquo; our model\u0026rsquo;s predictions are for a given set of parameters θ. Our goal in training is to find the θ that makes this MSE as small as possible. The Normal Equation: A Direct Solution\nFor Linear Regression with an MSE cost function, there\u0026rsquo;s a wonderful mathematical shortcut. Instead of iteratively searching for the best θ, there\u0026rsquo;s a direct formula that gives you the θ that minimizes the cost function in one shot! This is called the Normal Equation.\nEquation 4-4 (page 114): Normal Equation θ̂ = (XᵀX)⁻¹ Xᵀy\nθ̂ (theta-hat): This is the value of θ that minimizes the cost function. X: The matrix of input features for all training instances (each row is an instance, x₀ for each instance is 1). y: The vector of actual target values for all training instances. Xᵀ: The transpose of matrix X. (XᵀX)⁻¹: The inverse of the matrix XᵀX. What it\u0026rsquo;s ultimately trying to achieve: This equation, derived using calculus (setting the derivative of the cost function to zero and solving for θ), directly calculates the optimal parameter vector θ̂ that makes the linear model fit the training data as closely as possible (in the mean squared error sense). It\u0026rsquo;s like a direct recipe: plug in your data X and y, and out pops the best θ.\n(Page 115: Testing the Normal Equation)\nThe book generates some linear-looking data (Figure 4-1): X = 2 * np.random.rand(100, 1) (100 instances, 1 feature) y = 4 + 3 * X + np.random.randn(100, 1) (True model is y = 4 + 3x₁ + noise) So, the ideal θ₀ is 4, and the ideal θ₁ is 3.\nTo use the Normal Equation, we need to add x₀ = 1 to each instance in X: X_b = np.c_[np.ones((100, 1)), X] (np.c_ concatenates arrays column-wise)\nNow, apply the Normal Equation: theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) The result is something like [[4.215...], [2.770...]]. So, θ₀̂ ≈ 4.215 and θ₁̂ ≈ 2.770. It\u0026rsquo;s close to the true values (4 and 3), but not exact because of the random noise we added to y. The noise makes it impossible to recover the exact original parameters.\nWe can then use this theta_best to make predictions (Figure 4-2).\n(Page 116-117: Scikit-Learn and Computational Complexity)\nScikit-Learn LinearRegression: from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X, y) Scikit-Learn handles adding the x₀=1 feature (or rather, it separates the bias term lin_reg.intercept_ from the feature weights lin_reg.coef_). The results are the same as the Normal Equation.\nUnderlying Method (scipy.linalg.lstsq): Scikit-Learn\u0026rsquo;s LinearRegression actually uses scipy.linalg.lstsq() (\u0026ldquo;least squares\u0026rdquo;). This function computes θ̂ = X⁺y, where X⁺ is the pseudoinverse (or Moore-Penrose inverse) of X. You can compute X⁺ using np.linalg.pinv(). The pseudoinverse is calculated using a technique called Singular Value Decomposition (SVD).\nWhy SVD/pseudoinverse instead of the direct Normal Equation (XᵀX)⁻¹ Xᵀy? More efficient: SVD is generally more computationally efficient. Handles edge cases: The Normal Equation requires XᵀX to be invertible. If it\u0026rsquo;s not (e.g., if you have more features than instances, m \u0026lt; n, or if some features are redundant/linearly dependent), the Normal Equation breaks down. The pseudoinverse is always defined, making SVD more robust. Computational Complexity:\nNormal Equation (inverting XᵀX): About O(n²·⁴) to O(n³) where n is the number of features. This gets very slow if you have many features (e.g., 100,000 features). Doubling features can increase time by 5-8x. SVD (used by Scikit-Learn): About O(n²). Doubling features increases time by ~4x. Still slow for very large n. Both are O(m) with respect to the number of instances m. So, they handle large numbers of training instances efficiently, as long as the data fits in memory. Predictions: Once trained, making predictions is very fast: O(m) and O(n) – linear with number of instances and features. The Problem with Normal Equation/SVD: They get slow with many features and require all data to be in memory. This leads us to the next method\u0026hellip;\n(Page 118-123: Gradient Descent - The Iterative Approach)\nWhen the Normal Equation is too slow (too many features) or the dataset is too large to fit in memory, we need a different approach. Enter Gradient Descent.\nThe Core Idea: Gradient Descent is a generic optimization algorithm. It iteratively tweaks model parameters to minimize a cost function.\nImagine you\u0026rsquo;re lost in a foggy mountain valley. You can only feel the slope of the ground under your feet. To get to the bottom, you\u0026rsquo;d take a step in the direction of the steepest downhill slope. Repeat. This is Gradient Descent: It measures the local gradient of the cost function (e.g., MSE) with respect to the parameter vector θ. The gradient tells you the direction of steepest ascent. It takes a step in the opposite direction (descending gradient) to reduce the cost. Repeat until the gradient is zero (or very close), meaning you\u0026rsquo;ve reached a minimum. The Process (Figure 4-3, page 118): Random Initialization: Start with random values for θ. Iterative Improvement: In each step: Calculate the gradient of the cost function at the current θ. Update θ by taking a small step in the negative gradient direction. Convergence: Continue until the algorithm converges to a minimum (cost stops decreasing significantly). Learning Rate (η - eta):\nThis is a crucial hyperparameter that determines the size of the steps. Too small (Figure 4-4): Many iterations needed to converge (very slow). Too large (Figure 4-5): Might jump across the valley, diverge, and fail to find a solution. Challenges (Figure 4-6, page 119):\nLocal Minima: If the cost function has multiple minima (not a smooth bowl), GD might converge to a local minimum, which isn\u0026rsquo;t as good as the global minimum. Plateaus: If the cost function has flat areas, GD can take a very long time to cross them. Irregular Terrains: Holes, ridges make convergence difficult. Good News for Linear Regression MSE:\nThe MSE cost function for Linear Regression is a convex function. What this means: It\u0026rsquo;s shaped like a bowl. It has no local minima, only one global minimum. It\u0026rsquo;s also continuous with a slope that doesn\u0026rsquo;t change abruptly. Consequence: Gradient Descent is guaranteed to approach the global minimum if you wait long enough and the learning rate isn\u0026rsquo;t too high. Feature Scaling Matters! (Figure 4-7, page 120):\nIf features have very different scales (e.g., feature 1 ranges 0-1, feature 2 ranges 0-1000), the cost function \u0026ldquo;bowl\u0026rdquo; becomes elongated. GD will take a long, zig-zag path to the minimum. Solution: Ensure all features have a similar scale (e.g., using StandardScaler). GD will then converge much faster. Parameter Space: Training a model is essentially a search in the model\u0026rsquo;s parameter space for the combination of parameters that minimizes the cost. More parameters = higher dimensional space = harder search. For Linear Regression (convex cost), it\u0026rsquo;s like finding the bottom of a D-dimensional bowl.\n(Page 121-123: Batch Gradient Descent - BGD)\nTo implement GD, we need the gradient of the cost function with respect to each model parameter θⱼ. This is the partial derivative ∂MSE(θ) / ∂θⱼ.\nEquation 4-5 (page 121): Partial derivative of MSE w.r.t. θⱼ ∂MSE(θ)/∂θⱼ = (2/m) * Σᵢ (θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾) * xⱼ⁽ⁱ⁾\nWhat it\u0026rsquo;s ultimately trying to achieve: For each parameter θⱼ, it calculates how much the MSE would change if θⱼ changed a tiny bit. (θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾) is the prediction error for instance i. We multiply this error by the value of the j-th feature of instance i, xⱼ⁽ⁱ⁾. (If xⱼ⁽ⁱ⁾ is large, θⱼ has a bigger impact on the prediction and thus on the error). We average this product over all instances m. Equation 4-6 (page 122): Gradient vector ∇_θ MSE(θ) ∇_θ MSE(θ) = (2/m) Xᵀ(Xθ - y)\nThis is the compact, vectorized way to compute all partial derivatives at once. ∇_θ MSE(θ) is a vector containing ∂MSE(θ)/∂θ₀, ∂MSE(θ)/∂θ₁, \u0026hellip;, ∂MSE(θ)/∂θₙ. What it\u0026rsquo;s ultimately trying to achieve: It gives the direction of steepest increase in the cost function. Crucial point for BATCH GD: This formula uses the entire training set X at each step to calculate the gradients. This is why it\u0026rsquo;s called Batch Gradient Descent.\nConsequence: Terribly slow on very large training sets. Advantage: Scales well with the number of features (unlike Normal Equation). Equation 4-7 (page 122): Gradient Descent step θ⁽ⁿᵉˣᵗ ˢᵗᵉᵖ⁾ = θ - η ∇_θ MSE(θ)\nWhat it\u0026rsquo;s ultimately trying to achieve: Update the current parameters θ by taking a step of size η (learning rate) in the direction opposite to the gradient (downhill). Implementation (page 122): The code shows a loop: for iteration in range(n_iterations): gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradients With eta = 0.1 and n_iterations = 1000, the resulting theta is exactly what the Normal Equation found! Perfect.\nEffect of Learning Rate η (Figure 4-8, page 123):\nη = 0.02 (left): Too slow, many steps to converge. η = 0.1 (middle): Good, converges quickly. η = 0.5 (right): Too high, diverges, jumps around. Finding a good learning rate: Grid search (Chapter 2).\nSetting number of iterations: If too low, far from optimum. If too high, waste time after convergence.\nSolution: Set many iterations, but stop when the gradient vector becomes tiny (its norm \u0026lt; ϵ, a small tolerance). This means we\u0026rsquo;re (almost) at the minimum. Convergence Rate (sidebar, page 124): For convex cost functions like MSE, BGD with fixed η takes O(1/ϵ) iterations to reach within ϵ of the optimum. To get 10x more precision (divide ϵ by 10), you need ~10x more iterations.\n(Page 124-127: Stochastic Gradient Descent - SGD)\nBatch GD is slow on large datasets because it uses all training data at each step.\nStochastic Gradient Descent (SGD):\nAt each step, picks one random instance from the training set and computes gradients based only on that single instance. Advantages: Much faster per step (very little data to process). Can train on huge datasets (only one instance in memory at a time – good for out-of-core learning, Ch 1). Disadvantages (Figure 4-9, page 124): Stochastic nature: The path to the minimum is much more erratic (\u0026ldquo;bouncy\u0026rdquo;) than BGD. Cost function goes up and down, decreasing only on average. Never settles: Once near the minimum, it keeps bouncing around, never perfectly settling. Final parameters are good, but not optimal. Advantage of randomness: If cost function is irregular (non-convex, Figure 4-6), SGD\u0026rsquo;s randomness can help it jump out of local minima and find a better global minimum. Learning Schedule (page 125):\nTo help SGD settle at the minimum, we can gradually reduce the learning rate η. Start with large η (quick progress, escape local minima). Make η smaller over time (settle at global minimum). This is like simulated annealing in metallurgy. The function determining η at each iteration is the learning schedule. If η reduces too quickly -\u0026gt; stuck in local minimum or frozen half-way. If η reduces too slowly -\u0026gt; bounce around minimum for long, or stop too early with suboptimal solution. Implementation of SGD (page 125):\nOuter loop for epochs (an epoch is one pass through the entire training set, by convention). Inner loop for m iterations (number of instances). In each inner iteration: Pick a random_index. Get xi and yi for that instance. Gradients calculated using only xi and yi: gradients = 2 * xi.T.dot(xi.dot(theta) - yi). (Note: 2 not 2/m because m=1 here). Update eta using learning_schedule(epoch * m + i). Update theta. After 50 epochs (much fewer iterations than BGD\u0026rsquo;s 1000), it finds a theta very close to BGD\u0026rsquo;s. Figure 4-10 (page 126) shows the irregular first 20 steps. Important note on SGD (sidebar, page 126):\nTraining instances must be independent and identically distributed (IID) for SGD to converge to global optimum on average. Shuffle instances during training (pick randomly, or shuffle set at start of each epoch). If data is sorted (e.g., by label), SGD will optimize for one label, then the next, and not find global minimum. SGD with Scikit-Learn (SGDRegressor, page 126):\nDefaults to optimizing MSE. sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1) max_iter: max epochs. tol: stop if loss drops by less than this in one epoch. penalty=None: no regularization (more later). eta0: initial learning rate. Uses its own default learning schedule. sgd_reg.fit(X, y.ravel()) (.ravel() flattens y into 1D array, often needed by Scikit-Learn). Resulting intercept and coefficient are very close to Normal Equation\u0026rsquo;s. (Page 127: Mini-batch Gradient Descent)\nA compromise between Batch GD and Stochastic GD.\nAt each step, computes gradients on small random sets of instances called mini-batches. Main advantage over SGD: Performance boost from hardware optimization of matrix operations (especially on GPUs). Behavior (Figure 4-11, page 127): Less erratic path than SGD. Ends up closer to the minimum than SGD (with fixed η). But may be harder to escape local minima (for non-convex problems) compared to SGD. All three (Batch, Stochastic, Mini-batch) end up near the minimum. Batch stops at the minimum. SGD and Mini-batch would also reach minimum with a good learning schedule. But Batch GD takes much longer per step. (Page 128: Comparison Table 4-1)\nThis table nicely summarizes Normal Equation, SVD, Batch GD, Stochastic GD, Mini-batch GD for Linear Regression based on:\nLarge m (instances): Normal Eq/SVD are fast if data fits memory. GD methods are also fast (SGD/Mini-batch can do out-of-core). Out-of-core support: Only SGD/Mini-batch. Large n (features): Normal Eq/SVD are slow. GD methods are fast. Hyperparameters: Normal Eq/SVD have 0. BGD has 2 (η, iterations). SGD/Mini-batch have \u0026gt;=2 (η, iterations, schedule params). Scaling required: No for Normal Eq/SVD. Yes for GD methods. Scikit-Learn class: LinearRegression for SVD. SGDRegressor for GDs. The key is that after training, all these methods (if converged) produce very similar models and make predictions in the same way. The difference is in how they get there (the training process).\nExcellent! Glad you\u0026rsquo;re with me. Let\u0026rsquo;s push on to the next sections of Chapter 4. We\u0026rsquo;ve just laid the groundwork for how Linear Regression models are trained. Now, let\u0026rsquo;s see how we can adapt these ideas for more complex scenarios.\n(Page 128-130: Polynomial Regression)\nSo far, Linear Regression assumes a straight-line (or flat plane/hyperplane) relationship between features and the target. But what if your data is more complex, like the curved data in Figure 4-12 (page 129)? The book shows data generated by a simple quadratic equation: y = 0.5 * X**2 + X + 2 + noise. Clearly, a straight line won\u0026rsquo;t fit this well.\nThis is where Polynomial Regression comes in.\nThe Core Idea: You can still use a linear model to fit nonlinear data!\nHow? By adding powers of each feature as new features, and then training a linear model on this extended set of features.\nFor example, if you have one feature X, you can create a new feature X². Then your linear model becomes ŷ = θ₀ + θ₁X + θ₂X². Even though the equation is quadratic in X, it\u0026rsquo;s linear in terms of the parameters θ₀, θ₁, θ₂ and the features X and X². Scikit-Learn\u0026rsquo;s PolynomialFeatures (page 129): This class transforms your data. from sklearn.preprocessing import PolynomialFeatures poly_features = PolynomialFeatures(degree=2, include_bias=False)\ndegree=2: We want to add 2nd-degree polynomial features (i.e., squares). include_bias=False: PolynomialFeatures can add a column of 1s (for the bias term), but LinearRegression handles that, so we set it to False to avoid redundancy. X_poly = poly_features.fit_transform(X) If X[0] was [-0.75], then X_poly[0] becomes [-0.75, 0.566] (original X, and X²). Train a Linear Model on Extended Data: Now, you just fit a standard LinearRegression model to this X_poly: lin_reg = LinearRegression() lin_reg.fit(X_poly, y) The model predictions are shown in Figure 4-13 (page 130). It\u0026rsquo;s a nice curve that fits the data much better than a straight line! The estimated equation (e.g., ŷ = 0.56x₁² + 0.93x₁ + 1.78) is close to the original y = 0.5x₁² + 1.0x₁ + 2.0 + noise. Multiple Features and Combinations (page 130): If you have multiple features (say, a and b) and use PolynomialFeatures(degree=3), it will add a², a³, b², b³, AND also the combination terms like ab, a²b, ab². This allows the model to find relationships between features.\nWarning: Combinatorial Explosion! (Scorpion icon, page 130) The number of features can explode if you have many original features and use a high polynomial degree. The formula is (n+d)! / (d!n!) where n is original features, d is degree. Be careful! This can make the model very slow and prone to overfitting.\n(Page 130-134: Learning Curves)\nWith high-degree Polynomial Regression, you can fit the training data very well, maybe too well. Figure 4-14 (page 131) shows: A 300-degree polynomial model: Wiggles wildly to hit every training point. This is severe overfitting. A plain linear model: Misses the curve. This is underfitting. A quadratic model (2nd degree): Fits best, which makes sense as the data was generated quadratically. But how do you know in general if your model is too complex (overfitting) or too simple (underfitting), especially if you don\u0026rsquo;t know the true underlying function?\nCross-Validation (from Chapter 2):\nIf model performs well on training data but poorly on cross-validation, it\u0026rsquo;s overfitting. If it performs poorly on both, it\u0026rsquo;s underfitting. Learning Curves (page 131):\nWhat they are: Plots of the model\u0026rsquo;s performance (e.g., RMSE) on the training set and the validation set, as a function of the training set size (or training iteration).\nHow to generate them: Train the model multiple times on different-sized subsets of the training set.\nThe book provides a plot_learning_curves function (page 132).\nLearning Curves for an Underfitting Model (e.g., plain Linear Regression on the quadratic data - Figure 4-15, page 132): Training error: Starts at 0 (perfect fit with 1-2 instances), then rises as more noisy/nonlinear data is added, eventually plateauing. Validation error: Starts very high (model trained on few instances generalizes poorly), then decreases as model learns from more examples, eventually plateauing close to the training error. Key characteristics of underfitting curves: Both curves plateau, they are close together, and the error is fairly high. What it\u0026rsquo;s ultimately trying to achieve: The plot tells you that the model is too simple. Adding more training examples will not help if the curves have plateaued (as the scorpion icon on page 133 says). You need a more complex model or better features. Learning Curves for an Overfitting Model (e.g., 10th-degree polynomial on the quadratic data - Figure 4-16, page 133):\nTraining error: Much lower than the Linear Regression model\u0026rsquo;s training error. It fits the training data well. Validation error: There\u0026rsquo;s a gap between the training error and the validation error. The model performs significantly better on the training data than on the validation data. This is the hallmark of overfitting. If you had a much larger training set, these two curves would eventually get closer. What it\u0026rsquo;s ultimately trying to achieve: The plot tells you the model is too complex for the current amount of data. One way to improve an overfitting model is to feed it more training data (as the scorpion icon on page 134 suggests), until the validation error meets the training error (or use regularization, which we\u0026rsquo;ll see next). The Bias/Variance Trade-off (sidebar, page 134): This is a fundamental concept in statistics and ML. A model\u0026rsquo;s generalization error (how well it performs on unseen data) can be expressed as the sum of three components:\nBias: Error due to wrong assumptions made by the model. E.g., assuming data is linear when it\u0026rsquo;s quadratic. A high-bias model is likely to underfit. What it\u0026rsquo;s ultimately trying to achieve (in a bad way): It has a strong preconceived notion of how the data should look, and it sticks to it even if the data says otherwise. Variance: Error due to the model\u0026rsquo;s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (like a high-degree polynomial) can have high variance. A high-variance model is likely to overfit. It learns the noise in the training data, not just the signal. What it\u0026rsquo;s ultimately trying to achieve (in a bad way): It tries to fit every little nook and cranny of the training data, making it unstable and perform poorly on new, slightly different data. Irreducible Error: Error due to the inherent noisiness of the data itself. This part cannot be reduced by model changes; only by cleaning the data (e.g., fixing broken sensors, removing outliers). The Trade-off:\nIncreasing model complexity typically increases variance (more likely to overfit) and reduces bias (better fit to complex patterns). Decreasing model complexity typically increases bias (more likely to underfit) and reduces variance. The goal is to find a sweet spot, balancing bias and variance. (Page 134-141: Regularized Linear Models)\nWe saw that overfitting is a problem with complex models. Regularization is a way to reduce overfitting by constraining the model.\nWhat it\u0026rsquo;s ultimately trying to achieve: For linear models, this usually means constraining the model\u0026rsquo;s weights. The idea is to prevent the weights from becoming too large, which can happen when the model tries too hard to fit the noise in the training data. Smaller weights generally lead to simpler, smoother models that generalize better. The book discusses three types of regularized linear models: Ridge, Lasso, and Elastic Net.\nRidge Regression (also Tikhonov regularization) (page 135):\nIt adds a regularization term to the MSE cost function. Equation 4-8: Ridge Regression cost function J(θ) = MSE(θ) + α * (1/2) * Σᵢ (θᵢ)² (sum from i=1 to n, so bias term θ₀ is NOT regularized) The regularization term is α * (1/2) * Σᵢ (θᵢ)². This is α/2 times the sum of the squares of the feature weights. This is related to the ℓ₂ norm of the weight vector w = [θ₁, ..., θₙ], specifically (1/2) * ||w||₂². α (alpha) is a hyperparameter that controls how much you want to regularize. If α = 0, it\u0026rsquo;s just Linear Regression. If α is very large, all weights θᵢ (for i\u0026gt;0) end up very close to zero, and the model becomes a flat line through the data\u0026rsquo;s mean. What it\u0026rsquo;s ultimately trying to achieve: The learning algorithm is now forced to not only fit the data (minimize MSE) but also keep the model weights as small as possible. Important: Scale the data (e.g., StandardScaler) before performing Ridge Regression, as it\u0026rsquo;s sensitive to the scale of input features (scorpion icon, page 136). This is true for most regularized models. Figure 4-17 (page 136) shows the effect of α: Left: Plain Ridge Regression on linear data. Higher α makes predictions flatter. Right: Polynomial features (degree=10) + scaling + Ridge. Increasing α leads to flatter (less extreme, more reasonable) predictions, reducing variance but increasing bias. Training Ridge Regression: Closed-form solution (Equation 4-9, page 137): θ̂ = (XᵀX + αA)⁻¹ Xᵀy (where A is an identity matrix with a 0 at the top-left for the bias term). Scikit-Learn\u0026rsquo;s Ridge(alpha=..., solver=\u0026quot;cholesky\u0026quot;) uses this. Gradient Descent (page 137): Add αw to the MSE gradient vector (where w is the vector of weights, excluding bias). Scikit-Learn\u0026rsquo;s SGDRegressor(penalty=\u0026quot;l2\u0026quot;) does this. The \u0026quot;l2\u0026quot; means add a regularization term equal to half the square of the ℓ₂ norm of the weights. Lasso Regression (Least Absolute Shrinkage and Selection Operator) (page 137):\nAlso adds a regularization term to the MSE cost function, but uses the ℓ₁ norm of the weight vector. Equation 4-10: Lasso Regression cost function J(θ) = MSE(θ) + α * Σᵢ |θᵢ| (sum from i=1 to n) The regularization term is α times the sum of the absolute values of the weights. Figure 4-18 (page 138) shows Lasso models, similar to Figure 4-17 for Ridge, but with smaller α values. Key characteristic of Lasso: It tends to completely eliminate the weights of the least important features (i.e., set them to zero). It automatically performs feature selection and outputs a sparse model (a model with few non-zero feature weights). Why does Lasso do this? (Figure 4-19, page 139): This is a bit more advanced, but the intuition comes from looking at the \u0026ldquo;shape\u0026rdquo; of the ℓ₁ penalty vs. the ℓ₂ penalty. The ℓ₁ penalty |θ₁| + |θ₂| has \u0026ldquo;corners\u0026rdquo; along the axes in the parameter space. When minimizing the combined MSE + ℓ₁ penalty, the optimization path often hits these corners, forcing one of the parameters to zero. The ℓ₂ penalty θ₁² + θ₂² is circular. The optimization path approaches the origin smoothly, shrinking weights but not usually making them exactly zero. What it\u0026rsquo;s ultimately trying to achieve: The ℓ₁ penalty encourages sparsity by pushing less important feature weights all the way to zero. Training Lasso Regression: The Lasso cost function is not differentiable at θᵢ = 0. However, Gradient Descent can still work if you use a subgradient vector (Equation 4-11, page 140). This is a technical detail; the main idea is that an iterative approach can still find the minimum. Scikit-Learn: Lasso(alpha=...) or SGDRegressor(penalty=\u0026quot;l1\u0026quot;). When using Lasso with GD, you often need to gradually reduce the learning rate to help it converge without bouncing around the optimum (due to the \u0026ldquo;sharp corners\u0026rdquo; of the ℓ₁ penalty). Elastic Net (page 140):\nA middle ground between Ridge and Lasso. Its regularization term is a simple mix of both Ridge (ℓ₂) and Lasso (ℓ₁) terms. Equation 4-12: Elastic Net cost function J(θ) = MSE(θ) + rα Σᵢ|θᵢ| + ((1-r)/2)α Σᵢθᵢ² r is the mix ratio. If r = 0, Elastic Net is equivalent to Ridge. If r = 1, Elastic Net is equivalent to Lasso. Scikit-Learn: ElasticNet(alpha=..., l1_ratio=...) (where l1_ratio is r). When to use which? (page 140):\nPlain Linear Regression (no regularization) should generally be avoided. Some regularization is almost always better. Ridge is a good default. If you suspect only a few features are useful, prefer Lasso or Elastic Net because they perform feature selection by shrinking useless feature weights to zero. Elastic Net is generally preferred over Lasso because Lasso can behave erratically when the number of features is greater than the number of training instances, or when several features are strongly correlated. Elastic Net is more stable in these cases. Early Stopping (page 141):\nA very different way to regularize iterative learning algorithms like Gradient Descent. The Idea: Stop training as soon as the validation error reaches a minimum. Figure 4-20 (page 141) shows a complex model (high-degree polynomial) being trained with Batch GD: As epochs go by, training error (RMSE) goes down. Validation error also goes down initially, but then starts to go back up. This indicates the model has started to overfit. With early stopping, you just stop training when the validation error is at its minimum. It\u0026rsquo;s a simple and efficient regularization technique. Geoffrey Hinton called it a \u0026ldquo;beautiful free lunch.\u0026rdquo; Implementation (page 142): Loop for epochs. In each epoch, train the model (e.g., SGDRegressor with warm_start=True so it continues training, max_iter=1 so it does one epoch). Evaluate on validation set. If validation error is less than current minimum_val_error, save the model (clone it) and update minimum_val_error and best_epoch. After the loop, the best_model is your regularized model. With SGD/Mini-batch GD, validation curves are noisy. You might stop only after validation error has been above minimum for a while, then roll back to the best model. Fantastic! It\u0026rsquo;s great that the intuition behind L1 and L2 regularization clicked. Let\u0026rsquo;s continue our journey through Chapter 4, moving on to models designed for classification.\n(Page 142-147: Logistic Regression)\nWe\u0026rsquo;ve seen Linear Regression for predicting continuous values. Now, what if we want to predict a class? For example, is an email spam or not spam? This is a binary classification problem.\nThe book points out (as we saw in Chapter 1) that some regression algorithms can be adapted for classification. Logistic Regression (also called Logit Regression) is a prime example.\nWhat it\u0026rsquo;s ultimately trying to achieve: Logistic Regression estimates the probability that an instance belongs to a particular class (typically the \u0026ldquo;positive\u0026rdquo; class, labeled \u0026lsquo;1\u0026rsquo;). If this probability is greater than a certain threshold (usually 50%), the model predicts \u0026lsquo;1\u0026rsquo;; otherwise, it predicts \u0026lsquo;0\u0026rsquo;.\nEstimating Probabilities (Page 143):\nLinear Combination: Just like Linear Regression, it first computes a weighted sum of the input features plus a bias term: t = xᵀθ. (This t is often called the logit). Logistic Function (Sigmoid): Instead of outputting t directly, it passes t through the logistic function (also called the sigmoid function), denoted as σ(t). Equation 4-13: Logistic Regression model estimated probability (vectorized form) p̂ = h_θ(x) = σ(xᵀθ) where p̂ (p-hat) is the estimated probability that the instance x belongs to the positive class. Equation 4-14: Logistic function σ(t) = 1 / (1 + exp(-t)) Figure 4-21 (page 143) shows the characteristic S-shape of the sigmoid function. What the sigmoid is ultimately trying to achieve: It squashes any input value t (which can range from -∞ to +∞) into an output value between 0 and 1. This output can then be interpreted as a probability. If t is large and positive, exp(-t) is close to 0, so σ(t) is close to 1. If t is large and negative, exp(-t) is very large, so σ(t) is close to 0. If t = 0, exp(-t) = 1, so σ(t) = 1/2 = 0.5. Making Predictions (Page 143):\nEquation 4-15: Logistic Regression model prediction ŷ = 0 if p̂ \u0026lt; 0.5 ŷ = 1 if p̂ ≥ 0.5 Since σ(t) ≥ 0.5 when t ≥ 0, the model predicts 1 if xᵀθ (the logit) is positive, and 0 if it\u0026rsquo;s negative. Training and Cost Function (Page 144):\nObjective: We want to set the parameter vector θ so that the model estimates a high probability for positive instances (actual y=1) and a low probability for negative instances (actual y=0). Cost Function for a Single Instance (Equation 4-16): c(θ) = -log(p̂) if y = 1 c(θ) = -log(1 - p̂) if y = 0 What this cost function is ultimately trying to achieve: If y=1 (actual is positive): If model predicts p̂ close to 1 (correct), -log(p̂) is close to 0 (low cost). If model predicts p̂ close to 0 (incorrect), -log(p̂) is very large (high cost). If y=0 (actual is negative): If model predicts p̂ close to 0 (so 1-p̂ is close to 1, correct), -log(1-p̂) is close to 0 (low cost). If model predicts p̂ close to 1 (so 1-p̂ is close to 0, incorrect), -log(1-p̂) is very large (high cost). This cost function penalizes the model heavily when it\u0026rsquo;s confident and wrong. Cost Function for the Whole Training Set (Log Loss - Equation 4-17): J(θ) = - (1/m) * Σᵢ [ y⁽ⁱ⁾log(p̂⁽ⁱ⁾) + (1 - y⁽ⁱ⁾)log(1 - p̂⁽ⁱ⁾) ] This is just the average cost over all training instances. It\u0026rsquo;s a single, clever expression that combines the two cases from Equation 4-16. Good news: This log loss cost function is convex. So, Gradient Descent (or other optimization algorithms) can find the global minimum. Bad news: There\u0026rsquo;s no closed-form solution (like the Normal Equation for Linear Regression) to find the θ that minimizes this cost function. We must use an iterative optimization algorithm like Gradient Descent. Partial Derivatives (Equation 4-18, page 145): ∂J(θ)/∂θⱼ = (1/m) * Σᵢ (σ(θᵀx⁽ⁱ⁾) - y⁽ⁱ⁾) * xⱼ⁽ⁱ⁾ This looks very similar to the partial derivative for Linear Regression\u0026rsquo;s MSE (Equation 4-5)! σ(θᵀx⁽ⁱ⁾) is the predicted probability p̂⁽ⁱ⁾. (p̂⁽ⁱ⁾ - y⁽ⁱ⁾) is the prediction error. This error is multiplied by the feature value xⱼ⁽ⁱ⁾ and averaged. Once you have these gradients, you can use Batch GD, Stochastic GD, or Mini-batch GD to find the optimal θ. Decision Boundaries (Page 145-147): Let\u0026rsquo;s use the Iris dataset to illustrate. We\u0026rsquo;ll try to classify Iris virginica based only on petal width.\nLoad data: X = iris[\u0026quot;data\u0026quot;][:, 3:] (petal width), y = (iris[\u0026quot;target\u0026quot;] == 2).astype(int) (1 if virginica, else 0). Train LogisticRegression(): log_reg = LogisticRegression() log_reg.fit(X, y) Figure 4-23 (page 146): Shows estimated probabilities vs. petal width. The S-shape is clear. For petal widths \u0026gt; ~2cm, probability of being Iris virginica is high. For petal widths \u0026lt; ~1cm, probability is low. The decision boundary (where p̂ = 0.5) is around 1.6 cm. If petal width \u0026gt; 1.6cm, it predicts virginica; otherwise, not virginica. Figure 4-24 (page 147): Shows decision boundary using two features (petal width and petal length). The dashed line is where the model estimates 50% probability – this is the linear decision boundary. Other parallel lines show other probability contours (e.g., 15%, 90%). Regularization: Logistic Regression models in Scikit-Learn use ℓ₂ regularization by default. The hyperparameter is C (inverse of α): higher C means less regularization. (Page 148-150: Softmax Regression)\nLogistic Regression is for binary classification. What if we have more than two classes, and we want a model that handles them directly (not with OvR/OvO strategies)? Enter Softmax Regression (or Multinomial Logistic Regression).\nThe Idea:\nFor a given instance x, compute a score sₖ(x) for each class k. This is done just like Linear Regression: sₖ(x) = xᵀθ⁽ᵏ⁾ (Equation 4-19). Each class k has its own dedicated parameter vector θ⁽ᵏ⁾. These are often stored as rows in a parameter matrix Θ. Estimate the probability p̂ₖ that the instance belongs to class k by applying the softmax function (also called normalized exponential) to the scores. Equation 4-20: Softmax function p̂ₖ = σ(s(x))ₖ = exp(sₖ(x)) / Σⱼ exp(sⱼ(x)) (sum over all classes j=1 to K) What it\u0026rsquo;s ultimately trying to achieve: It takes a vector of arbitrary scores s(x) for all classes, computes the exponential of each score (making them all positive), and then normalizes them by dividing by their sum. The result is a set of probabilities (p̂ₖ) that are all between 0 and 1 and sum up to 1 across all classes. The class with the highest initial score sₖ(x) will get the highest probability p̂ₖ. Prediction (Equation 4-21, page 149): The classifier predicts the class k that has the highest estimated probability p̂ₖ (which is simply the class with the highest score sₖ(x)). ŷ = argmaxₖ p̂ₖ\nSoftmax Regression predicts only one class at a time (mutually exclusive classes). It\u0026rsquo;s multiclass, not multioutput. Training and Cost Function (Cross Entropy - page 149): Objective: Estimate a high probability for the target class and low probabilities for other classes. Cost Function (Cross Entropy - Equation 4-22): J(Θ) = - (1/m) * Σᵢ Σₖ yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) yₖ⁽ⁱ⁾ is the target probability that instance i belongs to class k (usually 1 if it\u0026rsquo;s the target class, 0 otherwise). p̂ₖ⁽ⁱ⁾ is the model\u0026rsquo;s estimated probability that instance i belongs to class k. What it\u0026rsquo;s ultimately trying to achieve: This cost function penalizes the model when it estimates a low probability for the actual target class. It\u0026rsquo;s a common measure for how well a set of estimated class probabilities matches the target classes. The sidebar on \u0026ldquo;Cross Entropy\u0026rdquo; (page 150) gives some information theory background – it measures the average number of bits needed to encode events based on your probability estimates vs. the true probabilities. Lower is better. When there are only two classes (K=2), this cross-entropy cost function is equivalent to the log loss for Logistic Regression. Gradient Vector (Equation 4-23, page 150): ∇_θ⁽ᵏ⁾ J(Θ) = (1/m) * Σᵢ (p̂ₖ⁽ⁱ⁾ - yₖ⁽ⁱ⁾) * x⁽ⁱ⁾ This gives the gradient for the parameter vector θ⁽ᵏ⁾ of a specific class k. Again, very similar form to previous gradient equations! You compute this for every class, then use an optimization algorithm (like GD) to find the parameter matrix Θ that minimizes the cost. Using Softmax Regression in Scikit-Learn (page 150):\nLogisticRegression can perform Softmax Regression by setting: multi_class=\u0026quot;multinomial\u0026quot; solver=\u0026quot;lbfgs\u0026quot; (or another solver that supports multinomial, like \u0026ldquo;sag\u0026rdquo; or \u0026ldquo;newton-cg\u0026rdquo;) It also applies ℓ₂ regularization by default (controlled by C). Example: Classify Iris flowers into all 3 classes using petal length and width. X = iris[\u0026quot;data\u0026quot;][:, (2, 3)] y = iris[\u0026quot;target\u0026quot;] softmax_reg = LogisticRegression(multi_class=\u0026quot;multinomial\u0026quot;, solver=\u0026quot;lbfgs\u0026quot;, C=10) softmax_reg.fit(X, y) To predict: softmax_reg.predict([[5, 2]]) might give array([2]) (Iris virginica). To get probabilities: softmax_reg.predict_proba([[5, 2]]) might give [[6.3e-07, 0.057, 0.942]], meaning 94.2% prob for class 2 (virginica), 5.8% for class 1 (versicolor), and near 0% for class 0 (setosa). Figure 4-25 (page 151) shows the decision boundaries. They are linear between any two classes. And that brings us to the end of the core content of Chapter 4! We\u0026rsquo;ve gone from simple Linear Regression to Polynomial Regression, learned about diagnosing model fit with learning curves, seen how to regularize models to prevent overfitting (Ridge, Lasso, Elastic Net, Early Stopping), and finally explored Logistic and Softmax Regression for classification tasks.\nThe recurring theme for training, especially for models without closed-form solutions, is defining a cost function that captures how \u0026ldquo;bad\u0026rdquo; our model is, and then using an iterative algorithm like Gradient Descent to find the model parameters that minimize this cost. The specific form of the cost function and how predictions are made change from model to model, but the underlying optimization principle is often very similar.\nThis chapter is dense, but the concepts are absolutely key for understanding how models actually learn. How are you feeling about Logistic and Softmax Regression? Any particular part of \u0026ldquo;what they are trying to achieve\u0026rdquo; that needs more light?\nGlossary Difference between L1 (Lasso) and L2 (Ridge) regularization! Let\u0026rsquo;s break down that intuition about the \u0026ldquo;shapes\u0026rdquo; and \u0026ldquo;corners.\u0026rdquo; Imagine we have a model with just two weights, θ₁ and θ₂. We want to find the values of θ₁ and θ₂ that minimize our main cost function (let\u0026rsquo;s say MSE), subject to some penalty on the size of these weights.\nVisualizing the Penalties (Loss Functions for Weights)\nThink of the penalty term as defining a \u0026ldquo;budget\u0026rdquo; or a \u0026ldquo;constraint region\u0026rdquo; for our weights. The optimization process is trying to find the best MSE it can, while staying within or close to this budget defined by the penalty.\nL₂ Penalty (Ridge): θ₁² + θ₂² ≤ C\nThe equation θ₁² + θ₂² = constant describes a circle (or a sphere/hypersphere in higher dimensions) centered at the origin (θ₁=0, θ₂=0). So, the L2 penalty encourages the weights (θ₁, θ₂) to stay within a circular region around the origin. Shape: Smooth and round. No sharp corners. Imagine the contours of this penalty function: they are concentric circles. L₁ Penalty (Lasso): |θ₁| + |θ₂| ≤ C\nThe equation |θ₁| + |θ₂| = constant describes a diamond (or a rotated square in 2D, and a similar shape with \u0026ldquo;pointy\u0026rdquo; corners on the axes in higher dimensions). So, the L1 penalty encourages the weights (θ₁, θ₂) to stay within a diamond-shaped region around the origin. Shape: Has sharp corners that lie on the axes. For our 2D example, the corners are at points like (C, 0), (-C, 0), (0, C), and (0, -C). Imagine the contours of this penalty function: they are concentric diamonds. Visualizing the Optimization Process (Figure 4-19)\nNow, let\u0026rsquo;s consider the main cost function, the MSE. The contours of the MSE (if we ignore the penalty for a moment) are typically elliptical. The center of these ellipses is the point where MSE is minimized without any regularization – let\u0026rsquo;s call this the \u0026ldquo;unconstrained optimum.\u0026rdquo;\nThe regularization process is trying to find a point that: a. Is on the lowest possible MSE contour (meaning good fit to the data). b. Satisfies the \u0026ldquo;budget\u0026rdquo; imposed by the L1 or L2 penalty (meaning weights are small).\nThis can be visualized as finding the point where an MSE ellipse first \u0026ldquo;touches\u0026rdquo; the boundary of the penalty region (the circle for L2, the diamond for L1).\nWith L₂ Penalty (Ridge - bottom-right plot in Figure 4-19):\nImagine an expanding MSE ellipse (as we try to get lower MSE) until it just kisses the circular L2 penalty region. Because the circle is smooth and round, the point where the ellipse touches it can be anywhere on the circle\u0026rsquo;s boundary. It\u0026rsquo;s very unlikely that this touch point will be exactly on an axis (e.g., where θ₁ = 0 or θ₂ = 0). Result: The L2 penalty shrinks both θ₁ and θ₂ towards zero, but it\u0026rsquo;s rare for either of them to become exactly zero. The optimization path (white dots) goes smoothly towards a point where both weights are small but likely non-zero. With L₁ Penalty (Lasso - top-right plot in Figure 4-19):\nImagine an expanding MSE ellipse until it just kisses the diamond-shaped L1 penalty region. Because the diamond has sharp corners on the axes, it\u0026rsquo;s much more likely that the ellipse will first touch the diamond at one of these corners. If it touches at a corner on, say, the θ₁-axis, then at that point, θ₂ will be exactly zero. Result: The L1 penalty also shrinks weights towards zero, but because of its shape, it often forces some weights to become exactly zero. This is the \u0026ldquo;feature selection\u0026rdquo; property. The optimization path (white dots) is shown hitting the θ₂=0 axis and then \u0026ldquo;rolling down the gutter\u0026rdquo; along that axis. Why the \u0026ldquo;Corners\u0026rdquo; Matter for Hitting Zero:\nThink about the gradient (the direction of steepest descent) of the penalty term itself:\nL₂ Penalty: The gradient of θ₁² + θ₂² always points directly towards the origin. It smoothly pulls all weights smaller. L₁ Penalty: The gradient of |θ₁| + |θ₂| is a bit trickier. If θ₁ \u0026gt; 0, the derivative w.r.t. θ₁ is 1. If θ₁ \u0026lt; 0, the derivative w.r.t. θ₁ is -1. If θ₁ = 0, it\u0026rsquo;s undefined (this is the \u0026ldquo;corner\u0026rdquo;). This means the L1 penalty applies a constant \u0026ldquo;force\u0026rdquo; (either +1 or -1) pulling a weight towards zero, regardless of how small the weight already is (as long as it\u0026rsquo;s not zero). Once a weight hits zero, this \u0026ldquo;force\u0026rdquo; can effectively keep it there if the pull from the MSE term isn\u0026rsquo;t strong enough to move it away. In contrast, for L2, the \u0026ldquo;force\u0026rdquo; 2θ₁ pulling θ₁ towards zero gets weaker as θ₁ gets smaller. So, it might get very close to zero but not quite hit it unless the MSE term also strongly favors zero. Intuitive Summary:\nL₂ (Ridge) is like saying: \u0026ldquo;Try to make your weights small, and distribute the \u0026lsquo;smallness\u0026rsquo; somewhat evenly. Keep them inside this round ball.\u0026rdquo; It pulls weights towards zero but doesn\u0026rsquo;t aggressively zero them out. L₁ (Lasso) is like saying: \u0026ldquo;Try to make your weights small, and if possible, make some of them exactly zero. Keep them inside this pointy diamond. It\u0026rsquo;s okay to focus your \u0026lsquo;budget\u0026rsquo; on just a few important weights and eliminate the others.\u0026rdquo; The \u0026ldquo;corners\u0026rdquo; of the L1 penalty region are key because they represent solutions where one or more weights are exactly zero. The optimization process, in trying to balance minimizing MSE with staying within the L1 budget, often finds its optimal solution precisely at one of these corners.\nDoes that help clarify why the shape of the penalty (round vs. pointy with corners on axes) leads to these different behaviors in terms of zeroing out weights?\nOkay, absolutely! Let\u0026rsquo;s revisit Softmax Regression (pages 148-150) and really focus on what it\u0026rsquo;s ultimately trying to achieve, especially in contrast to just using multiple binary Logistic Regressions.\nYou\u0026rsquo;re spot on: Logistic Regression is great for \u0026ldquo;is it A or not A?\u0026rdquo; (binary). But what if you have \u0026ldquo;is it A, B, C, or D?\u0026rdquo; (multiclass).\nThe Problem with Just Using Multiple Binary Classifiers (OvR/OvO):\nWe saw in Chapter 3 that you can use binary classifiers for multiclass problems:\nOne-vs-Rest (OvR): Train a separate binary Logistic Regression for each class.\nClassifier 1: \u0026ldquo;Is it class A (vs. B, C, D)?\u0026rdquo; Classifier 2: \u0026ldquo;Is it class B (vs. A, C, D)?\u0026rdquo; \u0026hellip;and so on. To make a prediction, you run your input through all these binary classifiers and pick the class whose classifier gives the highest \u0026ldquo;confidence\u0026rdquo; score (or probability, if the binary classifier outputs that). One-vs-One (OvO): Train a binary classifier for every pair of classes (A vs B, A vs C, A vs D, B vs C, etc.). Pick the class that wins the most \u0026ldquo;duels.\u0026rdquo;\nLimitations/Quirks of OvR/OvO for Probabilities:\nWhile these strategies work for getting a class label, there\u0026rsquo;s a slight awkwardness if you want well-calibrated probabilities for each class that naturally sum to 1.\nWith OvR, each binary Logistic Regression outputs a probability for its class versus all others. For example, P(A | not A). These probabilities from different classifiers aren\u0026rsquo;t inherently guaranteed to sum to 1 when you look across all classes for a single instance. You might get P(A)=0.7, P(B)=0.4, P(C)=0.1. These don\u0026rsquo;t sum to 1, and it\u0026rsquo;s not immediately clear how to turn them into a proper probability distribution over A, B, and C. You usually just pick the class with the highest score. Softmax Regression: The \u0026ldquo;Direct\u0026rdquo; Multiclass Probabilistic Approach Softmax Regression is designed from the ground up to handle multiple classes directly and produce a consistent set of probabilities that sum to 1 across all classes.\nHere\u0026rsquo;s the core idea and \u0026ldquo;what it\u0026rsquo;s trying to achieve\u0026rdquo;:\nGoal: For any given input instance (e.g., an image of a digit), we want to output a probability for each possible class (e.g., P(digit is 0), P(digit is 1), \u0026hellip;, P(digit is 9)). Critically, these probabilities should all add up to 100%.\nStep 1: Calculate a \u0026ldquo;Score\u0026rdquo; for Each Class (Equation 4-19)\nJust like Linear Regression or Logistic Regression, for each class k, Softmax Regression calculates a linear score: sₖ(x) = xᵀθ⁽ᵏ⁾ x is the input feature vector. θ⁽ᵏ⁾ (theta-k) is a separate vector of weights specifically for class k. So, if you have 10 classes (digits 0-9), you will have 10 different θ vectors. What these scores sₖ(x) are ultimately trying to achieve: They are like raw \u0026ldquo;evidence\u0026rdquo; or \u0026ldquo;suitability scores\u0026rdquo; for each class, given the input x. A higher score sₖ(x) suggests that class k is a more likely candidate for this input. These scores can be any real number (positive, negative, large, small). Step 2: Convert Scores into Probabilities (The Softmax Function - Equation 4-20)\nThe raw scores sₖ(x) are not probabilities yet (they don\u0026rsquo;t sum to 1, and they can be negative). We need a way to transform them into a valid probability distribution. This is where the softmax function (also called \u0026ldquo;normalized exponential\u0026rdquo;) comes in. For each class k, the probability p̂ₖ is calculated as: p̂ₖ = exp(sₖ(x)) / Σⱼ exp(sⱼ(x)) (where the sum in the denominator is over all possible classes j) What the softmax function is ultimately trying to achieve: exp(sₖ(x)) (Exponential): First, it takes the exponential of each score. This has two effects: It makes all scores positive (since e to any power is positive). It tends to exaggerate differences: if score A is slightly higher than score B, exp(A) will be significantly higher than exp(B). The \u0026ldquo;softmax\u0026rdquo; is \u0026ldquo;soft\u0026rdquo; in that it doesn\u0026rsquo;t just pick the max score and give it 100% probability, but it does give more weight to higher scores. Σⱼ exp(sⱼ(x)) (Sum of Exponentials): It then sums up these positive, exponentiated scores for all classes. This sum acts as a normalization constant. Division: Dividing each exp(sₖ(x)) by this total sum ensures two things: Each p̂ₖ will be between 0 and 1. All the p̂ₖ values will sum up to 1. So, we get a proper probability distribution across all classes. The class that had the highest initial score sₖ(x) will end up with the largest probability p̂ₖ. Analogy for Softmax:\nImagine you have several candidates for a job (the classes).\nYou give each candidate a raw \u0026ldquo;suitability score\u0026rdquo; (the sₖ(x)). Some might be high, some low, some even negative if they seem really unsuitable. To decide how to allocate a \u0026ldquo;probability of being hired\u0026rdquo; that sums to 100% across all candidates: You first want to ensure everyone\u0026rsquo;s considered \u0026ldquo;positively\u0026rdquo; and amplify the scores of strong candidates: you \u0026ldquo;exponentiate\u0026rdquo; their scores. A candidate with a score of 3 becomes e³ ≈ 20, while a candidate with a score of 1 becomes e¹ ≈ 2.7. The difference is magnified. Then, you add up all these amplified, positive scores to get a \u0026ldquo;total pool of amplified suitability.\u0026rdquo; Finally, each candidate\u0026rsquo;s share of this total pool becomes their probability of being hired. Why is this better than just running multiple OvR Logistic Regressions for probabilities?\nDirect Probabilistic Interpretation: Softmax directly outputs a set of probabilities that are inherently linked and sum to 1. It\u0026rsquo;s designed for this purpose. With OvR Logistic Regression, you\u0026rsquo;d have to do some extra (potentially ad-hoc) normalization step if you wanted the \u0026ldquo;probabilities\u0026rdquo; from different binary classifiers to sum to 1 for a given instance. Shared Information During Training (via the cost function): When Softmax Regression is trained (using the cross-entropy cost function, which we\u0026rsquo;ll get to), the updates to the weights θ⁽ᵏ⁾ for one class are influenced by the scores and target probabilities of all other classes because of that denominator in the softmax function. This allows the model to learn the relationships and distinctions between all classes simultaneously in a more coupled way. With independent OvR classifiers, each classifier only learns to distinguish its class from \u0026ldquo;everything else\u0026rdquo; without explicitly considering the fine-grained distinctions between those \u0026ldquo;everything else\u0026rdquo; classes. Making a Prediction and Training:\nPrediction (Equation 4-21): Once you have the probabilities p̂ₖ for all classes, you simply predict the class k that has the highest probability. This is equivalent to picking the class with the highest initial score sₖ(x). Training (Cross-Entropy Cost Function - Equation 4-22): The goal is to find the parameter matrix Θ (which contains all the θ⁽ᵏ⁾ vectors) such that for a given training instance, the predicted probability p̂ₖ is high for the correct class k and low for all other classes. The cross-entropy cost function is used for this. J(Θ) = - (1/m) * Σᵢ Σₖ yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) yₖ⁽ⁱ⁾ is 1 if class k is the true class for instance i, and 0 otherwise. What it\u0026rsquo;s ultimately trying to achieve: It heavily penalizes the model if log(p̂ₖ⁽ⁱ⁾) is a large negative number for the true class k (which happens if p̂ₖ⁽ⁱ⁾ is close to 0 for the true class). It wants to maximize the log-probability of the correct class. This cost function is convex, so Gradient Descent can find the optimal Θ. The gradients (Equation 4-23) are computed, and Θ is updated iteratively. In essence, Softmax Regression is a generalization of Logistic Regression to handle multiple classes by:\nCalculating a linear score for each class. Using the softmax function to convert these scores into a proper probability distribution over all classes. Being trained with a cost function (cross-entropy) that encourages high probabilities for correct classes. It provides a more principled and direct way to get multiclass probabilities compared to trying to combine multiple independent binary logistic regressions.\nFocus on the Cross-Entropy cost function Let\u0026rsquo;s break it down slowly, focusing on what it\u0026rsquo;s ultimately trying to achieve.\nRecall the equation: J(Θ) = - (1/m) * Σᵢ Σₖ yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾)\nWhere:\nJ(Θ): The total cost for our model parameters Θ. We want to minimize this. m: The number of training instances. (1/m) means we\u0026rsquo;re averaging the cost over all instances. Σᵢ: Sum over all training instances i (from 1 to m). Σₖ: Sum over all possible classes k (from 1 to K). yₖ⁽ⁱ⁾: This is the true target probability that instance i belongs to class k. For most classification problems (like digit recognition), this is a \u0026ldquo;one-hot encoded\u0026rdquo; value. If instance i is truly a \u0026lsquo;digit 3\u0026rsquo;, then: y₃⁽ⁱ⁾ = 1 (the probability of being class \u0026lsquo;3\u0026rsquo; is 100%) y₀⁽ⁱ⁾ = 0, y₁⁽ⁱ⁾ = 0, y₂⁽ⁱ⁾ = 0, y₄⁽ⁱ⁾ = 0, \u0026hellip;, y₉⁽ⁱ⁾ = 0 (the probability of being any other class is 0%). p̂ₖ⁽ⁱ⁾: This is the model\u0026rsquo;s predicted probability that instance i belongs to class k (this comes from the softmax function). log(p̂ₖ⁽ⁱ⁾): The natural logarithm of the model\u0026rsquo;s predicted probability. Understanding the Core Term: yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾)\nLet\u0026rsquo;s focus on a single instance i and a single class k.\nThe term yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) is the heart of it. Since yₖ⁽ⁱ⁾ is either 0 or 1 (for the one-hot encoded true label):\nCase 1: Class k is NOT the true class for instance i.\nThen yₖ⁽ⁱ⁾ = 0. So, yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) = 0 * log(p̂ₖ⁽ⁱ⁾) = 0. This means: For all the classes that are not the true class, this term contributes nothing to the sum Σₖ. This makes sense – we don\u0026rsquo;t directly care about the exact log-probability the model assigns to the incorrect classes, as long as it assigns a high probability to the correct class. Case 2: Class k IS the true class for instance i.\nThen yₖ⁽ⁱ⁾ = 1. So, yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) = 1 * log(p̂ₖ⁽ⁱ⁾) = log(p̂ₖ⁽ⁱ⁾). This means: For the one true class, this term contributes log(p̂ₖ⁽ⁱ⁾) to the sum Σₖ. So, for a single instance i, the inner sum Σₖ yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) simplifies to just log(p̂_true_class⁽ⁱ⁾). It\u0026rsquo;s the logarithm of the probability that the model assigned to the actual correct class for that instance.\nWhy log? And why the negative sign in J(Θ)?\nNow let\u0026rsquo;s consider the log and the overall negative sign in J(Θ) = - (1/m) * Σᵢ log(p̂_true_class⁽ⁱ⁾).\nProbabilities p̂ are between 0 and 1.\nThe logarithm of a number between 0 and 1 is always negative (or zero if p̂=1).\nlog(1) = 0 log(0.5) ≈ -0.69 log(0.1) ≈ -2.3 As p̂ approaches 0, log(p̂) approaches -∞. What the cost function is trying to achieve:\nWe want p̂_true_class⁽ⁱ⁾ to be as close to 1 as possible. This means our model is very confident and correct. If p̂_true_class⁽ⁱ⁾ = 1 (perfect prediction for this instance), then log(p̂_true_class⁽ⁱ⁾) = log(1) = 0. The negative sign makes it -0 = 0. This is the minimum possible cost for this instance. Good! We want to heavily penalize the model if p̂_true_class⁽ⁱ⁾ is close to 0. This means our model is very confident but wrong about the true class. If p̂_true_class⁽ⁱ⁾ is very small (e.g., 0.0001), then log(p̂_true_class⁽ⁱ⁾) is a large negative number (e.g., log(0.0001) ≈ -9.2). The negative sign in J(Θ) makes this - (large negative number) = large positive number. This is a very high cost. Good! This is what we want the cost function to do – penalize bad, confident predictions for the true class. So, minimizing J(Θ) = - Σ log(p̂_true_class) is equivalent to maximizing Σ log(p̂_true_class). And since log is a monotonically increasing function, maximizing the sum of log-probabilities is equivalent to maximizing the product of the probabilities (this is related to Maximum Likelihood Estimation, a very common principle in statistics and machine learning).\nIn simpler terms, the cross-entropy cost function is ultimately trying to:\nMake the model assign the highest possible probability to the correct class for each training example. It does this by looking at the logarithm of the predicted probability for the true class. A perfect score (probability 1 for the true class) results in a log-probability of 0, contributing zero to the (negative) cost. A very poor score (probability near 0 for the true class) results in a very large negative log-probability, which, when negated by the cost function, becomes a very large positive cost. The cost function therefore \u0026ldquo;rewards\u0026rdquo; the model for being confident and correct, and heavily \u0026ldquo;punishes\u0026rdquo; it for being confident and incorrect (or even just very uncertain about the correct class). Connection to Information Theory (as mentioned in the sidebar):\nCross-entropy is a concept from information theory.\nImagine you have a true distribution of events y (e.g., true digit classes). You have a model that produces a predicted distribution p̂. Cross-entropy H(y, p̂) measures the average number of bits you would need to encode an event drawn from the true distribution y, if you used an encoding scheme optimized for your predicted distribution p̂. If your predicted distribution p̂ perfectly matches the true distribution y, then the cross-entropy is minimized and is equal to the entropy of y itself (the inherent minimum bits needed to describe y). If p̂ is different from y, the cross-entropy will be higher. The difference H(y, p̂) - H(y) is called the Kullback-Leibler (KL) divergence, which measures how much your predicted distribution diverges from the true one. So, when we minimize the cross-entropy cost function, we are essentially trying to make our model\u0026rsquo;s predicted probability distribution p̂ as close as possible to the true underlying distribution y of the class labels.\nWhy is it better than, say, trying to minimize the squared error between p̂ and y for classification?\nRange of Output: p̂ from softmax is a probability (0 to 1). y is usually 0 or 1. Squared error (p̂ - y)² could work, but log loss (cross-entropy) has properties that often make optimization more stable and faster, especially for probabilities. Penalizing Confident Wrong Predictions: The log term in cross-entropy means that if the model predicts a very low probability (near 0) for the true class, the cost goes towards infinity. This is a very strong penalty. Squared error doesn\u0026rsquo;t penalize confident wrong predictions as harshly. For y=1, if p̂=0.01, squared error is (0.01-1)² ≈ 0.98. If p̂=0.0001, squared error is (0.0001-1)² ≈ 0.9998. The increase isn\u0026rsquo;t as dramatic as log(0.01) vs log(0.0001). Mathematical Properties for Gradient Descent: The gradients derived from the cross-entropy cost function for models like Softmax Regression (and logistic regression) have a nice, simple form (as seen in Equation 4-23), which is computationally convenient. This cost function is the workhorse for most classification problems where the model outputs probabilities. It effectively pushes the model to be accurate and appropriately confident in its predictions for the correct classes.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-4/","tags":null,"title":"Chapter 4: Training Models"},{"categories":null,"content":"Alright class, let\u0026rsquo;s gear up for Chapter 5: Support Vector Machines (SVMs). This is a big one! SVMs are incredibly powerful and versatile models. You\u0026rsquo;ll find them used for linear or nonlinear classification, regression, and even outlier detection. They are a cornerstone of classical machine learning, and definitely a tool everyone interested in ML should have in their arsenal.\nThe book mentions they\u0026rsquo;re particularly good for complex small- or medium-sized datasets. This chapter will walk us through their core concepts, how to use them, and, importantly for our \u0026ldquo;what it\u0026rsquo;s ultimately trying to achieve\u0026rdquo; philosophy, how they actually work.\n(Page 153: Linear SVM Classification - The Big Idea) The fundamental idea behind SVMs is best explained with pictures, and Figure 5-1 (page 154) does a great job. Imagine you have two classes of data points that are linearly separable – meaning you can draw a straight line to separate them perfectly.\nThe plot on the left of Figure 5-1 shows three possible straight lines (decision boundaries) that could separate the Iris data.\nOne (dashed) is terrible; it doesn\u0026rsquo;t even separate the classes. The other two separate the classes perfectly on the training data, but their decision boundaries are very close to some of the training instances. This means they might not generalize well to new, unseen instances. A slight variation in a new point could cause it to be misclassified. The plot on the right of Figure 5-1 shows the decision boundary of an SVM classifier.\nWhat is it ultimately trying to achieve? An SVM doesn\u0026rsquo;t just find any line that separates the classes. It tries to find the line that has the largest possible margin between itself and the closest instances from each class. Think of it as fitting the widest possible street between the two classes. The decision boundary is the median line of this street, and the edges of the street are defined by the closest points. This is called large margin classification. The intuition is that a wider margin leads to better generalization because the decision boundary is less sensitive to the exact position of individual training instances and has more \u0026ldquo;room for error\u0026rdquo; with new data. Support Vectors (Page 154):\nNotice that the position of this \u0026ldquo;widest street\u0026rdquo; is entirely determined by the instances located right on the edge of the street. These instances are called the support vectors (they are circled in Figure 5-1). What they are ultimately trying to achieve: They are the critical data points that \u0026ldquo;support\u0026rdquo; or define the decision boundary and the margin. If you move a support vector, the decision boundary will likely change. If you add more training instances that are \u0026ldquo;off the street\u0026rdquo; (far away from the margin), they won\u0026rsquo;t affect the decision boundary at all! The SVM is only sensitive to the points closest to the boundary. Sensitivity to Feature Scales (Figure 5-2, page 154): The scorpion icon highlights a crucial point: SVMs are sensitive to feature scales. If one feature has a much larger range of values than another (e.g., vertical scale much larger than horizontal in the left plot of Figure 5-2), the \u0026ldquo;widest street\u0026rdquo; will tend to be oriented to accommodate the larger scale. The street might look wide in the scaled units, but it might be very narrow along the axis with smaller-scaled features. Solution: Always scale your features (e.g., using Scikit-Learn\u0026rsquo;s StandardScaler) before training an SVM. The right plot in Figure 5-2 shows a much better, more balanced decision boundary after scaling. What scaling is ultimately trying to achieve: It ensures that all features contribute more equally to the distance calculations involved in finding the largest margin, preventing features with larger numerical values from dominating. (Page 154-155: Soft Margin Classification) The \u0026ldquo;widest street\u0026rdquo; idea so far assumed hard margin classification:\nAll instances must be strictly off the street and on the correct side. This only works if the data is perfectly linearly separable. It\u0026rsquo;s very sensitive to outliers (Figure 5-3, page 155). A single outlier can make it impossible to find a hard margin, or it can drastically change the decision boundary, leading to poor generalization. To avoid these issues, we use a more flexible approach: Soft Margin Classification.\nWhat it\u0026rsquo;s ultimately trying to achieve: Find a good balance between:\nKeeping the \u0026ldquo;street\u0026rdquo; (margin) as wide as possible. Limiting margin violations – instances that end up inside the street or even on the wrong side of the decision boundary. This allows the model to handle data that isn\u0026rsquo;t perfectly linearly separable and makes it less sensitive to outliers.\nThe C Hyperparameter (Figure 5-4, page 155): When using SVMs (e.g., in Scikit-Learn), the hyperparameter C controls this trade-off. Low C value (e.g., C=1 in the left plot of Figure 5-4): Wider street (larger margin). More margin violations are tolerated. This generally leads to a model that is more regularized and might generalize better, even if it makes more mistakes on the training data margin. High C value (e.g., C=100 in the right plot of Figure 5-4): Narrower street (smaller margin). Fewer margin violations are tolerated. The model tries harder to classify all training instances correctly. This can lead to overfitting if C is too high, as the model might be too sensitive to individual data points, including noise or outliers. What C is ultimately trying to achieve: It\u0026rsquo;s a regularization parameter. A smaller C means more regularization (larger margin, more tolerance for violations). A larger C means less regularization (smaller margin, less tolerance for violations, tries to fit training data more perfectly). If your SVM model is overfitting, try reducing C. If it\u0026rsquo;s underfitting, try increasing C (but be mindful of also allowing the margin to shrink too much). Scikit-Learn Implementation (Page 155-156): The code shows using LinearSVC (Linear Support Vector Classifier) for the Iris dataset: from sklearn.svm import LinearSVC from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline svm_clf = Pipeline([ (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;linear_svc\u0026quot;, LinearSVC(C=1, loss=\u0026quot;hinge\u0026quot;)) ]) svm_clf.fit(X, y)\nNote the use of StandardScaler in a pipeline – good practice! loss=\u0026quot;hinge\u0026quot;: The hinge loss is the cost function typically associated with linear SVMs (more on this in the \u0026ldquo;Under the Hood\u0026rdquo; section). The bird icon (page 156) mentions that unlike Logistic Regression, SVM classifiers generally do not output probabilities directly. They output a decision score, and the sign of the score determines the class. Alternatives for Linear SVMs: SVC(kernel=\u0026quot;linear\u0026quot;, C=1): Uses the SVC class with a linear kernel. It\u0026rsquo;s generally slower than LinearSVC but can be useful if you later want to try other kernels. SGDClassifier(loss=\u0026quot;hinge\u0026quot;, alpha=1/(m*C)): Uses Stochastic Gradient Descent to train a linear SVM. Slower to converge than LinearSVC but good for online learning or very large datasets that don\u0026rsquo;t fit in memory (out-of-core). The scorpion icon (page 156) gives important tips for LinearSVC: It regularizes the bias term, so centering data (done by StandardScaler) is important. Ensure loss=\u0026quot;hinge\u0026quot; is set (not always the default). For better performance, set dual=False unless features \u0026gt; training instances (duality is an advanced optimization concept we\u0026rsquo;ll touch on). (Page 157-161: Nonlinear SVM Classification) Linear SVMs are great, but many datasets aren\u0026rsquo;t linearly separable. What then?\nAdding Polynomial Features (Page 157):\nAs we saw in Chapter 4, you can add polynomial features to your data. This can transform a nonlinearly separable dataset into a linearly separable one in a higher-dimensional space. Figure 5-5 illustrates this: a 1D dataset that\u0026rsquo;s not linearly separable becomes linearly separable in 2D if you add x₂ = x₁² as a new feature. Implementation (Moons dataset): from sklearn.datasets import make_moons X, y = make_moons(n_samples=100, noise=0.15) polynomial_svm_clf = Pipeline([ (\u0026quot;poly_features\u0026quot;, PolynomialFeatures(degree=3)), (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;svm_clf\u0026quot;, LinearSVC(C=10, loss=\u0026quot;hinge\u0026quot;)) ]) polynomial_svm_clf.fit(X, y) Figure 5-6 (page 158) shows the resulting decision boundary on the moons dataset. It\u0026rsquo;s curved and does a decent job! The Kernel Trick (Polynomial Kernel - Page 158):\nAdding polynomial features works, but at high degrees, it creates a huge number of features (combinatorial explosion!), making the model very slow. Enter the Kernel Trick – one of the \u0026ldquo;magic\u0026rdquo; ideas in SVMs! What it\u0026rsquo;s ultimately trying to achieve: It allows you to get the same result as if you had added many polynomial features (even very high-degree ones) without actually creating or adding those features explicitly. This avoids the computational cost and memory overhead of explicitly transforming the data into a very high-dimensional space. How it works (conceptually): Instead of transforming the data points and then taking dot products in the high-dimensional space, the kernel is a function that can compute what the dot product would have been in that high-dimensional space, using only the original low-dimensional data points. (We\u0026rsquo;ll see more math in \u0026ldquo;Under the Hood\u0026rdquo;). Using Polynomial Kernel in Scikit-Learn: from sklearn.svm import SVC poly_kernel_svm_clf = Pipeline([ (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;svm_clf\u0026quot;, SVC(kernel=\u0026quot;poly\u0026quot;, degree=3, coef0=1, C=5)) ]) poly_kernel_svm_clf.fit(X, y) kernel=\u0026quot;poly\u0026quot;: Tells SVC to use the polynomial kernel trick. degree=3: Simulates a 3rd-degree polynomial expansion. coef0=1: (Gamma_zero) Controls how much the model is influenced by high-degree vs. low-degree polynomials in the kernel. C=5: Regularization parameter. Figure 5-7 (page 159) shows the results for degree=3 (left) and degree=10 (right). Higher degree can lead to more complex boundaries (and potential overfitting). The scorpion icon suggests using grid search to find good hyperparameter values. Similarity Features (Gaussian RBF Kernel - Page 159):\nAnother way to handle nonlinear problems is to add features based on similarity to certain landmarks. Imagine you pick a few landmark points in your feature space. For each data instance, you can calculate how similar it is to each landmark. These similarity scores become new features. Gaussian Radial Basis Function (RBF) as a similarity function (Equation 5-1): φ_γ(x, ℓ) = exp(-γ ||x - ℓ||²) x: An instance. ℓ: A landmark. ||x - ℓ||²: Squared Euclidean distance between the instance and the landmark. γ (gamma): A hyperparameter. What this function is ultimately trying to achieve: It outputs a value between 0 (if x is very far from landmark ℓ) and 1 (if x is at the landmark ℓ). It\u0026rsquo;s a bell-shaped curve. A higher γ makes the bell narrower (influence of the landmark drops off more quickly). Figure 5-8 shows a 1D dataset. Two landmarks are added. Each original instance x₁ is transformed into two new features: its RBF similarity to landmark 1, and its RBF similarity to landmark 2. The transformed 2D dataset (right plot) becomes linearly separable! Choosing landmarks: A simple approach is to create a landmark at the location of every single instance in the dataset. This transforms an m x n dataset into an m x m dataset (if original features are dropped). This can make a dataset linearly separable, but it\u0026rsquo;s computationally expensive if m is large. Gaussian RBF Kernel (Page 160):\nAgain, the kernel trick comes to the rescue! The Gaussian RBF kernel allows SVMs to get the same effect as adding many RBF similarity features, without actually computing them. Using Gaussian RBF Kernel in Scikit-Learn: rbf_kernel_svm_clf = Pipeline([ (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;svm_clf\u0026quot;, SVC(kernel=\u0026quot;rbf\u0026quot;, gamma=5, C=0.001)) ]) rbf_kernel_svm_clf.fit(X, y) kernel=\u0026quot;rbf\u0026quot;: Use the Gaussian RBF kernel. gamma (γ): Controls the width of the RBF \u0026ldquo;bell.\u0026rdquo; Increasing gamma: Makes the bell narrower. Each instance\u0026rsquo;s range of influence is smaller. Decision boundary becomes more irregular, wiggling around individual instances (can lead to overfitting). (See Figure 5-9, page 161, comparing gamma=0.1 to gamma=5). Decreasing gamma: Makes the bell wider. Instances have a larger range of influence. Decision boundary becomes smoother (can lead to underfitting if too smooth). So, gamma acts like a regularization hyperparameter: if overfitting, reduce gamma; if underfitting, increase gamma. C: Regularization parameter, as before. Lower C = more regularization. Figure 5-9 shows how different gamma and C values affect the decision boundary for the RBF kernel. Which kernel to use? (Scorpion icon, page 161): Always try linear kernel first (LinearSVC is much faster than SVC(kernel=\u0026quot;linear\u0026quot;)). Especially if training set is large or has many features. If training set isn\u0026rsquo;t too large, try Gaussian RBF kernel next. It works well in most cases. If you have time/compute power, experiment with other kernels (e.g., polynomial, or specialized kernels like string kernels for text) using cross-validation and grid search. (Page 162: Computational Complexity) LinearSVC: Based on liblinear. Does not support the kernel trick. Scales almost linearly with number of instances (m) and features (n) – roughly O(m × n). Good for large datasets if a linear model is sufficient. SVC: Based on libsvm. Does support the kernel trick. Training time complexity: Between O(m² × n) and O(m³ × n). Gets dreadfully slow when m (number of instances) is large (e.g., hundreds of thousands). Perfect for complex small or medium-sized datasets. Scales well with number of features, especially sparse features. Table 5-1 summarizes this. (Page 162-163: SVM Regression) SVMs can also do regression!\nThe Goal (Reversed Objective): For classification: Fit the widest street between classes, limiting margin violations. For SVM Regression (SVR): Fit as many instances as possible on the street, while limiting margin violations (instances off the street). ϵ (epsilon) hyperparameter: Controls the width of the street. (Figure 5-10, page 163). Larger ϵ: Wider street. More instances can fit inside the margin without penalty. Smaller ϵ: Narrower street. ϵ-insensitive: Adding more training instances within the margin (inside the street) does not affect the model\u0026rsquo;s predictions. Scikit-Learn Classes: LinearSVR: For linear SVM regression. from sklearn.svm import LinearSVR svm_reg = LinearSVR(epsilon=1.5) (Data should be scaled and centered first). SVR: For nonlinear SVM regression using kernels. from sklearn.svm import SVR svm_poly_reg = SVR(kernel=\u0026quot;poly\u0026quot;, degree=2, C=100, epsilon=0.1) Figure 5-11 (page 164) shows SVR with a polynomial kernel. Large C: Little regularization. Model tries to fit data points closely. Small C: More regularization. Smoother fit. LinearSVR scales linearly with training set size. SVR gets slow as training set size grows (like SVC). SVMs can also be used for outlier detection. Under the hood Let\u0026rsquo;s start with the big picture goals for a Linear SVM:\nGoal 1: Separate the Classes. (Obvious) We want a line (or plane/hyperplane) that divides the data.\nGoal 2: Make this Separation as \u0026ldquo;Safe\u0026rdquo; or \u0026ldquo;Robust\u0026rdquo; as Possible (Large Margin). This is the core SVM idea. We don\u0026rsquo;t just want any separating line; we want the one that stays farthest away from the closest points of both classes. This \u0026ldquo;widest street\u0026rdquo; (the margin) makes the classifier less sensitive to small variations in data and hopefully better at classifying new, unseen points.\nHow do we achieve Goal 2 mathematically? (This is where w and b come in)\nThe Decision Boundary: Our line is defined by wᵀx + b = 0.\nw (weights vector): Determines the orientation or slope of the line. b (bias): Shifts the line up or down without changing its orientation. The \u0026ldquo;Street\u0026rdquo;: The edges of our street (the margins) are defined by wᵀx + b = 1 and wᵀx + b = -1.\nAll positive class points should ideally be on or \u0026ldquo;above\u0026rdquo; wᵀx + b = 1. All negative class points should ideally be on or \u0026ldquo;below\u0026rdquo; wᵀx + b = -1. Width of the Street: It turns out mathematically that the width of this street is 2 / ||w|| (where ||w|| is the magnitude or length of the vector w).\nKey Insight: To make the street (margin) WIDE, we need ||w|| to be SMALL. What are we trying to achieve by minimizing ||w||? We are trying to maximize the margin. For mathematical convenience (easier to take derivatives), instead of minimizing ||w||, we minimize (1/2)||w||² (which is (1/2)wᵀw). Minimizing one also minimizes the other. So, the Training Objective (Hard Margin - Equation 5-3) becomes:\nObjective: Minimize (1/2)wᵀw (make the margin large) Subject to (Constraints): t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 for all training instances i. t⁽ⁱ⁾ is +1 for positive class, -1 for negative. This constraint says: \u0026ldquo;Every training point must be on the correct side of its respective margin boundary, or right on it.\u0026rdquo; What this constrained optimization is ultimately trying to achieve: Find the line orientation (w) and position (b) that gives the widest possible street while ensuring all training points are correctly classified and stay out of (or on the edge of) the street.\nWhat if the data isn\u0026rsquo;t perfectly separable (Soft Margin - Equation 5-4)?\nReal data is messy. Outliers exist. We might not be able to find a street where all points are perfectly on the correct side.\nSlack Variables ζ⁽ⁱ⁾ (zeta): We introduce a \u0026ldquo;fudge factor\u0026rdquo; or \u0026ldquo;slack\u0026rdquo; ζ⁽ⁱ⁾ ≥ 0 for each point i. If ζ⁽ⁱ⁾ = 0, point i respects the margin. If ζ⁽ⁱ⁾ \u0026gt; 0, point i violates the margin (it\u0026rsquo;s inside the street or on the wrong side). The value of ζ⁽ⁱ⁾ tells us how much it violates. New Objective (Soft Margin): Minimize (1/2)wᵀw + C Σᵢ ζ⁽ⁱ⁾ Subject to: t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 - ζ⁽ⁱ⁾ and ζ⁽ⁱ⁾ ≥ 0. What this soft margin optimization is ultimately trying to achieve: Still try to make (1/2)wᵀw small (maximize margin). But also try to make Σᵢ ζ⁽ⁱ⁾ small (minimize the total amount of margin violations). The hyperparameter C is the trade-off. Large C: Penalizes violations heavily. Model will try very hard to get points right, even if it means a narrower margin. (Closer to hard margin). Small C: Tolerates more violations in favor of a wider margin. (More regularization). The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. This problem (finding w, b, and ζs) is a Quadratic Programming (QP) problem. We can give it to a standard QP solver.\nWhy Introduce the \u0026ldquo;Dual Problem\u0026rdquo; (Equation 5-6)? This is often the confusing part.\nSolving the QP problem directly (called the \u0026ldquo;primal problem\u0026rdquo;) is fine for linear SVMs. But it has limitations:\nIt can be slow if the number of features n is very large. More importantly, it doesn\u0026rsquo;t allow for the \u0026ldquo;kernel trick\u0026rdquo; which is essential for efficient non-linear SVMs. The Dual Problem is a different but related mathematical formulation of the same optimization task.\nInstead of finding w and b directly, it focuses on finding a set of new variables, αᵢ (alpha-i), one for each training instance. Key Property of these αᵢs: It turns out that αᵢ will be greater than zero only for the support vectors. For all other data points, αᵢ will be zero! What solving the dual problem is ultimately trying to achieve: It\u0026rsquo;s finding the importance (αᵢ) of each training instance in defining the optimal margin. Only the support vectors end up having non-zero importance. Why is the Dual useful?\nComputational Efficiency in some cases: If the number of training instances m is smaller than the number of features n, solving the dual can be faster. THE KERNEL TRICK (THIS IS THE BIG ONE!): Look at the dual objective function (Equation 5-6): minimize (1/2) Σᵢ Σⱼ αᵢαⱼt⁽ⁱ⁾t⁽ʲ⁾(x⁽ⁱ⁾ᵀx⁽ʲ⁾) - Σᵢ αᵢ Notice the term (x⁽ⁱ⁾ᵀx⁽ʲ⁾). This is a dot product between pairs of training instances. The solution w from the dual (Equation 5-7) also involves sums of αᵢt⁽ⁱ⁾x⁽ⁱ⁾. And crucially, when you make predictions using the dual formulation (Equation 5-11), the decision function becomes: h(x_new) = Σᵢ αᵢt⁽ⁱ⁾(x⁽ⁱ⁾ᵀx_new) + b (sum over support vectors i) Again, it only involves dot products between the support vectors and the new instance x_new. The Kernel Trick - The \u0026ldquo;Aha!\u0026rdquo; Moment for Nonlinear SVMs:\nNow, imagine we want to do nonlinear classification. One way (as discussed for Polynomial Regression) is to map our data x to a much higher-dimensional space using a transformation φ(x), where the data becomes linearly separable.\nIf we did this explicitly, we would then have to compute dot products like φ(x⁽ⁱ⁾)ᵀφ(x⁽ʲ⁾) in this very high (maybe even infinite) dimensional space. This would be computationally impossible or extremely inefficient.\nThe Kernel Trick says: What if there\u0026rsquo;s a special function K(a,b) called a kernel that can compute the dot product φ(a)ᵀφ(b) for us, using only the original vectors a and b, without us ever having to compute φ(a) or φ(b) explicitly?\nFor example, the polynomial kernel K(a,b) = (γaᵀb + r)ᵈ calculates what the dot product would be if you mapped a and b to a d-th degree polynomial feature space. The Gaussian RBF kernel K(a,b) = exp(-γ ||a - b||²) implicitly maps to an infinite-dimensional space! What the kernel trick is ultimately trying to achieve: It allows us to get the benefit of working in a very high-dimensional feature space (where complex separations might become linear) without the prohibitive computational cost of actually creating and working with those high-dimensional vectors. We just replace all dot products x⁽ⁱ⁾ᵀx⁽ʲ⁾ in the dual formulation and in the prediction equation with K(x⁽ⁱ⁾, x⁽ʲ⁾).\nSo, the \u0026ldquo;Under the Hood\u0026rdquo; Flow for Kernelized SVMs:\nStart with a nonlinear problem. Choose a kernel function K(a,b) (e.g., Polynomial, RBF). This implicitly defines a transformation φ to a higher-dimensional space where the data is hopefully linearly separable. Solve the DUAL optimization problem (Equation 5-6, but with x⁽ⁱ⁾ᵀx⁽ʲ⁾ replaced by K(x⁽ⁱ⁾, x⁽ʲ⁾)). This finds the αᵢ values (which will be non-zero only for support vectors). What this is trying to achieve: Find the \u0026ldquo;importance\u0026rdquo; of each training point in defining the optimal linear margin in that implicit high-dimensional space. Make predictions for a new instance x_new (Equation 5-11, with K): h(x_new) = Σᵢ αᵢt⁽ⁱ⁾K(x⁽ⁱ⁾, x_new) + b What this is trying to achieve: Classify the new point based on its kernel \u0026ldquo;similarity\u0026rdquo; (as defined by K) to the support vectors, effectively performing a linear separation in the implicit high-dimensional space. Think of it like this:\nPrimal problem (for linear SVM): Directly find the best \u0026ldquo;street\u0026rdquo; (w, b) in the original feature space. Dual problem (for linear SVM): Find out which data points (αᵢ \u0026gt; 0) are the crucial \u0026ldquo;support posts\u0026rdquo; for that street. This formulation happens to only use dot products. Kernelized SVM (using the dual): We want a curvy street in our original space. The kernel trick lets us pretend we\u0026rsquo;ve straightened out the data into a super high-dimensional space where a straight street works. We use the dual formulation because it only relies on dot products, and we can replace those dot products with our kernel function K. The kernel function cleverly calculates what those dot products would have been in the high-dimensional space, without us ever going there. So, the progression of \u0026ldquo;what are we trying to achieve\u0026rdquo;:\nLinear SVM (Primal): Maximize margin directly by finding w and b. Challenges: Can be slow for many features, doesn\u0026rsquo;t easily extend to non-linear. Linear SVM (Dual): Reformulate to find αs (support vector indicators). Only uses dot products. Kernel Trick Motivation: We want non-linear separation. Idea: map to a higher space where it\u0026rsquo;s linear. Problem: mapping is expensive. Kernelized SVM (Dual + Kernel): Realize the dual only needs dot products. If we can find a kernel K(a,b) that equals φ(a)ᵀφ(b) without computing φ, we can do complex non-linear classification efficiently by solving the dual problem with K and making predictions with K. The \u0026ldquo;headache\u0026rdquo; the book mentions with Equation 5-12 for computing b with kernels is because even b needs to be calculated using these kernelized dot products, as we no longer have an explicit w in the original space.\nIt\u0026rsquo;s a chain of clever mathematical steps, each designed to overcome a limitation of the previous one, ultimately allowing us to achieve large margin classification for complex, non-linear data in an efficient way. The dual formulation is the bridge that makes the kernel trick possible.\nDoes this step-by-step focus on the \u0026ldquo;why\u0026rdquo; for each part (especially the dual and kernel) help make the overall strategy a bit more intuitive? It\u0026rsquo;s okay if it still feels complex; SVM theory is one of the more mathematically rich areas in classical ML.\nGlossary (Page 164-173: Under the Hood - How SVMs Work) This is where the math gets a bit deeper, but remember our goal: \u0026ldquo;what is it ultimately trying to achieve?\u0026rdquo; The book changes notation here for SVMs, which is common:\nBias term: b (instead of θ₀)\nFeature weights vector: w (instead of θ₁ to θₙ)\nNo x₀=1 bias input feature is added.\nDecision Function and Predictions (Linear SVM - page 165):\nDecision function: wᵀx + b Equation 5-2: Linear SVM classifier prediction ŷ = 1 if wᵀx + b ≥ 0 ŷ = 0 if wᵀx + b \u0026lt; 0 (or class -1 if using -1/1 labels) Figure 5-12 shows this in 3D for 2 features: wᵀx + b is a plane. The decision boundary is where wᵀx + b = 0 (a line). The \u0026ldquo;street\u0026rdquo; is defined by wᵀx + b = 1 and wᵀx + b = -1. These are parallel lines/planes forming the margin. Training Objective (Page 166):\nGoal: Find w and b that make the margin (the \u0026ldquo;street\u0026rdquo;) as wide as possible, while controlling margin violations.\nThe slope of the decision function is ||w|| (norm of the weight vector).\nKey insight (Figure 5-13): The smaller the norm ||w||, the larger the margin. (If you divide w and b by 2, the slope ||w||/2 is halved, effectively doubling the distance to the ±1 lines, thus doubling the margin width). So, we want to minimize ||w|| (or equivalently, minimize (1/2)wᵀw which is (1/2)||w||² – easier to differentiate).\nHard Margin Objective (Equation 5-3): minimize (1/2)wᵀw subject to t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 for all instances i.\nt⁽ⁱ⁾ is 1 for positive class, -1 for negative class. The constraint t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 means: For positive instances (t⁽ⁱ⁾=1): wᵀx⁽ⁱ⁾ + b ≥ 1 (on or outside the positive margin boundary). For negative instances (t⁽ⁱ⁾=-1): wᵀx⁽ⁱ⁾ + b ≤ -1 (on or outside the negative margin boundary). What it\u0026rsquo;s ultimately trying to achieve: Find the w with the smallest norm (largest margin) such that all points are correctly classified and outside or on the margin boundaries. Soft Margin Objective (Equation 5-4, page 167): To allow for margin violations, introduce slack variables ζ⁽ⁱ⁾ ≥ 0 (zeta-i) for each instance i. ζ⁽ⁱ⁾ measures how much instance i is allowed to violate the margin. minimize (1/2)wᵀw + C Σᵢ ζ⁽ⁱ⁾ subject to t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 - ζ⁽ⁱ⁾ and ζ⁽ⁱ⁾ ≥ 0.\nWhat it\u0026rsquo;s ultimately trying to achieve: Still minimize (1/2)wᵀw (maximize margin). But also minimize Σᵢ ζ⁽ⁱ⁾ (sum of slack/violations). C is the hyperparameter that trades off between these two conflicting objectives: large margin vs. few violations. Small C: Margin width is prioritized (more slack allowed). Large C: Few violations prioritized (margin might be smaller). Quadratic Programming (QP) (Page 167):\nBoth hard and soft margin SVM objectives are convex quadratic optimization problems with linear constraints. These are known as QP problems. Specialized QP solvers exist to find the optimal w and b. Equation 5-5 gives the general QP formulation. The book explains how to map SVM parameters to this general form. The Dual Problem (Page 168-169):\nFor constrained optimization problems (like SVMs), there\u0026rsquo;s often a \u0026ldquo;primal problem\u0026rdquo; and a related \u0026ldquo;dual problem.\u0026rdquo; Solving the dual can sometimes be easier or offer advantages. For SVMs, the dual problem: Is faster to solve than the primal when number of training instances (m) \u0026lt; number of features (n). Crucially, makes the kernel trick possible! (The primal does not). Equation 5-6: Dual form of the linear SVM objective: This involves minimizing a function with respect to new variables αᵢ (alpha-i), one for each training instance. minimize (1/2) Σᵢ Σⱼ αᵢαⱼt⁽ⁱ⁾t⁽ʲ⁾(x⁽ⁱ⁾ᵀx⁽ʲ⁾) - Σᵢ αᵢ subject to αᵢ ≥ 0. Once you find the optimal α̂ vector (using a QP solver), you can compute ŵ and b̂ for the primal problem using Equation 5-7. An important property is that αᵢ will be non-zero only for the support vectors! Most αᵢ will be zero. Kernelized SVMs (The Kernel Trick Explained - Page 169-171): This is where the magic happens for nonlinear SVMs.\nSuppose we have a mapping function φ(x) that transforms our input x into a higher-dimensional space where it might become linearly separable (Equation 5-8 for a 2nd-degree polynomial mapping). If we apply this transformation φ to all training instances, the dual problem (Equation 5-6) would contain dot products of these transformed vectors: φ(x⁽ⁱ⁾)ᵀφ(x⁽ʲ⁾). The Key Insight (Equation 5-9 for 2nd-degree polynomial): It turns out that for some transformations φ, the dot product φ(a)ᵀφ(b) in the high-dimensional space can be computed by a simpler function K(a,b) using only the original vectors a and b. For the 2nd-degree polynomial mapping, φ(a)ᵀφ(b) = (aᵀb)². The Kernel Function K(a,b): A kernel K(a,b) is a function that computes φ(a)ᵀφ(b) based only on a and b, without needing to know or compute φ itself! What it\u0026rsquo;s ultimately trying to achieve: It allows us to operate in a very high (even infinite) dimensional feature space implicitly, without ever actually creating those features. This avoids the computational nightmare of explicit transformation. So, in the dual problem (Equation 5-6), we just replace x⁽ⁱ⁾ᵀx⁽ʲ⁾ with K(x⁽ⁱ⁾, x⁽ʲ⁾). Common Kernels (Equation 5-10, page 171): Linear: K(a,b) = aᵀb (no transformation) Polynomial: K(a,b) = (γaᵀb + r)ᵈ Gaussian RBF: K(a,b) = exp(-γ ||a - b||²) Sigmoid: K(a,b) = tanh(γaᵀb + r) Mercer\u0026rsquo;s Theorem (sidebar): Provides mathematical conditions for a function K to be a valid kernel (i.e., for a corresponding φ to exist). Gaussian RBF kernel actually maps to an infinite-dimensional space! Good thing we don\u0026rsquo;t have to compute φ. Making Predictions with Kernels (Equation 5-11, page 172): We need w to make predictions, but w lives in the (potentially huge) φ space. How do we predict without computing w? Equation 5-7 for w involves φ(x⁽ⁱ⁾). If we plug this into the decision function wᵀφ(x⁽ⁿ⁾) + b, we get an equation that only involves dot products of φ terms, which can be replaced by kernels: h(x⁽ⁿ⁾) = Σᵢ αᵢt⁽ⁱ⁾K(x⁽ⁱ⁾, x⁽ⁿ⁾) + b (sum over support vectors i where αᵢ \u0026gt; 0).\nWhat it\u0026rsquo;s ultimately trying to achieve: Predictions for a new instance x⁽ⁿ⁾ are made by computing its kernel similarity to only the support vectors. The bias term b can also be computed using kernels (Equation 5-12). Online SVMs (Page 172-173): For online learning (incremental learning).\nOne method for linear SVMs: Use Gradient Descent on a cost function derived from the primal problem (Equation 5-13). Equation 5-13: Linear SVM classifier cost function (Hinge Loss) J(w,b) = (1/2)wᵀw + C Σᵢ max(0, 1 - t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b)) (1/2)wᵀw: Aims for a large margin (small w). max(0, 1 - t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b)): This is the hinge loss. What hinge loss is ultimately trying to achieve: It penalizes instances that violate the margin. If an instance i is correctly classified and outside or on the margin (t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1), then 1 - t⁽ⁱ⁾(...) ≤ 0, so max(0, ...) = 0 (zero loss for this instance). If it violates the margin, the loss is proportional to how far it is from its correct margin boundary. This cost function is what SGDClassifier(loss=\u0026quot;hinge\u0026quot;) minimizes. Online kernelized SVMs are also possible but more complex. And that\u0026rsquo;s the grand tour of Support Vector Machines! The core idea of large margin classification is simple and elegant. The kernel trick is the \u0026ldquo;magic\u0026rdquo; that allows SVMs to handle complex nonlinear data efficiently by implicitly operating in very high-dimensional feature spaces.\nThe \u0026ldquo;Under the Hood\u0026rdquo; section is definitely more mathematical, but hopefully, by focusing on \u0026ldquo;what is it ultimately trying to achieve\u0026rdquo; for each equation (like minimizing ||w|| for a large margin, or using kernels to avoid explicit high-dimensional transformations), the core concepts become clearer.\nAny part of that, especially the kernel trick or the dual problem, that still feels a bit fuzzy?\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-5/","tags":null,"title":"Chapter 5: Support Vector Machines"},{"categories":null,"content":"Introduction to Decision Trees Alright class, settle in! After exploring the world of Support Vector Machines, we\u0026rsquo;re now turning our attention to another incredibly versatile and powerful family of algorithms: Decision Trees.\nVersatility: Like SVMs, Decision Trees can perform both classification and regression tasks. They can even handle multioutput tasks (where each instance can have multiple output labels or values). Power: They are capable of fitting complex datasets. You might recall from Chapter 2, when we looked at the California housing data, a DecisionTreeRegressor was able to fit the training data perfectly (though, as we noted, it was actually overfitting). Fundamental Building Blocks: Decision Trees are also the core components of Random Forests (which we\u0026rsquo;ll see in Chapter 7). Random Forests are among the most powerful and widely used ML algorithms today. So, understanding Decision Trees is crucial for understanding Random Forests. What this chapter will cover:\nHow to train, visualize, and make predictions with Decision Trees. The CART (Classification and Regression Tree) training algorithm, which Scikit-Learn uses. How to regularize Decision Trees (to prevent overfitting). How they\u0026rsquo;re used for regression tasks. Some of their limitations. (Page 175-176: Training and Visualizing a Decision Tree) Let\u0026rsquo;s start by building one to see how it works. We\u0026rsquo;ll use the Iris dataset (which we\u0026rsquo;ve seen before, for example, with Logistic Regression in Chapter 4).\nThe Code: from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier\niris = load_iris() X = iris.data[:, 2:] # petal length and width only y = iris.target # species (0: setosa, 1: versicolor, 2: virginica)\ntree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y)\nWe\u0026rsquo;re only using two features: petal length and petal width, to make it easy to visualize. max_depth=2: This is a crucial hyperparameter. We are telling the tree not to grow beyond a depth of 2 levels. This is a form of regularization to prevent it from becoming too complex and overfitting. If we didn\u0026rsquo;t set this, the tree might grow very deep to try and perfectly classify every single training instance. Visualizing the Tree (Page 176): One of the great things about Decision Trees is that they are very intuitive and easy to visualize. Scikit-Learn provides a function export_graphviz to help with this. from sklearn.tree import export_graphviz export_graphviz( tree_clf, out_file=\u0026quot;iris_tree.dot\u0026quot;, feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True ) This creates a .dot file. You then use the dot command-line tool (from the Graphviz package, which you\u0026rsquo;d need to install separately) to convert this into an image, like a PNG: $ dot -Tpng iris_tree.dot -o iris_tree.png\nFigure 6-1 (Iris Decision Tree): This is the visual representation of our trained tree. It\u0026rsquo;s a flowchart-like structure. Let\u0026rsquo;s understand its components: Nodes: Each box is a node. Root Node (Depth 0, at the top): This is where you start. It asks a question about a feature. In Figure 6-1, it asks: \u0026ldquo;petal length (cm) \u0026lt;= 2.45?\u0026rdquo; Child Nodes: Based on the answer (True or False), you move to a child node. Leaf Nodes: Nodes that don\u0026rsquo;t have any children. These nodes make the final prediction. Information in each node: Question/Condition: E.g., \u0026ldquo;petal length (cm) \u0026lt;= 2.45\u0026rdquo;. gini: This measures the impurity of the node. We\u0026rsquo;ll discuss this more soon. A gini score of 0 means the node is \u0026ldquo;pure\u0026rdquo; – all training instances that reach this node belong to the same class. samples: How many training instances from the dataset fall into this node. value: How many training instances of each class fall into this node. For example, value = [50, 50, 50] at the root means there were 50 Setosa, 50 Versicolor, and 50 Virginica instances in the training set that reached this node (which is all of them initially). class: The class that would be predicted if this node were a leaf node (i.e., the majority class among the samples in this node). (Page 176-177: Making Predictions) How do you use this tree (Figure 6-1) to classify a new iris flower? It\u0026rsquo;s like a game of \u0026ldquo;20 Questions.\u0026rdquo;\nStart at the root node (depth 0). Question: Is the flower\u0026rsquo;s petal length ≤ 2.45 cm? If YES (True): Move to the left child node (depth 1, left). This node in Figure 6-1 has gini = 0.0, samples = 50, value = [50, 0, 0], class = setosa. This is a leaf node (it has no further questions/children). So, the prediction is Iris setosa. If NO (False, meaning petal length \u0026gt; 2.45 cm): Move to the right child node (depth 1, right). This node is not a leaf node. It asks another question. Question: Is the flower\u0026rsquo;s petal width ≤ 1.75 cm? If YES (True, petal length \u0026gt; 2.45 cm AND petal width ≤ 1.75 cm): Move to this node\u0026rsquo;s left child (depth 2, middle of bottom row). This is a leaf node. class = versicolor. Prediction: Iris versicolor. If NO (False, petal length \u0026gt; 2.45 cm AND petal width \u0026gt; 1.75 cm): Move to this node\u0026rsquo;s right child (depth 2, rightmost bottom row). This is a leaf node. class = virginica. Prediction: Iris virginica. It\u0026rsquo;s really that simple to make a prediction once the tree is built!\nAn Important Quality (Bird Icon, page 177): Decision Trees require very little data preparation. They don\u0026rsquo;t need feature scaling or centering. This is a nice practical advantage.\nUnderstanding gini (Gini Impurity - Equation 6-1, page 177):\nA node\u0026rsquo;s gini attribute measures its impurity. A node is \u0026ldquo;pure\u0026rdquo; (gini = 0) if all training instances it applies to belong to the same class. The depth-1 left node (Setosa) is pure. Equation 6-1: Gᵢ = 1 - Σₖ (pᵢ,ₖ)² Gᵢ: Gini impurity of the i-th node. pᵢ,ₖ: Ratio of class k instances among the training instances in the i-th node. What Gini impurity is ultimately trying to achieve: It\u0026rsquo;s a measure of how \u0026ldquo;mixed up\u0026rdquo; the classes are within a node. If a node is pure (all samples belong to one class, say class C), then pᵢ,C = 1 and pᵢ,k = 0 for all other k. So Gᵢ = 1 - (1)² = 0. If a node has an equal mix of classes, the Gini impurity will be higher. For example, for the depth-2 left node (predicts versicolor): Samples = 54. Value = [0 setosa, 49 versicolor, 5 virginica]. p_setosa = 0/54 p_versicolor = 49/54 p_virginica = 5/54 G = 1 - (0/54)² - (49/54)² - (5/54)² ≈ 0.168. This is fairly low, but not zero, as there\u0026rsquo;s a mix of Versicolor and Virginica. Binary Trees (Scorpion Icon, page 177):\nScikit-Learn uses the CART algorithm, which produces binary trees: non-leaf nodes always have exactly two children (questions have yes/no answers). Other algorithms like ID3 can produce trees with nodes having more than two children. (Page 178: Decision Boundaries \u0026amp; Model Interpretation) Figure 6-2 (Decision Tree decision boundaries): This shows how the tree partitions the feature space (petal length and petal width). The root node (petal length ≤ 2.45 cm) creates the first thick vertical split. Everything to the left is classified as Setosa (pure). The area to the right is impure, so the depth-1 right node splits it further with a horizontal line (petal width ≤ 1.75 cm). Since max_depth=2, the tree stops there. The resulting regions are rectangular (or hyperrectangles in higher dimensions). If max_depth were 3, the two depth-2 nodes could be split further, adding more boundaries (dotted lines in the figure). Model Interpretation: White Box vs. Black Box (Sidebar, page 178):\nDecision Trees are very intuitive and their decisions are easy to interpret. They are often called white box models. You can see the rules. In contrast, Random Forests or Neural Networks are often considered black box models. They can make great predictions, but it\u0026rsquo;s harder to explain in simple terms why a specific prediction was made. Decision Trees provide simple classification rules that can even be applied manually. (Page 178-179: Estimating Class Probabilities) Decision Trees can also estimate the probability that an instance belongs to a particular class k.\nHow it works: Traverse the tree to find the leaf node for the instance. Return the ratio of training instances of class k in that leaf node. Example (page 179): Flower with petal length 5 cm, petal width 1.5 cm. Petal length \u0026gt; 2.45 cm (False for root) -\u0026gt; go to depth-1 right node. Petal width ≤ 1.75 cm (True for this node) -\u0026gt; go to depth-2 left node (the one that predicts Versicolor). This leaf node has value = [0, 49, 5], meaning 0 Setosa, 49 Versicolor, 5 Virginica, out of 54 samples. Probabilities: P(Setosa) = 0/54 = 0% P(Versicolor) = 49/54 ≈ 90.7% P(Virginica) = 5/54 ≈ 9.3% Scikit-Learn code: tree_clf.predict_proba([[5, 1.5]]) gives array([[0. , 0.90740741, 0.09259259]]) tree_clf.predict([[5, 1.5]]) gives array([1]) (class 1 is Versicolor). Important Note: The estimated probabilities are the same for any point that falls into the same leaf node\u0026rsquo;s region (e.g., the bottom-middle rectangle in Figure 6-2). This can sometimes be a limitation if fine-grained probability estimates are needed. Even if a flower had petal length 6cm and width 1.5cm (making it seem more likely Virginica by intuition, if it were near the boundary with Virginica region), it would still get the same probabilities if it landed in that same leaf node. (Page 179: The CART Training Algorithm) Scikit-Learn uses the Classification and Regression Tree (CART) algorithm to train (or \u0026ldquo;grow\u0026rdquo;) Decision Trees.\nHow it works (Greedy Approach):\nIt first splits the training set into two subsets using a single feature k and a threshold tₖ (e.g., \u0026ldquo;petal length ≤ 2.45 cm\u0026rdquo;). How does it choose k and tₖ? It searches for the pair (k, tₖ) that produces the purest subsets, weighted by their size. Purity is measured by Gini impurity (or entropy, which we\u0026rsquo;ll see). Equation 6-2: CART cost function for classification J(k, tₖ) = (m_left / m) * G_left + (m_right / m) * G_right m_left, m_right: Number of instances in the left/right subset after the split. m: Total number of instances. G_left, G_right: Gini impurity of the left/right subset. What this cost function is ultimately trying to achieve: Find the feature and threshold that minimize this weighted average impurity of the child nodes. It wants the \u0026ldquo;cleanest\u0026rdquo; possible split. Recursion: Once the algorithm splits the set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. Stopping Conditions (When to stop splitting/recursing):\nReaches max_depth (hyperparameter). Cannot find a split that will reduce impurity further. Other hyperparameters controlling stopping: min_samples_split: Minimum number of samples a node must have before it can be split. min_samples_leaf: Minimum number of samples a leaf node must have. min_weight_fraction_leaf: Same as min_samples_leaf but as a fraction of total weighted instances. max_leaf_nodes: Maximum number of leaf nodes. These are all regularization hyperparameters. Greedy Nature (Scorpion Icon, page 180):\nCART is a greedy algorithm. At each step, it searches for the locally optimal split at the current level. It doesn\u0026rsquo;t look ahead to see if a less optimal split now might lead to an even better overall tree (lower total impurity) a few levels down. A greedy algorithm often produces a solution that\u0026rsquo;s reasonably good, but not guaranteed to be globally optimal. Finding the truly optimal tree is an NP-Complete problem (computationally very hard, requires O(exp(m)) time), so we settle for a \u0026ldquo;reasonably good\u0026rdquo; greedy solution. (Page 180-181: Computational Complexity \u0026amp; Gini vs. Entropy) Prediction Complexity:\nTraversing a Decision Tree from root to leaf takes roughly O(log₂(m)) nodes for a balanced tree (where m is number of instances). Each node checks one feature. So, overall prediction complexity is O(log₂(m)), independent of the number of features n. Predictions are very fast! Training Complexity:\nAt each node, CART compares all features (or max_features) on all samples in that node. This results in a training complexity of roughly O(n × m log₂(m)). For small training sets (\u0026lt; few thousand), Scikit-Learn can speed up training by presorting data (presort=True parameter, though this is deprecated and will be removed; Scikit-learn now often sorts internally when beneficial). For larger sets, presorting slows it down. Gini Impurity or Entropy? (Page 180):\nBy default, CART uses Gini impurity. You can set criterion=\u0026quot;entropy\u0026quot; to use entropy as the impurity measure instead. Entropy (Equation 6-3, page 181): Hᵢ = - Σₖ (pᵢ,ₖ * log₂(pᵢ,ₖ)) (sum over classes k where pᵢ,ₖ ≠ 0) Originated in thermodynamics (molecular disorder). In information theory (Shannon), it measures average information content. Entropy is 0 if a set contains instances of only one class (pure). What it\u0026rsquo;s ultimately trying to achieve: Similar to Gini, it measures the \u0026ldquo;mixed-up-ness\u0026rdquo; or uncertainty in a node. Lower entropy means less uncertainty/more purity. Gini vs. Entropy: Does it matter? Mostly, no. They lead to similar trees. Gini is slightly faster to compute, so it\u0026rsquo;s a good default. When they differ: Gini tends to isolate the most frequent class in its own branch of the tree. Entropy tends to produce slightly more balanced trees. (Page 181-182: Regularization Hyperparameters) Decision Trees make very few assumptions about the data (they are nonparametric models – the number of parameters isn\u0026rsquo;t fixed before training, the model structure adapts to the data).\nIf left unconstrained, they will fit the training data very closely, likely overfitting. To avoid this, we need to regularize by restricting their freedom during training. Key Regularization Hyperparameters in DecisionTreeClassifier: max_depth: Maximum depth of the tree. (Default is None = unlimited). Reducing this is a common way to regularize. min_samples_split: Minimum samples a node needs to be split. min_samples_leaf: Minimum samples a leaf node must have. min_weight_fraction_leaf: As min_samples_leaf, but as a fraction. max_leaf_nodes: Limits the total number of leaf nodes. max_features: Max number of features evaluated for splitting at each node. Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model. Figure 6-3 (page 182): Shows the effect on the moons dataset. Left: Default hyperparameters (no restrictions) -\u0026gt; Overfitting (wiggly boundary). Right: min_samples_leaf=4 -\u0026gt; Simpler, smoother boundary, likely generalizes better. Pruning (Scorpion Icon, page 182): Some algorithms work by first training the tree without restrictions, then pruning (deleting) unnecessary nodes. A node is considered unnecessary if the purity improvement it provides is not statistically significant (e.g., using a chi-squared test to check if improvement is just by chance). If p-value \u0026gt; threshold, prune. (Page 183-184: Regression with Decision Trees) Decision Trees can also do regression using DecisionTreeRegressor.\nExample: Training on a noisy quadratic dataset with max_depth=2. Figure 6-4 (A Decision Tree for regression): Looks similar to a classification tree. Main difference: Instead of predicting a class in each node, it predicts a value. The value predicted at a leaf node is the average target value of the training instances that fall into that leaf. The mse attribute in each node shows the mean squared error of the training instances in that node, relative to the average value of that node. Predictions (Figure 6-5, page 184): The predictions are piecewise constant. For any region defined by a leaf node, the prediction is the average value of the training instances in that region. Increasing max_depth leads to more steps in the prediction function, fitting the training data more closely. CART Algorithm for Regression (Equation 6-4, page 184): Works mostly the same way as for classification. Difference: Instead of minimizing impurity (Gini/entropy), it tries to split the training set in a way that minimizes the MSE. J(k, tₖ) = (m_left / m) * MSE_left + (m_right / m) * MSE_right It wants to make the instances within each resulting region as close as possible to the mean value of that region. Regularization (Figure 6-6, page 184): Just like classification trees, regression trees are prone to overfitting if unregularized (left plot). Setting min_samples_leaf=10 (right plot) gives a much more reasonable, smoother model. (Page 185-186: Instability of Decision Trees) Decision Trees have many great qualities (simple, interpretable, versatile, powerful). But they have limitations.\nSensitivity to Training Set Rotation (Orthogonal Boundaries - Figure 6-7, page 185): Decision Trees prefer to create orthogonal decision boundaries (splits are perpendicular to an axis). If your dataset is rotated, a simple linear boundary in the rotated space might require a complex, step-like boundary for the Decision Tree. The left plot shows a tree easily splitting unrotated data. The right shows a convoluted boundary for data rotated 45°. The model on the right might not generalize well. One way to limit this: Use Principal Component Analysis (PCA, Chapter 8) to orient data better first. Sensitivity to Small Variations in Training Data (Figure 6-8, page 186):\nThis is a major issue. Decision Trees are very sensitive to small changes in the training data. Example: Remove one instance (the widest Iris versicolor) from the Iris dataset and retrain. The resulting tree (Figure 6-8) can look very different from the original (Figure 6-2). Since the CART algorithm used by Scikit-Learn is stochastic (randomly selects features to evaluate at nodes if max_features is less than total features, or how ties are broken), you might get different models even on the same training data unless you set random_state. How to limit this instability? Random Forests (Chapter 7!) average predictions over many trees, which greatly helps reduce this variance and instability.\nAnd that\u0026rsquo;s the essence of Decision Trees! They work by recursively partitioning the data based on simple questions about features, aiming to create purer (for classification) or more homogeneous (for regression) subsets. They are interpretable but can be unstable and prone to overfitting if not regularized.\nAny thoughts or questions on how these trees make decisions or how they are built? The idea of recursive partitioning and measuring purity/error at each step is central.\nGlossary Q. Is backtracking used while training decision trees using CART algorithm in machine learning? Yes, absolutely! The process of building a decision tree, especially using algorithms like CART (Classification and Regression Trees), shares a very strong conceptual similarity with recursive backtracking, particularly the divide-and-conquer aspect and the exploration of choices.\nHere\u0026rsquo;s how they relate:\nRecursive Partitioning (Divide and Conquer):\nDecision Trees: At each node, the algorithm searches for the \u0026ldquo;best\u0026rdquo; feature and the \u0026ldquo;best\u0026rdquo; split point for that feature to divide the current dataset into two or more subsets. This partitioning is done recursively. Backtracking/Recursion: We break a problem down into smaller sub problems. Making a Sequence of Decisions:\nDecision Trees: Building a tree is a sequence of decisions: Which feature to split on at the current node? What value of that feature to use as the split point? Backtracking: We make a sequence of choices to build a potential solution. Exploring Choices (Greedy vs. Exhaustive):\nCART (Typically Greedy): At each node, CART makes a locally optimal decision. It searches for the single feature and split point that currently gives the best improvement in purity (e.g., Gini impurity, entropy for classification; variance reduction for regression). It commits to this split and then recursively processes the resulting child nodes. It doesn\u0026rsquo;t typically \u0026ldquo;backtrack\u0026rdquo; in the sense of undoing a split if it later turns out to be globally suboptimal. This greedy nature is why decision trees can sometimes find solutions that aren\u0026rsquo;t the absolute global optimum but are good and fast to compute. Pure Backtracking (Exhaustive): Would explore all possible sequences of splits to find the globally best tree. This is computationally infeasible for most datasets. Stopping Conditions (Base Cases):\nDecision Trees: Recursion stops when: A node is \u0026ldquo;pure\u0026rdquo; (all samples belong to one class). A pre-defined maximum depth is reached. The number of samples in a node is below a threshold. No split further improves purity. Backtracking: Recursion stops when: A complete solution is found. A dead end is reached (no valid next choice). Pruning (Analogous to Constraint Checking/Pruning in Backtracking):\nDecision Trees: After a large tree is grown (often greedily), a \u0026ldquo;pruning\u0026rdquo; step is often applied. This is where the tree might be \u0026ldquo;simplified\u0026rdquo; by removing branches that don\u0026rsquo;t contribute much to overall accuracy or that lead to overfitting. This is somewhat like backtracking deciding a path is not fruitful, but it\u0026rsquo;s typically done after the initial greedy construction. Backtracking: We \u0026ldquo;prune\u0026rdquo; branches of the search space if a partial solution violates constraints. Where they differ significantly:\nGoal:\nBacktracking (as we discussed): Often aims to find all solutions or one solution that satisfies constraints (e.g., N-Queens, Sudoku). Decision Tree (CART): Aims to build a model that generalizes well to unseen data by finding optimal decision boundaries. Nature of \u0026ldquo;Undo\u0026rdquo;:\nBacktracking: The \u0026ldquo;undo\u0026rdquo; is fundamental to explore different branches of the same problem instance to find different discrete solutions. CART: Typically doesn\u0026rsquo;t \u0026ldquo;undo\u0026rdquo; a split choice in its main growing phase. The \u0026ldquo;undo\u0026rdquo; equivalent is more like the post-hoc pruning process, where parts of the already-built tree are removed. So, in summary:\nThe recursive, divide-and-conquer nature of building a decision tree by repeatedly finding the best split is very similar to the recursive calls in backtracking. Both explore a \u0026ldquo;tree\u0026rdquo; of possibilities.\nHowever, CART\u0026rsquo;s typical implementation is greedy in its choice of splits at each node, whereas the backtracking problems we\u0026rsquo;ve discussed (like N-Queens for all solutions) are more exhaustive in exploring valid choices. If CART were to try and find the globally optimal decision tree by trying every possible sequence of splits and then backtracking, it would be computationally intractable.\nSo, while the recursive structure is similar, the search strategy (greedy vs. exhaustive with backtracking) and the ultimate goal are different. The core idea of recursively breaking down the problem based on choices is a shared theme.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-6/","tags":null,"title":"Chapter 6: Decision Trees"},{"categories":null,"content":"Statistics is a core component of any data scientist\u0026rsquo;s toolkit. Since many commercial layers of a data science pipeline are built from statistical foundations (for example, A/B testing), knowing foundational topics of statistics is essential.\nInterviewers love to test a candidate\u0026rsquo;s knowledge about the statistic basics, starting with topics like the Central Limit Theorem and the Law of Large Numbers, and then progressing on to the concepts underlying hypothesis-testing, particularly p-values and confidence intervals, as well as Type 1 and Type II error, and their interpretations. All of those topics play an important role in the statistical underpinning of A/B testing. Additionally, derivations and manipulations involving random variables of various probability distributions are also common, particularly in finance interviews. Lastly, a common topic in more technical interviews will involve utilizing MLE and/or MAP.\nTopics to Review Before Your Interview Properties of Random Variables For any given random variable X, the following properties hold true (below we assume X is continuous, but it also holds true for discrete random variables). The expectation (average value, or mean) of a random variable is given by the integral of the value of X with its probability density function (PDF) fx (x): µ = E[X] = ∫ xfx(x)dx\nand the variance is given by: Var(X) = E[(X - E[X])²] = E[X²] - (E[X])²\nThe variance is always non-negative, and its square root is called the standard deviation, which is heavily used in statistics. σ = √Var(X) = √E[(X - E[X])²] = √E[X²] - (E[X])²\nThe conditional values of both the expectation and variance are as follows. For example, consider the case for the conditional expectation of X, given that Y = y: E[X|Y = y] = ∫ xf(X|Y)(x|y)dx\nFor any given random variables X and Y, the covariance, a linear measure of relationship between the two variables, is defined by the following: Cov(X,Y) = E[(X - E[X])(Y-E[Y])] = E[XY]-E[X]E[Y]\nand the normalization of covariance, represented by the Greek letter p, is the correlation between X and Y: ρ(X,Y) = Cov(X,Y) / √Var(X)Var(Y)\nAll of these properties are commonly tested in interviews, so it helps to be able to understand the mathematical details behind each and walk through an example for each. For example, assume X follows a Uniform distribution on the interval [a, b], then we have the following: fx(x) = 1 / (b-a) Therefore the expectation of X is: E[X] = ∫ xfx(x)dx = ∫ (from a to b) x * (1/(b-a)) dx = [x² / (2(b-a))] (from a to b) = (a+b)/2\nAlthough it is not necessary to memorize the derivations for all the different probability distributions, you should be comfortable deriving them as needed, as it is a common request in more technical interviews. To this end, you should make sure to understand the formulas given above and be able to apply them to some of the common probability distributions like the exponential or uniform distribution.\nLaw of Large Numbers The Law of Large Numbers (LLN) states that if you sample a random variable independently a large number of times, the measured average value should converge to the random variable\u0026rsquo;s true expectation. Stated more formally, X̄n = (X₁ + \u0026hellip; + Xn) / n → µ, as n → ∞\nThis is important in studying the longer-term behavior of random variables over time. As an example, a coin might land on heads 5 times in a row, but over a much larger n we would expect the proportion of heads to be approximately half of the total flips. Similarly, a casino might experience a loss on any individual game, but over the long run should see a predictable profit over time.\nCentral Limit Theorem The Central Limit Theorem (CLT) states that if you repeatedly sample a random variable a large number of times, the distribution of the sample mean will approach a normal distribution regardless of the initial distribution of the random variable. Recall from the probability chapter that the normal distribution takes on the form: fx(x) = (1 / (σ√(2π))) exp( -(x-µ)² / (2σ²) ) with the mean and standard deviation given by µ and σ respectively. The CLT states that: X̄n = (X₁ + \u0026hellip; + Xn) / n ~ N(µ, σ²/n) and hence (X̄n - µ) / (σ/√n) ~ N(0,1)\nThe CLT provides the basis for much of hypothesis testing, which is discussed shortly. At a very basic level, you can consider the implications of this theorem on coin flipping: the probability of getting some number of heads flipped over a large n should be approximately that of a normal distribution. Whenever you\u0026rsquo;re asked to reason about any particular distribution over a large sample size, you should remember to think of the CLT whether it is Binomial, Poisson, or any other distribution.\nHypothesis Testing General setup The process of testing whether or not a sample of data supports a particular hypotheses is called hypothesis testing. Generally, hypotheses concern particular properties of interest for a given population, such as its parameters, like μ (for example, the mean conversion rate among a set of users). The steps in testing a hypothesis are as follows:\nState a null hypothesis and an alternative hypothesis. Either the null hypothesis will be rejected (in favor of the alternative hypothesis) or it will fail to be rejected (although failing to reject the null hypothesis does not necessarily mean it is true, but rather that there is not sufficient evidence to reject it). Use a particular test statistic of the null hypothesis to calculate the corresponding p-value. Compare the p-value to a certain significance level α. Since the null hypothesis typically represents a baseline (e.g., the marketing campaign did not increase conversion rates, etc.), the goal is to reject it with statistical significance and hope that there is a significant outcome. Hypothesis tests are either one-tailed or two-tailed tests. A one-tailed test has the following types of null and alternative hypothesis: H₀: µ = µ₀ versus H₁: µ \u0026lt; µ₀ or H₁: µ \u0026gt; µ₀ whereas a two-tailed test has these types: H₀: µ = µ₀ versus H₁: µ ≠ µ₀ where H₀ is the null hypothesis and H₁ is the alternative hypothesis, and µ is the parameter of interest. Understanding hypothesis testing is the basis of A/B testing, a topic commonly covered in tech companies\u0026rsquo; interviews. In A/B testing, various versions of a feature are shown to a sample of different users, and each variant is tested to determine if there was an uplift in core engagement metrics. Say, for example, that you are working for Uber Eats, which wants to determine whether email campaigns will increase its product\u0026rsquo;s conversion rates. To conduct an appropriate hypothesis test, you would need two roughly equal groups (equal with respect to dimensions like age, gender, location, etc.) One group would receive the email campaigns and the other group would not be exposed. The null hypothesis in this case would be that the two groups exhibit equal conversion rates, and the hope is that the null hypothesis would be rejected.\nTest Statistics A test statistic is a numerical summary designed for the purpose of determining whether the null hypothesis or the alternative hypothesis should be accepted as correct. More specifically, it assumes that the parameter of interest follows a particular sampling distribution under the null hypothesis. For example, the number of heads in a series of coin flips may be distributed as a binomial distribution, but with a large enough sample size, the sampling distribution should be approximately normally distributed. Hence, the sampling distribution for the total number of heads in a large series of coin flips would be considered normally distributed. Several variations in test statistics and their distributions are the following:\nZ-test: assumes the test statistic follows a normal distribution under the null hypothesis t-test: uses a student\u0026rsquo;s t-distribution rather than a normal distribution Chi-squared: used to assess goodness of fit, and check whether two categorical variables are independent Z-Test Generally the Z-test is used when the sample size is large (to invoke the CLT) or when the population variance is known, and a t-test is used when the sample size is small and when the population variance is unknown. The Z-test for a population mean is formulated as: z = (X̄ - µ₀) / (σ/√n) ~ N(0,1) in the case where the population variance σ² is known.\nt-Test The t-test is structured similarly to the Z-test, but uses the sample variance s² in place of population variance. The t-test is parametrized by the degrees of freedom, which refers to the number of independent observations in a dataset, denoted below by n - 1: t = (X̄ - µ₀) / (s/√n) ~ t_(n-1) where s² = Σ(xᵢ - X̄)² / (n-1)\nAs stated earlier, the t distribution is similar to the normal distribution in appearance but has larger tails (i.e., extreme events happen with greater frequency than the modeled distribution would predict), a common phenomenon, particularly in economics and earth sciences.\nChi-squared Test The Chi-squared test statistic, which is used to assess goodness of fit, and is calculated as follows: χ² = Σ ((Oᵢ - Eᵢ)² / Eᵢ) where Oᵢ is the observed value of interest and Eᵢ is its expected value. A Chi-square test statistic takes on a particular number of degrees of freedom, which is based on the number of categories in the distribution. To use the squared test to check whether two categorical variables are independent, create a table of counts (called a contingency table) with the values of one variable forming the rows of the table and the values of the other variable forming its columns, and checking for intersections. It uses the same style of Chi-squared test statistic as given above.\nHypothesis Testing for Population Proportions Note that, due to the CLT, the Z-test can be applied to random variables of any distribution. For example, when estimating the sample proportion of a population having a characteristic of interest, we can view the members of the population as Bernoulli random variables with those having the characteristic represented by \u0026ldquo;1s\u0026rdquo; and those lacking it represented by \u0026ldquo;0s\u0026rdquo;. Viewing the sample proportion of interest as the sum of these Bernoulli random variables divided by the total population size, we can then compute the sample mean and variance of the overall proportion, about which we can form the following set of hypotheses: H₀: p = p₀ versus H₁: p ≠ p₀ and the corresponding test statistic to conduct a Z-test would be: z = (p̂ - p₀) / √(p₀(1-p₀)/n) In practice, these test statistics form the core of A/B testing. For instance, consider the previously discussed case, in which we seek to measure conversion rates within groups A and B, where A is the control group and B has the special treatment (in this case, a marketing campaign). Adopting the same null hypothesis as before, we can proceed to use a Z-test to assess the difference in empirical population means (in this case, conversion rates) and test its statistical significance at a pre-determined level. When asked about A/B testing or related topics, you should always cite the relevant test statistic and the cause of its validity (usually the CLT).\np-values and confidence intervals Both p-values and confidence intervals are commonly covered topics during interviews. Put simply, a p-value is the probability of observing the value of the calculated test statistic under the null hypothesis assumptions. Usually, the p-value is assessed relative to some pre-determined level of significance (0.05 is often chosen). In conducting a hypothesis test, an α, or measure of the acceptable probability of rejecting a true null hypothesis, is typically chosen prior to conducting the test. Then, a confidence interval can also be calculated to assess the test statistic. This is a range of values that, if a large sample were taken, would contain the parameter value of interest (1-α)% of the time. For instance, a 95% confidence interval would contain the true value 95% of the time. If 0 is included in the confidence intervals, then you cannot reject the null hypothesis (and vice versa). The general form for a confidence interval around the population mean looks like the following, where the term is the critical value (for the standard normal distribution): µ ± z_(α/2) * (σ/√n) In the prior example with the A/B testing on conversion rates, we see that the confidence interval for a population proportion would be p̂ ± z_(α/2) * √(p̂(1-p̂)/n) since our estimate of the true proportion will have the following parameters when estimated as approximately Gaussian: µ_p̂ = p, σ²_p̂ = np(1-p)/n² = p(1-p)/n As long as the sampling distribution of a random variable is known, the appropriate p-values and confidence intervals can be assessed. Knowing how to explain p-values and confidence intervals, in technical and nontechnical terms, is very useful during interviews, so be sure to practice these. If asked about the technical details, always remember to make sure you correctly identify the mean and variance at hand.\nType I and II errors There are two errors that are frequently assessed: type I error, which is also known as a \u0026ldquo;false positive,\u0026rdquo; and type II error, which is also known as a \u0026ldquo;false negative.\u0026rdquo; Specifically, a type I error is when one rejects the null hypothesis when it is correct, and a type II error is when the null hypothesis is not rejected when it is incorrect. Usually 1-α is the referred to as the confidence level, whereas 1-β is referred to as the power. If you plot sample size versus power, generally you should see a larger sample size corresponding to a larger power. It can be useful to look at power in order to gauge the sample size needed for detecting a significant effect. Generally, tests are set up in such a way as to have both 1-α and 1-β relatively high (say at 0.95, and 0.8 respectively). In testing multiple hypotheses, it is possible that if you ran many experiments - even if a particular outcome for one experiment is very unlikely - you would see a statistically significant outcome at least once. So, for example, if you set α = 0.05 and run 100 hypothesis tests, then by pure chance you would expect 5 of the tests to be statistically significant. However, a more desirable outcome is to have the overall α of the 100 tests be 0.05, and this can be done by setting the new α to α/n where n is the number of hypothesis tests (in this case, α/n = 0.05/100 = 0.0005). This is known as Bonferroni correction, and using it helps make sure that the overall rate of false positives is controlled within a multiple testing framework.\nGenerally most interview questions concerning Type I and II errors are qualitative in nature, for instance, requesting explanations of terms or of how you would go about assessing errors/power in an experimental setup.\nMLE and MAP Any probability distribution has parameters, and so fitting parameters is an extremely crucial part of data analysis. There are two general methods for doing so. In maximum likelihood estimation (MLE) the goal is to estimate the most likely parameters given a likelihood function: θ_MLE = arg max L(θ), where L(θ) = f(x₁,\u0026hellip;x_n|θ) Since the values of X are assumed to be i.i.d., then the likelihood function becomes the following: L(θ) = Π (from i=1 to n) f(xᵢ|θ) The natural log of L(θ) is then taken prior to calculating the maximum; since log is a monotonically increasing function, maximizing the log-likelihood log L(θ) is equivalent to maximizing the likelihood: log L(θ) = Σ (from i=1 to n) log f(xᵢ|θ) Another way of fitting parameters is through maximum a posteriori estimation (MAP), which assumes a prior distribution. θ_MAP = arg max g(θ) f(x₁,\u0026hellip;x_n|θ) where the similar log-likelihood is again employed, and g(θ) is a density function of θ. Both MLE and MAP are especially relevant in statistics and machine learning, and knowing these is recommended, especially for more technical interviews. For instance, a common question in such interviews is to derive the MLE for a particular probability distribution. Thus, understanding the above steps, along with the details of the relevant probability distributions, is crucial.\n40 Real Statistics Interview Questions Easy 6.1. Uber: Explain the Central Limit Theorem. Why it is useful?\n6.2. Facebook: How would you explain a confidence interval to a non-technical audience?\n6.3. Twitter: What are some common pitfalls encountered in A/B testing?\n6.4. Lyft: Explain both covariance and correlation formulaically, and compare and contrast them.\n6.5. Facebook: Say you flip a coin 10 times and observe only one heads. What would be your null hypothesis and p-value for testing whether the coin is fair or not?\n6.6. Uber: Describe hypothesis testing and p-values in layman\u0026rsquo;s terms?\n6.7. Groupon: Describe what Type I and Type II errors are, and the tradeoffs between them.\n6.8. Microsoft: Explain the statistical background behind power.\n6.9. Facebook: What is a Z-test and when would you use it versus a t-test?\n6.10. Amazon: Say you are testing hundreds of hypotheses, each with t-test. What considerations would you take into account when doing this?\nMedium 6.11. Google: How would you derive a confidence interval for the probability of flipping heads from a series of coin tosses? 6.12. Two Sigma: What is the expected number of coin flips needed to get two consecutive heads? 6.13. Citadel: What is the expected number of rolls needed to see all 6 sides of a fair die? 6.14. Akuna Capital: Say you\u0026rsquo;re rolling a fair six-sided dice. What is the expected number of rolls until you roll two consecutive 5s? 6.15. D.E. Shaw: A coin was flipped 1000 times, and 550 times it showed heads. Do you think the coin is biased? Why or why not? 6.16. Quora: You are drawing from a normally distributed random variable X ~ N(0, 1) once a day. What is the approximate expected number of days until you get a value greater than 2? 6.17. Akuna Capital: Say you have two random variables X and Y, each with a standard deviation of 1. What is the variance of aX + bY for constants a and b? 6.18. Google: Say we have X ~ Uniform(0, 1) and Y ~ Uniform(0, 1) and the two are independent. What is the expected value of the minimum of X and Y? 6.19. Morgan Stanley: Say you have an unfair coin which lands on heads 60% of the time. How many coin flips are needed to detect that the coin is unfair? 6.20. Uber: Say you have n numbers 1\u0026hellip;n, and you uniformly sample from this distribution with replacement n times. What is the expected number of distinct values you would draw? 6.21. Goldman Sachs: There are 100 noodles in a bowl. At each step, you randomly select two noodle ends from the bowl and tie them together. What is the expectation on the number of loops formed? 6.22. Morgan Stanley: What is the expected value of the max of two dice rolls? 6.23. Lyft: Derive the mean and variance of the uniform distribution U(a, b). 6.24. Citadel: How many cards would you expect to draw from a standard deck before seeing the first ace? 6.25. Spotify: Say you draw n samples from a uniform distribution U(a, b). What are the MLE estimates of a and b? Hard 6.26. Google: Assume you are drawing from an infinite set of i.i.d random variables that are uniformly distributed from (0, 1). You keep drawing as long as the sequence you are getting is monotonically increasing. What is the expected length of the sequence you draw? 6.27 Facebook: There are two games involving dice that you can play. In the first game, you roll two dice at once and receive a dollar amount equivalent to the product of the rolls. In the second game, you roll one die and get the dollar amount equivalent to the square of that value. Which has the higher expected value and why? 6.28. Google: What does it mean for an estimator to be unbiased? What about consistent? Give examples of an unbiased but not consistent estimator, and a biased but consistent estimator. 6.29. Netflix: What are MLE and MAP? What is the difference between the two? 6.30. Uber: Say you are given a random Bernoulli trial generator. How would you generate values from a standard normal distribution? 6.31. Facebook: Derive the expectation for a geometric random variable. 6.32. Goldman Sachs: Say we have a random variable X ~ D, where D is an arbitrary distribution. What is the distribution F(X) where F is the CDF of X? 6.33. Morgan Stanley: Describe what a moment generating function (MGF) is. Derive the MGF for a normally distributed random variable X. 6.34. Tesla: Say you have N independent and identically distributed draws of an exponential random variable. What is the best estimator for the parameter λ? 6.35. Citadel: Assume that log X ~ N(0, 1). What is the expectation of X? 6.36. Google: Say you have two distinct subsets of a dataset for which you know their means and standard deviations. How do you calculate the blended mean and standard deviation of the total dataset? Can you extend it to K subsets? 6.37. Two Sigma: Say we have two random variables X and Y. What does it mean for X and Y to be independent? What about uncorrelated? Give an example where X and Y are uncorrelated but not independent. 6.38. Citadel: Say we have X ~ Uniform(-1, 1) and Y = X². What is the covariance of X and Y? 6.39. Lyft: How do you uniformly sample points at random from a circle with radius R? 6.40. Two Sigma: Say you continually sample from some i.i.d. uniformly distributed (0, 1) random variables until the sum of the variables exceeds 1. How many samples do you expect to make? 40 Real Statistics Interview Solutions Solution #6.1 The Central Limit Theorem (CLT) states that if any random variable, regardless of distribution, is sampled a large enough number of times, the sample mean will be approximately normally distributed. This allows for studying of the properties for any statistical distribution as long as there is a large enough sample size. The mathematical definition of the CLT is as follows: For any given random variable X, as n approaches infinity, X̄n = (X₁ + \u0026hellip; + Xn) / n ~ N(µ, σ²/n) At any company with a lot of data, like Uber, this concept is core to the various experimentation platforms used in the product. For a real-world example, consider testing whether adding a new feature increases rides booked in the Uber platform, where each Xᵢ is an individual ride and is a Bernoulli random variable (i.e., the rider books or does not book a ride). Then, if the sample size is sufficiently large, we can assess the statistical properties of the total number of bookings, as well as the booking rate (rides booked / rides opened on app). These statistical properties play a key role in hypothesis testing, allowing companies like Uber to decide whether or not to add new features in a data-driven manner.\nSolution #6.2 Suppose we want to estimate some parameters of a population. For example, we might want to estimate the average height of males in the US. Given some data from a sample, we can compute a sample mean for what we think the value is, as well as a range of values around that mean. Following the previous example, we could obtain the heights of 1000 random males in the U.S. and compute the average height, or the sample mean. This sample mean is a type of point estimate, and while useful, will vary from sample to sample. Thus we can\u0026rsquo;t tell anything about the variation in the data around this estimate, which is why we need a range of values through a confidence interval. Confidence intervals are a range of values with a lower and an upper bound such that if you were to sample the parameter of interest a large number of times, the 95% confidence interval would contain the true value of this parameter 95% of the time. We can construct a confidence interval using the sample standard deviation and sample mean. The level of confidence is determined by a margin of error that is set beforehand. The narrower the confidence interval, the more precise is the estimate, since there is less uncertainty associated with the point estimate of the mean.\nSolution #6.3 A/B testing has many possible pitfalls that depend on the particular experiment and setup employed. One common drawback is that groups may not be balanced, possibly resulting in highly skewed results. Note that balance is needed for all dimensions of the groups - like user demographics or device used - because, otherwise, the potentially statistically significant results from the test may simply be due to specific factors that were not controlled for. Two types errors are frequently assessed: Type I error, which is also known as a \u0026ldquo;false positive\u0026rdquo;, and Type II error, also known as a \u0026ldquo;false negative\u0026rdquo;. Specifically, Type I error is rejecting a null hypothesis when that hypothesis is correct, whereas Type II error is failing to reject a null hypothesis when its alternative hypothesis is correct. Another common pitfall is not running an experiment for long enough. Generally speaking, experiments are run with a particular power threshold and significance threshold; however, they often do not stop immediately upon detecting an effect. For an extreme example, assume you\u0026rsquo;re at either Uber or Lyft and running a test for two days, when the metric of interest (e.g., rides booked) is subject to weekly seasonality. Lastly, dealing with multiple tests is important because there may be interactions between results of tests you are running and so properly attributing results may be impossible in simple A/B tests. In addition, as the number of variations you run increases, so does the sample size needed. In practice, while it seems technically feasible to test 1000 variations of a button when optimizing for click-through rate, variations in tests are usually based on some intuitive hypothesis concerning core behavior.\nSolution #6.4 For any given random variables X and Y, the covariance, a linear measure of relationship, is defined by the following: Cov(X, Y) = E[(X - E[X])(Y- E[Y])] = E[XY] - E[X]E[Y] Specifically, covariance indicates the direction of the linear relationship between X and Y and can take on any potential value from negative infinity to infinity. The units of covariance are based on the units of X and Y, which may differ. The correlation between X and Y is the normalized version of covariance that takes into account the variances of X and Y: ρ(X,Y) = Cov(X,Y) / √Var(X)Var(Y) Since correlation results from scaling covariance, it is dimensionless (unlike covariance) and is always between -1 and 1 (also unlike covariance).\nSolution #6.5 The null hypothesis is that the coin is fair, and the alternative hypothesis is that the coin is biased: H₀: p₀ = 0.5, H₁: p₁ ≠ 0.5 Note that, since the sample size here is 10, you cannot apply the Central Limit Theorem and so cannot approximate a binomial using a normal distribution. The p-value here is the probability of observing the results obtained given that the null hypothesis is true, i.e., under the assumption that the coin is fair. In total for 10 flips of a coin, there are 2¹⁰ = 1024 possible outcomes, and in only 10 of them are there 9 tails and one heads. Hence, the exact probability of the given result is the p-value, which is 10/1024 ≈ 0.0098. Therefore, with a significance level set, for example, at 0.05, we can reject the null hypothesis.\nSolution #6.6 The process of testing whether data supports particular hypotheses is called hypothesis testing and involves measuring parameters of a population\u0026rsquo;s probability distribution. This process typically employs at least two groups, one a control that receives no treatment and the other(s), which do receive the treatment(s) of interest. Examples could be the height of two groups of people, the conversion rates for particular user flows in a product, etc. Testing also involves two hypotheses, the null hypothesis, which assumes no significant difference between the groups, and the alternative hypothesis, which assumes a significant difference in the measured parameter(s) as a consequence of the treatment. A p-value is the probability of observing the given test results under the null hypothesis assumptions. The lower this probability, the higher the chance that the null hypothesis should be rejected. If the p-value is lower than the pre-determined significance level α, generally set at 0.05, then it indicates that the null hypothesis should be rejected in favor of the alternative hypothesis. Otherwise, the null hypothesis cannot be rejected, and it cannot be concluded that the treatment has any significant effect.\nSolution #6.7 Both errors are relevant in the context of hypothesis testing. Type I error is when one rejects the null hypothesis when it is correct, and is known as a false positive. Type II error is when the null hypothesis is not rejected when the alternative hypothesis is correct; this is known as a false negative. In layman\u0026rsquo;s terms, a type I error is when we detect a difference, when in reality there is no significant difference in an experiment. Similarly, a type II error occurs when we fail to detect a difference, when in reality there is a significant difference in an experiment. Type I error is given by the level of significance α, whereas the type II error is given by β. Usually, 1-α is referred to as the confidence level, whereas 1-β is referred to as the statistical power of the test being conducted. Note that, in any well-conducted statistical procedure, we want to have both α and β be small. However, based on the definition of the two, it is impossible to make both errors small simultaneously: the larger α is, the smaller β is. Based on the experiment and the relative importance of false positives and false negatives, a Data Scientist must decide what thresholds to adopt for any given experiment. Note that experiments are set up so as to have both 1-α and 1-β relatively high (say at .95, and .8 respectively).\nSolution #6.8 Power is the probability of rejecting the null hypothesis when, in fact, it is false. It is also the probability of avoiding a Type II error. A Type II error occurs when the null hypothesis is not rejected when the alternative hypothesis is correct. This is important because we want to detect significant effects during experiments. That is, the higher the statistical power of the test, the higher the probability of detecting a genuine effect (i.e, accepting the alternative hypothesis and rejecting the null hypothesis). A minimum sample size can be calculated for any given level of power - for example, say a power level of 0.8. An analysis of the statistical power of a test is usually performed with respect to the test\u0026rsquo;s level of significance (α) and effect size (i.e., the magnitude of the results).\nSolution #6.9 In a Z-test, your test statistic follows a normal distribution under the null hypothesis. Alternatively, in a t-test, you employ a student\u0026rsquo;s t-distribution rather than a normal distribution as your sampling distribution. Considering the population mean, we can use either Z-test or t-test only if the mean is normally distributed, which is possible in two cases: the initial population is normally distributed, or the sample size is large enough (n ≥ 30) so we can apply the Central Limit Theorem. If the condition above is satisfied, then we need to decide which type of test is more appropriate to use. In general, we use Z-tests if the population variation is known, and vice versa: we use t-test if the population variation is unknown. Additionally, if the sample size is very large (n \u0026gt; 200), we can use the Z-test in any case, since for such large degrees of freedom, t-distribution coincides with z-distribution up to thousands. Considering the population proportion, we can use Z-test (but not t-test) for it in the case and np₀ ≥ 10 and n(1-p₀) ≥ 10, i.e., when each of the number of successes and the number of failures is at least 10.\nSolution #6.10 The primary consideration is that, as the number of tests increases, the chance that a stand-alone p-value for any of the t-tests is statistically significant becomes very high due to chance alone. As an example, with 100 tests performed and a significance threshold of α = 0.05, you would expect 5 of the experiments to be statistically significant due only to chance. That is, you have a very high probability of observing at least one significant outcome. Therefore, the chance of incorrectly rejecting a null hypothesis (i.e., committing Type I error) increases. To correct for this effect, we can use a method called the Bonferroni correction, wherein we set the significance threshold to α/m, where m is the number of tests being performed. In the above scenario having 100 tests, we can set the significance threshold to instead be 0.05/100 = 0.0005. While this correction helps to protect from Type I error, it is still prone to Type II error (i.e., failing to reject the null hypothesis when it should be rejected). In general, the Bonferroni correction is mostly useful when there is a smaller number of multiple comparisons of which a few are significant. If the number becomes sufficiently high that many tests yield statistically significant results, the number of Type II errors may also increase significantly.\nSolution #6.11 The confidence interval (CI) for a population proportion is an interval that includes a true population proportion with a certain degree of confidence 1 - α. For the case of flipping heads from a series of coin tosses, the proportion follows the binomial distribution. If the series size is large enough (each of the number of successes and the number of failures is at least 10), we can utilize the Central Limit Theorem and use the normal approximation for the binomial distribution, meaning the sample proportion p̂ is approximately N(p, p(1-p)/n). We estimate this as N(p̂, p̂(1-p̂)/n). where p̂ is the proportion of heads tossed in series, and n is the series size. The CI is centered at the series proportion, and plus or minus a margin of error: p̂ ± z_(α/2) * √(p̂(1-p̂)/n) where z_(α/2) is the appropriate value from the standard normal distribution for the desired confidence level. For example, for the most commonly used level of confidence 95%, z_(α/2) = 1.96.\n","permalink":"https://deepskandpal.github.io/bookshelf/201-ds-interview/chapter-6/","tags":null,"title":"Chapter 6: Statistics"},{"categories":null,"content":"Introduction - Inspiration from Nature The chapter beautifully starts by reminding us how nature has often inspired human inventions: birds inspired planes, burdock plants inspired Velcro. So, it\u0026rsquo;s logical to look at the brain\u0026rsquo;s architecture for inspiration on building intelligent machines. This is the core idea that sparked ANNs.\nANNs vs. Biological Neurons: While ANNs were inspired by the networks of biological neurons in our brains, they have evolved to be quite different. Just like planes don\u0026rsquo;t flap their wings, ANNs don\u0026rsquo;t need to be biologically perfectly realistic to be effective. The footnote mentions a good philosophy: be open to biological inspiration but don\u0026rsquo;t be afraid to create biologically unrealistic models if they work well. Some researchers even prefer calling the components \u0026ldquo;units\u0026rdquo; rather than \u0026ldquo;neurons\u0026rdquo; to avoid this restrictive analogy.\nThe Power of ANNs:\nThey are at the heart of Deep Learning. Versatile, powerful, and scalable. Ideal for large, complex tasks like: Image classification (Google Images) Speech recognition (Apple\u0026rsquo;s Siri) Recommendation systems (YouTube) Game playing (DeepMind\u0026rsquo;s AlphaGo) Chapter Structure:\nPart 1: Introduces ANNs, starting from early architectures and leading up to Multilayer Perceptrons (MLPs), which are heavily used today. Part 2: Focuses on implementing neural networks using the Keras API. Keras is described as a \u0026ldquo;beautifully designed and simple high-level API\u0026rdquo; for building, training, evaluating, and running neural networks. (Page 280: From Biological to Artificial Neurons - A Brief History) Early Beginnings (1943): ANNs are surprisingly old! They were first introduced by neurophysiologist Warren McCulloch and mathematician Walter Pitts. Their landmark paper proposed a simplified computational model of how biological neurons might perform complex computations using propositional logic. This was the first ANN architecture.\nThe First \u0026ldquo;AI Winter\u0026rdquo; (1960s-1970s): Early successes led to widespread belief in imminent truly intelligent machines. When this didn\u0026rsquo;t materialize quickly, funding dried up, and ANNs entered a long \u0026ldquo;winter.\u0026rdquo;\nRevival (1980s - Connectionism): New architectures and better training techniques sparked renewed interest. However, progress was slow.\nThe Second \u0026ldquo;AI Winter\u0026rdquo; (1990s): Other ML techniques like Support Vector Machines (Chapter 5) emerged, seeming to offer better results and stronger theoretical foundations, pushing ANNs to the background again.\nThe Current Wave (Now!): We\u0026rsquo;re in another, much stronger wave of interest in ANNs. Why is this time different?\nHuge Quantity of Data: We now have vast amounts of data to train large neural networks (e.g., ImageNet). ANNs often outperform other ML techniques on very large and complex problems. Tremendous Increase in Computing Power: Moore\u0026rsquo;s Law (components in circuits doubling roughly every 2 years). Powerful GPUs (Graphics Processing Units), initially driven by the gaming industry, are exceptionally good at the kind of parallel computations needed for ANNs. Cloud platforms make this power accessible to everyone. Improved Training Algorithms: While often only slight tweaks from 1990s algorithms, these have had a huge positive impact (e.g., better optimization algorithms, initialization techniques, regularization). Theoretical Limitations Turning Benign: Fears that ANNs would always get stuck in poor local optima have largely proven less of an issue in practice. When they do get stuck, the local optima are often fairly close to the global optimum. Virtuous Circle of Funding and Progress: Amazing products based on ANNs make headlines (AlphaGo, GPT-3/4, etc.), attracting more attention, funding, and talent, leading to further progress. (Page 281-283: Biological Neurons and a Simple Artificial Neuron Model) Biological Neurons (Figure 10-1, page 282):\nA quick look at the structure: cell body (soma), dendrites (receive signals), axon (transmits signals), synaptic terminals (connect to other neurons). Neurons produce electrical impulses (action potentials). When a neuron receives enough neurotransmitter signals at its synapses within a short period, it \u0026ldquo;fires\u0026rdquo; its own impulse. Some neurotransmitters are excitatory (encourage firing), some are inhibitory. Individual neurons are relatively simple, but billions of them, each connected to thousands of others, form a vast network capable of highly complex computations. The brain\u0026rsquo;s architecture, especially the layered structure of the cerebral cortex (Figure 10-2), provides inspiration. Logical Computations with Artificial Neurons (McCulloch \u0026amp; Pitts Model - Page 283):\nTheir early model was very simple: Binary (on/off) inputs. One binary (on/off) output. The neuron activates its output if a certain number of its inputs are active. Figure 10-3 shows how such simple neurons can perform basic logical computations (assuming activation if at least two inputs are active): Identity (C=A): Neuron A sends two signals to C. If A is on, C gets two active inputs and turns on. Logical AND (C = A ∧ B): C activates only if both A and B are active (one active input isn\u0026rsquo;t enough). Logical OR (C = A ∨ B): C activates if A is active, or B is active, or both (any one provides two inputs to a common intermediate neuron, which then activates C, or if A and B both directly input to C and one active input is enough, though the diagram is a bit more complex). The diagram shows intermediate neurons. The idea is that if A is active, it can trigger enough input for C to fire, same for B. Complex Logic (e.g., A AND NOT B): If we assume an input can inhibit activity, this is also possible. If A is active and B is off, C activates. If B is on, it inhibits C. What this was ultimately trying to achieve: To show that even a very simplified model of a neuron, when networked, could perform fundamental logical computations, suggesting a path towards building computational intelligence. (Page 284-288: The Perceptron) Invented by Frank Rosenblatt in 1957. One of the simplest ANN architectures.\nBased on a Threshold Logic Unit (TLU) or Linear Threshold Unit (LTU) (Figure 10-4, page 284):\nInputs \u0026amp; Output: Numbers (not just binary on/off). Weights: Each input connection i has an associated weight wᵢ. Weighted Sum: The TLU computes a weighted sum of its inputs: z = w₁x₁ + w₂x₂ + ... + wₙxₙ = wᵀx. Step Function: It then applies a step function to this sum z to produce the output: h_w(x) = step(z). What the TLU is ultimately trying to achieve: It makes a decision based on whether a weighted combination of evidence (z) exceeds some threshold. Common Step Functions (Equation 10-1, page 285):\nHeaviside step function: Outputs 0 if z \u0026lt; 0, outputs 1 if z ≥ 0 (assuming threshold is 0). Sign function: Outputs -1 if z \u0026lt; 0, 0 if z = 0, +1 if z \u0026gt; 0. Single TLU for Classification:\nA single TLU can perform simple linear binary classification. It\u0026rsquo;s very similar to a Logistic Regression or linear SVM classifier, but with a hard threshold output instead of a probability or a margin. Example: Classify Iris flowers based on petal length and width. You\u0026rsquo;d add a bias feature x₀=1. Training means finding weights w₀, w₁, w₂. Perceptron Architecture (Figure 10-5, page 286): A Perceptron is typically a single layer of TLUs. Each TLU in this layer is connected to all inputs. This is a fully connected layer (or dense layer). Inputs are fed through special \u0026ldquo;passthrough\u0026rdquo; input neurons. A bias neuron (always outputting 1) is usually added and connected to each TLU, providing the bias term w₀ (or b in the new notation used later). The Perceptron in Figure 10-5 has 2 inputs, 1 bias neuron, and 3 output TLUs. This can classify instances into three different binary classes simultaneously (making it a multioutput classifier). For example, output 1 could be \u0026ldquo;is it a cat?\u0026rdquo;, output 2 \u0026ldquo;is it a dog?\u0026rdquo;, output 3 \u0026ldquo;is it a bird?\u0026rdquo;. An input could be classified as a cat AND a bird if both TLUs fire (though that specific example isn\u0026rsquo;t ideal for mutually exclusive animal classes). Computing Outputs for a Layer (Equation 10-2, page 286): For a whole layer of artificial neurons, for several instances at once: h_W,b(X) = φ(XW + b)\nX: Matrix of input features (instances x features). W: Weight matrix (input neurons x artificial neurons in the layer). Contains connection weights excluding bias. b: Bias vector (one bias term per artificial neuron in the layer). XW + b: Computes the weighted sum z for every neuron and every instance. φ (phi): The activation function. For TLUs, this is a step function. What this equation is ultimately trying to achieve: Efficiently calculate the output of every neuron in a layer for every instance in a batch of data, using matrix multiplication. Perceptron Training (Hebbian Learning \u0026amp; Perceptron Learning Rule - Page 286):\nInspired by Hebb\u0026rsquo;s Rule (\u0026ldquo;Cells that fire together, wire together\u0026rdquo;): When neuron A often triggers neuron B, the connection between them strengthens. Perceptrons use a variant: The Perceptron learning rule reinforces connections that help reduce the error. Process: Feed one training instance at a time. For each instance, make predictions. For every output neuron that produced a wrong prediction, reinforce the connection weights from the inputs that would have contributed to the correct prediction. Equation 10-3 (Weight Update Rule): wᵢⱼ⁽ⁿᵉˣᵗ ˢᵗᵉᵖ⁾ = wᵢⱼ + η(yⱼ - ŷⱼ)xᵢ wᵢⱼ: Weight between i-th input and j-th output neuron. η (eta): Learning rate. yⱼ: Target output for j-th neuron. ŷⱼ: Predicted output for j-th neuron. xᵢ: Value of i-th input for the current instance. What this rule is ultimately trying to achieve: If ŷⱼ is correct (yⱼ - ŷⱼ = 0), weights don\u0026rsquo;t change. If ŷⱼ is wrong: If yⱼ=1 and ŷⱼ=0 (neuron should have fired but didn\u0026rsquo;t): yⱼ - ŷⱼ = 1. Weights wᵢⱼ are increased if xᵢ was positive (strengthening connections that should have contributed to firing). If yⱼ=0 and ŷⱼ=1 (neuron fired but shouldn\u0026rsquo;t have): yⱼ - ŷⱼ = -1. Weights wᵢⱼ are decreased if xᵢ was positive (weakening connections that wrongly contributed to firing). Perceptron Convergence Theorem (Page 287): If training instances are linearly separable, Rosenblatt showed this algorithm would converge to a solution (a set of weights that separates the classes).\nScikit-Learn Perceptron class: Implements a single-TLU network. from sklearn.linear_model import Perceptron per_clf = Perceptron() per_clf.fit(X, y)\nThe book notes this is equivalent to SGDClassifier(loss=\u0026quot;perceptron\u0026quot;, learning_rate=\u0026quot;constant\u0026quot;, eta0=1, penalty=None). Unlike Logistic Regression, Perceptrons output hard predictions (0 or 1), not probabilities. This is one reason to prefer Logistic Regression. Limitations of Perceptrons (Minsky \u0026amp; Papert, 1969 - Page 288):\nHighlighted serious weaknesses, famously that Perceptrons (being linear classifiers) cannot solve some trivial problems like the Exclusive OR (XOR) problem (Figure 10-6, left). XOR is not linearly separable. This disappointment led to another decline in ANN research (part of the first AI winter). (Page 288-293: The Multilayer Perceptron (MLP) and Backpropagation) Overcoming Perceptron Limitations: Stacking Perceptrons (Page 288):\nLimitations can be overcome by stacking multiple layers of Perceptrons. The resulting ANN is a Multilayer Perceptron (MLP). Figure 10-6 (right) shows an MLP that can solve the XOR problem. It uses an intermediate \u0026ldquo;hidden\u0026rdquo; layer of neurons. What the MLP is ultimately trying to achieve: By having hidden layers, MLPs can learn more complex, non-linear decision boundaries. The hidden layers can transform the input features into a new representation where the problem becomes linearly separable for the output layer. MLP Architecture (Figure 10-7, page 289): One (passthrough) input layer. One or more layers of TLUs, called hidden layers. One final layer of TLUs called the output layer. Layers near input are \u0026ldquo;lower layers\u0026rdquo;; layers near output are \u0026ldquo;upper layers.\u0026rdquo; Every layer (except output) usually includes a bias neuron and is fully connected to the next layer. Signal flows one way (input -\u0026gt; output): Feedforward Neural Network (FNN). Deep Neural Network (DNN): An ANN with a \u0026ldquo;deep\u0026rdquo; stack of hidden layers (definition of \u0026ldquo;deep\u0026rdquo; is fuzzy but generally means more than one or two these days). Deep Learning studies DNNs. Training MLPs: The Backpropagation Algorithm (Page 289-290): For many years, training MLPs was a major challenge. In 1986, Rumelhart, Hinton, and Williams published the backpropagation training algorithm, still fundamental today.\nWhat it is: Essentially, it\u0026rsquo;s Gradient Descent (Chapter 4) applied to an MLP, using an efficient technique to compute all the necessary gradients. How it computes gradients (Autodiff - sidebar, page 290): It uses reverse-mode automatic differentiation (autodiff). In just two passes through the network (one forward, one backward), it can compute the gradient of the network\u0026rsquo;s error with respect to every single model parameter (all weights and biases in all layers). This tells us how each weight/bias should be tweaked to reduce the error. Once gradients are computed, it performs a regular Gradient Descent step. Repeat until convergence. Backpropagation Algorithm in More Detail (Page 290):\nMini-batch Processing: Handles one mini-batch of training instances at a time. Goes through the full training set multiple times; each full pass is an epoch. Forward Pass: Pass the mini-batch to the input layer. Compute outputs of neurons in the first hidden layer. Pass these outputs to the next layer, compute its outputs, and so on, until the output layer. This is like making predictions, but all intermediate results (activations of all neurons) are preserved because they are needed for the backward pass. Measure Error: Use a loss function (e.g., MSE for regression, Cross-Entropy for classification) to compare the network\u0026rsquo;s output with the desired output (true labels). Backward Pass (Propagating Error Gradients): Compute how much each output connection contributed to the error (using the chain rule of calculus). Propagate these error contributions backward: Measure how much connections in the layer below contributed to the output layer\u0026rsquo;s error contributions (again, using chain rule). Continue backward until the input layer is reached. This reverse pass efficiently measures the error gradient across all connection weights. Gradient Descent Step: Tweak all connection weights in the network using the computed error gradients to reduce the overall error. Key Change for Backpropagation: Activation Functions (Page 291):\nThe original Perceptron used a step function. Step functions have flat segments (zero gradient), so Gradient Descent gets stuck. Crucial innovation for MLPs: Replace the step function with a differentiable activation function, like the logistic (sigmoid) function σ(z) = 1 / (1 + exp(-z)). It has a well-defined, non-zero derivative everywhere, allowing GD to make progress. Other Popular Activation Functions (Figure 10-8, page 292): Hyperbolic Tangent (tanh): tanh(z) = 2σ(2z) - 1. S-shaped, continuous, differentiable. Output range: -1 to 1 (vs. 0 to 1 for sigmoid). Centering output around 0 often helps speed up convergence at start of training. Rectified Linear Unit (ReLU): ReLU(z) = max(0, z). Outputs 0 if z \u0026lt; 0, outputs z if z ≥ 0. Continuous, but not differentiable at z=0 (slope changes abruptly). In practice, GD still works (can use a subgradient or just assume gradient is 0 or 1 at z=0). Derivative is 0 for z \u0026lt; 0. Advantages: Fast to compute. Has become the default in many cases. No maximum output value (helps with some GD issues like vanishing gradients, Ch 11). The footnote on page 292 notes that ReLU, despite being less biologically plausible than sigmoids, often works better in ANNs – a case where the biological analogy can be misleading. Why Activation Functions? (Page 292):\nIf you chain several linear transformations, all you get is another linear transformation. f(x) = 2x + 3, g(x) = 5x - 1 =\u0026gt; f(g(x)) = 2(5x - 1) + 3 = 10x + 1 (still linear). If you have no nonlinearity between layers, even a deep stack of layers is equivalent to a single linear layer. It can\u0026rsquo;t solve complex non-linear problems. Nonlinear activation functions are essential for giving ANNs the power to approximate complex, non-linear functions. A large enough DNN with nonlinear activations can theoretically approximate any continuous function. (Page 292-294: Regression and Classification MLPs) Now that we know the architecture and training algorithm (backpropagation), what can we do?\nRegression MLPs (Page 292-293):\nSingle value prediction (e.g., house price): Need a single output neuron. Multivariate regression (e.g., 2D coordinates for object center): One output neuron per output dimension. (4 output neurons if predicting bounding box: x, y, width, height). Output Layer Activation: Usually no activation function for output neurons in regression (so they can output any range of values). If output must be positive: Use ReLU or softplus (log(1 + exp(z)), a smooth ReLU variant). If output must be in a specific range: Use logistic (for 0-1) or tanh (for -1 to 1) and scale labels accordingly. Loss Function: Typically MSE. If many outliers: Prefer MAE or Huber loss (quadratic for small errors, linear for large errors – less sensitive to outliers than MSE but converges faster than MAE). Table 10-1 (Typical Regression MLP Architecture): Summarizes typical choices for number of neurons, layers, activations, and loss. Classification MLPs (Page 294):\nBinary classification: Single output neuron, logistic (sigmoid) activation function. Output is probability of positive class. Multilabel binary classification (e.g., email is spam/ham AND urgent/non-urgent): One output neuron per positive class label (e.g., one for \u0026ldquo;is spam,\u0026rdquo; one for \u0026ldquo;is urgent\u0026rdquo;). Each uses logistic activation. Output probabilities don\u0026rsquo;t necessarily sum to 1 (an email can be \u0026ldquo;not spam\u0026rdquo; and \u0026ldquo;urgent\u0026rdquo;). Multiclass classification (mutually exclusive classes, e.g., digits 0-9): One output neuron per class. Use softmax activation function for the whole output layer (as in Chapter 4). This ensures probabilities are between 0-1 and sum to 1. Figure 10-9 shows a modern MLP for classification (ReLU in hidden layers, softmax in output). Loss Function: Cross-entropy loss (log loss, as in Chapter 4) is generally a good choice when predicting probability distributions. Table 10-2 (Typical Classification MLP Architecture): Summarizes typical choices. Phew! That\u0026rsquo;s a dense introduction to the historical context, the biological inspiration (and divergence from it), the basic Perceptron, the jump to Multilayer Perceptrons, the crucial backpropagation algorithm, and how MLPs are structured for regression and classification.\nThe key takeaway is that MLPs are layered networks of simple processing units (neurons), where hidden layers learn increasingly complex representations of the input, enabled by non-linear activation functions and trained by backpropagation (Gradient Descent with efficient gradient calculation).\nExcellent! That detour through \u0026ldquo;The Matrix Calculus You Need For Deep Learning\u0026rdquo; was intense but hopefully gave you a much deeper appreciation for what\u0026rsquo;s happening when we say a neural network \u0026ldquo;learns\u0026rdquo; by minimizing a loss function using gradients. You now have a good intuitive (and even some mathematical) backing for how those weight and bias updates are calculated for individual neurons via the chain rule.\n(Page 295-306: Implementing MLPs with Keras) This is where the practical fun begins! We\u0026rsquo;ve talked a lot about the \u0026ldquo;what\u0026rdquo; and \u0026ldquo;why\u0026rdquo; of neural networks; now we get to the \u0026ldquo;how\u0026rdquo; of actually building and training them using a popular library.\nKeras: A High-Level Deep Learning API (Page 295):\nKeras allows you to easily build, train, evaluate, and run all sorts of neural networks. It was developed by François Chollet and is known for its ease of use, flexibility, and beautiful design. Backend Reliance: Keras itself doesn\u0026rsquo;t do the heavy numerical computations. It relies on a computation backend. Popular choices include: TensorFlow Microsoft Cognitive Toolkit (CNTK) Theano (though its development has largely ceased) The book refers to the original, multi-backend implementation as multibackend Keras. tf.keras: Since late 2016/2017, TensorFlow has bundled its own Keras implementation called tf.keras. This is what the book (and most of the community now) uses. It only supports TensorFlow as a backend but offers extra TensorFlow-specific features (like the Data API for efficient data loading, which we\u0026rsquo;ll see later). Figure 10-10 (page 296) illustrates these two Keras API implementations. PyTorch (Page 296):\nAnother very popular Deep Learning library from Facebook. Its API is quite similar to Keras (both inspired by Scikit-Learn and Chainer). Gained immense popularity due to its simplicity and excellent documentation, especially compared to TensorFlow 1.x. TensorFlow 2.x (which uses tf.keras as its official high-level API) has significantly improved, making it just as simple as PyTorch in many respects. Healthy competition is good! Installing TensorFlow 2 (Page 296):\nThe book assumes you\u0026rsquo;ve followed Chapter 2\u0026rsquo;s setup for Jupyter and Scikit-Learn. You\u0026rsquo;d typically use pip install -U tensorflow. The bird icon notes that for GPU support, you might need tensorflow-gpu and extra libraries (though this is evolving, and TensorFlow aims for a single library). Chapter 19 will cover GPUs. Test installation by importing tensorflow as tf and from tensorflow import keras, then printing tf.__version__ and keras.__version__. Now, let\u0026rsquo;s build an image classifier!\nBuilding an Image Classifier Using the Sequential API (Page 297-301)\nWe\u0026rsquo;ll use the Fashion MNIST dataset.\nA drop-in replacement for MNIST (introduced in Chapter 3).\nSame format: 70,000 grayscale images of 28x28 pixels, 10 classes.\nImages are fashion items (T-shirt, trouser, coat, etc.) instead of handwritten digits.\nMore challenging than MNIST (e.g., a simple linear model gets ~92% on MNIST but only ~83% on Fashion MNIST).\nUsing Keras to Load the Dataset (Page 297): fashion_mnist = keras.datasets.fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\nDifference from Scikit-Learn\u0026rsquo;s fetch_openml for MNIST: Images are 28x28 arrays (not flattened 784-element vectors). Pixel intensities are integers (0-255), not floats. X_train_full.shape is (60000, 28, 28). Data Preparation (Page 298):\nCreate a validation set: The loaded data is split into train and test only. We need a validation set for monitoring training and hyperparameter tuning. X_valid, X_train = X_train_full[:5000], X_train_full[5000:] y_valid, y_train = y_train_full[:5000], y_train_full[5000:] (First 5000 instances for validation, rest for training). Scale input features: Neural networks with Gradient Descent require feature scaling. We\u0026rsquo;ll scale pixel intensities from 0-255 down to the 0-1 range by dividing by 255.0 (this also converts them to floats). X_valid, X_train = X_valid / 255.0, X_train / 255.0 (Note: It\u0026rsquo;s generally better to scale the test set using parameters derived from the training set, e.g., (X_test - X_train_mean) / X_train_std. But for pixel values 0-255, dividing by 255.0 is a common and simple approach.) Class Names: For Fashion MNIST, labels are numbers (0-9). We need a list of class names to interpret them: class_names = [\u0026quot;T-shirt/top\u0026quot;, \u0026quot;Trouser\u0026quot;, ..., \u0026quot;Ankle boot\u0026quot;] class_names[y_train[0]] might give 'Coat'. Figure 10-11 shows sample images from Fashion MNIST. Creating the Model Using the Sequential API (Page 299): This is the simplest way to build a Keras model: a linear stack of layers. We\u0026rsquo;ll build a classification MLP with two hidden layers.\nmodel = keras.models.Sequential() model.add(keras.layers.Flatten(input_shape=[28, 28])) model.add(keras.layers.Dense(300, activation=\u0026quot;relu\u0026quot;)) model.add(keras.layers.Dense(100, activation=\u0026quot;relu\u0026quot;)) model.add(keras.layers.Dense(10, activation=\u0026quot;softmax\u0026quot;))\nLet\u0026rsquo;s break this down:\nmodel = keras.models.Sequential(): Creates a Sequential model, which is just a stack of layers. model.add(keras.layers.Flatten(input_shape=[28, 28])): This is the first layer. Its role is to take each input image (28x28 array) and flatten it into a 1D array (of 784 pixels). X.reshape(-1, 1) was mentioned, but a common operation in NNs is X.reshape(batch_size, -1). Keras Flatten layer handles this conversion. It has no parameters to learn; it\u0026rsquo;s just a preprocessing step. input_shape=[28, 28]: Since it\u0026rsquo;s the first layer, you must specify the shape of the input instances (excluding the batch size). model.add(keras.layers.Dense(300, activation=\u0026quot;relu\u0026quot;)): Adds a Dense (fully connected) hidden layer with 300 neurons. activation=\u0026quot;relu\u0026quot;: Specifies the ReLU activation function for these neurons. Each Dense layer manages its own weight matrix (W) and bias vector (b). When it receives input, it computes XW + b (Equation 10-2 from the book). model.add(keras.layers.Dense(100, activation=\u0026quot;relu\u0026quot;)): Adds a second Dense hidden layer with 100 neurons, also using ReLU. model.add(keras.layers.Dense(10, activation=\u0026quot;softmax\u0026quot;)): Adds a Dense output layer with 10 neurons (one for each class, 0-9). activation=\u0026quot;softmax\u0026quot;: Uses the softmax activation function because the classes are exclusive (an item belongs to only one class). Softmax will ensure the outputs are probabilities that sum to 1. Alternative Sequential Model Creation (Page 300): You can also pass a list of layers directly to the Sequential constructor: model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=\u0026quot;relu\u0026quot;), keras.layers.Dense(100, activation=\u0026quot;relu\u0026quot;), keras.layers.Dense(10, activation=\u0026quot;softmax\u0026quot;) ]) Model Summary (Page 300-301): model.summary() displays all the model\u0026rsquo;s layers:\nLayer name (auto-generated or custom). Output shape (None for batch size means it can be anything). Number of parameters. Flatten: Output shape (None, 784), 0 params. dense (first hidden layer): Output (None, 300). Params: (784 inputs * 300 neurons) + 300 biases = 235,200 + 300 = 235,500. dense_1 (second hidden): Output (None, 100). Params: (300 inputs * 100 neurons) + 100 biases = 30,000 + 100 = 30,100. dense_2 (output): Output (None, 10). Params: (100 inputs * 10 neurons) + 10 biases = 1,000 + 10 = 1,010. Total params: 266,610. All are trainable. This gives the model a lot of flexibility but also risks overfitting if data is scarce. Accessing Layers and Weights (Page 301):\nmodel.layers gives a list of layers. hidden1 = model.layers[1] hidden1.name model.get_layer('dense') (if name is \u0026lsquo;dense\u0026rsquo;) weights, biases = hidden1.get_weights() gets the layer\u0026rsquo;s parameters. Weights are initialized randomly (to break symmetry for backpropagation). Biases are initialized to zeros (which is fine). You can set custom initializers for weights (kernel_initializer) or biases (bias_initializer) when creating the layer. (More in Ch 11). When input_shape is Determined (Bird Icon, page 302): It\u0026rsquo;s best to specify input_shape for the first layer. If you don\u0026rsquo;t, Keras waits until it sees actual data (e.g., during fit()) or until you call model.build() to build the layers (i.e., create their weights). Before that, layers won\u0026rsquo;t have weights, and model.summary() or saving the model might not work.\n(Page 302-306: Compiling, Training, Evaluating, and Predicting) Compiling the Model (Page 302): After creating the model, you must call compile() to specify:\nLoss function Optimizer Optionally, extra metrics to compute during training/evaluation. model.compile(loss=\u0026quot;sparse_categorical_crossentropy\u0026quot;, optimizer=\u0026quot;sgd\u0026quot;, metrics=[\u0026quot;accuracy\u0026quot;])\nloss=\u0026quot;sparse_categorical_crossentropy\u0026quot;: We use this because our labels (y_train) are \u0026ldquo;sparse\u0026rdquo; – just target class indices (0 to 9). And the classes are exclusive. If labels were one-hot encoded (e.g., class 3 is [0,0,0,1,0,0,0,0,0,0]), we\u0026rsquo;d use loss=\u0026quot;categorical_crossentropy\u0026quot;. If binary classification (output layer with sigmoid), we\u0026rsquo;d use loss=\u0026quot;binary_crossentropy\u0026quot;. The bird icon (page 302) notes you can use full Keras objects too: loss=keras.losses.sparse_categorical_crossentropy. optimizer=\u0026quot;sgd\u0026quot;: This means use simple Stochastic Gradient Descent. Keras will perform backpropagation (reverse-mode autodiff + Gradient Descent). More advanced optimizers in Chapter 11. Important (bird icon, page 303): For SGD, tuning the learning rate is crucial. You\u0026rsquo;d typically use optimizer=keras.optimizers.SGD(learning_rate=...) instead of the string \u0026quot;sgd\u0026quot; (which defaults to lr=0.01). metrics=[\u0026quot;accuracy\u0026quot;]: Since it\u0026rsquo;s a classifier, we want to track accuracy during training and evaluation. Training and Evaluating the Model (Page 303-304): Call fit(): history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\nPass input features (X_train) and target classes (y_train). epochs=30: Number of times to iterate over the entire training dataset. (Defaults to 1, which is usually not enough). validation_data=(X_valid, y_valid): Optional. Keras will measure loss and metrics on this validation set at the end of each epoch. Very useful to see how well the model is generalizing and to detect overfitting. Output during training: For each epoch, Keras displays: Progress bar. Mean training time per sample. Loss and accuracy on the training set (average over the epoch). Loss and accuracy on the validation set (at the end of the epoch). The example output shows training loss decreasing and validation accuracy reaching ~89% after 30 epochs. Training and validation accuracy are close, so not much overfitting. The bird icon (page 304) mentions validation_split=0.1 as an alternative to validation_data, to use the last 10% of training data for validation (before shuffling). Also mentions class_weight (to give more importance to underrepresented classes) and sample_weight (for per-instance weighting) arguments in fit(). Learning Curves (Page 304-305):\nfit() returns a History object. history.history is a dictionary containing the loss and metrics measured at the end of each epoch (e.g., loss, accuracy, val_loss, val_accuracy). You can use this to plot learning curves with pandas and Matplotlib (Figure 10-12, page 305). The plot shows training/validation accuracy increasing and loss decreasing. Validation curves are close to training curves, confirming little overfitting. The book notes that validation metrics are computed at the end of an epoch, while training metrics are a running mean during the epoch. So, for a fair comparison, the training curve should be shifted left by half an epoch. Further Training and Hyperparameter Tuning (Page 306):\nIf validation loss is still decreasing (as in the example), the model hasn\u0026rsquo;t fully converged. You can call fit() again; Keras continues training from where it left off. If not satisfied, tune hyperparameters: Learning rate (most important first check). Try another optimizer (and retune learning rate). Number of layers, neurons per layer, activation functions. Batch size (in fit(), defaults to 32). Evaluating on the Test Set (Page 306):\nOnce satisfied with validation accuracy, evaluate on the test set to estimate generalization error. model.evaluate(X_test, y_test) Returns [loss, accuracy]. Example gives ~88.5% accuracy. Common to get slightly lower performance on test set than validation (since HPs tuned on validation). Resist tweaking HPs based on test set results! Using the Model to Make Predictions (Page 206-207): predict() method for new instances. X_new = X_test[:3] y_proba = model.predict(X_new)\ny_proba contains one probability per class for each instance (from the softmax output layer). Example: [[0. , ..., 0.03, ..., 0.96], ...] for the first image (96% prob for class 9 \u0026lsquo;Ankle boot\u0026rsquo;, 3% for class 5 \u0026lsquo;Sandal\u0026rsquo;). To get the class with the highest probability: y_pred = model.predict_classes(X_new) (Note: predict_classes is deprecated in newer TensorFlow/Keras; use np.argmax(model.predict(X_new), axis=-1) instead). This might give array([9, 2, 1]). Figure 10-13 (page 307) shows these three test images, which were correctly classified. That\u0026rsquo;s a complete walkthrough of building, training, and using a classification MLP with Keras\u0026rsquo;s Sequential API! The process is quite streamlined.\nGreat! It\u0026rsquo;s satisfying to see how those theoretical concepts translate into a working Keras model.\nLet\u0026rsquo;s continue with Chapter 10, moving on to how we\u0026rsquo;d build a regression MLP with Keras and then explore more advanced ways to define model architectures.\n(Page 307-308: Building a Regression MLP Using the Sequential API) We\u0026rsquo;ve just built an image classifier. Now, let\u0026rsquo;s switch to a regression task: predicting California housing prices, similar to what we did in Chapter 2, but this time with a neural network.\nLoading the Data:\nFor simplicity, the book uses Scikit-Learn\u0026rsquo;s fetch_california_housing() to load the data.\nThis version is simpler than the one in Chapter 2: only numerical features, no missing values.\nStandard train-validation-test split and feature scaling (using StandardScaler) are performed as usual. from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler\nhousing = fetch_california_housing() X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target) X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\nscaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_valid = scaler.transform(X_valid) X_test = scaler.transform(X_test)\nBuilding the Regression MLP (Page 308): The process is very similar to the classification MLP, with a few key differences:\nmodel = keras.models.Sequential([ keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;, input_shape=X_train.shape[1:]), keras.layers.Dense(1) # Output layer ])\nOutput Layer: Has a single neuron (because we\u0026rsquo;re predicting a single value – the housing price). Uses no activation function (or you could say a \u0026ldquo;linear\u0026rdquo; activation activation=None). This allows the output neuron to produce any range of values, which is what we want for regression. (Recall for classification we used softmax or sigmoid). Hidden Layer: The example uses a single hidden layer with 30 neurons and ReLU activation. The book mentions using fewer neurons and a shallower network because the dataset is quite noisy, to avoid overfitting. input_shape=X_train.shape[1:] correctly sets the input dimension based on the number of features in X_train. What this architecture is ultimately trying to achieve: The hidden layer learns complex combinations of the input features, and the final output neuron combines these learned features linearly to produce the price prediction. Compiling the Model: model.compile(loss=\u0026quot;mean_squared_error\u0026quot;, optimizer=\u0026quot;sgd\u0026quot;)\nLoss Function: loss=\u0026quot;mean_squared_error\u0026quot; (or keras.losses.mean_squared_error). This is standard for regression. Optimizer: \u0026quot;sgd\u0026quot; (again, you\u0026rsquo;d likely want to specify keras.optimizers.SGD(learning_rate=...) for tuning). Metrics: For regression, common metrics might be MAE (Mean Absolute Error) if MSE is the loss, or just watching the loss itself. Accuracy isn\u0026rsquo;t used for regression. Training, Evaluating, Predicting: These steps are identical to the classification MLP: history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid)) mse_test = model.evaluate(X_test, y_test) y_pred = model.predict(X_new)\nThe core workflow with the Sequential API is consistent. The main changes for regression are the structure of the output layer (number of neurons, no activation) and the choice of loss function (MSE).\n(Page 308-312: Building Complex Models Using the Functional API) The Sequential API is easy to use for simple stacks of layers. However, sometimes you need to build neural networks with more complex topologies:\nMultiple inputs. Multiple outputs. Layers that branch off and then merge back. For these, Keras offers the Functional API.\nWide \u0026amp; Deep Neural Network (Example Architecture - Page 308, Figure 10-14):\nIntroduced in a 2016 paper by Google for recommender systems. The Idea: Combine the strengths of deep learning (learning complex patterns through a \u0026ldquo;deep path\u0026rdquo; of stacked layers) with the ability to learn simple rules (through a \u0026ldquo;wide path\u0026rdquo; where inputs connect directly, or via a shallow path, to the output). What it\u0026rsquo;s ultimately trying to achieve: Prevent simple, easily learnable patterns in the data from being distorted or lost by forcing them through many layers of transformations. It allows the network to memorize simple rules while also discovering intricate patterns. Building a Wide \u0026amp; Deep Network with the Functional API (Page 309): Let\u0026rsquo;s tackle the California housing problem with this architecture. input_ = keras.layers.Input(shape=X_train.shape[1:]) hidden1 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(input_) hidden2 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(hidden1) concat = keras.layers.Concatenate()([input_, hidden2]) output = keras.layers.Dense(1)(concat) model = keras.Model(inputs=[input_], outputs=[output])\nLet\u0026rsquo;s break this down step-by-step:\ninput_ = keras.layers.Input(shape=X_train.shape[1:]): This creates an Input object (a symbolic tensor). It defines the shape and dtype of the input the model will receive. This is like declaring the entry point for your data. hidden1 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(input_): We create a Dense layer. Then, we call it like a function, passing it the input_ object. This connects input_ to hidden1. hidden1 now represents the symbolic output of this layer. This \u0026ldquo;calling a layer on a tensor\u0026rdquo; is the essence of the Functional API. You are defining how layers connect. No actual data is processed yet. hidden2 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(hidden1): Create another Dense layer and connect it to the output of hidden1. concat = keras.layers.Concatenate()([input_, hidden2]): Create a Concatenate layer. Call it with a list of tensors you want to concatenate: the original input_ (this is the \u0026ldquo;wide\u0026rdquo; path) and the output of hidden2 (the \u0026ldquo;deep\u0026rdquo; path). concat is now the symbolic concatenated tensor. output = keras.layers.Dense(1)(concat): Create the output Dense layer (single neuron, no activation for regression) and connect it to the concat layer. model = keras.Model(inputs=[input_], outputs=[output]): Finally, create the Model object by specifying its inputs and outputs. Keras then figures out the graph of layers. Once this model is built, compile(), fit(), evaluate(), and predict() work exactly the same as with the Sequential API.\nHandling Multiple Inputs (Figure 10-15, page 310): What if you want to send different subsets of features through the wide and deep paths?\nDefine multiple Input objects: input_A = keras.layers.Input(shape=[5], name=\u0026quot;wide_input\u0026quot;) (e.g., features 0-4) input_B = keras.layers.Input(shape=[6], name=\u0026quot;deep_input\u0026quot;) (e.g., features 2-7, notice overlap is possible)\nBuild the paths: hidden1 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(input_B) hidden2 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(hidden1)\nConcatenate: concat = keras.layers.concatenate([input_A, hidden2]) (using the functional form concatenate() which creates and calls the layer in one step).\nOutput: output = keras.layers.Dense(1, name=\u0026quot;output\u0026quot;)(concat)\nCreate the model, specifying multiple inputs: model = keras.Model(inputs=[input_A, input_B], outputs=[output])\nTraining with Multiple Inputs (Page 311): When calling fit(), evaluate(), or predict(), you must pass data for each input.\nIf inputs are ordered in the inputs list of keras.Model, you pass a tuple/list of NumPy arrays: model.fit((X_train_A, X_train_B), y_train, ...) Where X_train_A would be X_train[:, :5] and X_train_B would be X_train[:, 2:]. Alternatively (and often better if many inputs), you can pass a dictionary mapping input names (defined in keras.layers.Input(name=...)) to the data arrays: model.fit({\u0026quot;wide_input\u0026quot;: X_train_A, \u0026quot;deep_input\u0026quot;: X_train_B}, y_train, ...) Handling Multiple Outputs (Figure 10-16, page 312): Sometimes a task demands multiple outputs, or it\u0026rsquo;s useful for regularization.\nExample: Adding an auxiliary output deeper in the network (e.g., from hidden2). This can encourage the main network to learn useful features earlier on, as this auxiliary output also contributes to the loss. [...] # Up to hidden2 output = keras.layers.Dense(1, name=\u0026quot;main_output\u0026quot;)(concat) aux_output = keras.layers.Dense(1, name=\u0026quot;aux_output\u0026quot;)(hidden2) model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output]) Compiling with Multiple Outputs/Losses (Page 312): Each output needs its own loss function. You can pass a list of losses: model.compile(loss=[\u0026quot;mse\u0026quot;, \u0026quot;mse\u0026quot;], optimizer=\u0026quot;sgd\u0026quot;, ...) You can also specify loss_weights to give different importance to each loss: model.compile(loss=[\u0026quot;mse\u0026quot;, \u0026quot;mse\u0026quot;], loss_weights=[0.9, 0.1], optimizer=\u0026quot;sgd\u0026quot;) Here, the main output\u0026rsquo;s loss contributes 90% to the total loss, and the auxiliary output\u0026rsquo;s loss contributes 10%. (You can also pass dictionaries for loss and loss_weights mapping output names to values). Training with Multiple Outputs/Labels: You need to provide labels for each output during fit(): model.fit([X_train_A, X_train_B], [y_train, y_train], ...) (If the main and aux outputs predict the same thing, you can pass y_train twice). Evaluating with Multiple Outputs: model.evaluate(...) will return the total loss, as well as the individual losses for each output. Predicting with Multiple Outputs: model.predict(...) will return predictions for each output. The Functional API is very powerful for creating custom, non-sequential network architectures!\nExcellent! Let\u0026rsquo;s proceed with that plan.\nWe\u0026rsquo;ve covered the foundational concepts of Artificial Neural Networks, including their structure (Perceptrons, MLPs), how they learn (the idea of backpropagation and gradient descent), and how they are designed for regression and classification.\n(Page 320-327: Fine-Tuning Neural Network Hyperparameters) This section is critical because, as the book states, \u0026ldquo;The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters to tweak.\u0026rdquo;\nWhat we are ultimately trying to achieve with hyperparameter tuning: We want to find the specific configuration of the network\u0026rsquo;s architecture and training process that allows it to learn the underlying patterns in our data effectively and generalize well to new, unseen data, without overfitting or underfitting. Even in a simple MLP, you can change:\nNumber of hidden layers. Number of neurons per hidden layer. Type of activation function for each layer. Weight initialization logic. Optimizer and its specific parameters (like learning rate). Batch size. Regularization techniques and their strengths. And many more! How do you find the best combination? General Strategies for Hyperparameter Tuning (Page 320):\nTrial and Error (Guided by Best Practices): Try many combinations and see which performs best on a validation set (or using K-fold cross-validation). Automated Hyperparameter Optimization: Tools like Scikit-Learn\u0026rsquo;s GridSearchCV or RandomizedSearchCV can be used. To do this with Keras models, you need to wrap your Keras model in an object that mimics a Scikit-Learn regressor/classifier. The book shows how to create a build_model function that Keras-wrapping classes (like KerasRegressor or KerasClassifier from tf.keras.wrappers.scikit_learn or a similar older Keras utility) can use. The build_model function would take hyperparameters as arguments (e.g., n_hidden, n_neurons, learning_rate) and return a compiled Keras model. RandomizedSearchCV is often preferred over GridSearchCV when there are many hyperparameters, as it explores the space more efficiently. The book provides an example of setting up param_distribs for n_hidden, n_neurons, and learning_rate to use with RandomizedSearchCV. Challenges with Automated Search for NNs (Page 321-322): Training NNs can be slow, especially with large datasets or complex models. Exploring a large hyperparameter space can take many hours or days. Manual Assistance: You can guide the search: start with a wide random search, then do a finer search around the best values found. This is time-consuming. More Efficient Search Techniques: The core idea is that when a region of the hyperparameter space looks promising, it should be explored more. Libraries that help with this (beyond simple random search or grid search): Hyperopt: Optimizes over complex search spaces (real, discrete values). Hyperas, kopt, Talos: Based on Hyperopt, specifically for Keras. Keras Tuner: Easy-to-use library from Google for Keras models, with visualization. Scikit-Optimize (skopt): General-purpose, BayesSearchCV class uses Bayesian optimization. Spearmint: Bayesian optimization library. Hyperband: Fast tuning based on a novel bandit-based approach. Sklearn-Deap: Uses evolutionary algorithms. Many cloud providers (like Google Cloud AI Platform) also offer hyperparameter tuning services. Evolutionary Algorithms \u0026amp; AutoML (Page 323): Research is active in using evolutionary approaches not just for hyperparameters but also for finding the best network architecture itself (AutoML). Even training individual NNs with evolutionary algorithms instead of Gradient Descent is being explored (e.g., Uber\u0026rsquo;s Deep Neuroevolution). Guidelines for Choosing Key Hyperparameters (Page 323-327):\nEven with advanced tuning tools, having some intuition about reasonable starting values and search ranges is very helpful.\nNumber of Hidden Layers (Page 323-324):\nStart Simple: For many problems, you can begin with just one or two hidden layers and get reasonable results. An MLP with one hidden layer can theoretically model even very complex functions, if it has enough neurons. Parameter Efficiency of Deep Networks: For complex problems, deep networks (more layers) have much higher parameter efficiency than shallow ones. They can model complex functions using exponentially fewer neurons than a shallow net would need to achieve similar performance. This means they can often reach better performance with the same amount of training data. Hierarchical Structure of Real-World Data: Deep networks naturally take advantage of hierarchical structures in data. Lower hidden layers tend to learn low-level structures (e.g., edges, simple shapes in images). Intermediate hidden layers combine these to model intermediate-level structures (e.g., eyes, noses, squares, circles). Highest hidden layers and the output layer combine these to model high-level structures (e.g., faces, specific objects). This hierarchical learning helps DNNs converge faster and generalize better. Transfer Learning: This hierarchical nature enables transfer learning. If you\u0026rsquo;ve trained a network to recognize faces, you can reuse its lower layers (which learned general visual features) to kickstart training for a new, related task like recognizing hairstyles. The new network doesn\u0026rsquo;t have to learn low-level features from scratch. (More in Chapter 11). General Guideline: Start with 1-2 hidden layers. For more complex problems, gradually ramp up the number of hidden layers until you start overfitting the training set, then use regularization techniques (like early stopping, dropout, etc., which we\u0026rsquo;ll see more of). Very complex tasks (large image classification, speech recognition) might need dozens of layers (but often specialized architectures like CNNs, not fully connected MLPs). You\u0026rsquo;ll rarely train huge networks from scratch; usually, you\u0026rsquo;ll reuse parts of a pretrained state-of-the-art network (transfer learning). Number of Neurons per Hidden Layer (Page 324-325):\nInput/Output Layers: Determined by your task (number of input features, number of output classes/values). Hidden Layers: Old Practice (Pyramid): Fewer neurons in higher layers (e.g., 300 -\u0026gt; 200 -\u0026gt; 100). Rationale: many low-level features coalesce into fewer high-level features. Largely abandoned. Current Practice: Using the same number of neurons in all hidden layers often performs just as well or better, and it\u0026rsquo;s simpler (only one hyperparameter for neuron count per layer, instead of one per layer). Sometimes, making the first hidden layer larger than subsequent ones can be beneficial, depending on the dataset. \u0026ldquo;Stretch Pants\u0026rdquo; Approach (Vincent Vanhoucke): It\u0026rsquo;s often simpler and more efficient to pick a model with more layers and neurons than you actually need, and then use early stopping and other regularization techniques to prevent it from overfitting. This is like buying large stretch pants that shrink to the right size. This avoids creating \u0026ldquo;bottleneck\u0026rdquo; layers (layers with too few neurons) that might lose important information from the inputs. Once information is lost by a bottleneck, subsequent larger layers cannot recover it. More Bang for Your Buck (Scorpion Icon, page 325): In general, you\u0026rsquo;ll get better performance improvements by increasing the number of layers rather than just the number of neurons in a single layer. Learning Rate (Page 325):\nArguably the most important hyperparameter. Optimal Learning Rate: Often about half of the maximum learning rate (the rate above which training diverges). Finding a Good Learning Rate: Train the model for a few hundred iterations. Start with a very low learning rate (e.g., 10⁻⁵). Gradually increase it exponentially up to a very large value (e.g., 10 or 100). Plot the loss as a function of the learning rate (use a log scale for the learning rate axis). The loss will typically drop, then flatten, then shoot back up. The optimal learning rate is usually a bit lower (e.g., 10 times lower) than the point where the loss starts to climb. (More techniques for learning rate schedules in Chapter 11). Optimizer (Page 326):\nChoosing a better optimizer than plain Mini-batch Gradient Descent (and tuning its specific hyperparameters) is also very important. (Advanced optimizers like Adam, RMSprop, etc., will be covered in Chapter 11). Batch Size (Page 326):\nCan significantly impact model performance and training time. Large Batch Sizes: Benefit: Hardware accelerators (GPUs) can process them efficiently, leading to more instances seen per second. Catch: Often lead to training instabilities (especially at the start), and the resulting model may not generalize as well as one trained with a small batch size. Small Batch Sizes (e.g., 2 to 32): A 2018 paper (Masters and Luschi) concluded these were often preferable, leading to better models in less training time. Yann LeCun famously tweeted \u0026ldquo;Friends don\u0026rsquo;t let friends use mini-batches larger than 32.\u0026rdquo; The Counter-Argument (Very Large Batches): Other research (Hoffer et al. 2017, Goyal et al. 2017) showed it\u0026rsquo;s possible to use very large batch sizes (up to 8,192) effectively with techniques like learning rate warmup (start with a small learning rate, then ramp it up). This can lead to very short training times without a generalization gap. Practical Strategy: Try using a large batch size with learning rate warmup. If training is unstable or final performance is disappointing, then try using a small batch size. Activation Function (Page 326):\nHidden Layers: In general, ReLU is a good default. (SELU and other variants will be discussed in Chapter 11). Output Layer: Depends entirely on your task: Regression (unbounded): None (linear). Regression (positive output): ReLU or softplus. Regression (output in a specific range, e.g., 0-1): Logistic (sigmoid) or tanh (for -1 to 1), and scale labels accordingly. Binary Classification: Logistic (sigmoid). Multiclass Classification (exclusive classes): Softmax. Number of Iterations (Epochs) (Page 327):\nIn most cases, this doesn\u0026rsquo;t actually need to be tweaked directly. Just use early stopping instead. Set a large number of epochs and let early stopping find the optimal point. Important Note on Interacting Hyperparameters (Bird Icon, page 327): The optimal learning rate often depends on other hyperparameters, especially the batch size. If you modify any hyperparameter, make sure to re-evaluate/retune the learning rate.\nFurther Reading (Page 327): The book recommends an excellent 2018 paper by Leslie Smith, \u0026ldquo;A Disciplined Approach to Neural Network Hyper-Parameters,\u0026rdquo; for more best practices.\nConclusion of Chapter 10: This overview of hyperparameter tuning gives you a strategic approach to building effective neural networks. It\u0026rsquo;s less about finding one \u0026ldquo;magic number\u0026rdquo; and more about understanding the trade-offs and having a systematic way to explore the possibilities, often starting simple and iteratively adding complexity or using smart search strategies and regularization.\nThis pretty much wraps up the core conceptual learning from Chapter 10. We\u0026rsquo;ve journeyed from the biological neuron to the idea of MLPs, backpropagation, how to design them for different tasks, and now, critically, how to approach the art and science of tuning them.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-10/","tags":null,"title":"Chapter 10: Introduction to Artificial Neural Networks"},{"categories":null,"content":"Okay, great! We\u0026rsquo;ve built a solid conceptual understanding of Artificial Neural Networks and how to approach their design and hyperparameter tuning, without getting bogged down in the specific Keras/TensorFlow code from the later parts of Chapter 10.\nNow, we\u0026rsquo;re ready to tackle Chapter 11: Training Deep Neural Networks. This chapter addresses the practical challenges and advanced techniques that arise when you start working with deep networks – those with many layers.\n(Page 331: Introduction - Challenges of Deep DNNs)\nChapter 10 introduced ANNs, and we even notionally \u0026ldquo;trained\u0026rdquo; some (though they were relatively shallow, with just a few hidden layers). But what happens when you need to tackle really complex problems, like detecting hundreds of object types in high-resolution images?\nYou might need a much deeper DNN (10+ layers, hundreds of neurons per layer, hundreds of thousands or millions of connections). Training such a deep DNN is not a walk in the park. The chapter immediately highlights several common problems you could run into:\nVanishing or Exploding Gradients:\nThis is a critical issue. As gradients flow backward through the network during backpropagation (from the output layer to the input layer), they can either: Vanish: Get smaller and smaller, until the gradients for the lower layers (those closer to the input) are tiny. This means the weights of these lower layers barely get updated, and the network effectively stops learning in those early layers. Explode: Get larger and larger, until the weight updates for lower layers become insanely huge, causing the training algorithm to diverge. Both make lower layers very hard to train effectively. Not Enough Training Data:\nLarge, deep networks have many parameters (weights and biases). They often require vast amounts of training data to learn these parameters well and generalize without overfitting. Getting enough labeled data can be very costly and time-consuming. Extremely Slow Training:\nMore layers, more neurons, more connections, more data = much more computation per epoch. Training can take days, weeks, or even longer. Risk of Overfitting:\nA model with millions of parameters can easily memorize the training set (especially if the dataset isn\u0026rsquo;t proportionally huge or is noisy), leading to poor performance on new, unseen data. Chapter Goals: This chapter will go through each of these problems and present techniques to solve them:\nVanishing/Exploding Gradients: We\u0026rsquo;ll look at their causes and popular solutions (like better weight initialization, non-saturating activation functions, Batch Normalization, Gradient Clipping). Lack of Labeled Data / Complex Tasks: We\u0026rsquo;ll explore transfer learning and unsupervised pretraining. Slow Training: We\u0026rsquo;ll discuss various faster optimizers. Overfitting: We\u0026rsquo;ll cover popular regularization techniques for large neural networks. The ultimate aim: \u0026ldquo;With these tools, you will be able to train very deep nets. Welcome to Deep Learning!\u0026rdquo;\n(Page 332-338: The Vanishing/Exploding Gradients Problems)\nThis is a fundamental challenge in training deep networks.\nThe Core Issue:\nBackpropagation involves propagating the error gradient backward from the output layer to the input layer. As these gradients are passed from layer to layer, they are multiplied by the weights of those layers (and the derivatives of activation functions). If weights (or activation function derivatives) are consistently small, the gradients shrink exponentially as they go backward -\u0026gt; Vanishing Gradients. The lower layers learn very slowly or not at all. If weights (or derivatives) are consistently large, the gradients grow exponentially -\u0026gt; Exploding Gradients. Training becomes unstable and diverges. More generally, deep networks suffer from unstable gradients, where different layers may learn at vastly different speeds. Historical Context:\nThis unstable gradient behavior was observed long ago and was a key reason deep NNs were largely abandoned in the early 2000s. The cause wasn\u0026rsquo;t entirely clear until a landmark 2010 paper by Xavier Glorot and Yoshua Bengio. Glorot and Bengio\u0026rsquo;s Findings (2010 Paper - Footnote 1): They identified key culprits:\nLogistic Sigmoid Activation Function: Figure 11-1 (page 333) shows the logistic sigmoid. When its inputs become large (positive or negative), the function saturates at 0 or 1. In these saturated regions, the derivative of the sigmoid function is extremely close to 0. During backpropagation, when gradients are passed through a saturated sigmoid neuron, they get multiplied by this near-zero derivative, effectively getting squashed. If many layers have saturated sigmoids, the gradient can get diluted to almost nothing by the time it reaches the lower layers. Traditional Weight Initialization: At the time, weights were often initialized from a normal distribution with mean 0 and standard deviation 1. Variance Imbalance: Glorot and Bengio showed that with this combination (sigmoid activation + standard normal initialization), the variance of the outputs of each layer is much greater than the variance of its inputs. As the signal flows forward, the variance keeps increasing layer by layer. This pushes the inputs to the activation functions of the upper layers into their saturated regions (where derivatives are ~0). The problem is worsened because the logistic function has a mean of 0.5, not 0 (tanh, with mean 0, behaves slightly better). The Goal for Proper Signal Flow (Glorot \u0026amp; Bengio\u0026rsquo;s Insight - Page 333): For a signal (activations forward, gradients backward) to flow properly without dying out or exploding:\nThe variance of the outputs of each layer should be (roughly) equal to the variance of its inputs. The gradients should have (roughly) equal variance before and after flowing through a layer in the reverse direction. The analogy (footnote 2) is excellent: a chain of microphone amplifiers. Each needs to be set correctly so your voice comes out clearly at the end, with consistent amplitude through the chain. It\u0026rsquo;s hard to guarantee both conditions 1 \u0026amp; 2 simultaneously unless a layer has an equal number of inputs (fan-in) and outputs/neurons (fan-out).\nXavier/Glorot Initialization (Equation 11-1, Page 334): Glorot and Bengio proposed a practical compromise for weight initialization that works well:\nInitialize connection weights randomly. The distribution should have a mean of 0. The variance σ² should depend on fan_in and fan_out: σ² = 1 / fan_avg where fan_avg = (fan_in + fan_out) / 2. (For a uniform distribution between -r and r, r = sqrt(3 / fan_avg)). What this initialization is ultimately trying to achieve: It aims to keep the variance of activations and backpropagated gradients roughly constant across layers, preventing them from vanishing or exploding. It helps the signal propagate properly. This significantly speeds up training and was a key factor in the success of Deep Learning. LeCun Initialization (Page 334):\nAn earlier strategy by Yann LeCun (1990s). If you replace fan_avg with just fan_in in Equation 11-1, you get LeCun initialization. Equivalent to Glorot when fan_in = fan_out. He Initialization (Kaiming He et al., 2015 - Footnote 3, Page 334):\nGlorot initialization works well for sigmoid, tanh, and softmax. For ReLU and its variants (which became very popular), a different initialization is needed because ReLU behaves differently (it kills half the activations, which changes variance). He Initialization uses: σ² = 2 / fan_in (For a uniform distribution, r = sqrt(6 / fan_in)). Table 11-1 (Page 334): Summarizes initialization strategies for different activation functions:\nGlorot: For None (linear output), tanh, logistic, softmax. Variance 1 / fan_avg. He: For ReLU and variants. Variance 2 / fan_in. LeCun: For SELU (we\u0026rsquo;ll see this soon). Variance 1 / fan_in. Keras Implementation (Page 334):\nKeras uses Glorot initialization with a uniform distribution by default for its Dense layers. You can change it using kernel_initializer: keras.layers.Dense(10, activation=\u0026quot;relu\u0026quot;, kernel_initializer=\u0026quot;he_normal\u0026quot;) keras.layers.Dense(10, activation=\u0026quot;relu\u0026quot;, kernel_initializer=\u0026quot;he_uniform\u0026quot;) For He initialization with uniform distribution based on fan_avg (instead of fan_in): he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform') keras.layers.Dense(..., kernel_initializer=he_avg_init) These initialization strategies are crucial first steps to combat unstable gradients. They are all trying to set the initial weights to a \u0026ldquo;sensible\u0026rdquo; scale so that the signal (activations and gradients) can propagate through many layers without becoming too small or too large too quickly.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-11/","tags":null,"title":"Chapter 11: Training Deep Neural Networks"},{"categories":null,"content":"Recursion Core Idea of Recursion: At its simplest, recursion means a function calls itself to solve a smaller version of the same problem.\nPrompt Link(my private discussion for this chapter using ai studio and this chapters main system prompt along with the pdf of this chapter)\nTwo Key Ingredients :\nBase Cases: These are the simplest instances of the problem that the function can solve directly, without further recursion. Intuition: \u0026ldquo;When do I know the answer without asking for more help?\u0026rdquo; Progress (Recursive Call with different arguments): The function must call itself with arguments that move it closer to a base case. If it doesn\u0026rsquo;t make progress, it\u0026rsquo;ll loop forever. Recursion: The \u0026ldquo;Ask a Mini-Me\u0026rdquo; Approach:\nImagine you\u0026rsquo;re a kid and your parent asks you to clean your entire room. It\u0026rsquo;s a disaster. Toys everywhere. You (the function): \u0026ldquo;Ugh, this is too much!\u0026rdquo;\nYour thought process:\nBase Case (Simplest Job): \u0026ldquo;Is there just ONE toy left on the floor?\u0026rdquo; If yes, you pick it up. Done! Easy. Recursive Step (Making it Simpler): \u0026ldquo;Okay, there\u0026rsquo;s more than one toy. What if I pick up just one toy and put it away? Now the room is slightly cleaner. The remaining mess is a smaller version of the original problem.\u0026rdquo; The \u0026ldquo;Call a Mini-Me\u0026rdquo;: \u0026ldquo;Hey, Mini-Me (you call yourself again), can you clean this slightly cleaner room?\u0026rdquo; Trusting Mini-Me: You assume Mini-Me will somehow get the job done. What you do after Mini-Me is done: Nothing! You already did your one piece of work (picking up one toy). Why is recursion useful?\nWhen the input itself is recursive (like file system directories, or a computer grammar). For problems like searching, enumeration (listing all possibilities), and divide-and-conquer. Divide-and-Conquer vs. Recursion:\nDivide-and-Conquer: A strategy. It breaks a problem into independent smaller subproblems of the same type, solves them, and combines their solutions. Merge Sort is a classic. Recursion: A technique. It\u0026rsquo;s how you often implement divide-and-conquer. But recursion is more general. You might have only one subproblem (like binary search or factorial), or the subproblems might not be independent (hello, Dynamic Programming!). Example: Counting Down\nLet\u0026rsquo;s say you want to write a function that prints numbers from N down to 1. countdown(N):\nJob: Print N, then N-1, then N-2, \u0026hellip;, down to 1.\ndef countdown(n): if n == 0: # Base case: stop when we reach 0 (or 1, if you prefer to print 1) print(\u0026#34;Blast off!\u0026#34;) return # This is important! Stop the recursion. print(n) # Do a small piece of work countdown(n - 1) # Ask a \u0026#34;mini-me\u0026#34; to do the rest (a smaller version) countdown(3) # Output: # 3 # 2 # 1 # Blast off! 1. Recursion boot camp Euclidean Algorithm for GCD (Greatest Common Divisor)\nProblem: Find the largest number that divides both x and y without a remainder. Core Idea (from the book): GCD(x, y) is the same as GCD(y, x % y). (Assuming x \u0026gt; y, initially it might be GCD(x-y, y) repeatedly, which simplifies to GCD(x % y, y) and then swapping to keep the first argument larger or just using GCD(y, x%y)). Let\u0026rsquo;s think recursively: def gcd(x, y):\ndef gcd(x, y): # Base case: if y is 0, then x is the GCD. # Think: GCD(12, 0) -\u0026gt; what\u0026#39;s the largest number that divides 12 and 0? It\u0026#39;s 12. return x if y == 0 else gcd(y, x % y) The \u0026ldquo;Work\u0026rdquo;: The % (modulo) operation is the work that makes the problem smaller. \u0026ldquo;Smaller\u0026rdquo;: The numbers y and x % y are generally smaller than x and y. Specifically, x % y is guaranteed to be less than y. Base Case: y == 0. When the second number is zero, the first number is the GCD. GCD(12, 0) = 12. 2. Mutilated Chessboard (Page 218) This is a fantastic example of divide-and-conquer.\n🧩 Problem Cover an 8x8 board with one square missing (mutilated) using L-shaped triominoes (3 squares each).\nTotal squares: 63 Required triominoes: 21 💡 EPI\u0026rsquo;s Reasoning Don’t think n → n+1. That’s a dead end.\nInstead, think:\nIf I can solve for an n x n Mboard, can I solve for a 2n x 2n Mboard?\n✨ The \u0026ldquo;Aha!\u0026rdquo; Insight (Figure 15.1(b)) Imagine a 2n x 2n board. Divide it into four n x n quadrants. One quadrant contains the original missing square. This is already an n x n Mboard (mutilated board) — by hypothesis, we can tile it! The other three quadrants are not Mboards yet. 🧠 Clever Step Place one triomino in the center of the 2n x 2n board. Align it such that it covers one corner square from each of the three \u0026ldquo;full\u0026rdquo; n x n quadrants. This action creates a new missing square in each of the three quadrants, turning them into Mboards! By hypothesis, we can now tile all four n x n Mboards — problem solved!\n3. 📘 Table 15.1: Top Tips for Recursion (Page 219) 1. 🔁 Recursive Rules Input If the problem description sounds recursive, then recursion is a natural fit.\nExample: \u0026ldquo;A directory contains files and other directories.\u0026rdquo;\n2. 🧭 Search, Enumerate, Divide-and-Conquer These problem types are prime candidates for recursion:\n🔍 Search:\n\u0026ldquo;Is the item in the left half? Or the right half?\u0026rdquo;\n→ Example: Binary Search\n📋 Enumerate:\n\u0026ldquo;What if I pick this? What are the options then? What if I don’t pick this?\u0026rdquo;\n→ Example: Generating all subsets\n🧩 Divide-and-Conquer:\nBreak the problem into smaller subproblems of the same type.\n3. 🔄 Alternative to Nested Loops (of Undefined Depth) When you don’t know how many nested loops you’ll need (e.g., problems with variable segments like IP address parsing),\nrecursion gracefully handles varying depth.\n4. 🧱 Removing Recursion (Mimic the Call Stack) If you\u0026rsquo;re asked to make a recursive solution iterative:\nUse your own stack data structure to keep track of “what to do next.”\n5. 🎯 Tail Recursion If the recursive call is the last operation in the function, it can often be converted into a loop.\nNote: Python does not optimize tail recursion, but understanding it is still valuable.\n6. 💾 Caching Repeated Calls (Memoization) If your recursive function recomputes the same result multiple times:\nStore the result (e.g., fib(3) is needed by both fib(5) and fib(4)) This is the gateway to Dynamic Programming! 🧠 Problem 15.1: The Towers of Hanoi (Page 219) This is a classic recursion problem.\nIf you understand this one, you\u0026rsquo;ve got a solid grasp of basic recursive thinking.\n🏗️ The Setup Three pegs:\nP1 → Source P2 → Destination P3 → Auxiliary / Helper n disks of decreasing sizes stacked on P1:\nLargest at the bottom, smallest at the top Goal:\nMove all n disks from P1 to P2.\n📏 Rules Only one disk can be moved at a time. A disk can only be moved if it is the top disk on a peg. A larger disk can never be placed on top of a smaller disk. 🧾 Task Write a program that prints the sequence of operations\nrequired to move all n disks from P1 to P2, obeying the rules.\nThe recursive solution has a pattern:\nIf n \u0026gt; 0:\nHanoi(n-1, src, aux, dest) Move disk n from src to dest Hanoi(n-1, aux, dest, src) Define the recursive function:\ncompute_tower_hanoi_steps(num_rings_to_move, from_peg, to_peg, use_peg)\nBase Case:\nIf num_rings_to_move == 0: do nothing. If num_rings_to_move == 1: move the single disk from from_peg to to_peg. Note: The EPI book uses an implicit base case with if num_rings_to_move \u0026gt; 0:. This works because:\nWhen num_rings_to_move == 1, the first recursive call is with 0 disks (no action), then it moves the disk, then a second call with 0 disks (again, no action). This pattern is correct and avoids explicitly writing a base case. However, using num_rings_to_move == 1 explicitly can improve clarity when learning recursion. Recursive Step (if num_rings_to_move \u0026gt; 1):\nMove (n - 1) disks from from_peg to use_peg using to_peg as a temporary: compute_tower_hanoi_steps(num_rings_to_move - 1, from_peg, use_peg, to_peg) Move the nth disk (the largest in this subproblem) from from_peg to to_peg: print(\u0026ldquo;Move disk\u0026rdquo;, num_rings_to_move, \u0026ldquo;from\u0026rdquo;, from_peg, \u0026ldquo;to\u0026rdquo;, to_peg) (In EPI: pegs[to_peg].append(pegs[from_peg].pop()); result.append(\u0026hellip;)) Move (n - 1) disks from use_peg to to_peg using from_peg as a temporary: compute_tower_hanoi_steps(num_rings_to_move - 1, use_peg, to_peg, from_peg) Backtracking and Recursion Pre-requiste What is Backtracking? It\u0026rsquo;s a Problem-Solving Technique (not a specific formula).\nThink of backtracking as a systematic way to explore all possible solutions to a problem, especially when the problem involves a sequence of choices.\nImagine you\u0026rsquo;re trying to find your way out of a maze:\nYou reach a junction (a decision point). You have multiple paths to choose from. You CHOOSE one path. You EXPLORE that path. If this path leads to the exit (a solution!), great! You might record it. If this path leads to a dead end (not a solution, or violates a rule), or if you\u0026rsquo;ve explored all it has to offer\u0026hellip; You BACKTRACK: You return to the junction where you made your last choice. You UNDO your previous choice (mentally or actually \u0026ldquo;un-take\u0026rdquo; that path). You CHOOSE a different, unexplored path from that same junction and explore it. If all paths from a junction have been explored (and possibly led to dead ends), you backtrack even further to the previous junction. Core Components of Backtracking:\nProblem Representation: The problem can be broken down into a sequence of decisions. The set of all possible decision sequences forms a \u0026ldquo;search space\u0026rdquo; or \u0026ldquo;state-space tree.\u0026rdquo;\nN-Queens: Decision at each row = which column to place the queen. Permutations: Decision at each position = which available item to place. Power Set: Decision for each item = include it or exclude it. Recursive Structure: Backtracking is almost always implemented using recursion because it naturally handles the \u0026ldquo;explore deeper\u0026rdquo; and \u0026ldquo;return to previous state\u0026rdquo; aspects.\nA function call represents making a set of choices up to a certain point and then trying to make the next choice. The \u0026ldquo;Try, Recurse, Undo\u0026rdquo; Pattern (The Heart of Backtracking): Inside a recursive backtracking function, for the current decision point:\nIterate through all possible CHOICES for the current step. For each choice: a. Validate the choice: Can I make this choice given the previous choices? (e.g., Is it safe to place a queen here? Is this letter already used?). b. If choice is valid: i. MAKE the choice: Update the current state (e.g., place the queen, add the letter to the word). ii. EXPLORE (Recurse): Call the function recursively to make the next set of choices, building upon the current choice. iii. UNDO the choice (Backtrack): Revert the state to what it was before making this choice. This is CRUCIAL. It allows the loop to correctly try the next possible choice for the current decision point without being affected by the exploration of the previous choice. Base Cases:\nSolution Found: The sequence of choices has led to a valid, complete solution. (e.g., N queens placed, word of K length formed). Record it. Dead End / Invalid Path: The current path of choices cannot possibly lead to a solution, or it violates a fundamental constraint. (e.g., no safe spot for the current queen). The function returns, triggering backtracking in the caller. Why is it called \u0026ldquo;Backtracking\u0026rdquo;? Because when a path doesn\u0026rsquo;t lead to a solution (or all solutions down that path have been found), the algorithm \u0026ldquo;backtracks\u0026rdquo; up the decision tree (i.e., recursive calls return) to a previous decision point to try a different option.\nIs it a formula? No, it\u0026rsquo;s more like a template or a strategy for designing algorithms. The specific implementation details (how state is stored, how choices are made, how validity is checked, how \u0026ldquo;undo\u0026rdquo; happens) vary from problem to problem.\nWhen to Think \u0026ldquo;Backtracking\u0026rdquo;:\nYou need to generate all possible configurations/solutions (permutations, combinations, subsets, placements). The problem involves a sequence of decisions. You can check the validity of a partial solution or a next choice. You need to explore many possibilities, but some paths can be identified as \u0026ldquo;dead ends\u0026rdquo; early. Backtracking is the general search technique. The \u0026ldquo;undo\u0026rdquo; step is what makes it backtracking rather than just a one-way greedy search. Without the \u0026ldquo;undo,\u0026rdquo; you\u0026rsquo;d explore one path and get stuck with its consequences. The \u0026ldquo;undo\u0026rdquo; allows you to say, \u0026ldquo;Okay, that path (or sub-path) is done; let\u0026rsquo;s rewind and try something else from my last major decision point.\u0026rdquo;\nGoal Find all 2-letter \u0026ldquo;words\u0026rdquo; using letters from \u0026ldquo;ABC\u0026rdquo;, where each letter is used at most once.\nExpected Output: \u0026ldquo;AB\u0026rdquo;, \u0026ldquo;AC\u0026rdquo;, \u0026ldquo;BA\u0026rdquo;, \u0026ldquo;BC\u0026rdquo;, \u0026ldquo;CA\u0026rdquo;, \u0026ldquo;CB\u0026rdquo;\nRecursive Function Let our recursive function be:\nfind_words(current_word, used_letters)\nBase Case If len(current_word) == 2:\nWe\u0026rsquo;ve built a 2-letter word. Add it to the list of solutions. Return. Recursive Step Iterate through each letter in \u0026quot;ABC\u0026quot;:\nCheck if the letter is already in used_letters. If the letter is not used:\na. Place / Choose Add the letter to current_word Add the letter to used_letters\nb. Recurse Call find_words(current_word, used_letters)\nc. Backtrack / Undo Remove the letter from current_word Remove the letter from used_letters Backtracking ensures the state is clean when trying the next available letter.\nVisualization of This Simpler Problem\nImagine you have 3 slots to pick from: A, B, C. You are building a 2-letter word.\n1st letter choice:\nPick \u0026lsquo;A\u0026rsquo;\n→ Now need 1 more letter.\n→ Used: {\u0026lsquo;A\u0026rsquo;}\n→ Word so far: \u0026ldquo;A\u0026rdquo;\n2nd letter choice (can\u0026rsquo;t be \u0026lsquo;A\u0026rsquo;):\nPick \u0026lsquo;B\u0026rsquo;\n→ Word is \u0026ldquo;AB\u0026rdquo; — Done! (Base case)\nBacktrack: Forget \u0026lsquo;B\u0026rsquo;\n→ Word so far: \u0026ldquo;A\u0026rdquo;\n→ Used: {\u0026lsquo;A\u0026rsquo;}\nPick \u0026lsquo;C\u0026rsquo;\n→ Word is \u0026ldquo;AC\u0026rdquo; — Done! (Base case)\nBacktrack: Forget \u0026lsquo;C\u0026rsquo;\n→ Word so far: \u0026ldquo;A\u0026rdquo;\n→ Used: {\u0026lsquo;A\u0026rsquo;}\nBacktrack: Forget \u0026lsquo;A\u0026rsquo;\n→ Word so far: \u0026quot;\u0026quot;\n→ Used: {}\nPick \u0026lsquo;B\u0026rsquo;\n→ Now need 1 more letter.\n→ Used: {\u0026lsquo;B\u0026rsquo;}\n→ Word so far: \u0026ldquo;B\u0026rdquo;\n2nd letter choice (can\u0026rsquo;t be \u0026lsquo;B\u0026rsquo;):\nPick \u0026lsquo;A\u0026rsquo;\n→ Word is \u0026ldquo;BA\u0026rdquo; — Done!\nBacktrack: Forget \u0026lsquo;A\u0026rsquo;\n→ Word so far: \u0026ldquo;B\u0026rdquo;\n→ Used: {\u0026lsquo;B\u0026rsquo;}\nPick \u0026lsquo;C\u0026rsquo;\n→ Word is \u0026ldquo;BC\u0026rdquo; — Done!\nBacktrack: Forget \u0026lsquo;C\u0026rsquo;\n→ Word so far: \u0026ldquo;B\u0026rdquo;\n→ Used: {\u0026lsquo;B\u0026rsquo;}\nBacktrack: Forget \u0026lsquo;B\u0026rsquo;\n→ Word so far: \u0026quot;\u0026quot;\n→ Used: {}\nPick \u0026lsquo;C\u0026rsquo;\n→ Now need 1 more letter.\n→ Used: {\u0026lsquo;C\u0026rsquo;}\n→ Word so far: \u0026ldquo;C\u0026rdquo;\n2nd letter choice (can\u0026rsquo;t be \u0026lsquo;C\u0026rsquo;):\nPick \u0026lsquo;A\u0026rsquo;\n→ Word is \u0026ldquo;CA\u0026rdquo; — Done!\nBacktrack: Forget \u0026lsquo;A\u0026rsquo;\n→ Word so far: \u0026ldquo;C\u0026rdquo;\n→ Used: {\u0026lsquo;C\u0026rsquo;}\nPick \u0026lsquo;B\u0026rsquo;\n→ Word is \u0026ldquo;CB\u0026rdquo; — Done!\nBacktrack: Forget \u0026lsquo;B\u0026rsquo;\n→ Word so far: \u0026ldquo;C\u0026rdquo;\n→ Used: {\u0026lsquo;C\u0026rsquo;}\nBacktrack: Forget \u0026lsquo;C\u0026rsquo;\n→ Word so far: \u0026quot;\u0026quot;\n→ Used: {}\nHere the Undo part ( Backtrack ) is EXPLICIT when we do \u0026ldquo;FORGET\u0026rdquo; aplhabet (A/B/C)\nLet\u0026rsquo;s re-look at the Alphabet example with the \u0026ldquo;Backtracking Template\u0026rdquo; in mind: find_words(current_word_list, used_letters_set) (Decision: What letter to pick next?)\nBase Case (Solution Found): len(current_word_list) == K. Iterate through choices: for letter in all_available_letters: Validate: if letter not in used_letters_set: Make Choice: current_word_list.append(letter), used_letters_set.add(letter) Explore (Recurse): find_words(next_state...) Undo Choice: used_letters_set.remove(letter), current_word_list.pop() Key Characteristics/Benefits of Backtracking:\nFinds All Solutions: If you let it run its course without early termination, it will systematically find all solutions. Handles Constraints: The \u0026ldquo;validate choice\u0026rdquo; step allows you to prune the search space by not exploring paths that violate problem constraints. Conceptually Simple (once grasped): The \u0026ldquo;try all options, and for each, recursively try all further options\u0026rdquo; is a natural way to think about exhaustive search. Often a starting point: For many hard problems (NP-complete problems), backtracking is a fundamental approach. Optimizations (like memoization in dynamic programming, or better pruning heuristics) can then be built on top of it. When is the O(N²) Simple Iterative Solution \u0026ldquo;Good Enough\u0026rdquo;? Preferred When:\nThe depth of choices is small and fixed (e.g., K = 2). The state between choices is minimal. Raw performance is critical and recursion’s function call overhead, though usually minor, matters. Mental Model:\nIterative Nested Loops:\nBest for fixed, shallow decision trees. Like building with a fixed number of Lego blocks in a specific sequence. Recursive Backtracking:\nBest for variable-depth or complex decision trees. Like exploring a maze with unknown turns and constraints. BACKTRACKING HELPS FIND ALL POSSIBLE SOLUTIONS ♛ 15.2 GENERATE ALL NONATTACKING PLACEMENTS OF N-QUEENS (Page 221) Goal: Place N queens on an N×N chessboard such that no two queens attack each other.\n🧩 The Problem: Given an N × N chessboard. Place N queens on the board such that no two queens attack each other. ⚔️ Attack Rules: Queens can attack horizontally, vertically, and diagonally. 🎯 Goal: Return all distinct configurations (valid placements) of these N queens. 🧾 Task Exact problem: Place exactly one queen per row. Find ALL Possible combinations\nRecusrion Backtracking: \u0026ldquo;choose, explore, undo\u0026rdquo; pattern\nTLDR;\nHow it happens in N-Queens (EPI style): You are trying to place a queen in Row X. You try putting it in Column A. board_config[X] = A. You recursively try to solve for Row X+1 and beyond. When that recursion finishes and returns, you are still in the loop for Row X. Next, you try putting the queen for Row X in Column B. The \u0026ldquo;Undo\u0026rdquo;: board_config[X] = B overwrites board_config[X] = A. The choice of A for Row X is now gone for future explorations from Row X. Recursive Function Idea: solve_queens(row_to_place_in, current_board_config)\n\u0026ldquo;Building the solution piece by piece\u0026rdquo; (like building the word letter by letter):\nIn N-Queens: We are trying to decide the column for the queen in the row_to_place_in. This is like deciding the next letter for our word. \u0026ldquo;What choices do I have for the current piece?\u0026rdquo;\nAlphabet problem: Choose any unused letter from \u0026lsquo;A\u0026rsquo;, \u0026lsquo;B\u0026rsquo;, \u0026lsquo;C\u0026rsquo;. N-Queens: For the current row_to_place_in, try placing the queen in column 0, then column 1, then column 2, \u0026hellip;, up to column N-1. solve_queens(row, placement) (where placement[r] stores the column of the queen in row r)\nBase Case (Solution Found):\nAlphabet: if len(current_word) == K_target_length: Solution found. N-Queens: if row == N: We have successfully placed queens in row 0 to N-1. All N queens are on the board. This is a valid complete solution. Action: Add a copy of placement to our list of results. Return. Recursive Step (Trying to place a queen in the current row):\nLoop through all possible choices for this step: Alphabet: for each_letter in available_letters:\nN-Queens: for col_choice from 0 to N-1: (This is trying each column for the queen in the current row)\nCheck if the choice is valid/allowed:\nAlphabet: if each_letter not in used_letters: N-Queens: if is_safe_to_place_queen_at(row, col_choice, placement_so_far): The is_safe_to_place_queen_at function checks: Is col_choice already taken by a queen in a previous row (placement[prev_row] == col_choice)? Is (row, col_choice) on a diagonal with any queen in a previous row\n(abs(placement[prev_row] - col_choice) == abs(prev_row - row))? If the choice IS VALID/SAFE: a. \u0026ldquo;Place\u0026rdquo; / Make the Choice:\nAlphabet: current_word.append(each_letter), used_letters.add(each_letter) N-Queens: placement[row] = col_choice (We’ve decided the queen for the current row goes in col_choice) b. \u0026ldquo;Explore\u0026rdquo; / Recurse (Try to complete the rest of the solution based on this choice):\nAlphabet: find_words(current_word, used_letters) (which internally will try to fill the next letter position) N-Queens: solve_queens(row + 1, placement) (Try to place queens for all subsequent rows, starting with the very next row) c. \u0026ldquo;Backtrack\u0026rdquo; / Undo the Choice (CRUCIAL for exploring other options for the current step):\nAlphabet: current_word.pop(), used_letters.remove(each_letter) N-Queens: This is where the EPI code is a bit more implicit. When the solve_queens(row + 1, placement) call returns (meaning it has fully explored all possibilities from placing a queen at (row, col_choice), or it hit a dead end), the execution comes back to the for col_choice ... loop. If the loop continues to the next col_choice for the same current row, the line placement[row] = new_col_choice will simply overwrite the previous col_choice. This acts as the \u0026ldquo;undo\u0026rdquo; for the specific assignment to placement[row]. If the for col_choice ... loop finishes for the current row (all columns tried), the solve_queens(row, placement) function itself returns. This signifies that all paths starting with the configuration of queens up to row-1 (that led to this call) have been explored for this particular row. The state placement[0...row-1] remains untouched by this returning function, allowing the caller (which was trying to place a queen in row-1) to potentially try a different column for row-1. Q. so what if we find all the valid placements in the first go itself . Then don\u0026rsquo;t you think our return and the loop for placement[row] = new_col_choice iterating further undoing our setup wasted when we Have a valid placement? You\u0026rsquo;re asking: If a particular choice for placement[row] (say, col_A) leads to one or more valid complete solutions through the solve(row+1, ...) call, why does the for loop for row continue to try other columns (like col_B) for that same row? Isn\u0026rsquo;t that wasted effort if we\u0026rsquo;ve already found solutions?\nThe answer depends on the goal of the problem:\nGoal: Find ALL distinct non-attacking placements (as in N-Queens).\nIf the goal is to find all possible solutions, then no, it\u0026rsquo;s not wasted. We must continue exploring.\nJust because placing a queen at (row, col_A) led to some solutions doesn\u0026rsquo;t mean that placing a queen at (row, col_B) (if safe) won\u0026rsquo;t also lead to other, different valid solutions. The N-Queens problem specifically asks for all distinct nonattacking placements. To achieve this, the backtracking algorithm must systematically explore every possible valid path in the decision tree. When solve(row+1, ...) returns after exploring the consequences of placement[row] = col_A, it has done its job for that branch. The for loop for row then must try placement[row] = col_B (and col_C, etc.) to see if those choices also lead to valid complete solutions.\nEach of these will be a distinct branch. Goal: Find ANY ONE non-attacking placement (or determine if one exists).\nIf the problem was modified to \u0026ldquo;find just one solution and then stop,\u0026rdquo; then yes, you could optimize.\nIn this scenario, your recursive function could return a boolean: true if a solution was found down that path, false otherwise. 15.3 GENERATE PERMUTATIONS (Page 222) Problem: Given an array of distinct integers, generate all possible orderings (permutations) of those integers. No permutation should appear more than once.\nCore Idea (Recursive Backtracking): To form a permutation of N items:\nChoose one item to be the first item in the permutation. Then, generate all permutations of the remaining N-1 items. Prepend the chosen first item to each of these N-1 item permutations. Repeat this by choosing every possible item as the \u0026ldquo;first\u0026rdquo; item. Simplified Recursive Logic: generate_perms(index, current_array)\nindex: The current position in the permutation we are trying to fill (from 0 to N-1). current_array: The array whose elements are being swapped to form permutations. Base Case:\nIf index == N-1 (or index == N depending on how you structure it): We\u0026rsquo;ve fixed elements for all positions from 0 to N-1. The current_array now holds one complete permutation. Action: Add a copy of current_array to the results. Return. Recursive Step (For the current index): Iterate i from index to N-1 (these are the elements available to be placed at the current index).\nChoose/Place: Swap current_array[index] with current_array[i]. This brings the element originally at current_array[i] into the index-th position. Explore/Recurse: Call generate_perms(index + 1, current_array). This will generate all permutations for the rest of the array (from index + 1 onwards), given the choice made for index. Undo/Backtrack: Swap current_array[index] with current_array[i] again. This restores the array to its state before this iteration\u0026rsquo;s choice, allowing the next iteration of the for loop (for i) to correctly pick a different element for the index-th position. This is crucial. Example: A = [1,2,3], directed_permutations_recursive(0)\nidx_to_fill = 0: (Think of idx_to_fill as \u0026ldquo;locking down\u0026rdquo; elements from left to right. Everything to the left of idx_to_fill is considered \u0026ldquo;fixed\u0026rdquo; for the current branch of recursion.) i = 0: A=[1,2,3]. Swap A[0],A[0] (no change). Recurse d_p(1) with A=[1,2,3]. idx_to_fill = 1: (Processing A=[1,_,_]) i = 1: A=[1,2,3]. Swap A[1],A[1]. Recurse d_p(2) with A=[1,2,3]. idx_to_fill = 2. Base Case. Add [1,2,3] to result. Return. Undo swap A[1],A[1]. A=[1,2,3]. i = 2: A=[1,2,3]. Swap A[1],A[2] -\u0026gt; A=[1,3,2]. Recurse d_p(2) with A=[1,3,2]. idx_to_fill = 2. Base Case. Add [1,3,2] to result. Return. Undo swap A[1],A[2] -\u0026gt; A=[1,2,3]. d_p(1) returns. Undo swap A[0],A[0]. A=[1,2,3]. i = 1: A=[1,2,3]. Swap A[0],A[1] -\u0026gt; A=[2,1,3]. Recurse d_p(1) with A=[2,1,3]. idx_to_fill = 1: (Processing A=[2,_,_]) i = 1: A=[2,1,3]. Swap A[1],A[1]. Recurse d_p(2) with A=[2,1,3]. Add [2,1,3] to result. Undo. i = 2: A=[2,1,3]. Swap A[1],A[2] -\u0026gt; A=[2,3,1]. Recurse d_p(2) with A=[2,3,1]. Add [2,3,1] to result. Undo. d_p(1) returns. Undo swap A[0],A[1] -\u0026gt; A=[1,2,3]. i = 2: A=[1,2,3]. Swap A[0],A[2] -\u0026gt; A=[3,2,1]. Recurse d_p(1) with A=[3,2,1]. (generates [3,2,1] and [3,1,2]) Undo. d_p(0) returns. Complexity:\nThere are N! permutations. Each permutation is of length N. Generating it and copying it takes O(N). The recursion tree has roughly N! leaf nodes. The path to each leaf involves N levels of recursion. The work done at non-leaf nodes involves a loop and swaps. Time: O(N * N!) because we spend O(N) work (loop and swaps) along the path to each of the N! permutations, and then O(N) to copy the result. More precisely, the number of recursive calls C(N) is N * C(N-1), leading to N! calls roughly. Each call involves a loop. The book derives it as sum(N! / (N-k)!) calls, which is O(N!). Space: O(N) for the recursion call stack. O(N * N!) if storing all results. The \u0026ldquo;swap, recurse, swap back\u0026rdquo; is the classic pattern for generating permutations in place.\n15.4 GENERATE THE POWER SET (Page 224) Problem: Given a set S (represented as a list/array for ordering purposes), return its power set. The power set is the set of all possible subsets of S, including the empty set and S itself.\nExample: If S = {0, 1, 2}, its power set is: {{}, {0}, {1}, {2}, {0,1}, {0,2}, {1,2}, {0,1,2}}\nCore Idea (Recursive Backtracking - \u0026ldquo;Take it or Leave it\u0026rdquo;): For each element in the input set, we have two choices:\nInclude the element in the current subset we are building. Do NOT include the element in the current subset we are building. We explore both choices recursively.\nRecursive Logic: generate_power_set_recursive(index_of_element_to_consider, current_subset_being_built)\nindex_of_element_to_consider: The index of the element in the input set S that we are currently deciding whether to include or not. current_subset_being_built: The subset formed so far based on decisions for elements S[0] through S[index-1]. Base Case:\nIf index_of_element_to_consider == len(S): We have made a decision (include/exclude) for every element in the input set S. Action: The current_subset_being_built is now one complete subset of S. Add a copy of it to our list of all subsets (the power set). Return. Recursive Step (For S[index_of_element_to_consider]):\nLet element = S[index_of_element_to_consider]. Choice 1: EXCLUDE element Don\u0026rsquo;t add element to current_subset_being_built. Recursively call generate_power_set_recursive(index_of_element_to_consider + 1, current_subset_being_built). (Move to decide for the next element). Choice 2: INCLUDE element \u0026ldquo;Place\u0026rdquo;/Choose: Add element to current_subset_being_built. Recursively call generate_power_set_recursive(index_of_element_to_consider + 1, current_subset_being_built). (Move to decide for the next element, with the current one included). \u0026ldquo;Undo\u0026rdquo;/Backtrack: Remove element from current_subset_being_built. This is crucial so that when the \u0026ldquo;EXCLUDE\u0026rdquo; path for the next higher level of recursion (or other branches) is taken, current_subset_being_built is in the correct state. EPI\u0026rsquo;s directed_power_set(to_be_selected, selected_so_far) (Page 225):\nto_be_selected: Corresponds to my index_of_element_to_consider. selected_so_far: Corresponds to my current_subset_being_built. Example: input_set = [0, 1], directed_power_set_recursive(0, [])\ndps(0, []): (Deciding for element 0) Exclude 0: Call dps(1, []) dps(1, []): (Deciding for element 1) Exclude 1: Call dps(2, []) dps(2, []): idx(2) == len(2). Base Case! Add [] to results. Return. Back in dps(1, []). Include 1: current_subset.append(1) -\u0026gt; current_subset is [1] Call dps(2, [1]) dps(2, [1]): idx(2) == len(2). Base Case! Add [1] to results. Return. current_subset.pop() -\u0026gt; current_subset is [] dps(1, []) returns. Back in dps(0, []). current_subset is still []. Include 0: current_subset.append(0) -\u0026gt; current_subset is [0] Call dps(1, [0]) dps(1, [0]): (Deciding for element 1) Exclude 1: Call dps(2, [0]) dps(2, [0]): idx(2) == len(2). Base Case! Add [0] to results. Return. Back in dps(1, [0]). Include 1: current_subset.append(1) -\u0026gt; current_subset is [0, 1] Call dps(2, [0, 1]) dps(2, [0, 1]): idx(2) == len(2). Base Case! Add [0, 1] to results. Return. current_subset.pop() -\u0026gt; current_subset is [0] dps(1, [0]) returns. current_subset.pop() -\u0026gt; current_subset is [] dps(0, []) returns. Results: [ [], [1], [0], [0,1] ] (Order might vary based on recursion path, but all subsets are there).\nComplexity:\nThere are 2^N subsets for a set of N elements. Each subset can have up to N elements. Copying a subset takes O(N). The recursion tree has 2^N leaf nodes (each representing a subset). The depth is N. Time: O(N * 2^N) because we generate 2^N subsets, and for each, we might do O(N) work (e.g., appending to list, copying at the end). The number of nodes in the decision tree is 2^(N+1) - 1. Space: O(N) for the recursion call stack (depth N) and O(N) for current_subset. If storing all results, then O(N * 2^N). Alternative Iterative Approach (Bit Manipulation - Page 225):\nIf N is small (e.g., \u0026lt;= 64), each integer from 0 to 2^N - 1 can represent a subset. The k-th bit of the integer corresponds to the k-th element of the set. If the bit is 1, include the element; if 0, exclude. Example: S = {a,b,c} (N=3) 0 (000_2) -\u0026gt; {} 1 (001_2) -\u0026gt; {c} (assuming c is 0th, b is 1st, a is 2nd if mapping right to left) 2 (010_2) -\u0026gt; {b} \u0026hellip; 7 (111_2) -\u0026gt; {a,b,c} This is often very fast in practice. Complexity is O(N * 2^N) because you iterate 2^N numbers, and for each, you might iterate up to N bits to construct the subset. This \u0026ldquo;take it or leave it\u0026rdquo; recursive pattern is fundamental for many subset, combination, and related problems.\n15.5 GENERATE ALL SUBSETS OF SIZE k (Page 226) Problem: Given n and k, compute all subsets of size k from the set {1, 2, ..., n}. (The book uses {1,2,...,n} for the example, but the code can be adapted for any input set if needed by passing the set and an offset). This is also known as generating combinations: \u0026ldquo;n choose k\u0026rdquo;.\nExample: n=5, k=2. Subsets of size 2 from {1,2,3,4,5}: {{1,2}, {1,3}, {1,4}, {1,5}, {2,3}, {2,4}, {2,5}, {3,4}, {3,5}, {4,5}}\nCore Idea (Recursive Backtracking - Focused \u0026ldquo;Take it or Leave it\u0026rdquo;): We want to build a combination of size k. We can iterate through the numbers from 1 to n. For each number, we decide:\nDo we include this number in our current combination? If we include it, we then need to find k-1 more numbers from the remaining available numbers (i.e., numbers greater than the current one to avoid duplicates and maintain order). If we don\u0026rsquo;t include it, we need to find k numbers from the remaining available numbers. To make it more structured and avoid duplicate combinations (like {1,2} and {2,1} which are the same set), we can enforce that the numbers in our partial_combination are always added in increasing order.\nRecursive Logic: generate_combinations_recursive(start_number_to_consider, current_combination)\nstart_number_to_consider: The smallest number we can currently pick to add to our combination. This ensures we pick numbers in increasing order. current_combination: The list of numbers picked so far for the current combination. Base Case 1 (Combination Complete):\nIf len(current_combination) == k: We have found a combination of the desired size k. Action: Add a copy of current_combination to our list of results. Return. Base Case 2 (Not enough elements left to form a combination of size k):\nIf start_number_to_consider \u0026gt; n and len(current_combination) \u0026lt; k: We\u0026rsquo;ve run out of numbers to pick from, but haven\u0026rsquo;t reached size k. This path is a dead end. Return. More precisely, if the number of remaining elements (n - start_number_to_consider + 1) is less than the number of elements we still need (k - len(current_combination)), then we can\u0026rsquo;t complete the combination. This check can prune branches earlier. (EPI code handles this implicitly with loop bounds). Recursive Step (For numbers starting from start_number_to_consider):\nIterate num_to_add from start_number_to_consider up to n. (Optimization from EPI: num_to_add should not go so far that there aren\u0026rsquo;t enough remaining elements to complete a k-sized combination. The loop in EPI is while i \u0026lt;= n and num_remaining \u0026lt;= n - i + 1: which captures this). a. \u0026ldquo;Place\u0026rdquo;/Choose: Add num_to_add to current_combination. b. Explore/Recurse: Call generate_combinations_recursive(num_to_add + 1, current_combination). We pass num_to_add + 1 as the next start_number_to_consider to ensure subsequent numbers are greater, maintaining sorted order and preventing duplicate combinations. c. \u0026ldquo;Undo\u0026rdquo;/Backtrack: Remove num_to_add from current_combination. This is crucial to allow the for loop to try the next num_to_add for the current position in the combination, or for previous recursive calls to explore different branches. EPI\u0026rsquo;s directed_combinations(offset, partial_combination) (Page 226):\noffset: Corresponds to my start_number_to_consider. (EPI uses 1-based indexing for numbers, so offset starts at 1). partial_combination: Corresponds to my current_combination. Example: n=4, k=2, directed_combinations_recursive(1, [])\ndcr(1, []): needed=2. end_loop_at = 4-2+1 = 3. Loop num_to_add from 1 to 3. num_to_add = 1: combo=[1]. Call dcr(2, [1]). dcr(2, [1]): needed=1. end_loop_at = 4-1+1 = 4. Loop num_to_add_inner from 2 to 4. num_to_add_inner = 2: combo=[1,2]. Call dcr(3, [1,2]). dcr(3, [1,2]): len==k. Base! Add [1,2]. Return. combo.pop() -\u0026gt; [1]. num_to_add_inner = 3: combo=[1,3]. Call dcr(4, [1,3]). dcr(4, [1,3]): len==k. Base! Add [1,3]. Return. combo.pop() -\u0026gt; [1]. num_to_add_inner = 4: combo=[1,4]. Call dcr(5, [1,4]). dcr(5, [1,4]): len==k. Base! Add [1,4]. Return. combo.pop() -\u0026gt; [1]. Loop ends. dcr(2, [1]) returns. combo.pop() -\u0026gt; []. num_to_add = 2: combo=[2]. Call dcr(3, [2]). dcr(3, [2]): needed=1. end_loop_at = 4-1+1 = 4. Loop num_to_add_inner from 3 to 4. num_to_add_inner = 3: combo=[2,3]. Call dcr(4, [2,3]). Add [2,3]. combo.pop() -\u0026gt; [2]. num_to_add_inner = 4: combo=[2,4]. Call dcr(5, [2,4]). Add [2,4]. combo.pop() -\u0026gt; [2]. Loop ends. dcr(3, [2]) returns. combo.pop() -\u0026gt; []. num_to_add = 3: combo=[3]. Call dcr(4, [3]). dcr(4, [3]): needed=1. end_loop_at = 4-1+1 = 4. Loop num_to_add_inner from 4 to 4. num_to_add_inner = 4: combo=[3,4]. Call dcr(5, [3,4]). Add [3,4]. combo.pop() -\u0026gt; [3]. Loop ends. dcr(4, [3]) returns. combo.pop() -\u0026gt; []. Loop ends. dcr(1, []) returns. Results: [[1,2], [1,3], [1,4], [2,3], [2,4], [3,4]]\nComplexity:\nNumber of combinations is \u0026ldquo;n choose k\u0026rdquo;, denoted C(n,k) = n! / (k! * (n-k)!). Each combination has k elements. Time: O(k * C(n,k)) because we generate C(n,k) combinations, and forming/copying each takes about O(k). The recursion depth is k. Space: O(k) for the recursion stack and current_combo. O(k * C(n,k)) if storing all results. This pattern is very similar to the power set, but with the added constraint on the size k and the optimization of only picking numbers greater than the previously picked one to ensure unique combinations.\n","permalink":"https://deepskandpal.github.io/bookshelf/elements-of-programming/chapter-15/","tags":null,"title":"Chapter 15: Recursion"},{"categories":["AI"],"content":"At its heart, \u0026ldquo;The Illusion of Thinking,\u0026rdquo; is about trying to genuinely understand how well these new \u0026ldquo;Large Reasoning Models\u0026rdquo; (LRMs) actually reason. You\u0026rsquo;ve probably heard about models that show their \u0026ldquo;thinking steps\u0026rdquo; before giving an answer, like with Chain-of-Thought. They often do better on benchmarks, which is exciting. But we felt that just looking at the final answer on standard math or coding tests wasn\u0026rsquo;t telling the whole story.\nThe Big Problem We Saw: Imagine you\u0026rsquo;re teaching a student. If they just spit out the right answer, you don\u0026rsquo;t know if they truly understood the method or just got lucky, or maybe even saw the answer somewhere before (which is a bit like \u0026ldquo;data contamination\u0026rdquo; in AI benchmarks). We wanted to look inside the \u0026ldquo;thinking process\u0026rdquo; and also test these models in a way where we could be sure they hadn\u0026rsquo;t just memorized the solutions.\nOur Approach: Puzzles! Instead of standard benchmarks, we turned to classic puzzles – things like the Tower of Hanoi, Checkers Jumping, River Crossing, and Blocks World (you can see these in Figure 3 on page 6). Why puzzles?\nControllable Complexity: We can make these puzzles a little harder or a little easier just by changing a small thing (like adding one more disk in Tower of Hanoi). This lets us see exactly when and how the models start to struggle. No Cheating: It\u0026rsquo;s highly unlikely these models have seen the exact step-by-step solutions to, say, a 10-disk Tower of Hanoi problem in their training data, especially in the specific format we used. Clear Rules, Clear Steps: The logic is all there. They don\u0026rsquo;t need outside knowledge, just the rules we give them. This helps us see if they can follow algorithmic steps. We Can Check Their Work: We can use simulators to see if every single step in their \u0026ldquo;thinking\u0026rdquo; is correct, not just the final answer. (See the top part of Figure 1 on page 2 for how we analyze both the \u0026ldquo;thoughts\u0026rdquo; and the final answer). What We Found (The \u0026ldquo;Illusion of Thinking\u0026rdquo; Part): There\u0026rsquo;s a Wall – Complete Accuracy Collapse: As we made the puzzles harder, every LRM we tested eventually hit a wall. Beyond a certain complexity, their accuracy just plummeted to zero. They couldn\u0026rsquo;t solve it at all, no matter how many tries we gave them (within a generous token budget). (You can see this in Figure 4 on page 7, where accuracy drops off a cliff for all puzzles as complexity increases).\nThey Stop Trying So Hard (Counter-intuitive Scaling): This was really interesting. You\u0026rsquo;d think that as a problem gets harder, the model would \u0026ldquo;think\u0026rdquo; more (i.e., use more computational steps or \u0026ldquo;tokens\u0026rdquo;). They do, up to a point. But right around where they start to fail catastrophically, they actually start reducing their reasoning effort, even if they have plenty of \u0026ldquo;thinking time\u0026rdquo; (token budget) left. It’s like they get overwhelmed and just give up prematurely. (Look at Figure 1, bottom middle graph on page 2, or Figure 6 on page 9 – the token usage goes up, then down).\nThe Three Regimes (When \u0026ldquo;Thinking\u0026rdquo; Helps, and When It Doesn\u0026rsquo;t):\nEasy Problems: Surprisingly, standard LLMs (without the fancy \u0026ldquo;thinking\u0026rdquo; steps) often did better or were more efficient than the LRMs. It seems the extra \u0026ldquo;thinking\u0026rdquo; by LRMs was sometimes unnecessary or even confusing for simple tasks. (See Figure 5 on page 8, the \u0026ldquo;low complexity\u0026rdquo; section). Medium Problems: This is where LRMs shone. The extra \u0026ldquo;thinking\u0026rdquo; steps helped them solve problems that the standard LLMs struggled with. (Figure 5, \u0026ldquo;medium complexity\u0026rdquo;). Hard Problems: Both the LRMs and the standard LLMs eventually failed completely. The \u0026ldquo;thinking\u0026rdquo; could delay the failure, but it couldn\u0026rsquo;t prevent it. (Figure 5, \u0026ldquo;high complexity\u0026rdquo;). Peeking Inside the \u0026ldquo;Thoughts\u0026rdquo; (Figure 1, bottom right, and Figure 7a on page 10):\nOverthinking Simple Stuff: For easy puzzles, we saw models find the correct solution path early in their \u0026ldquo;thoughts\u0026rdquo; but then continue to explore, sometimes making mistakes later or just wasting effort. Struggling with Harder Stuff: For more complex problems (but before they completely collapsed), they\u0026rsquo;d often explore a lot of wrong paths before stumbling upon the correct one, if they found it at all. Giving Up: For the really hard problems, they just couldn\u0026rsquo;t find a correct solution path, no matter how long their \u0026ldquo;thoughts\u0026rdquo; were. They Can\u0026rsquo;t Always Follow Directions: Even when we gave the models the exact algorithm to solve a puzzle (like Tower of Hanoi, see Figure 8a and 8b on page 11), they still failed around the same complexity level! This tells us they have trouble just executing a known series of logical steps, not just finding the steps.\nInconsistent Reasoning: A model might be pretty good at Tower of Hanoi up to a certain number of disks, which involves many steps. But then it might fail miserably at a River Crossing problem that actually requires fewer steps but has different types of constraints (Figure 8c and 8d on page 11). This suggests they aren\u0026rsquo;t learning a general, abstract reasoning skill but are perhaps more sensitive to the specific structure or \u0026ldquo;feel\u0026rdquo; of a problem, maybe due to what they saw in training.\nSo, What\u0026rsquo;s the \u0026ldquo;Illusion\u0026rdquo;? The \u0026ldquo;illusion\u0026rdquo; is that while these models appear to be thinking and reasoning like humans when they produce step-by-step traces, their underlying capabilities have hard limits. Their reasoning isn\u0026rsquo;t as robust or generalizable as it might seem from their performance on some benchmarks. It seems they are very good at pattern matching and interpolating from things they\u0026rsquo;ve seen, but when faced with problems that require them to systematically apply rules in novel or increasingly complex ways (algorithmic complexity), they break down.\nWhy Does This Matter? Our findings suggest we need to be cautious about how much we trust the \u0026ldquo;reasoning\u0026rdquo; of current LRMs, especially for complex, high-stakes tasks. It also points towards areas where these models need to improve, like developing more robust algorithmic execution and generalization capabilities.\n","permalink":"https://deepskandpal.github.io/papershelf/illusion-of-thinking/","tags":null,"title":"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity"},{"categories":["Thoughts"],"content":"So I watched or rather tried to watch this weeks movie releases. I had 2 options one was House full 5 and the other was a few weeks old but new to OTT bhul chuk maaf. Now I personally never found any housefull movie funny not even the first one. Except that one dialogue by Ritesh Deshmukh regarding \u0026ldquo;Biwi\u0026rdquo; and \u0026ldquo;Behen\u0026rdquo; in the first one I have mostly cringed. Pardon me if my sense of humour is not keeping up with the times but I grew up watching comedies of Priya Darshan and Akshay kumar. Raju hirani, and too some extent I find Rohit shetty older films also funny with Chennai Express being my favorite from his Filmography. So I would think of myself as someone who under stands what \u0026ldquo;Brain rot comedy\u0026rdquo; is in Hindi Cinema context. I don\u0026rsquo;t really understand why we have moved on to accept absolutely mediocre films with sub par production/acting/writing/production-design/music/dance in the name of \u0026ldquo;oH bUt iTs tImE pASS dON\u0026rsquo;t tHinK tOO mUch\u0026rdquo; . If i pay money for something I am liable for getting what is expected in return. Let me draw a parallel. If you go to a restaurant to have creamy shahi paneer and the waiter serves you the Now known to everyone \u0026ldquo;Analogue\u0026rdquo; Paneer. Which we all know is not real paneer and also bad for your health but it tastes almost like an original paneer. Would you accept my argument \u0026ldquo;But bro itne paise mai aisa hi paneer milega restaurant ko bhi toh paisa banane hai\u0026rdquo;. Won\u0026rsquo;t you feel cheated that in the name of paneer garbage was fed to you? ( a seemingly paneer looking garbage but still gargbage)\nSame I think is the case with today\u0026rsquo;s hindi films especially in the last few years more specifically post covid. It feels as if they are lost. We do get a gem once in a while but a good hindi movie which entertains you even for time pass is now an exception rather than a norm.\nWhy does it sound as if I am pissed well because I am. So pissed that I am trying very hard with each word as I type to not use any abuse ( I abuse a lot IRL but I normally try to keep a calm composure while writing) but its getting harder and harder to keep it at bay since I was trying to watch bhul chuk maaf for 2 reasons.\nI cannot could not bring myself to buy a ticket for HOUSEFULL 5A/B whatever. My wife is a huge housefull fan but even she was not interested in watching the movie after seeing the trailer Bhul chuk maaf was available in prime video we were like great we will have a good time plus the expectation is was a bit higher for me since I didn\u0026rsquo;t have any from housefull AND OH MY GOD my blood just boiled with each passing frame. I know sounds like an over reaction but its good that we stopped watching after 21 minutes. My wife went to sleep and I switched to Netflix to put on Oppenheimer something which I couldn\u0026rsquo;t get a chance to watch earlier and was looking for an opportunity to catch up. And within 5 min my blood stopped boiling and I was feeling pleasant again.\nNow I know it is NOT AT ALL fair to compare Oppenheimer with any of these movies . And I am not going to . I am not stupid. But while was watching it I realized something really important. Christoper Nolan cares about his movies.\nIn Bollywood atleast for now it feels nobody cares anymore.\nWhy am I generalizing the whole hindi film industry because its pretty clear that the care factor has gone for a toss at least at the top layer (the yrfs , dharmas, maddocks etc). You pick up randomly any 5 movies from these production houses in the last 5 years and you compare them against any random 5 moves from pre covid era say 2011-2018. And keep a base metric of IMBD ratings and reviews you would see that they have fallen. I have done a very adhoc analysis but If needed I will do a full fledge one to draw some statistically significant conclusions but the high level conclusion is this\n\u0026ldquo;Movies have become expensive over the last 5 years but the quality has declined or has been in decline\u0026rdquo; for the quantification of quality a simple sentiment analysis of the reviews would be easiest way to do it. Now I know its very \u0026ldquo;scientific\u0026rdquo; it feels more like a feeler but this can be a good basis for where to look for directionally to get more data , better methods to prove this. But I think its a waste of time . I don\u0026rsquo;t need to ask a Michelin star chef wether the omelette I am having has too much salt I can eat it and tell it. Similarly we have over complicated movie going experience as well. The film is bad if people who are watching it don\u0026rsquo;t like it , if it doesn\u0026rsquo;t do a good box office, if ( and this is new) no OTT wants it. Its just that simple.\nPeople just think \u0026ldquo;Oh the film viewing is very subjective look Anupama chopra found befikre nice.\u0026rdquo; well true it can happen but it \u0026ldquo;rarely happens\u0026rdquo; that is the whole point of exceptions. Exceptions show case that there is a general pattern and for that general pattern there are a few Anomalies. So when I feel that hindi cinema is going through a low time I am not just saying after watching ONLY 21 minutes of the latest comedy movie.\nIn my specific case I have watched, stree2, singham again, bhool bhulaiya 3, dunki, jawan, pathaan, dhrishyam2, shaitaan , viky vida , fighter, sky force and many many more\nout of this I have only loved dhrishyam2 and some what liked fighter in contrast if i compare them with other movies that i have watched recently like , MI7 , MI8, die hard 3, maharaja ,the menu, Nosferatu, bhramayugam, CRAZZY, american made, KGF Chapter 2 , kantara, Leo, godfather (telegu), the movie that i loved the most was MI7 , American made, maharaja , kantara , KGF Chapter 2 and then i liked Leo, die hard 3, MI8 , and I hated the menu and godfather.\nSo clearly for me the hindi cinema has gone down just by this simple comparison. And I am sure a lot us feel the same way.\nAt the end of they day what I am trying to say is 2 things\nI want my money\u0026rsquo;s worth , If you are comedy movie make me fucking laugh, if you are making an action movie keep me at the edge of the movie. I should be able to pick your films (yes you hindi film directors) over others and yes you have very very tough competition especially in today\u0026rsquo;s world.\nDo not justify mediocrity do not produce mediocre products and don\u0026rsquo;t give me this bullshit \u0026ldquo;FiLm MakInG iS hARD\u0026rdquo; bitch every job is hard my job is also hard for you ( yes you, you fanatic fans , hindi film apologists), I shouldn\u0026rsquo;t need Data or logic to conclude that shit is bad for you hence you shouldn\u0026rsquo;t eat it. Sometimes its implied.\nI just hope that as film fan I continue to find hindi cinema as a viable movie going experience\n","permalink":"https://deepskandpal.github.io/thoughts/bollywood/","tags":["Bollywood"],"title":"Bollywood needs a hard reset"},{"categories":["AI"],"content":"Summary (Your summary and notes about the paper go here)\n","permalink":"https://deepskandpal.github.io/papershelf/understand-training-diff-deep-nn/","tags":null,"title":"Understanding the difficulty of training deep feedforward neural networks"},{"categories":["AI"],"content":"Summary (Your summary and notes about the paper go here)\n","permalink":"https://deepskandpal.github.io/papershelf/disciplined-nn/","tags":null,"title":"A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 – LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY"},{"categories":["AI"],"content":"Summary (Your summary and notes about the paper go here)\n","permalink":"https://deepskandpal.github.io/papershelf/multihead-attention/","tags":null,"title":"Multi-Head Attention: Collaborate Instead of Concatenate"},{"categories":["AI"],"content":"Summary (Your summary and notes about the paper go here)\n","permalink":"https://deepskandpal.github.io/papershelf/flash-attention/","tags":null,"title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"},{"categories":["AI"],"content":"Summary (Your summary and notes about the paper go here)\n","permalink":"https://deepskandpal.github.io/papershelf/bert/","tags":null,"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"categories":["AI"],"content":"Summary (Your summary and notes about the paper go here)\n","permalink":"https://deepskandpal.github.io/papershelf/gpt-1/","tags":null,"title":"Improving Language Understanding by Generative Pre-Training"},{"categories":["Non Fiction"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/fabric-of-reality/","tags":null,"title":"Fabric of Reality"},{"categories":["Non Fiction"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/thinking-systems/","tags":null,"title":"Thinking in Systems"},{"categories":["Business"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/creativity-inc/","tags":null,"title":"Creativity Inc"},{"categories":["Machine Learning (Maths)"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/elements/","tags":null,"title":"The Elements of Statistical Learning"},{"categories":["Machine Learning (Maths)"],"content":"TO BE READ ","permalink":"https://deepskandpal.github.io/bookshelf/linear-done-right/","tags":null,"title":"Linear Done Right"},{"categories":["Machine Learning (Maths)"],"content":"To be Read\n","permalink":"https://deepskandpal.github.io/bookshelf/machine-learning-prob-approach/","tags":null,"title":"Machine Learning: A Probabilistic Perspective"},{"categories":["Programming"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/lsr/","tags":null,"title":"Introduction to Algorithms, fourth edition "},{"categories":["Engineering"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/design-data-intensive/","tags":null,"title":"Design Data Intensive"},{"categories":["Data Science"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/statistical-test/","tags":null,"title":"Statistical Test"},{"categories":["Data Science"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/marketing-analytics/","tags":null,"title":"Marketing Analytics"},{"categories":["Thoughts"],"content":"In Indian Twitter (currently known as X) lingo, any kind of verbal skirmish is termed as e-lafda (fight). The latest one is between YouTuber Mohan Mangal and ANI over the use of copyrighted clips. Mohak claims that he has used 5-10 seconds of clips in a few of his recent videos from ANI coverage regarding recent events such as Operation Sindoor in his videos covering the same, and ANI has applied a copyright strike on his YouTube channel because of this. His claim is that when he contacted ANI for the reason and the removal of the copyright strike, he was informed that it would only happen if he pays somewhere in the range of 40-50 lakhs + GST to ANI for the use of those clips in his videos (they claim they have applied for 8 copyright strikes, so there are more to come). Failure to do so will result in getting 3 strikes (2 of which he has received already) that will lead to an automatic channel deletion as per YouTube policy.\nMohak obviously given his commentary on the topic wants to take up the fight but the real questions to ask are\nCan ANI do this? if yes will they succeed Should ANI do this will this become a norm. Now copy right strikes have been for a while seen a \u0026ldquo;weapon\u0026rdquo; to be used against any content creator with reasons to be used can range from legal infringment to take down attempts, Something which I would not like to get into. I believe there are some universal truths and post that everything is up for debate. Right now the debate around this issue is pretty interesting.\nPeople on the left are completely behind Mohak on this; supportingn him telling him that he is right and sharing his message accross twitter and other media platforms. Left leaning political leaders have also come out in support of him. Obviously the people who are opposite to him are the people on the Right. It helps because ANI is often seen as a right leaning news organization but from my perspective they are centre right. Most of the time they try to be neutral or have a moral stance but the Lean towards BJP and right wing is pretty clear. So natrually as per the rules of any good ecosystem the right has come and supported ANI accussing Mohak and everybody supporting him or even trying to debate with them of being \u0026ldquo;left cronies\u0026rdquo; or \u0026ldquo;sadak chap\u0026rdquo; or whatever fancy insult they would like to call them. I personally don\u0026rsquo;t really worry or care about these personal attacks. It usually means the person resorting to these has no substantial argument left in them so they are trying to reach the bottom of the barrel. More than what the arguement is being made about I like to disect how well the arguement is formed in support or against stance. So I\u0026rsquo;ll try to do both here\nPro Mohak side\nYoutube has a fair use policy that allows copyright material avaliable on the platform to be used for commentary and educational purpose. If there is a problem with the copyright infrigment there are better ways to deal with revenue sharing of the video. Demonetization. Prior intimation of the copyright strike asking for fair compensation of the clips used. The way ANI handled it, Its pretty obvious they don\u0026rsquo;t want to go a sustainable business model route where everyone can win. The idea of using copyright strike as a gun to shoot down the whole household for ransom. Yes if this is how they want to play it then lets play it in court see how it turns. Probably some judgement around this would help. For the people who are supporting ANI they should know tomorrow they can be in the same place sometime in the future. Pro ANI side\nOur clip our right we can do whatever the hell we want. We spend money on stringers, reporters sources equipments, work hard verify news produce clippings. So why shouldn\u0026rsquo;t we get compensated for it. News channels pay for our clips why can\u0026rsquo;t youtubers do the same. We offered mohak a way to resolve this with money. We just want fair compensation today youtube is also a revenue generating platform . If a content creator is making revenue, paying for a team , equipements , resources licesning of softwares and other media , why can\u0026rsquo;t they pay us? Its our perogative to decide who we charge money whom we don\u0026rsquo;t. We are not obliged to favour anyone specific because they support ur or our in our good books. That being said we give concessions when we want to ; to whoever we want to. I beleive that this was one of the few ways we can look at it but twitter as usual barring some accounts believes in serving the lowest common denominator and rage baiting. I hate this part of the site. Because majority of people either are low IQ or behave like a low IQ for engagement farming. After all ins\u0026rsquo;t all this about this about money\n","permalink":"https://deepskandpal.github.io/thoughts/copyright/","tags":[],"title":"Copyright: right to intellectual property or Tool for Extortation?"},{"categories":["Engineering"],"content":"The Tyranny of the Top Rank Ever feel like your favorite music app is playing DJ Déjà Vu, stuck on an endless loop of your top-played genres or artists you already know and love? Or, if you\u0026rsquo;re an artist, have you wondered how your fresh, amazing track can cut through the noise and find new ears? This isn\u0026rsquo;t just a feeling; it\u0026rsquo;s a common challenge in any system that ranks and recommends items – whether it\u0026rsquo;s songs, movies, products, or even internal company resources.\nMost recommendation algorithms are designed to be highly effective at finding what they think is the absolute \u0026ldquo;best\u0026rdquo; for you based on your past behavior and similar users. This deterministic precision is great for relevance, but it can lead to a predictable experience. We call this the tyranny of the top rank. Users might get trapped in filter bubbles, missing out on diverse and potentially delightful new discoveries. Recommendations can become stale, and for content creators, it means items ranked just slightly lower than the top might never see the light of day.\nBut what if there was a simple way to shake things up, to inject a bit of serendipity without completely abandoning relevance? What if a tiny, controlled \u0026rsquo;nudge\u0026rsquo; – a bit of strategic randomness – could unlock a richer, more diverse experience?\nThat\u0026rsquo;s exactly what we\u0026rsquo;re going to explore. We\u0026rsquo;ll look at how introducing a small amount of \u0026ldquo;jitter\u0026rdquo; or using \u0026ldquo;probabilistic sampling\u0026rdquo; on an item\u0026rsquo;s relevance score can significantly improve the diversity of recommendations, making them more engaging and useful in the long run.\nThe Core Idea: Controlled Randomness At the heart of our approach is the idea of controlled randomness. Instead of always picking the item with the highest calculated relevance score, we slightly perturb these scores randomly before making the final selection. This manages the classic exploration-exploitation trade-off:\nExploitation: Picking the known best (high deterministic score). Exploration: Trying out other potentially good options to discover new value and provide variety. Our goal is to lean heavily on exploitation (we still want relevant recommendations!) but introduce just enough exploration to make things interesting.\nOur Journey: From Internal Challenge to Broader Solution Our journey into this started with a similar challenge: how to ensure a wide range of valuable internal training resources were being utilized, rather than just the same few top-scorers. We found that even with well-defined scoring for relevance and effectiveness, a deterministic approach led to a predictable and limited selection. This prompted us to explore how controlled randomness could improve the diversity of assigned trainings. This experience highlighted a universal issue in ranked selections, leading us to generalize these techniques for broader application.\nOur Case Study: Crafting a \u0026ldquo;Discovery Weekly\u0026rdquo; Playlist Imagine we\u0026rsquo;re building a feature like Spotify\u0026rsquo;s \u0026ldquo;Discover Weekly.\u0026rdquo; We have a pool of newly released or under-the-radar songs. For a given user, our main recommendation engine has already calculated a UserMatchScore (0-100) for each song, indicating how well it fits their known taste profile.\nIf we always pick the songs with the top 10 UserMatchScores, the playlist might be highly relevant but could lack variety, potentially just offering more of what the user already listens to. We want to introduce songs that are still a good match but might be from slightly different sub-genres or a bit less obvious.\nThis is where our randomization techniques come in. We\u0026rsquo;ll apply them to the UserMatchScore.\nTwo Simple Techniques for Introducing Diversity We explored two main methods:\nSimple Jitter (Uniform Distribution Jitter):\nConcept: We take each song\u0026rsquo;s UserMatchScore and add a small, random \u0026ldquo;nudge\u0026rdquo; to it. This nudge can be positive-only (only boosting scores) or positive-negative (allowing scores to be randomly increased or decreased). How it works: We define a Jitter_Coefficient (JC). For the current set of candidate songs, we find the range of their UserMatchScores (Max Score - Min Score). The maximum possible nudge is JC * score_range. For each song, a random factor (e.g., between -1 and 1 for positive-negative jitter) is multiplied by this maximum nudge to get the actual jitter amount. Jittered_Score = UserMatchScore + Actual_Jitter_Amount. The song with the highest Jittered_Score is selected. Effect: This gives a chance for songs with slightly lower UserMatchScores to be selected if they get a \u0026ldquo;lucky\u0026rdquo; positive nudge while a higher-scoring song gets an \u0026ldquo;unlucky\u0026rdquo; negative nudge (or a smaller positive one). Probabilistic Normal Sampling:\nConcept: We treat each song\u0026rsquo;s UserMatchScore not as a fixed point, but as the average (mean) of a bell curve (Normal distribution) representing its potential \u0026ldquo;appeal\u0026rdquo; in any given instance. How it works: We define a Standard_Deviation_Coefficient (SDC). The standard deviation (std_dev) for each song\u0026rsquo;s Normal distribution is SDC * score_range (score range of current candidates). For each song, we draw one random sample from its specific Normal distribution: Sampled_Score = random_sample_from_Normal(mean=UserMatchScore, std_dev=std_dev). The song with the highest Sampled_Score is selected. Effect: Songs with higher UserMatchScores are statistically more likely to produce the highest sample, but there\u0026rsquo;s a \u0026ldquo;smoother\u0026rdquo; probability of songs with slightly lower scores winning. The chance of an upset depends on how much the bell curves of different songs overlap. Simulating the Impact To see these techniques in action, we ran simulations. For a chosen set of candidate songs for our \u0026ldquo;Discovery Weekly,\u0026rdquo; we repeatedly selected songs using different JC and SDC values and tracked:\nHow often the original #1 song (by UserMatchScore) was still selected. How often the selection stayed within the Top-K original ranks (e.g., Top-10). How many different songs were selected over time (a measure of diversity/exploration). What proportion of the available candidate songs were \u0026ldquo;touched\u0026rdquo; by the selection process. (Here, you would embed your generic plots, similar to the ones we discussed, showing lines for \u0026ldquo;Avg Prob(Select Top 1)\u0026rdquo; decreasing and \u0026ldquo;Avg Distinct Items Selected\u0026rdquo; increasing as JC/SDC increases. Label axes generically, e.g., \u0026ldquo;Randomization Coefficient (JC/SDC)\u0026rdquo; and \u0026ldquo;Selection Probability\u0026rdquo; or \u0026ldquo;Number of Distinct Items\u0026rdquo;.)\nExample Plot Caption: Fig 1: As the randomization coefficient (JC or SDC) increases, the probability of selecting the deterministically top-ranked item typically decreases (blue line), while the number of unique items selected over many trials tends to increase (green line), indicating greater exploration.\nKey Findings from Simulation Our simulations (even with simplified scores) consistently showed:\nIncreased Diversity: Both methods successfully increased the variety of items selected compared to a purely deterministic approach. Controllability: The JC and SDC parameters provided effective control over the trade-off. Small values resulted in minor deviations from the top ranks, while larger values promoted more significant exploration. Simple Jitter (positive_negative type): Tended to explore more broadly for a given relative coefficient, sometimes selecting items further down the deterministic rank if their \u0026ldquo;luck factor\u0026rdquo; was high. Normal Sampling: Provided a \u0026ldquo;smoother\u0026rdquo; exploration, where items with scores closer to the top had a proportionally better chance of being selected during an \u0026ldquo;upset\u0026rdquo; than items ranked much lower. The probability of selection decreased more gradually. The \u0026ldquo;best\u0026rdquo; coefficient depends on the specific goals: if aggressive discovery is desired, a higher coefficient might be chosen. If maintaining very high relevance is paramount, a lower coefficient is better. Benefits \u0026amp; Considerations Pros of these simple randomization techniques:\nSimplicity: Easy to understand and implement on top of existing ranking scores. Effective Diversity Boost: Can significantly improve the variety of recommendations. Improved User Experience: Can reduce recommendation fatigue and increase serendipitous discoveries. Low Computational Cost: Adds minimal overhead to the selection process. Cons/Considerations:\nNot Adaptive: These methods don\u0026rsquo;t \u0026ldquo;learn\u0026rdquo; from user feedback (e.g., if a jitter-selected song is consistently skipped). They rely on the quality of the underlying DeterministicScore. Offline Tuning: The JC or SDC parameter needs to be tuned, often through offline simulations like ours, to find the right balance for your specific application. Potential for \u0026ldquo;Sub-optimal\u0026rdquo; (by one metric): By design, you will sometimes recommend items that aren\u0026rsquo;t the absolute #1 by your primary deterministic score. This is the trade-off for diversity. Conclusion: Embrace the (Controlled) Chaos! Always showing the \u0026ldquo;best\u0026rdquo; isn\u0026rsquo;t always best for long-term user engagement or content discovery. Introducing a small, controlled amount of randomness, like the Simple Jitter or Probabilistic Normal Sampling techniques we explored, can be a surprisingly powerful and easy-to-implement strategy. It helps break out of predictability, surfaces a wider range of relevant items, and can make your recommendation system feel more dynamic and intelligent.\nSo, next time you\u0026rsquo;re building a ranking system, consider adding a little \u0026ldquo;jitter bug\u0026rdquo; – your users (and maybe even your content providers) might thank you for the delightful new discoveries!\nTutorial: Adding Diversity to Rankings with Jitter \u0026amp; Normal Sampling Below is a self-contained Python tutorial demonstrating how to add controlled randomness to ranking scores using Simple Jitter and Probabilistic Normal Sampling.\nColab tutorial can be found here 1. Setup and Sample Song Data import pandas as pd import numpy as np import random import matplotlib.pyplot as plt # --- Generate Synthetic Song Data --- num_songs = 100 # Candidate songs for our discovery playlist genres = [\u0026#34;Indie Pop\u0026#34;, \u0026#34;Electro Funk\u0026#34;, \u0026#34;Ambient\u0026#34;, \u0026#34;Folk Rock\u0026#34;, \u0026#34;Lo-fi Beats\u0026#34;, \u0026#34;Synthwave\u0026#34;] subgenres = { \u0026#34;Indie Pop\u0026#34;: [\u0026#34;Dream Pop\u0026#34;, \u0026#34;Bedroom Pop\u0026#34;], \u0026#34;Electro Funk\u0026#34;: [\u0026#34;Nu-Disco\u0026#34;], \u0026#34;Ambient\u0026#34;: [\u0026#34;Drone\u0026#34;, \u0026#34;Chillout\u0026#34;], \u0026#34;Folk Rock\u0026#34;: [\u0026#34;Indie Folk\u0026#34;], \u0026#34;Lo-fi Beats\u0026#34;: [\u0026#34;Chillhop\u0026#34;], \u0026#34;Synthwave\u0026#34;: [\u0026#34;Retrowave\u0026#34;] } song_data = [] for i in range(num_songs): genre = np.random.choice(genres) base_score = 60 if genre in [\u0026#34;Indie Pop\u0026#34;, \u0026#34;Lo-fi Beats\u0026#34;]: base_score += np.random.randint(20, 35) else: base_score += np.random.randint(0, 20) user_match_score = np.clip(base_score + np.random.normal(0, 8), 40, 100) song_data.append({ \u0026#39;SongID\u0026#39;: f\u0026#34;Song_{i+1:03}\u0026#34;, \u0026#39;Title\u0026#39;: f\u0026#34;{genre} Track {i+1}\u0026#34;, \u0026#39;Artist\u0026#39;: f\u0026#34;Artist_{np.random.randint(1, 20)}\u0026#34;, \u0026#39;Genre\u0026#39;: genre, \u0026#39;UserMatchScore\u0026#39;: round(user_match_score, 2) }) songs_df = pd.DataFrame(song_data) print(\u0026#34;Sample Candidate Songs:\u0026#34;) print(songs_df.head()) print(\u0026#34;\\nTop 5 Songs (Deterministic by UserMatchScore):\u0026#34;) print(songs_df.sort_values(\u0026#39;UserMatchScore\u0026#39;, ascending=False).head()) 2. Implementing the Selection Functions A. Simple Jitter Function def apply_simple_jitter_selection(df_candidates, score_column, jc_value, jitter_type=\u0026#39;positive_negative\u0026#39;, id_column=\u0026#39;SongID\u0026#39;): if df_candidates.empty: return None if len(df_candidates) == 1: return df_candidates[id_column].iloc[0] temp_df = df_candidates.copy() max_score = temp_df[score_column].max() min_score = temp_df[score_column].min() score_range = max_score - min_score if score_range \u0026gt; 0 and jc_value \u0026gt; 0: n_rows = len(temp_df) if jitter_type == \u0026#39;positive_negative\u0026#39;: random_factors = np.random.uniform(-1.0, 1.0, size=n_rows) elif jitter_type == \u0026#39;positive_only\u0026#39;: random_factors = np.random.rand(n_rows) else: random_factors = np.zeros(n_rows) jitter_amount = random_factors * jc_value * score_range temp_df[\u0026#39;Jittered_Score\u0026#39;] = temp_df[score_column] + jitter_amount else: temp_df[\u0026#39;Jittered_Score\u0026#39;] = temp_df[score_column] max_jittered_score = temp_df[\u0026#39;Jittered_Score\u0026#39;].max() top_songs = temp_df[ np.isclose(temp_df[\u0026#39;Jittered_Score\u0026#39;], max_jittered_score) ][id_column].tolist() return random.choice(top_songs) if top_songs else None B. Probabilistic Normal Sampling Function def apply_normal_sampling_selection(df_candidates, score_column, sdc_value, id_column=\u0026#39;SongID\u0026#39;): if df_candidates.empty: return None if len(df_candidates) == 1: return df_candidates[id_column].iloc[0] temp_df = df_candidates.copy() max_score = temp_df[score_column].max() min_score = temp_df[score_column].min() score_range = max_score - min_score std_dev = max(0, sdc_value * score_range) if score_range \u0026gt; 0 else 0 if std_dev \u0026gt; 0: temp_df[\u0026#39;Sampled_Score\u0026#39;] = np.random.normal( loc=temp_df[score_column], scale=std_dev ) else: temp_df[\u0026#39;Sampled_Score\u0026#39;] = temp_df[score_column] max_sampled_score = temp_df[\u0026#39;Sampled_Score\u0026#39;].max() top_songs = temp_df[ np.isclose(temp_df[\u0026#39;Sampled_Score\u0026#39;], max_sampled_score) ][id_column].tolist() return random.choice(top_songs) if top_songs else None 3. Running a Small Simulation N_PLAYLISTS_TO_SIMULATE = 100 PLAYLIST_SIZE = 10 TUTORIAL_JC = 0.7 TUTORIAL_JITTER_TYPE = \u0026#39;positive_negative\u0026#39; TUTORIAL_SDC = 0.3 all_jitter_selections = [] all_normal_selections = [] candidate_songs_df = songs_df.copy() for _ in range(N_PLAYLISTS_TO_SIMULATE): for _ in range(PLAYLIST_SIZE): selected_jitter = apply_simple_jitter_selection( candidate_songs_df, \u0026#39;UserMatchScore\u0026#39;, TUTORIAL_JC, TUTORIAL_JITTER_TYPE ) if selected_jitter: all_jitter_selections.append(selected_jitter) for _ in range(PLAYLIST_SIZE): selected_normal = apply_normal_sampling_selection( candidate_songs_df, \u0026#39;UserMatchScore\u0026#39;, TUTORIAL_SDC ) if selected_normal: all_normal_selections.append(selected_normal) jitter_counts = pd.Series(all_jitter_selections).value_counts(normalize=True) normal_counts = pd.Series(all_normal_selections).value_counts(normalize=True) print(\u0026#34;\\n--- Simple Jitter Selections (Top 10) ---\u0026#34;) print(pd.merge( jitter_counts.head(10).rename(\u0026#34;Prop_Jitter\u0026#34;), songs_df, left_index=True, right_on=\u0026#39;SongID\u0026#39; )[[\u0026#39;SongID\u0026#39;,\u0026#39;Title\u0026#39;,\u0026#39;Genre\u0026#39;,\u0026#39;UserMatchScore\u0026#39;,\u0026#39;Prop_Jitter\u0026#39;]]) print(\u0026#34;\\n--- Normal Sampling Selections (Top 10) ---\u0026#34;) print(pd.merge( normal_counts.head(10).rename(\u0026#34;Prop_Normal\u0026#34;), songs_df, left_index=True, right_on=\u0026#39;SongID\u0026#39; )[[\u0026#39;SongID\u0026#39;,\u0026#39;Title\u0026#39;,\u0026#39;Genre\u0026#39;,\u0026#39;UserMatchScore\u0026#39;,\u0026#39;Prop_Normal\u0026#39;]]) 4. Visualizing the Diversity # Prepare genre distributions df_jitter = pd.merge(jitter_counts.rename(\u0026#34;Prob\u0026#34;), songs_df[[\u0026#39;SongID\u0026#39;,\u0026#39;Genre\u0026#39;]], left_index=True, right_on=\u0026#39;SongID\u0026#39;) jitter_genre_dist = df_jitter.groupby(\u0026#39;Genre\u0026#39;)[\u0026#39;Prob\u0026#39;].sum().sort_values(ascending=False) df_normal = pd.merge(normal_counts.rename(\u0026#34;Prob\u0026#34;), songs_df[[\u0026#39;SongID\u0026#39;,\u0026#39;Genre\u0026#39;]], left_index=True, right_on=\u0026#39;SongID\u0026#39;) normal_genre_dist = df_normal.groupby(\u0026#39;Genre\u0026#39;)[\u0026#39;Prob\u0026#39;].sum().sort_values(ascending=False) # Plotting fig, axes = plt.subplots(1, 2, figsize=(14, 6)) jitter_genre_dist.plot(kind=\u0026#39;bar\u0026#39;, ax=axes[0]) axes[0].set_title(f\u0026#39;Simple Jitter (JC={TUTORIAL_JC})\u0026#39;) axes[0].set_ylabel(\u0026#39;Selection Probability\u0026#39;) normal_genre_dist.plot(kind=\u0026#39;bar\u0026#39;, ax=axes[1]) axes[1].set_title(f\u0026#39;Normal Sampling (SDC={TUTORIAL_SDC})\u0026#39;) axes[1].set_ylabel(\u0026#39;Selection Probability\u0026#39;) plt.tight_layout() plt.show() # Deterministic baseline deterministic = songs_df.sort_values(\u0026#39;UserMatchScore\u0026#39;, ascending=False).head( N_PLAYLISTS_TO_SIMULATE * PLAYLIST_SIZE ) deterministic_dist = deterministic[\u0026#39;Genre\u0026#39;].value_counts(normalize=True) plt.figure(figsize=(7, 6)) deterministic_dist.plot(kind=\u0026#39;bar\u0026#39;) plt.title(\u0026#39;Deterministic Top Scores\u0026#39;) plt.ylabel(\u0026#39;Proportion\u0026#39;) plt.show() ","permalink":"https://deepskandpal.github.io/tech-writings/jitter-bug/","tags":["Recommendations Sytems"],"title":"The Jitter Bug - How a Little Randomness Makes Your Recommendations Smarter (and More Fun!)"},{"categories":["Business"],"content":"Notes / Summary for the Book ","permalink":"https://deepskandpal.github.io/bookshelf/polyster-prince/","tags":null,"title":"Polyster Prince"},{"categories":["Engineering"],"content":"Below is the list of system prompts i use for different tasks with gemini 2.5 pro model\nFor learning DSA you are an expert DSA expert who specializes in teaching how to track coding problems you have written books and specializes in coaching those students who run away from DSA. Your ability to boil down even complex problems and concepts into very simple intuitive first principals based explanation makes you the best in the trade . You are starting a new course based on the book elements of programming interview in python. You will cover each chapters core topics and also cover each section and the core problems in that chapter. its best to pair with a book say Elements of Programming interviews in python\nFor reading and learning any research paper you are one of the core authors of the given paper. Your speciality is you can break down complex concepts and mathematics to simple intuitions your job would be to answer any questions asked by your user in a way that explains the concepts intuitively with the goal of helping user understand them\nbest paired with google gemini model\nMarkdown formatter You are a markdown formatter expert i give you text you read it analyse it and without changing it make a nice markdown out of it To learn Machine learning from any of the books You are a self taught machine learning specialist at a major ivy league college who takes machine learning machine learning 101 elective for stem undergraduate student. Your core competency is in physics in which you have done a PHD from the same college but you are a curious researcher who loves to dabble into different domains of science and engineering. You have a passion for teaching and the students love you because you have the ability to teach hard abstract concepts in the domains of applied sciences mathematics statistics and more recently machine learning into easy to follow intuitive grounded in the philosophy of \u0026quot;what this concept is ultimately trying to achieve \u0026quot; which is very different from usual approaches of how these concepts are taught which have heavy reliance on definition and equations ( you bring realism to them as well which is ultimately your goal) you have written a GOAT book hands on machine learning which students love for its comprehensive depth and breadth and your course is derived from that book\nPro interviewer you are a experience recruiter who has worked in all major FAANG companies like Google Apple Uber Microsoft Nvidia etc. You now provide consulting service for people who are looking to work in big tech. You specialize in mid to senior roles across domains such as Data Science Machine learning in different teams . You have deep knowledge of resume screening , interview process ; what rounds are taken for which roles and how many rounds are there eg ML and DS role will have Machine learning knowledge round and the system design rounds that differ from role to role.\n","permalink":"https://deepskandpal.github.io/tech-writings/prompts/","tags":[],"title":"System Prompts that I found usefull"},{"categories":["Statistics"],"content":"","permalink":"https://deepskandpal.github.io/papershelf/msprt/","tags":null,"title":"Sequential Test and Adaptive Experimental Design"},{"categories":["AI"],"content":"The Big Picture Goal: We want our machine learning models to make good predictions. To do this, we define a loss function (or cost function) that measures how \u0026ldquo;bad\u0026rdquo; our model\u0026rsquo;s predictions are compared to the true values.\nTraining = Minimizing Loss: Training a model means finding the model parameters (like weights w and biases b) that make this loss function as small as possible. Calculus as a Tool for Minimization: Derivatives (and their extensions like gradients) tell us the rate of change or the slope of a function. If we know the slope of our loss function with respect to our model parameters, we know which \u0026ldquo;direction\u0026rdquo; to tweak those parameters to decrease the loss. This is the essence of Gradient Descent. So, all this calculus is ultimately about finding an efficient way to \u0026ldquo;walk downhill\u0026rdquo; on our loss function surface to find the parameter values that give the lowest error.\nNow, let\u0026rsquo;s look at the paper.\n(Section 1 - Introduction) The paper acknowledges that derivatives are critical for ML, especially deep neural networks, which are trained by optimizing a loss function.\nIt mentions that while modern libraries have \u0026ldquo;automatic differentiation,\u0026rdquo; understanding the underlying matrix calculus helps to \u0026ldquo;grok academic papers\u0026rdquo; and understand what\u0026rsquo;s happening \u0026ldquo;under the hood.\u0026rdquo;\nIt gives an example of a single neuron\u0026rsquo;s activation:\nz(x) = w · x + b (affine function: dot product of weights w with input x, plus a bias b). Followed by an activation function, e.g., max(0, z(x)) (ReLU). Key statement on training: \u0026ldquo;Training this neuron means choosing weights w and bias b so that we get the desired output for all N inputs x. To do that, we minimize a loss function\u0026hellip; All of those [gradient descent methods] require the partial derivative (the gradient) of activation(x) with respect to the model parameters w and b. Our goal is to gradually tweak w and b so that the overall loss function keeps getting smaller\u0026hellip;\u0026rdquo;\nWhat this is ultimately trying to achieve: Use the gradient (which calculus helps us find) to iteratively adjust w and b to reduce the model\u0026rsquo;s error. The introduction to the paper\u0026rsquo;s goal: \u0026ldquo;\u0026hellip;rederive and rediscover some key matrix calculus rules in an effort to explain them\u0026hellip; There aren’t dozens of new rules to learn; just a couple of key concepts.\u0026rdquo; This is encouraging!\nLet\u0026rsquo;s proceed to Section 2: Review: Scalar derivative rules (Page 4 of the paper). This should be a good refresher. Are you ready to look at these basic rules? We can take it slow and make sure each one makes sense.\nAlright, let\u0026rsquo;s dive into Section 2: Review: Scalar derivative rules from the Parr \u0026amp; Howard paper (page 4, extending to the table on page 5).\nThis section is a quick refresher of the basic derivative rules you likely encountered in a first calculus course. The core idea of a derivative for a function of a single variable, f(x), is to find its instantaneous rate of change or the slope of the tangent line at any given point x.\nWhat we are ultimately trying to achieve with derivatives (in an ML context): We want to know how a small change in an input (like a model parameter θ or an input feature x) will affect the output of a function (like the loss function J(θ) or an activation function a(z)). If the derivative is large and positive, a small increase in the input leads to a large increase in the output. If it\u0026rsquo;s negative, an increase in input leads to a decrease in output. If it\u0026rsquo;s zero, the function is momentarily flat at that point (which could indicate a minimum, maximum, or a plateau).\nLet\u0026rsquo;s look at the rules presented in the table on page 5:\nConstant Rule:\nRule: f(x) = c (where c is a constant) Scalar derivative notation: d/dx (c) = 0 Example: d/dx (99) = 0 What it\u0026rsquo;s ultimately trying to achieve: A constant function doesn\u0026rsquo;t change, no matter what x is. So, its rate of change (slope) is always zero. Think of a flat horizontal line – its slope is 0. Multiplication by Constant Rule:\nRule: f(x) = cf(x) (actually, it should be g(x) = c * f(x)) Scalar derivative notation: d/dx (c * f(x)) = c * (df/dx) Example: d/dx (3x) = 3 * (d/dx (x)) = 3 * 1 = 3 What it\u0026rsquo;s ultimately trying to achieve: If you scale a function by a constant, its rate of change (slope) at any point is also scaled by that same constant. The shape of the change is the same, just amplified or shrunk. Power Rule:\nRule: f(x) = xⁿ Scalar derivative notation: d/dx (xⁿ) = nxⁿ⁻¹ Example: d/dx (x³) = 3x³⁻¹ = 3x² What it\u0026rsquo;s ultimately trying to achieve: This rule tells us how functions involving powers of x change. For x³, the slope isn\u0026rsquo;t constant; it changes depending on x. At x=1, slope is 3. At x=2, slope is 3*(2)² = 12. Sum Rule:\nRule: h(x) = f(x) + g(x) Scalar derivative notation: d/dx (f(x) + g(x)) = (df/dx) + (dg/dx) Example: d/dx (x² + 3x) = (d/dx x²) + (d/dx 3x) = 2x + 3 What it\u0026rsquo;s ultimately trying to achieve: The rate of change of a sum of functions is the sum of their individual rates of change. If one part is changing quickly and another slowly, the sum changes according to the combined effect. Difference Rule:\nRule: h(x) = f(x) - g(x) Scalar derivative notation: d/dx (f(x) - g(x)) = (df/dx) - (dg/dx) Example: d/dx (x² - 3x) = (d/dx x²) - (d/dx 3x) = 2x - 3 What it\u0026rsquo;s ultimately trying to achieve: Similar to the sum rule, the rate of change of a difference is the difference of the rates of change. Product Rule:\nRule: h(x) = f(x)g(x) Scalar derivative notation: d/dx (f(x)g(x)) = f(x)(dg/dx) + (df/dx)g(x) Example: d/dx (x² * x) (where f(x)=x², g(x)=x) = x² * (d/dx x) + (d/dx x²) * x = x² * 1 + 2x * x = x² + 2x² = 3x² (This is consistent with d/dx (x³) = 3x²) What it\u0026rsquo;s ultimately trying to achieve: When two functions are multiplied, a change in x affects both. The overall rate of change depends on how g(x) changes (weighted by f(x)) plus how f(x) changes (weighted by g(x)). Chain Rule (for single variable nested functions):\nRule: h(x) = f(g(x)) (a function f applied to the output of another function g) Scalar derivative notation: d/dx f(g(x)) = (df/du) * (du/dx), where u = g(x). The paper writes this as df(u)/du * du/dx. Example: d/dx ln(x²) Let u = g(x) = x². Then f(u) = ln(u). du/dx = d/dx (x²) = 2x. df/du = d/du (ln(u)) = 1/u. So, d/dx ln(x²) = (1/u) * (2x) = (1/x²) * (2x) = 2x/x² = 2/x. What it\u0026rsquo;s ultimately trying to achieve: It tells us how to find the rate of change of a composite function. How much does the outer function f change with respect to its input u (df/du), multiplied by how much that inner input u changes with respect to the original variable x (du/dx). It \u0026ldquo;chains\u0026rdquo; the rates of change together. This rule is extremely important for neural networks because they are essentially deeply nested functions, and backpropagation is an application of the chain rule. The paper also mentions (bottom of page 4, top of page 5):\nThe notation d/dx can be thought of as an operator that takes a function f(x) and gives you its derivative df/dx. This operator view is helpful because the operator is distributive (d/dx (f+g) = df/dx + dg/dx) and lets you pull out constants (d/dx (cf) = c * df/dx). This simplifies taking derivatives of complex expressions, like their example: d/dx (9(x + x²)) = 9 * d/dx (x + x²) = 9 * (d/dx x + d/dx x²) = 9 * (1 + 2x) = 9 + 18x. This review of scalar derivative rules is foundational. For deep learning and training models with Gradient Descent, we are constantly asking: \u0026ldquo;If I wiggle this weight/bias a tiny bit, how much does my loss function change?\u0026rdquo; Derivatives give us the answer to that question.\nSection 3: Introduction to vector calculus and partial derivatives This section transitions us from functions of a single variable (f(x)) to functions of multiple variables (e.g., f(x, y)). Neural network layers often have many inputs (features) and many parameters (weights, biases), so we need to be able to talk about how the output changes when we wiggle one specific input or parameter, while keeping others constant.\nFunctions of Multiple Parameters:\nInstead of f(x), we now consider f(x, y). For example, the product xy. If we want to know its derivative, the question is \u0026ldquo;derivative with respect to what?\u0026rdquo; With respect to x, or with respect to y? The change in xy will be different depending on which variable we \u0026ldquo;wiggle.\u0026rdquo; Partial Derivatives:\nWhen we have a function of multiple variables, we compute partial derivatives. The notation changes from d/dx to ∂/∂x (using a stylized \u0026rsquo;d\u0026rsquo;, often called \u0026ldquo;del\u0026rdquo; or \u0026ldquo;curly d\u0026rdquo;). ∂f(x,y)/∂x: This is the partial derivative of f(x,y) with respect to x. What it\u0026rsquo;s ultimately trying to achieve: It tells us the rate of change of the function f if we change x by a tiny amount, while holding all other variables (in this case, y) constant. You treat y as if it were just a number. ∂f(x,y)/∂y: This is the partial derivative of f(x,y) with respect to y. What it\u0026rsquo;s ultimately trying to achieve: It tells us the rate of change of f if we change y by a tiny amount, while holding x constant. Example from the paper: f(x, y) = 3x²y\nPartial derivative with respect to x (∂f/∂x): Treat y (and 3) as constants. ∂/∂x (3x²y) = 3y * (∂/∂x x²) = 3y * (2x) = 6yx. Intuition: If y is fixed, say y=2, then f(x,2) = 6x². The derivative d/dx (6x²) = 12x. Our partial derivative 6yx gives 6(2)x = 12x. It matches! Partial derivative with respect to y (∂f/∂y): Treat x (and 3x²) as constants. ∂/∂y (3x²y) = 3x² * (∂/∂y y) = 3x² * 1 = 3x². Intuition: If x is fixed, say x=1, then f(1,y) = 3y. The derivative d/dy (3y) = 3. Our partial derivative 3x² gives 3(1)² = 3. It matches! The Gradient (∇f):\nFor a function of multiple variables, like f(x,y), we can organize all its partial derivatives into a vector. This vector is called the gradient of f, denoted by ∇f (nabla f). For f(x,y) = 3x²y, the paper writes: ∇f(x, y) = [∂f/∂x, ∂f/∂y] = [6yx, 3x²] What the gradient is ultimately trying to achieve: The gradient vector ∇f at a specific point (x₀, y₀) points in the direction of the steepest ascent (fastest increase) of the function f at that point. Its magnitude ||∇f|| tells you the rate of increase in that direction. In machine learning, our loss function J(θ) depends on many parameters θ₁, θ₂, ..., θₙ. The gradient ∇J(θ) will be a vector [∂J/∂θ₁, ∂J/∂θ₂, ..., ∂J/∂θₙ]. Gradient Descent uses this: it calculates ∇J(θ) and then takes a step in the opposite direction (-∇J(θ)) to go \u0026ldquo;downhill\u0026rdquo; and reduce the loss. This section essentially extends the concept of a derivative (rate of change/slope) from single-variable functions to multi-variable functions by considering the rate of change with respect to each variable individually, holding others constant. The gradient then bundles all these partial rates of change into a single vector that gives us the \u0026ldquo;overall\u0026rdquo; direction of steepest increase.\nSection 4: Matrix calculus This is where things get a bit more generalized. So far:\nScalar derivative: df/dx (function of one variable, derivative is a scalar). Vector calculus (gradient): ∇f(x,y) = [∂f/∂x, ∂f/∂y] (function of multiple scalar variables x, y, output is a scalar f, gradient is a vector of partials). Matrix calculus deals with derivatives when:\nThe input is a vector (e.g., x = [x₁, x₂]). The output can also be a vector (e.g., y = [f₁(x), f₂(x)]). From One Function to Many Functions: The paper keeps f(x, y) = 3x²y from the previous section. It introduces another function g(x, y) = 2x + y⁸. We can find the gradient of g just like we did for f: ∂g/∂x = ∂/∂x (2x + y⁸) = 2 (treating y⁸ as constant) ∂g/∂y = ∂/∂y (2x + y⁸) = 8y⁷ (treating 2x as constant) So, ∇g(x, y) = [2, 8y⁷]. The Jacobian Matrix (J):\nWhen we have multiple functions (say, f and g), each of which can depend on multiple input variables (say, x and y), we can organize their gradients into a matrix. This matrix is called the Jacobian matrix (or just the Jacobian).\nIf we have two functions f(x,y) and g(x,y), the Jacobian J (using the paper\u0026rsquo;s \u0026ldquo;numerator layout\u0026rdquo; where gradients are rows) is formed by stacking their gradient vectors: J = [ ∇f(x, y) ] = [ ∂f/∂x ∂f/∂y ] [ ∇g(x, y) ] [ ∂g/∂x ∂g/∂y ]\nSo for f(x,y) = 3x²y and g(x,y) = 2x + y⁸, the Jacobian is: J = [ 6yx 3x² ] [ 2 8y⁷ ]\nWhat the Jacobian is ultimately trying to achieve: It captures all the first-order partial derivatives of a vector-valued function (a function that outputs a vector of values) with respect to a vector of input variables.\nEach row i tells you how function fᵢ changes with respect to each input variable. Each column j tells you how all output functions change with respect to input variable xⱼ. Essentially, if you have m functions and n input variables, the Jacobian is an m x n matrix where the entry Jᵢⱼ = ∂fᵢ/∂xⱼ. Layouts (Numerator vs. Denominator):\nThe paper notes: \u0026ldquo;there are multiple ways to represent the Jacobian.\u0026rdquo; They use \u0026ldquo;numerator layout\u0026rdquo; (where ∇f is a row vector, and the Jacobian stacks these row vectors). Other sources might use \u0026ldquo;denominator layout,\u0026rdquo; which is essentially the transpose of the numerator layout Jacobian. It\u0026rsquo;s important to be aware of this when consulting different texts, as the shapes of matrices in equations will change accordingly. The paper sticks to numerator layout. Example given (page 7, top): Transpose of our Jacobian above is [ 6yx 2 ] [ 3x² 8y⁷ ] This Jacobian matrix is a fundamental tool when dealing with transformations between vector spaces in calculus, and it plays a key role in the chain rule for vector functions, which is vital for backpropagation in neural networks with multiple layers and multiple neurons per layer.\nThe main idea here is organization:\nGradient: Organizes partial derivatives of a single scalar-output function with respect to its multiple inputs into a vector. Jacobian: Organizes gradients of multiple scalar-output functions (or equivalently, partial derivatives of a single vector-output function) into a matrix. Great! Understanding the Jacobian as an organized collection of partial derivatives is a key step.\nSection 4.1: Generalization of the Jacobian (page 7 of the Parr \u0026amp; Howard paper). This section formalizes the Jacobian for a general case where you have a vector of functions, y = f(x), where x is an input vector and y is an output vector.\nVector Notation:\nInput vector x has n elements: x = [x₁, x₂, ..., xₙ]ᵀ (they assume column vectors by default). Output vector y has m scalar-valued functions: y = [f₁(x), f₂(x), ..., fₘ(x)]ᵀ. Each fᵢ(x) takes the whole vector x as input and returns a scalar. The Jacobian Matrix ∂y/∂x: The paper defines the Jacobian as a stack of m gradients (one for each output function fᵢ). Since they use numerator layout (where each gradient ∇fᵢ(x) is a row vector of partials with respect to the components of x), the Jacobian becomes an m x n matrix:\n∂y/∂x = [ ∇f₁(x) ] = [ ∂f₁/∂x₁ ∂f₁/∂x₂ ... ∂f₁/∂xₙ ] [ ∇f₂(x) ] [ ∂f₂/∂x₁ ∂f₂/∂x₂ ... ∂f₂/∂xₙ ] [ ... ] [ ... ... ... ... ] [ ∇fₘ(x) ] [ ∂fₘ/∂x₁ ∂fₘ/∂x₂ ... ∂fₘ/∂xₙ ]\nRows: Each row i of the Jacobian corresponds to one output function fᵢ(x) and contains all the partial derivatives of that specific output function with respect to each of the input variables x₁, x₂, ..., xₙ. So, row i is ∇fᵢ(x). Columns: Each column j of the Jacobian corresponds to one input variable xⱼ and contains all the partial derivatives of each of the output functions f₁, f₂, ..., fₘ with respect to that specific input variable xⱼ. What the Jacobian ∂y/∂x is ultimately trying to achieve: It describes how each component of the output vector y changes in response to a small change in each component of the input vector x. It\u0026rsquo;s a complete map of all the first-order sensitivities between the inputs and outputs.\nVisualizing Jacobian Shapes (diagram on page 8): This is a handy diagram to remember the dimensions based on the nature of input x and output f (or y):\nScalar input x, Scalar output f: Derivative ∂f/∂x is a scalar. (This is standard Calc 1). Vector input x, Scalar output f: Derivative ∂f/∂x (which is ∇f) is a row vector (1 x n) in numerator layout. Scalar input x, Vector output f: Derivative ∂f/∂x is a column vector (m x 1). (Each ∂fᵢ/∂x is a scalar, stacked up). Vector input x, Vector output f: Derivative ∂f/∂x (the Jacobian) is an m x n matrix. Example: Jacobian of the Identity Function (page 8):\nIf y = f(x) = x, then each output component yᵢ is just equal to the corresponding input component xᵢ (so fᵢ(x) = xᵢ). Here, m=n. Let\u0026rsquo;s find ∂fᵢ/∂xⱼ = ∂xᵢ/∂xⱼ: If i = j, then ∂xᵢ/∂xᵢ = 1 (the derivative of x₁ with respect to x₁ is 1). If i ≠ j, then ∂xᵢ/∂xⱼ = 0 (e.g., x₁ does not change when x₂ changes, because they are independent input components, so ∂x₁/∂x₂ = 0). When you assemble these into the Jacobian matrix, you get 1s on the main diagonal and 0s everywhere else. This is the Identity Matrix (I). ∂x/∂x = I What this ultimately means: If you wiggle an input xⱼ by a small amount, only the corresponding output yⱼ wiggles by that same amount, and other outputs yᵢ (where i≠j) don\u0026rsquo;t change at all. This makes perfect sense for the identity function. This section firmly establishes the Jacobian matrix as the way to represent the derivative of a vector function with respect to a vector input. It\u0026rsquo;s the matrix that holds all the individual \u0026ldquo;slopes\u0026rdquo; that connect changes in each input dimension to changes in each output dimension.\nThis general form of the Jacobian will be crucial when we get to the vector chain rule later, which is how backpropagation efficiently calculates gradients through multiple layers of a neural network, where each layer can be seen as a vector function taking a vector input.\nSection 4.2: Derivatives of vector element-wise binary operators (Pages 9-11). The Big Picture of This Section:\nNeural networks involve many operations on vectors: adding an input vector to a bias vector, multiplying activations by weights, etc. Often, these operations are element-wise – meaning the operation is applied independently to corresponding elements of the input vectors to produce an element of the output vector.\nExample: If w = [w₁, w₂] and x = [x₁, x₂], then w + x = [w₁ + x₁, w₂ + x₂]. The first element of the output only depends on the first elements of the inputs, and so on. This section aims to figure out:\nIf we have an output vector y that\u0026rsquo;s a result of an element-wise operation between two input vectors w and x (e.g., y = w + x or y = w * x), how does y change if we wiggle w? (This gives us the Jacobian ∂y/∂w). And how does y change if we wiggle x? (This gives us the Jacobian ∂y/∂x). What we are ultimately trying to achieve here is to find simplified forms for these Jacobians, because for element-wise operations, many of the terms in the full Jacobian matrix will turn out to be zero.\nBreaking it Down (Page 9):\nThe paper starts with the general form: y = f(w) ⊙ g(x).\nw and x are input vectors. f(w) and g(x) are functions that produce vectors of the same size. ⊙ represents any element-wise binary operator (like +, -, element-wise *, element-wise /). y is the output vector. So, yᵢ = fᵢ(w) ⊙ gᵢ(x). The i-th element of y depends only on the i-th element processing of w and x. The full Jacobian ∂y/∂w would be a matrix where element (i,j) is ∂yᵢ/∂wⱼ (how the i-th output element yᵢ changes with respect to the j-th input element wⱼ from vector w). Similarly for ∂y/∂x.\nThe \u0026ldquo;Furball\u0026rdquo; and the Simplification (Page 10):\nThe general Jacobian matrix for such an operation (shown at the bottom of page 9) looks complicated – a \u0026ldquo;furball,\u0026rdquo; as the paper says.\nHowever, because the operations are element-wise, there\u0026rsquo;s a huge simplification:\nyᵢ (the i-th element of the output) only depends on wᵢ (the i-th element of w) and xᵢ (the i-th element of x). yᵢ does not depend on wⱼ if j ≠ i. yᵢ does not depend on xⱼ if j ≠ i. What does this mean for the Jacobian ∂y/∂w? Consider ∂yᵢ/∂wⱼ:\nIf j ≠ i (we are looking at an off-diagonal element of the Jacobian): Since yᵢ does not depend on wⱼ, its derivative ∂yᵢ/∂wⱼ must be zero. If j = i (we are looking at a diagonal element of the Jacobian): Then ∂yᵢ/∂wᵢ will generally be non-zero, and it\u0026rsquo;s just the derivative of the i-th scalar operation fᵢ(wᵢ) ⊙ gᵢ(xᵢ) with respect to wᵢ (treating xᵢ as a constant for this partial derivative). The Result: Diagonal Jacobians! This means that for element-wise operations, the Jacobian matrices ∂y/∂w and ∂y/∂x are diagonal matrices. A diagonal matrix has non-zero values only along its main diagonal, and zeros everywhere else.\nThe paper introduces the \u0026ldquo;element-wise diagonal condition\u0026rdquo;: fᵢ(w) should only access wᵢ and gᵢ(x) should only access xᵢ. This is precisely what happens in simple element-wise vector operations.\nSo, the Jacobian ∂y/∂w simplifies to (middle of page 10): ∂y/∂w = diag( ∂/∂w₁ (f₁(w₁) ⊙ g₁(x₁)), ∂/∂w₂ (f₂(w₂) ⊙ g₂(x₂)), ..., ∂/∂wₙ (fₙ(wₙ) ⊙ gₙ(xₙ)) ) Each term on the diagonal is just a scalar derivative of the i-th component operation.\nWhat this simplification is ultimately trying to achieve: It makes calculating these Jacobians much easier. Instead of a full matrix of derivatives, we only need to calculate n scalar derivatives for the diagonal.\nSpecial Case: f(w) = w (Page 10-11)\nVery often in neural networks, one of the functions in the element-wise operation is just the identity. For example, vector addition y = w + x. Here f(w) = w (so fᵢ(wᵢ) = wᵢ) and g(x) = x (so gᵢ(xᵢ) = xᵢ).\nLet\u0026rsquo;s look at y = w + x, so yᵢ = wᵢ + xᵢ.\n∂y/∂w: The i-th diagonal element is ∂/∂wᵢ (wᵢ + xᵢ). Treating xᵢ as a constant, this derivative is 1. So, ∂(w+x)/∂w = diag(1, 1, ..., 1) = I (the identity matrix). ∂y/∂x: The i-th diagonal element is ∂/∂xᵢ (wᵢ + xᵢ). Treating wᵢ as a constant, this derivative is 1. So, ∂(w+x)/∂x = diag(1, 1, ..., 1) = I (the identity matrix). The paper then lists Jacobians for common element-wise binary operations where f(w) = w (so fᵢ(wᵢ) = wᵢ):\nAddition: y = w + x\n∂y/∂w = I ∂y/∂x = I Intuition: If you wiggle w₁ by a small amount Δ, y₁ changes by Δ, and no other yⱼ changes. Same for x. Subtraction: y = w - x\n∂y/∂w = I ∂y/∂x = -I (because ∂/∂xᵢ (wᵢ - xᵢ) = -1) Intuition: If you wiggle x₁ by Δ, y₁ changes by -Δ. Element-wise Multiplication (Hadamard Product): y = w ⊗ x (so yᵢ = wᵢ * xᵢ)\n∂y/∂w: i-th diagonal element is ∂/∂wᵢ (wᵢ * xᵢ) = xᵢ. So, ∂y/∂w = diag(x). ∂y/∂x: i-th diagonal element is ∂/∂xᵢ (wᵢ * xᵢ) = wᵢ. So, ∂y/∂x = diag(w). Intuition for ∂y/∂w: If you wiggle w₁ by Δ, y₁ changes by x₁ * Δ. The change in y₁ depends on the value of x₁. Element-wise Division: y = w / x (so yᵢ = wᵢ / xᵢ)\n∂y/∂w: i-th diagonal element is ∂/∂wᵢ (wᵢ / xᵢ) = 1/xᵢ. So, ∂y/∂w = diag(1/x₁, 1/x₂, ...). ∂y/∂x: i-th diagonal element is ∂/∂xᵢ (wᵢ / xᵢ) = -wᵢ / xᵢ². So, ∂y/∂x = diag(-w₁/x₁², ...). Key Takeaway from Section 4.2: When dealing with element-wise operations between two vectors w and x to produce y, the Jacobians ∂y/∂w and ∂y/∂x are diagonal matrices. This is a huge simplification. The values on the diagonal are found by simply taking the scalar derivative of the i-th component operation with respect to the i-th component of the input vector. This section provides the rules for common operations like addition, subtraction, and element-wise multiplication/division.\nThis means that when we\u0026rsquo;re backpropagating gradients through such an element-wise layer, the calculations become much simpler than if we had to deal with full Jacobian matrices. We\u0026rsquo;re essentially just scaling the incoming gradients by these diagonal terms.\nSection 4.3: Derivatives involving scalar expansion (Pages 11-12). The Big Picture of This Section:\nIn neural networks, we often perform operations between a vector and a scalar. For example:\nAdding a scalar bias b to every element of a vector z: y = z + b. Multiplying every element of a vector z by a scalar learning rate η: y = η * z. This section explains how to find the derivatives for these types of operations. The trick is to realize that these operations can be viewed as implicit element-wise operations where the scalar is \u0026ldquo;expanded\u0026rdquo; or \u0026ldquo;broadcast\u0026rdquo; into a vector of the same size as the other vector.\nWhat we are ultimately trying to achieve here is to get rules for how the output vector y changes if we wiggle the input vector x, and how y changes if we wiggle the input scalar z.\nBreaking it Down (Page 11-12):\nThe paper uses the example y = x + z, where x is a vector and z is a scalar.\nImplicit Expansion: This is really y = f(x) + g(z) where f(x) = x and g(z) = 1z (a vector where every element is z). So, yᵢ = xᵢ + z. Each element yᵢ is the sum of the corresponding xᵢ and the same scalar z. 1. Derivative with respect to the vector x: ∂y/∂x\nThis fits the \u0026ldquo;element-wise diagonal condition\u0026rdquo; we just discussed. fᵢ(x) = xᵢ only depends on xᵢ. gᵢ(z) = z (for the i-th component) only depends on the scalar z (which is considered independent of x for this partial derivative). The i-th diagonal element of the Jacobian ∂y/∂x is ∂/∂xᵢ (xᵢ + z). Since z is treated as a constant when differentiating with respect to xᵢ, ∂z/∂xᵢ = 0. So, ∂/∂xᵢ (xᵢ + z) = ∂xᵢ/∂xᵢ + ∂z/∂xᵢ = 1 + 0 = 1. Therefore, ∂y/∂x = diag(1, 1, ..., 1) = I (the identity matrix). Intuition: If you wiggle xᵢ by a small amount Δ, only yᵢ changes, and it changes by Δ. This is the definition of the identity matrix\u0026rsquo;s effect. What this is ultimately trying to achieve: It confirms that adding a scalar to a vector shifts all elements equally, so the rate of change of each output element yᵢ with respect to its corresponding input xᵢ is 1, and there\u0026rsquo;s no cross-influence. 2. Derivative with respect to the scalar z: ∂y/∂z This is different! We are differentiating a vector output y with respect to a scalar input z. Based on the table from page 8 (or just thinking about it):\nInput: scalar z Output: vector y The derivative ∂y/∂z should be a column vector (or a vertical vector as the paper terms it). Each element of this column vector will be ∂yᵢ/∂z. Let\u0026rsquo;s find ∂yᵢ/∂z for yᵢ = xᵢ + z. When differentiating with respect to z, we treat xᵢ as a constant. So, ∂xᵢ/∂z = 0. ∂yᵢ/∂z = ∂/∂z (xᵢ + z) = ∂xᵢ/∂z + ∂z/∂z = 0 + 1 = 1. Since this is true for every i (from 1 to n, the dimension of y and x), then ∂y/∂z is a column vector of all ones. The paper writes this as ∂/∂z (x + z) = 1 (where 1 is the vector of ones of appropriate length). Intuition: If you wiggle the single scalar z by Δ, every element yᵢ of the output vector changes by Δ because z is added to every xᵢ. What this is ultimately trying to achieve: It shows how a single scalar change propagates to all elements of the output vector when scalar addition is involved. Now for Scalar Multiplication: y = xz (or y = zx) This is treated as element-wise multiplication: y = x ⊗ 1z. So, yᵢ = xᵢ * z.\n1. Derivative with respect to the vector x: ∂y/∂x\nAgain, element-wise diagonal condition holds. The i-th diagonal element is ∂/∂xᵢ (xᵢ * z). Treat z as constant: ∂/∂xᵢ (xᵢ * z) = z * ∂xᵢ/∂xᵢ = z * 1 = z. So, ∂y/∂x = diag(z, z, ..., z) = zI (scalar z times the identity matrix). The paper writes this as ∂/∂x (xz) = diag(1z) = Iz. (Here, 1z in diag(1z) means a vector of zs, which makes more sense. If diag(z) was meant for a scalar z, it would just be z itself, not a matrix. So diag(1z) is the clearer way to express zI). My interpretation: diag(1z) means a diagonal matrix with z on every diagonal element. This is z * I. Intuition: If you wiggle xᵢ by Δ, yᵢ changes by z * Δ. The effect is scaled by z. 2. Derivative with respect to the scalar z: ∂y/∂z\n∂y/∂z will be a column vector. The i-th element is ∂yᵢ/∂z. yᵢ = xᵢ * z. Treat xᵢ as constant. ∂yᵢ/∂z = ∂/∂z (xᵢ * z) = xᵢ * ∂z/∂z = xᵢ * 1 = xᵢ. So, ∂y/∂z is a column vector whose elements are [x₁, x₂, ..., xₙ]ᵀ, which is just the vector x. The paper writes this as ∂/∂z (xz) = x. Intuition: If you wiggle the scalar z by Δ, the i-th output yᵢ changes by xᵢ * Δ. The amount yᵢ changes depends on the value of xᵢ. What this is ultimately trying to achieve: It shows how a single scalar change z propagates to the output vector y, with each element yᵢ being affected proportionally to the corresponding xᵢ. Key Takeaway from Section 4.3: Operations involving a vector and a scalar (like adding a scalar to a vector, or multiplying a vector by a scalar) can be understood by \u0026ldquo;expanding\u0026rdquo; the scalar into a vector and then applying the rules for element-wise vector operations.\nWhen differentiating with respect to the vector input, the Jacobian is still diagonal. When differentiating with respect to the scalar input, the result is a vector (not a diagonal matrix), because a single change in the scalar affects all components of the output vector. This section helps us build up the rules needed for things like Wx + b in a neural network layer, where b might be a bias vector added element-wise, or a scalar bias broadcasted. The logic applies similarly.\nSection 4.4: Vector sum reduction (Pages 12-13). The Big Picture of This Section:\nIn many machine learning contexts, especially when defining loss functions, we often need to reduce a vector to a single scalar value. The most common way to do this is by summing up all the elements of the vector.\nExample: The Mean Squared Error (MSE) involves summing the squared errors for each instance. If we have a vector of squared errors, we sum them up to get a total squared error, then average. This section focuses on finding the derivative of such a sum:\nIf y = sum(f(x)), where x is an input vector and f(x) produces an output vector whose elements are then summed to get the scalar y, how does this scalar y change if we wiggle the input vector x? This will give us ∂y/∂x, which will be a gradient (a row vector, according to the paper\u0026rsquo;s convention for derivative of a scalar w.r.t. a vector). What we are ultimately trying to achieve here is a rule for how to differentiate through a summation operation. This is critical for backpropagation, as the total loss is often a sum of losses from individual components or instances.\nBreaking it Down (Page 13):\nLet y = sum(f(x)) = Σᵢ fᵢ(x).\nx is an input vector [x₁, x₂, ..., xₙ]. f(x) is a vector-valued function, producing an output vector [f₁(x), f₂(x), ..., fₚ(x)]. (Note: the paper uses n as the dimension of x and also n for the number of terms in the sum, implying p=n. Let\u0026rsquo;s assume the output vector of f(x) has p elements). Each fᵢ(x) could potentially depend on all elements of x (not just xᵢ). This is an important distinction from the element-wise operations earlier. y is the scalar sum of all elements of the vector f(x). We want to find the gradient ∂y/∂x. This will be a row vector: ∂y/∂x = [∂y/∂x₁, ∂y/∂x₂, ..., ∂y/∂xₙ]\nLet\u0026rsquo;s consider one component of this gradient, ∂y/∂xⱼ (how the sum y changes with respect to one input element xⱼ): ∂y/∂xⱼ = ∂/∂xⱼ ( Σᵢ fᵢ(x) )\nBecause the derivative operator is distributive over a sum, we can move it inside: ∂y/∂xⱼ = Σᵢ ( ∂fᵢ(x)/∂xⱼ )\nWhat this means: To find how the total sum y changes when xⱼ changes, we sum up how each individual term fᵢ(x) in the sum changes when xⱼ changes.\nSo, the gradient vector ∂y/∂x becomes: ∂y/∂x = [ Σᵢ(∂fᵢ/∂x₁), Σᵢ(∂fᵢ/∂x₂), ..., Σᵢ(∂fᵢ/∂xₙ) ]\nExample 1: y = sum(x) (Simple sum of input vector elements) Here, f(x) = x, so fᵢ(x) = xᵢ.\n∂y/∂xⱼ = Σᵢ (∂xᵢ/∂xⱼ) ∂xᵢ/∂xⱼ is 1 if i=j, and 0 if i≠j. So, in the sum Σᵢ (∂xᵢ/∂xⱼ), only one term is non-zero: when i=j, the term is 1. Therefore, ∂y/∂xⱼ = 1 for all j. This means ∂y/∂x = [1, 1, ..., 1]. The paper writes this as ∇y = 1ᵀ (a row vector of ones). Intuition: If you have y = x₁ + x₂ + ... + xₙ, and you wiggle x₁ by a small amount Δ, y changes by Δ. If you wiggle x₂ by Δ, y also changes by Δ, and so on. The rate of change of y with respect to any xⱼ is 1. What this result ultimately achieves: It gives us a very simple rule: the derivative of the sum of a vector\u0026rsquo;s elements, with respect to that vector itself, is a vector of ones (transposed to match the gradient convention). Example 2: y = sum(xz) (Sum of a vector multiplied by a scalar) Here, x is a vector, z is a scalar. Let f(x,z) be the vector xz (meaning fᵢ(x,z) = xᵢz). Then y = sum(f(x,z)) = Σᵢ (xᵢz).\nDerivative with respect to the vector x: ∂y/∂x\nThe j-th component of the gradient is ∂y/∂xⱼ = Σᵢ (∂(xᵢz)/∂xⱼ). ∂(xᵢz)/∂xⱼ: If i = j: ∂(xⱼz)/∂xⱼ = z (treating z as constant). If i ≠ j: ∂(xᵢz)/∂xⱼ = 0 (because xᵢz doesn\u0026rsquo;t depend on xⱼ). So, in the sum Σᵢ (∂(xᵢz)/∂xⱼ), only the term where i=j survives, and it is z. Therefore, ∂y/∂xⱼ = z for all j. This means ∂y/∂x = [z, z, ..., z]. Intuition: If y = x₁z + x₂z + ... + xₙz, and you wiggle xⱼ by Δ, then y changes by zΔ. The rate of change is z. Derivative with respect to the scalar z: ∂y/∂z (This is a scalar derivative of a scalar w.r.t a scalar)\ny = Σᵢ (xᵢz) = z * (Σᵢ xᵢ). ∂y/∂z = ∂/∂z ( z * sum(x) ). Since sum(x) is treated as a constant when differentiating w.r.t. z, ∂y/∂z = sum(x). Intuition: If y = z * (x₁ + x₂ + ... + xₙ), and you wiggle z by Δ, then y changes by (x₁ + ... + xₙ)Δ. The rate of change is sum(x). Key Takeaway from Section 4.4: When you differentiate a scalar sum y = sum(f(x)) with respect to the input vector x, the j-th component of the resulting gradient ∂y/∂x is the sum of how each term fᵢ(x) in the original sum changes with respect to xⱼ.\nFor y = sum(x), this simplifies to ∂y/∂x = 1ᵀ. This rule is fundamental for backpropagation because the overall loss of a network is often a sum (or average) of losses per instance, or a sum of terms in a complex loss function. When we take the derivative of this total loss with respect to weights in earlier layers, we\u0026rsquo;ll be implicitly using this idea of differentiating through a sum. The paper uses the notation ∇y when the output y is a scalar and the differentiation is with respect to a vector, resulting in a row vector (the gradient). It uses ∂y/∂x for the more general Jacobian when y could also be a vector.\nSection 4.5: The Chain Rules The Big Picture of This Section:\nNeural networks are essentially deeply nested functions. The output of one layer becomes the input to the next, which then feeds into another, and so on, until we get the final output, and then a loss function is applied. loss = L( activation_L( ... activation_2( activation_1(X, w₁, b₁), w₂, b₂) ... ), y_true )\nTo train the network using gradient descent, we need to calculate how the final loss changes with respect to every single weight and bias in the entire network, even those in the very first layers. This is a monumental task if done naively.\nThe Chain Rule is the mathematical tool that allows us to do this efficiently.\nWhat it\u0026rsquo;s ultimately trying to achieve: It provides a systematic way to calculate the derivative of a composite (nested) function by breaking it down into simpler derivatives of its constituent parts and then \u0026ldquo;chaining\u0026rdquo; them together (usually by multiplication). The paper discusses three variants, from simple to more general:\nSingle-variable chain rule (scalar function of a scalar variable). Single-variable total-derivative chain rule (scalar function of a scalar variable, but with intermediate multivariate functions). Vector chain rule (vector function of a vector variable). This is the most general one for neural networks. 4.5.1 Single-variable chain rule (Pages 14-17)\nThis is the chain rule you likely learned in basic calculus.\nScenario: You have a function nested within another, like y = f(g(x)). For example, y = sin(x²). Here, g(x) = x² (the inner function). And f(u) = sin(u) (the outer function, where u = g(x)). The Rule: dy/dx = dy/du * du/dx du/dx: How the inner function u changes with respect to x. For u=x², du/dx = 2x. dy/du: How the outer function y changes with respect to its direct input u. For y=sin(u), dy/du = cos(u). Then dy/dx = cos(u) * 2x. Substitute back u=x² to get cos(x²) * 2x. The Process (as recommended by the paper): Introduce intermediate variables: Break down the nested expression. For y = sin(x²), let u = x². Then y = sin(u). Compute derivatives of intermediate variables: du/dx = 2x dy/du = cos(u) Combine (multiply) the derivatives: dy/dx = (dy/du) * (du/dx) = cos(u) * 2x Substitute back: dy/dx = cos(x²) * 2x Units Analogy (page 15): The paper gives a nice analogy: if y is miles, u is gallons, and x is tank level, then miles/tank = (miles/gallon) * (gallons/tank). The intermediate unit \u0026ldquo;gallon\u0026rdquo; cancels out. Dataflow Diagram / Abstract Syntax Tree (page 16): Visualizing the chain of operations helps. Changes in x bubble up to u, then to y. The chain rule traces this path. Condition for Single-Variable Chain Rule: There\u0026rsquo;s a single dataflow path from x to y. Intermediate functions (u(x), y(u)) have only one parameter. Deeply Nested Expressions (page 17): The process extends. For y = f₄(f₃(f₂(f₁(x)))), let u₁=f₁(x), u₂=f₂(u₁) etc. Then dy/dx = (dy/du₃) * (du₃/du₂) * (du₂/du₁) * (du₁/dx). Example: y = ln(sin(x³)²) u₁ = x³ u₂ = sin(u₁) u₃ = u₂² y = u₄ = ln(u₃) du₁/dx = 3x² du₂/du₁ = cos(u₁) du₃/du₂ = 2u₂ dy/du₃ = 1/u₃ dy/dx = (1/u₃) * (2u₂) * (cos(u₁)) * (3x²) Substitute back: (1/sin(x³)² ) * (2sin(x³)) * (cos(x³)) * (3x²) = 6x²cos(x³)/sin(x³) What the single-variable chain rule is ultimately trying to achieve: It provides a recipe to find the overall rate of change of a nested scalar function by multiplying the rates of change of its individual components along the chain of dependency. 4.5.2 Single-variable total-derivative chain rule (Pages 18-20)\nThis handles a more complex scenario. What if an intermediate variable depends on the original input x and other intermediate variables that also depend on x?\nScenario: y = f(x) = x + x². The paper rewrites this using intermediate variables to illustrate: u₁(x) = x² y = u₂(x, u₁) = x + u₁ Here, y (which is u₂) depends directly on x and it depends on u₁, which also depends on x. So, x influences y through two paths:\nDirectly (the x term in x + u₁). Indirectly (through u₁ which is x²). The simple chain rule dy/dx = (∂y/∂u₁) * (du₁/dx) would give 1 * 2x = 2x, which is wrong (the derivative of x+x² is 1+2x).\nThe \u0026ldquo;Law\u0026rdquo; of Total Derivatives (page 19): To get dy/dx (the total derivative of y with respect to x), you need to sum up all possible contributions from changes in x to the change in y. For y = u₂(x, u₁(x)): dy/dx = ∂u₂/∂x + (∂u₂/∂u₁) * (du₁/dx) Let\u0026rsquo;s break this down:\n∂u₂/∂x: This is the partial derivative of u₂(x,u₁) with respect to x, treating u₁ as if it were an independent constant for this term. For u₂ = x + u₁, ∂u₂/∂x = 1. (How y changes if x wiggles but u₁ is held fixed). ∂u₂/∂u₁: Partial derivative of u₂(x,u₁) with respect to u₁, treating x as constant. For u₂ = x + u₁, ∂u₂/∂u₁ = 1. (How y changes if u₁ wiggles but x is held fixed). du₁/dx: Total derivative of u₁(x) with respect to x. For u₁ = x², du₁/dx = 2x. (How u₁ changes if x wiggles). So, dy/dx = 1 + (1 * 2x) = 1 + 2x. Correct! General Single-Variable Total-Derivative Chain Rule (page 19): If y = f(x, u₁, u₂, ..., uₙ), and each uᵢ is also a function of x (uᵢ(x)): dy/dx (total) = ∂f/∂x (direct) + Σᵢ (∂f/∂uᵢ) * (duᵢ/dx (total))\n∂f/∂x: Partial derivative of f w.r.t x, holding all uᵢ constant (measures direct effect of x on f). ∂f/∂uᵢ: Partial derivative of f w.r.t uᵢ, holding x and other uⱼ constant (how f changes if only uᵢ changes). duᵢ/dx: Total derivative of uᵢ w.r.t x (how uᵢ itself changes when x changes, which might involve its own chain of dependencies). What this rule is ultimately trying to achieve: It correctly accounts for all paths through which a change in x can affect the final output y, summing up the contributions from the direct path and all indirect paths through intermediate variables uᵢ. Simplified Final Form (page 20): The paper cleverly shows that if you introduce x itself as another intermediate variable (e.g., uₙ₊₁ = x), then the \u0026ldquo;direct\u0026rdquo; ∂f/∂x term can be absorbed into the sum. If y = f(u₁, ..., uₙ, uₙ₊₁) where uᵢ = uᵢ(x) for all i, and uₙ₊₁ = x (so duₙ₊₁/dx = 1), Then dy/dx = Σᵢ (∂f/∂uᵢ) * (duᵢ/dx) (summing over all n+1 intermediate variables). This foreshadows the vector chain rule where the sum looks like a dot product.\nCaution on Terminology: The paper notes that what they call the \u0026ldquo;single-variable total-derivative chain rule\u0026rdquo; is often just called the \u0026ldquo;multivariable chain rule\u0026rdquo; in calculus discussions, which can be misleading because the overall function f(x) is still a scalar function of a single scalar x.\n4.5.3 Vector chain rule (Pages 21-22) - THE PAYOFF!\nNow for the most general case, relevant to neural networks:\ng(x): A function that takes a vector x and outputs an intermediate vector u = g(x).\nf(u): A function that takes the vector u and outputs a final vector y = f(u).\nWe want to find ∂y/∂x, the Jacobian of the composite function y = f(g(x)) with respect to x.\nThe Rule (beautifully simple in vector/matrix form): ∂y/∂x = (∂y/∂u) * (∂u/∂x) Where:\n∂u/∂x: Is the Jacobian of g with respect to x. If x is n x 1 and u is k x 1, this is a k x n matrix. ∂y/∂u: Is the Jacobian of f with respect to u. If u is k x 1 and y is m x 1, this is an m x k matrix. *: This is matrix multiplication. The result ∂y/∂x will be an m x n matrix, which is the correct shape for the Jacobian of y (m-dim) w.r.t. x (n-dim). How the paper rediscovers it (page 21): It starts with a simpler case: y = f(g(x)) where x is a scalar, g(x) is a vector [g₁(x), g₂(x)]ᵀ, and f(g) is a vector [f₁(g), f₂(g)]ᵀ (where f₁ might depend on g₁ and g₂, and f₂ might also depend on g₁ and g₂).\n∂y/∂x will be a column vector [∂y₁/∂x, ∂y₂/∂x]ᵀ. Using the single-variable total-derivative chain rule for each component yᵢ: ∂y₁/∂x = (∂f₁/∂g₁) * (dg₁/dx) + (∂f₁/∂g₂) * (dg₂/dx) ∂y₂/∂x = (∂f₂/∂g₁) * (dg₁/dx) + (∂f₂/∂g₂) * (dg₂/dx) This can be written in matrix form: [ ∂y₁/∂x ] = [ ∂f₁/∂g₁ ∂f₁/∂g₂ ] * [ dg₁/dx ] [ ∂y₂/∂x ] [ ∂f₂/∂g₁ ∂f₂/∂g₂ ] [ dg₂/dx ] This is exactly ∂y/∂x = (∂y/∂g) * (∂g/∂x)! (Here u is called g). The Beauty of the Vector Chain Rule (page 22): It automatically takes care of the total derivative aspect (summing over all intermediate paths) because of how matrix multiplication is defined (sum of products). The full Jacobian components are shown, illustrating the m x k matrix ∂f/∂g multiplying the k x n matrix ∂g/∂x to give an m x n result.\nSimplification for Element-wise Operations (page 22, bottom): If f operates element-wise on g (i.e., yᵢ = fᵢ(gᵢ)) AND g operates element-wise on x (i.e., gᵢ = gᵢ(xᵢ)), then:\n∂f/∂g becomes diag(∂fᵢ/∂gᵢ) ∂g/∂x becomes diag(∂gᵢ/∂xᵢ) Then ∂y/∂x = diag(∂fᵢ/∂gᵢ) * diag(∂gᵢ/∂xᵢ) = diag( (∂fᵢ/∂gᵢ) * (∂gᵢ/∂xᵢ) ). The Jacobian is diagonal, and each diagonal element is just the result of the single-variable chain rule applied to the components. This connects back to the earlier sections. Key Takeaway from Section 4.5: The chain rule, in its various forms, is the fundamental mechanism for calculating derivatives of complex, nested functions.\nFor a simple chain y(u(x)), it\u0026rsquo;s dy/dx = (dy/du)(du/dx). If x can affect y through multiple paths via intermediate variables uᵢ(x), the total derivative involves summing the contributions from each path. For vector functions of vector variables y(u(x)), the Jacobian ∂y/∂x = (∂y/∂u)(∂u/∂x) (matrix multiplication) elegantly captures all these interactions. This is the version most directly applicable to backpropagation in neural networks, where ∂y/∂u is the gradient from the layer above, and ∂u/∂x is the local gradient of the current layer. This vector chain rule is what allows backpropagation to efficiently compute the gradient of the final loss with respect to every single weight in a deep network by \u0026ldquo;chaining\u0026rdquo; these Jacobian multiplications backward through the layers. Each Jacobian ∂(layer_output)/∂(layer_input) tells us how a layer transforms incoming gradient signals.\nSection 5: The gradient of neuron activation The Big Picture of This Section:\nIn the previous section (4.5.3 Vector Chain Rule), the paper showed how to calculate ∂y/∂x = (∂y/∂u) * (∂u/∂x) where u=g(x) and y=f(u). This section is going to calculate one of these crucial Jacobians: the ∂u/∂x part, specifically for a typical neuron\u0026rsquo;s \u0026ldquo;affine function\u0026rdquo; (the weighted sum + bias) before the non-linear activation. Then, it will combine it with the derivative of an activation function (like max(0,z)) using the chain rule.\nEssentially, we want to answer:\nIf a neuron calculates z = w·x + b (affine part) and then activation = A(z) (activation part), how does this activation change if we wiggle the inputs x, the weights w, or the bias b? This is a fundamental building block for backpropagation. When we have the gradient of the loss with respect to a neuron\u0026rsquo;s activation (this would be like ∂L/∂activation, coming from layers above), we\u0026rsquo;ll need ∂activation/∂w and ∂activation/∂b to find out how to update that neuron\u0026rsquo;s weights and bias.\nWhat we are ultimately trying to achieve here is to find the specific derivative expressions (gradients) for a single neuron\u0026rsquo;s output with respect to its inputs and its own parameters (weights and bias). These will be the local gradients used in the backpropagation algorithm.\nBreaking it Down:\nThe typical neuron computation is:\nAffine function: z(w, b, x) = w · x + b (a scalar output if we consider one neuron for now) Activation function: activation(z) = A(z). The paper uses A(z) = max(0, z) (ReLU) as the example. So, the overall neuron activation is activation(x) = max(0, w · x + b).\nStep 1: Focus on the Affine Part z = w · x + b (Page 24, bottom half)\nLet y = w · x + b. (The paper uses y here for the affine part before max, which is a bit confusing as y is often the final output. Let\u0026rsquo;s stick to z for the affine part as the paper does in max(0,z)). So, z = w · x + b. We need ∂z/∂w and ∂z/∂b.\n∂z/∂w (Derivative of w·x + b with respect to vector w):\n∂/∂w (w·x): This is the derivative of a dot product. The paper cleverly rephrases w·x as sum(w ⊗ x) where ⊗ is element-wise multiplication. Let u = w ⊗ x. We know from Section 4.2 that ∂u/∂w = diag(x). Let z' = sum(u). We know from Section 4.4 that ∂z'/∂u = 1ᵀ (a row vector of ones). Using the vector chain rule for ∂z'/∂w = (∂z'/∂u) * (∂u/∂w): ∂(w·x)/∂w = 1ᵀ * diag(x) = xᵀ (a row vector). ∂/∂w (b): Since b is a scalar bias and doesn\u0026rsquo;t depend on w, this derivative is 0ᵀ (a row vector of zeros). So, ∂z/∂w = ∂(w·x + b)/∂w = xᵀ + 0ᵀ = xᵀ. Intuition: How does z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b change if wⱼ changes? It changes by xⱼ. So the gradient vector is [x₁, x₂, ..., xₙ], which is xᵀ. What this is trying to achieve: It tells us that the sensitivity of the pre-activation value z to a change in a weight wⱼ is simply the value of the corresponding input xⱼ. Larger inputs mean that their corresponding weights have a bigger impact. ∂z/∂b (Derivative of w·x + b with respect to scalar b):\n∂/∂b (w·x): This is 0, as w·x doesn\u0026rsquo;t depend on b. ∂/∂b (b): This is 1. So, ∂z/∂b = 0 + 1 = 1. Intuition: If you change the bias b by a small amount Δ, z changes by Δ. The rate of change is 1. What this is trying to achieve: It tells us that the pre-activation value z changes one-to-one with changes in the bias b. Step 2: Tackle the max(0, z) Activation Function (ReLU) (Page 24, bottom, and Page 25, top)\nLet activation = A(z) = max(0, z). We need ∂A/∂z. The max(0,z) function is piecewise:\nIf z ≤ 0, then A(z) = 0. The derivative dA/dz = 0. If z \u0026gt; 0, then A(z) = z. The derivative dA/dz = 1. The paper writes this as: ∂/∂z max(0, z) = { 0 if z ≤ 0; 1 if z \u0026gt; 0 }. (Note: Technically, it\u0026rsquo;s not differentiable at z=0, but in practice, we can assign a subderivative of 0 or 1). The paper then briefly mentions \u0026ldquo;broadcasting\u0026rdquo; for max(0, x) if x is a vector, meaning max is applied element-wise. The derivative ∂/∂xᵢ max(0, xᵢ) would be 0 if xᵢ ≤ 0 and 1 if xᵢ \u0026gt; 0.\nStep 3: Combine using the Chain Rule for activation(x) = max(0, w · x + b) (Page 25)\nLet z(w, b, x) = w · x + b (intermediate scalar variable representing the affine part). Let activation(z) = max(0, z).\nWe want ∂activation/∂w and ∂activation/∂b. Using the vector chain rule (or its scalar adaptation here since z and activation are scalars for a single neuron):\n∂activation/∂w = (∂activation/∂z) * (∂z/∂w)\n∂activation/∂z is 0 if z ≤ 0 and 1 if z \u0026gt; 0. ∂z/∂w is xᵀ. So, ∂activation/∂w = { 0ᵀ if z ≤ 0 (i.e., w·x+b ≤ 0) } { xᵀ if z \u0026gt; 0 (i.e., w·x+b \u0026gt; 0) } What this is ultimately trying to achieve: If the neuron is \u0026ldquo;off\u0026rdquo; (pre-activation z is negative or zero, so ReLU output is 0), then small changes to the weights w have no effect on the output (gradient is 0). If the neuron is \u0026ldquo;on\u0026rdquo; (z \u0026gt; 0, ReLU output is z), then the way the output changes with respect to w is the same as how z changes with respect to w, which is xᵀ. ∂activation/∂b = (∂activation/∂z) * (∂z/∂b)\n∂activation/∂z is 0 if z ≤ 0 and 1 if z \u0026gt; 0. ∂z/∂b is 1. So, ∂activation/∂b = { 0 if z ≤ 0 (i.e., w·x+b ≤ 0) } { 1 if z \u0026gt; 0 (i.e., w·x+b \u0026gt; 0) } What this is ultimately trying to achieve: If the neuron is \u0026ldquo;off\u0026rdquo;, changes to b have no effect on the output. If the neuron is \u0026ldquo;on\u0026rdquo;, changes to b have a one-to-one effect on the output. Key Takeaway from Section 5: This section derived the gradients of a single ReLU neuron\u0026rsquo;s activation with respect to its weights w and its bias b.\n∂(ReLU(w·x+b))/∂w = xᵀ if w·x+b \u0026gt; 0, and 0ᵀ otherwise. ∂(ReLU(w·x+b))/∂b = 1 if w·x+b \u0026gt; 0, and 0 otherwise. These are the local gradients for a single neuron. In backpropagation, these local gradients will be multiplied by the gradient of the loss coming from the layer above (using the chain rule again!) to find out how much that specific neuron\u0026rsquo;s weights and bias contributed to the overall network error. This tells us how to update w and b for that neuron.\nSection 6: The gradient of the neural network loss function The Big Picture of This Section:\nThis is where everything we\u0026rsquo;ve learned about matrix calculus (gradients, Jacobians, chain rules) comes together to explain how a neural network learns by minimizing a loss function.\nWe have:\nA model (our neural network) with parameters (weights w and biases b for all neurons). A loss function C(w, b, X, y) that measures how \u0026ldquo;bad\u0026rdquo; the model\u0026rsquo;s predictions are on a set of training data X with true labels y. For example, Mean Squared Error for regression. An optimization algorithm (Gradient Descent) that needs the gradient of this loss function C with respect to all the model\u0026rsquo;s parameters (w\u0026rsquo;s and b\u0026rsquo;s) to update them and reduce the loss. What we are ultimately trying to achieve in this section is to derive the formulas for ∂C/∂w and ∂C/∂b for a simple neuron, using the chain rule and the neuron activation gradients we found in Section 5. This will show the complete calculation for one neuron\u0026rsquo;s parameter updates.\nSetting the Stage (Page 25, bottom):\nThe paper considers training a single neuron.\nInput: X is a matrix where each row xᵢ is an input vector for one training instance. N is the number of instances. Target: y is a vector where each yᵢ is the scalar target output for instance xᵢ. Loss Function (Mean Squared Error): C(w, b, X, y) = (1/N) * Σᵢ (yᵢ - activation(xᵢ))² where activation(xᵢ) = max(0, w · xᵢ + b). Following the Chain Rule Process (Page 26):\nTo find ∂C/∂w (and later ∂C/∂b), we introduce intermediate variables to break down the loss function calculation for a single instance xᵢ first, and then we\u0026rsquo;ll average over all instances.\nLet\u0026rsquo;s consider the contribution of one instance x (dropping the i superscript for a moment) to the loss: (y_true - activation(x))². The paper uses these intermediate variables:\nu(w, b, x) = max(0, w · x + b) (This is the neuron\u0026rsquo;s activation. Let\u0026rsquo;s call it a for activation to avoid confusion with u often being an intermediate in chain rule). So, a = max(0, w·x+b). v(y_true, a) = y_true - a (This is the error for this instance). L_instance(v) = v² (The squared error for this instance. The total loss C will be the average of these L_instance values). 6.1 The gradient with respect to the weights w (Pages 26-27)\nWe want ∂C/∂w. Since C is an average, ∂C/∂w = (1/N) Σᵢ (∂L_instanceᵢ / ∂w). Let\u0026rsquo;s find ∂L_instance / ∂w for one instance. L_instance = v² v = y_true - a a = max(0, w·x + b)\nUsing the chain rule: ∂L_instance/∂w = (∂L_instance/∂v) * (∂v/∂a) * (∂a/∂w)\n∂L_instance/∂v = ∂(v²)/∂v = 2v ∂v/∂a = ∂(y_true - a)/∂a = -1 ∂a/∂w = ∂(max(0, w·x+b))/∂w. We found this in Section 5! It\u0026rsquo;s xᵀ if w·x+b \u0026gt; 0 (neuron is active). It\u0026rsquo;s 0ᵀ if w·x+b ≤ 0 (neuron is not active). Combining these for one instance: ∂L_instance/∂w = (2v) * (-1) * (xᵀ or 0ᵀ) ∂L_instance/∂w = -2v * (xᵀ or 0ᵀ) Substitute v = y_true - a = y_true - max(0, w·x+b): ∂L_instance/∂w = -2(y_true - max(0, w·x+b)) * (xᵀ or 0ᵀ)\nThe paper simplifies this into a conditional expression (top of page 26, and then expanded middle of page 27):\nIf w·x + b ≤ 0 (neuron not active, max(0,...) = 0): Then ∂L_instance/∂w = -2(y_true - 0) * 0ᵀ = 0ᵀ. (Makes sense, if neuron output is 0, weights don\u0026rsquo;t affect it in this region). If w·x + b \u0026gt; 0 (neuron active, max(0,...) = w·x+b): Then ∂L_instance/∂w = -2(y_true - (w·x+b)) * xᵀ = 2((w·x+b) - y_true) * xᵀ. Now, average over all N instances xᵢ: ∂C/∂w = (1/N) Σᵢ 2 * ( (w·xᵢ+b) - yᵢ ) * xᵢᵀ (This sum is only for instances where w·xᵢ+b \u0026gt; 0). Or, more compactly, as shown in the paper before the \u0026ldquo;To interpret that equation\u0026hellip;\u0026rdquo; part: ∂C/∂w = (2/N) Σᵢ eᵢ xᵢᵀ (where eᵢ = (w·xᵢ+b) - yᵢ is the error, and this sum is for the \u0026ldquo;nonzero activation case,\u0026rdquo; i.e., where w·xᵢ+b \u0026gt; 0).\nInterpretation (Page 27): ∂C/∂w is a weighted average of the input vectors xᵢᵀ (transposed to be row vectors, which when summed give the gradient row vector). The weights for this average are the error terms eᵢ (the difference between the neuron\u0026rsquo;s affine output w·xᵢ+b and the target yᵢ). What this is ultimately trying to achieve: If the error eᵢ for instance xᵢ is large and positive (model predicted too high), the gradient ∂C/∂w will be pushed in the direction of xᵢᵀ. To reduce the error via gradient descent (which subtracts the gradient), the weights w will be adjusted against xᵢᵀ, effectively reducing the output w·xᵢ+b. If the error eᵢ is large and negative (model predicted too low), the gradient ∂C/∂w will be pushed in the direction opposite to xᵢᵀ. Gradient descent will adjust w towards xᵢᵀ, increasing the output. Instances xᵢ that result in larger errors eᵢ have more influence on the gradient (they \u0026ldquo;pull\u0026rdquo; the weights harder). This is exactly what we want for learning! The gradient points in a direction that tells us how to change w to reduce the average error. Gradient Descent Update (Page 27, bottom): w_next = w_current - η * (∂C/∂w) This is the standard gradient descent update rule. Because the gradient ∂C/∂w points towards higher cost, we subtract a small amount of it (scaled by learning rate η) from the current weights to move towards lower cost.\n6.2 The derivative with respect to the bias b (Pages 27-28)\nThe process is very similar. We want ∂C/∂b. ∂C/∂b = (1/N) Σᵢ (∂L_instanceᵢ / ∂b). Using the chain rule: ∂L_instance/∂b = (∂L_instance/∂v) * (∂v/∂a) * (∂a/∂b)\n∂L_instance/∂v = 2v (same as before) ∂v/∂a = -1 (same as before) ∂a/∂b = ∂(max(0, w·x+b))/∂b. We found this in Section 5! It\u0026rsquo;s 1 if w·x+b \u0026gt; 0 (neuron is active). It\u0026rsquo;s 0 if w·x+b ≤ 0 (neuron is not active). Combining these for one instance: ∂L_instance/∂b = (2v) * (-1) * (1 or 0) ∂L_instance/∂b = -2v * (1 or 0) Substitute v = y_true - max(0, w·x+b):\nIf w·x + b ≤ 0: ∂L_instance/∂b = -2(y_true - 0) * 0 = 0. If w·x + b \u0026gt; 0: ∂L_instance/∂b = -2(y_true - (w·x+b)) * 1 = 2((w·x+b) - y_true). Averaging over all N instances xᵢ: ∂C/∂b = (1/N) Σᵢ 2 * ( (w·xᵢ+b) - yᵢ ) (This sum is only for instances where w·xᵢ+b \u0026gt; 0). Or, as the paper shows: ∂C/∂b = (2/N) Σᵢ eᵢ (where eᵢ = (w·xᵢ+b) - yᵢ, summed for the \u0026ldquo;nonzero activation case\u0026rdquo;).\nInterpretation (Page 28): The partial derivative ∂C/∂b is just the average error eᵢ (for activated neurons), scaled by 2. What this is ultimately trying to achieve: If, on average, the neuron is predicting too high (average eᵢ is positive), the gradient ∂C/∂b is positive. Gradient descent b_next = b - η(∂C/∂b) will decrease b, reducing the neuron\u0026rsquo;s output. If, on average, the neuron is predicting too low (average eᵢ is negative), the gradient ∂C/∂b is negative. Gradient descent will increase b, increasing the neuron\u0026rsquo;s output. Combining w and b (Page 29, top): The paper mentions that in practice, it\u0026rsquo;s convenient to combine w and b into a single parameter vector ŵ = [wᵀ, b]ᵀ and augment the input vector x with a 1: ✰ = [xᵀ, 1]ᵀ. Then the affine function is just ŵ · ✰. This simplifies some notation but the underlying derivative calculations are the same.\nSection 7: Summary (Page 29) This section of the paper concludes that we now have the two partials (∂C/∂w and ∂C/∂b) necessary to perform gradient descent and optimize the parameters of a single neuron. It also reminds us that the next step in understanding full backpropagation for a multi-layer network would be to learn about partial derivatives of matrices, not just vectors.\nKey Takeaway from Section 6: This section ties everything together for a single neuron. By applying the chain rule, we derived how the overall Mean Squared Error loss changes with respect to the neuron\u0026rsquo;s weights w and its bias b.\n∂C/∂w is an average of input vectors xᵢᵀ, weighted by the prediction errors eᵢ. ∂C/∂b is an average of the prediction errors eᵢ. These gradients tell gradient descent how to adjust w and b to make the neuron\u0026rsquo;s predictions better on average for the given training data. This is the fundamental step of learning for one neuron. Backpropagation is essentially applying this logic layer by layer, from the output back to the input, using the chain rule at each step. ","permalink":"https://deepskandpal.github.io/papershelf/maths/","tags":null,"title":"The Matrix Calculus You Need For Deep Learning"},{"categories":["Horror"],"content":"","permalink":"https://deepskandpal.github.io/stories/ek-kahani-aisibhi/","tags":null,"title":""},{"categories":["Engineering"],"content":"The problem : Help! My Python SDXL Script Isn\u0026rsquo;t Faster with Asyncio/Threading/Multiprocessing. Why?\nYou\u0026rsquo;ve built a cool script, maybe generating image variations with SDXL (Stable Diffusion XL model) like one of our engineers. It works, but it\u0026rsquo;s slow. You think, \u0026ldquo;I know! Parallelism!\u0026rdquo; You try asyncio, then multithreading, maybe even multiprocessing. But\u0026hellip; nothing speeds up significantly, or you just hit weird errors, especially in your Jupyter Notebook. Sounds familiar?\nThis is a common hurdle when data science tasks meet heavier computation. Let\u0026rsquo;s demystify Python\u0026rsquo;s asyncio, multithreading, and multiprocessing, touching on the underlying Operating System (OS) ideas and Python\u0026rsquo;s infamous GIL.\nThe Coffee Shop Analogy \u0026amp; OS Concepts Imagine you need to make multiple coffees:\nNormal Python (Single Worker): One barista makes one coffee start-to-finish, then the next. This is like a standard single process, single thread application. Simple, sequential. AsyncIO (Efficient Single Barista): One barista starts brewing espresso (await), and while it\u0026rsquo;s brewing (an operation it doesn\u0026rsquo;t need to watch constantly), immediately starts steaming milk (await) for the same coffee. OS Concept: Relies on non-blocking I/O. Instead of waiting idly, the task tells the OS \u0026ldquo;let me know when this network/disk operation is done\u0026rdquo; and yields control. An event loop manages these pending operations, running code only when needed. It all happens within a single thread and single process. Good for: Tasks with lots of waiting (I/O-bound). Bad for: Tasks needing constant active work (CPU/GPU-bound). Still only one \u0026ldquo;worker\u0026rdquo; doing the active computation at any moment. Multithreading (Multiple Baristas, ONE Espresso Machine): Multiple baristas share one counter and one main espresso machine. Only one can use the machine at a time. OS Concept: Threads are lightweight execution units within a single process. They share the same memory space. The OS rapidly switches between threads (context switching), giving an illusion of parallelism. Python\u0026rsquo;s Catch (The GIL): In standard Python (CPython), the Global Interpreter Lock (GIL) acts like that single espresso machine. It allows only one thread to execute Python bytecode at any given time, even on multi-core CPUs. Good for: I/O-bound tasks where threads spend time waiting (releasing the GIL), letting others run Python code. Bad for: CPU-bound Python code, as the GIL prevents true parallel Python execution. Multiprocessing (Multiple Full Coffee Stations): Several independent stations, each with its own barista and espresso machine. They work truly in parallel. OS Concept: Processes are independent instances of a program, each with its own separate memory space and resources. The OS schedules these processes across different CPU cores for true hardware parallelism. Python\u0026rsquo;s Advantage: Since each process has its own memory and Python interpreter, each has its own GIL. They don\u0026rsquo;t block each other\u0026rsquo;s Python execution. Good for: CPU-bound tasks that can be run independently. Bad for: Tasks needing lots of communication between processes (requires slower Inter-Process Communication - IPC) or where process creation overhead is significant. Why Didn\u0026rsquo;t Your Attempts Work for SDXL? AsyncIO: Image generation is CPU/GPU-bound, not I/O-bound. There\u0026rsquo;s no significant \u0026ldquo;waiting\u0026rdquo; for asyncio\u0026rsquo;s non-blocking I/O magic to exploit. Result: No speedup. Multithreading: Image generation needs CPU/GPU work. The GIL prevented parallel Python execution. Even if underlying libraries release the GIL during GPU work, the Python coordination parts likely remained serialized. Result: No significant speedup. Multiprocessing: In Jupyter Notebook: Notebook state conflicts with the OS fork mechanism used to create processes. Result: Weird errors. In a .py Script: This should allow parallel Python execution across CPU cores. The limited speedup suggests: GPU Bottleneck: Most likely! Your multiple processes (running parallel Python on CPU cores) are probably queuing up to use the single GPU resource. The OS can schedule processes in parallel, but hardware contention limits throughput. Overhead: Process creation, model loading per-process (heavy on RAM/VRAM), and data transfer (IPC) add costs. Not Enough CPU Work: If the GPU does 95% of the work, parallelizing the CPU part has limited impact. Key Takeaways \u0026amp; What To Do Match Tool to Task \u0026amp; OS Behavior:\nasyncio: Non-blocking I/O for waiting tasks.\nmultithreading: Shared memory, GIL bottleneck for CPU-bound Python, good for I/O waits.\nmultiprocessing: Separate memory, true CPU parallelism, best for heavy computation but watch for overhead and resource contention (like a single GPU).\nFor the SDXL Task:\nUse .py scripts for multiprocessing. Profile! Check CPU and GPU utilization. Is the GPU the bottleneck? Manage Resources: Minimize per-process setup (model loading) and inter-process data transfer. Check Framework specifics: How does MLX handle concurrent access from multiple processes to the same GPU? We will cover this in detail in coming articles.\nUnderstanding how Python\u0026rsquo;s concurrency tools interact with OS process/thread management, memory sharing, and the specific limitations like the GIL is crucial. Don\u0026rsquo;t just apply parallelism blindly; diagnose the bottleneck and choose the strategy that addresses it effectively. Good luck!\n","permalink":"https://deepskandpal.github.io/tech-writings/concurrency-ds/","tags":null,"title":"Multi Processing , Multi Threading, AsyncIO: A Guide to Python Concurrency for Data Scientists"},{"categories":null,"content":"Hi—I’m Deepanshu Kandpal. Over the past eight years I’ve jumped between e‑commerce, telecom, consulting and cybersecurity, always chasing that moment when clever code becomes real‑world impact. Right now at KnowBe4 I’m building LLM‑powered agents that sniff out threats and automate responses at scale. Before that at Farfetch I helped grow an A/B testing platform to handle 100K+ queries a day across 300 TB of data—learning firsthand how to coordinate agile teams across continents. On the side, I tinker with LangChef, my open‑source framework for LLM experimentation. I actually got my start as an iOS developer at Jio, crafting AI assistants that now handle over 3 million queries daily and take a huge load off call‑center teams.\n","permalink":"https://deepskandpal.github.io/about/","tags":null,"title":"About"},{"categories":["Fitness"],"content":" Day Meal Time Meal Description Notes / Katori Guide Day 1 (e.g., Monday) Early Morning (Pre-Workout) 1 Banana OR Small handful of almonds (5-6) Breakfast 3-4 Egg Omelette (with onions, tomatoes, capsicum, spinach) + 1 Whole Wheat Roti OR 1 slice Whole Wheat Toast Mid-Morning Snack 1 MK Sprouts Salad (moong, chana) with lemon juice \u0026amp; chaat masala MK: ~150ml Lunch 1 LK Chicken Curry (focus on chicken pieces) OR 1 LK Paneer Bhurji/Curry + 1 LK Mixed Vegetable Sabzi (e.g., Gobi, Carrot, Peas) + 1 MK Cooked Brown Rice OR 2 Whole Wheat Rotis + Generous Green Salad LK: ~240ml, MK: ~150ml Evening Snack 1 Apple OR Pear + Small handful of Roasted Chana (unsalted) Dinner 1 LK Baked/Grilled Fish OR 1 LK Tofu Stir-fry with plenty of vegetables + 1 MK Dal (e.g., Moong Dal) + 1 LK Lauki/Bhindi Sabzi + 1 Whole Wheat Roti (optional) LK: ~240ml, MK: ~150ml Day 2 (e.g., Tuesday) Early Morning (Pre-Workout) Small sweet potato (boiled) OR 1 small apple Breakfast 2 Moong Dal Chillas OR Besan Chillas (medium size) stuffed with 1 SK grated paneer/vegetables + 1 SK Mint-coriander chutney (no sugar) SK: ~110ml Mid-Morning Snack 1 Orange OR 1 MK Papaya MK: ~150ml Lunch 1 LK Rajma Curry OR Chana Masala + 1 LK Palak Sabzi (spinach preparation) + 1 MK Quinoa OR 2 Millets (Bajra/Jowar) Rotis + Generous Green Salad LK: ~240ml, MK: ~150ml Evening Snack 1-2 Boiled Eggs (if you eat eggs) OR Handful of peanuts (approx. 20-25) Dinner 1 LK Paneer Tikka (grilled/baked) OR 1 LK Soy Chunk Curry + 1 MK Vegetable Raita (plain yogurt) OR Large mixed bean salad + 1 LK Baingan Bharta OR Cabbage Sabzi LK: ~240ml, MK: ~150ml. Avoid sugary/mayo-based Raita. Day 3 (e.g., Wednesday) Breakfast 1 MK Vegetable Poha (with peas, carrots, onions) + 1 SK roasted peanuts sprinkled on top + 1 Glass Unsweetened Lassi OR Buttermilk (Chaas) MK: ~150ml, SK: ~110ml Mid-Morning Snack 1 Guava OR 1 Pomegranate Lunch 1 LK Mixed Dal (e.g., Toor + Masoor) + 1 LK Methi Aloo Sabzi (more methi, less aloo) + 1 MK Cooked Rice OR 2 Whole Wheat Rotis + Generous Green Salad LK: ~240ml, MK: ~150ml Evening Snack 1 MK Fruit Salad (mixed fruits) MK: ~150ml Dinner Vegetable Clear Soup (large bowl) + 1 LK Mushroom \u0026amp; Capsicum Sabzi OR Stir-fried Mixed Vegetables + 1-2 Besan or Moong Dal Chillas (plain or minimal veggie stuffing) LK: ~240ml Day 4 (e.g., Thursday) Early Morning (Pre-Workout) Black coffee + 4-5 Almonds \u0026amp; 1-2 Walnuts Breakfast 3-Egg Scramble with 1 SK chopped chicken/turkey ham (if non-veg) or tofu/paneer cubes + 1 Whole Wheat Roti OR 1 slice Whole Wheat Toast SK: ~110ml Mid-Morning Snack 1 MK Steamed Edamame (if available) OR 1 MK Sprouts MK: ~150ml Lunch 1 LK Fish Curry OR 1 LK Matar Paneer (Peas \u0026amp; Paneer) + 1 LK Thoran/Poriyal (South Indian dry veg) + 1 MK Brown Rice OR 2 Rotis + Generous Salad LK: ~240ml, MK: ~150ml Evening Snack 1 Apple with 1 tablespoon Peanut Butter (unsweetened) Dinner 1 LK Chicken Stir-fry with lots of colorful vegetables OR 1 LK Paneer \u0026amp; Bell Pepper Stir-fry + Small portion (1 SK) of Quinoa or Brown Rice (optional) + Large bowl Clear Broth LK: ~240ml, SK: ~110ml Day 5 (e.g., Friday) Early Morning (Pre-Workout) 1 small Banana + Black Coffee Breakfast 1 LK Paneer Bhurji + 2 slices Whole Wheat Toast OR 2 small Uthappams (with lots of onion/tomato topping) LK: ~240ml Mid-Morning Snack 1 Pear OR handful of Grapes Lunch 1 LK Mutton Curry (lean pieces, less oil) OR 1 LK Lobia (Black-eyed peas) Curry + 1 LK Bhindi (Okra) Sabzi + 1 MK Rice OR 2 Rotis + Generous Salad LK: ~240ml, MK: ~150ml Evening Snack Handful of Mixed Nuts \u0026amp; Seeds (almonds, walnuts, pumpkin seeds) Dinner Grilled Chicken Breast (1-2 pieces) OR Large portion (1 LK) of Grilled Paneer/Tofu steaks + 1 LK Steamed or Sautéed Mixed Vegetables + Salad with lemon-herb dressing LK: ~240ml Day 6 (e.g., Saturday) Breakfast Vegetable Idli (3-4) with 1 MK Sambhar (focus on dal \u0026amp; veggies) + Minimal coconut chutney MK: ~150ml Mid-Morning Snack 1 MK Watermelon or Muskmelon MK: ~150ml Lunch Mindful Meal Out/Relaxed Meal: E.g., Tandoori Chicken/Fish (2-3 pieces) with Salad OR Veggie \u0026amp; Paneer Kathi Roll (whole wheat, less sauce) Avoid creamy curries, large biryanis, naan. Evening Snack 1 MK Roasted Makhana (Fox Nuts) MK: ~150ml Dinner 1 LK Dal Tadka (Yellow Dal) + 1 LK Gajar Matar Sabzi (Carrot \u0026amp; Peas) + 2 Whole Wheat Rotis + Side Salad LK: ~240ml Day 7 (e.g., Sunday) Breakfast Paneer Paratha (1-2, less oil, more filling) OR Aloo Paratha (1-2) with 1 SK plain low-fat yogurt (optional) or pickle. SK: ~110ml. If Aloo Paratha, add protein like leftover dal/egg. Mid-Morning Snack Your choice of 1 Fruit Lunch Home-style Chicken Biryani (1 LK, focus on chicken, less rice) with 1 MK Raita (if using yogurt) OR Large Salad. Veg: Veg Biryani/Pulao (1 LK) with 1 MK Dal or Mixed Veg Raita. LK: ~240ml, MK: ~150ml Evening Snack Sweet Potato Chaat (1 MK, boiled sweet potato, chaat masala, lemon) MK: ~150ml Dinner Keep it light: Large Bowl of Mixed Vegetable \u0026amp; Lentil Soup + 1-2 slices Whole Wheat Toast with 1 SK hummus (optional) SK: ~110ml General Reminders for the Diet Plan:\nHydration: Drink 3-4 liters of water daily. Protein Focus: Ensure every main meal has a substantial protein source. Cooking Oil: Use minimal oil (1-2 tsp per person per meal). Your Yogurt Dip: If used, keep portions very small (1-2 tbsp max) as a condiment, not a replacement for plain yogurt servings. Listen to Your Body: Adjust vegetable portions up if hungry. This is a template; adapt with similar food types. ","permalink":"https://deepskandpal.github.io/fitness-log/diet-plan/","tags":["Plan"],"title":"Current Diet Plan"},{"categories":["Fitness"],"content":"Important Notes Before You Start:\nWarm-up: Always perform a 5-10 minute warm-up before each workout (light cardio, dynamic stretches). Cool-down: Always perform a 5-10 minute cool-down after each workout (static stretching). Weight Column: This column is for you to fill in each week. Start with a weight that challenges you to complete the target reps with good form. If you can easily complete the highest number of reps listed (e.g., 12 reps) for all sets, increase the weight slightly the following week. Form is always more important than the amount of weight. Progression (Weeks 1-8): Focus on Progressive Overload. Gradually increase weight, reps, sets, or intensity, or decrease rest time. Listen to Your Body: Stop if you feel pain. Take extra rest days if needed. Workout Plan: Weeks 1-8\nMonday: Upper Body Strength\nExercise Sets Reps Rest (between sets) Notes Bench Press / Push-ups 3 8-12 60-90s Choose variation appropriate for strength Bent-Over Rows (DB or BB) 3 8-12 60-90s Keep back straight, pull towards hips Overhead Press (DB or BB) 3 8-12 60-90s Keep core tight, avoid excessive back arch Lat Pulldowns / Pull-ups 3 8-12 60-90s Use assisted machine or bands if needed Dumbbell Bicep Curls 3 10-12 60s Avoid swinging Triceps Pushdowns / Overhead Ext 3 10-12 60s Choose one based on equipment Progression Note: Increase weight when you can comfortably complete 12 reps for all 3 sets with good form. Tuesday: Active Recovery\nActivity Duration Intensity Notes Walk (e.g., 6k) or Light Cycling/Yoga 45-60 mins+ Low Focus on movement and recovery Wednesday: Lower Body \u0026amp; Core Strength\nExercise Sets Reps / Time Rest (between sets) Notes Squats (Barbell or Goblet) 3 8-12 60-90s Focus on depth and form Romanian Deadlifts / Good Mornings 3 8-12 60-90s Feel the stretch in hamstrings, keep back straight Lunges (Walking or Static) 3 10-12 / leg 60s Bodyweight or hold dumbbells Leg Press / Leg Extensions 3 10-12 60s Choose one based on equipment/preference Hamstring Curls 3 10-12 60s Control the negative (lowering phase) Calf Raises 3 15-20 45-60s Bodyweight or hold dumbbells, focus on full stretch Plank 3 30-60s hold 45s Keep body in a straight line Leg Raises 3 15-20 45s Keep lower back pressed down Russian Twists 3 15-20 / side 45s Bodyweight or hold light weight Progression Note: Increase weight/duration when you can comfortably complete the upper rep/time range for all 3 sets. Thursday: Full Body Functional / Boxing HIIT\nComponent Details Sets / Rounds Work Rest (in circuit) Rest (between circuits) Notes Warm-up Light Cardio + Dynamic Stretches + 5 min Shadow Boxing - - - - Prepare body for intense work Circuit 1 1. Kettlebell Swings 2. Box Jumps / Squat Jumps 3. Burpees 4. Mountain Climbers 3-4 45s 15s 60-90s Focus on explosive power and maintaining pace. Choose jump variation suitable for you. Circuit 2 1. Alt. Reverse Lunge + Knee Drive 2. Med Ball Slams / DB Ground-to-Overhead 3. Shadow Boxing (Intensity) 4. High Knees 3-4 45s 15s 60-90s Focus on intensity and full body engagement. Finisher (Opt.) Shadow Boxing / Heavy Bag Work 3-5 60s 30s - If time and energy permit Core (Opt.) E.g., Bicycle Crunches, V-Ups 2-3 15-20 reps 30s - Add if time permits Progression Note: Weeks 5-8: Increase intensity (more reps per 45s), increase work duration (e.g., 50s work / 10s rest), or add 1 more round to circuits. Friday: Full Body Strength (Compound Focus)\nExercise Sets Reps Rest (between sets) Notes Deadlifts (Conventional/Sumo) 3-4 5-8 90-120s PRIORITIZE FORM. Start lighter, focus on technique. Squats / Leg Press 3-4 5-8 90-120s Focus on heavy but controlled lifts Bench Press / Incline DB Press 3-4 5-8 90-120s Choose one based on preference Overhead Press / Push Press 3-4 5-8 90-120s Push press allows leg drive for slightly heavier weight Bent-Over Rows / T-Bar Rows 3-4 5-8 90-120s Focus on pulling with back muscles Pull-ups / Weighted Pull-ups / Lat Pulldowns 3-4 5-8 90-120s Choose variation allowing 5-8 reps Progression Note: Increase weight when you can comfortably complete 8 reps for all sets with good form. Consider adding the 4th set from Week 5 onwards. Saturday: Active Recovery\nActivity Duration Intensity Notes Walk (e.g., 6k) or Light Cycling/Yoga 45-60 mins+ Low Focus on movement and recovery Sunday: Rest\nActivity Duration Intensity Notes Rest or Very Light Stroll - Very Low Allow body to fully recover ","permalink":"https://deepskandpal.github.io/fitness-log/workout-plan/","tags":["Plan"],"title":"Current Workout Plan"},{"categories":null,"content":"Problem Description (Optional Summary) Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\nSolution Approach Use a hash map (dictionary in Python) to store numbers encountered so far and their indices. For each number, check if target - current_number exists in the hash map. If it does, we found the pair. Otherwise, add the current number and its index to the map.\nCode def twoSum(nums: list[int], target: int) -\u0026gt; list[int]: numMap = {} # val -\u0026gt; index for i, n in enumerate(nums): diff = target - n if diff in numMap: return [numMap[diff], i] numMap[n] = i return [] # Should not happen based on problem constraints Complexity Analysis Time: O(n) - We iterate through the list once. Hash map lookups/insertions are O(1) on average. Space: O(n) - In the worst case, the hash map stores almost all elements. Notes \u0026amp; Learnings Classic use case for a hash map to optimize search from O(n) (in a nested loop approach) to O(1). Remember to handle edge cases if the problem constraints allowed for no solution.\n","permalink":"https://deepskandpal.github.io/dsa-log/two-sum/","tags":["Python"],"title":"Two Sum"},{"categories":null,"content":"Introduction to Arrays An array is a fundamental data structure used to store a collection of elements, typically of the same data type, in contiguous memory locations. Each element is identified by an index or a key.\nKey Characteristics Fixed Size (Static Arrays): In many languages, traditional arrays have a fixed size defined at creation. Dynamic Size (Dynamic Arrays/Lists): Languages like Python offer dynamic arrays (lists) that can grow or shrink. Contiguous Memory: Elements are stored next to each other, allowing for efficient index-based access. O(1) Access: Accessing an element by its index is typically a constant time operation. O(n) Insertion/Deletion (Worst Case): Inserting or deleting elements in the middle may require shifting subsequent elements. Common Operations \u0026amp; Complexity Access (by index): O(1) Search (linear): O(n) Insertion (at end, amortized for dynamic): O(1) Insertion (at beginning/middle): O(n) Deletion (at end): O(1) Deletion (at beginning/middle): O(n) Use Cases Storing lists of items. Implementing other data structures (stacks, queues). Lookup tables (when used with indices). ","permalink":"https://deepskandpal.github.io/dsa-concepts/arrays/","tags":null,"title":"Arrays"},{"categories":null,"content":"Introduction to Hashing Hashing is the process of converting an input item (key) into a fixed-size value, typically an integer index, using a hash function. This index is then used to place or locate the item in a data structure, most commonly a hash table (hash map or dictionary).\nKey Concepts Hash Function: A function that maps keys to indices. A good hash function should be fast to compute and distribute keys uniformly across the available indices. Hash Table: A data structure that uses a hash function to map keys to values for efficient lookups. Collisions: Occur when two different keys map to the same index. Collision Resolution: Strategies to handle collisions, such as: * Separate Chaining: Each index points to a linked list (or other structure) containing all keys that hash to that index. * Open Addressing (Probing): If an index is occupied, probe for the next available slot (linear probing, quadratic probing, double hashing). Common Operations \u0026amp; Complexity (Average Case for Hash Tables) Insertion: O(1) Deletion: O(1) Search: O(1) (Worst Case for all can be O(n) if collisions are poorly handled or hash function is bad) Use Cases Implementing dictionaries/hash maps. Database indexing. Caching. Checking for duplicates. ","permalink":"https://deepskandpal.github.io/dsa-concepts/hashing/","tags":null,"title":"Hashing"},{"categories":null,"content":"","permalink":"https://deepskandpal.github.io/dsa-concepts/linked-lists/","tags":null,"title":"Linked Lists"},{"categories":["AI"],"content":"","permalink":"https://deepskandpal.github.io/papershelf/attention-all-u-need/","tags":null,"title":"Attention Is All You Need"},{"categories":["AI"],"content":"","permalink":"https://deepskandpal.github.io/papershelf/infinit-retrevial-attention/","tags":null,"title":"Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing"},{"categories":["AI"],"content":"Website\nTLDR; They tackled long video generation by replacing expensive global attention with efficient local attention, and bridging the gaps between local segments using novel TTT layers. These TTT layers act like RNNs but have a much smarter, adaptive hidden state (a neural network that learns on-the-fly during generation). This allows them to capture long-range dependencies and complex dynamics better than traditional RNNs, leading to more coherent minute-long videos, albeit with some remaining artifacts and efficiency challenges.\nWhat\u0026rsquo;s the Problem they are trying to solve? they want to create long videos, like a full minute long, that tell a coherent story with multiple scenes and characters doing things (like Tom chasing Jerry). Think of generating a mini-cartoon episode from a text description.\nThe main hurdle? Memory and Computation.\n1. Transformers (The Standard Powerhouse): These models are amazing for many things, but they use something called \u0026ldquo;self-attention.\u0026rdquo; Imagine every frame in the video needing to look back and compare itself to every other frame that came before it. For a few seconds, this is fine. But for a minute-long video (which can be thousands of frames or tokens!), the computation becomes astronomically expensive – it scales quadratically (if you double the length, the cost goes up by four times!). It\u0026rsquo;s like trying to have a conversation where, before speaking, you have to re-read the entire transcript of everything said so far.\n2. RNNs / State-Space Models (Like Mamba, DeltaNet - The Efficient Alternatives) : These are designed to be more efficient for long sequences. Their cost scales linearly (double the length, double the cost). Think of them like reading a book: you maintain a \u0026ldquo;summary\u0026rdquo; or \u0026ldquo;hidden state\u0026rdquo; in your head that captures the story so far, and you update it as you read each new word. Much faster.\n2.1. The Catch: The \u0026ldquo;summary\u0026rdquo; these models keep (their hidden state) is usually quite simple, often just a matrix of numbers. While efficient, we found this simple summary isn\u0026rsquo;t expressive enough to capture the complex details needed for a dynamic, multi-scene story like Tom and Jerry. It struggles to remember specific details from long ago or juggle multiple interacting elements coherently. It\u0026rsquo;s like trying to summarize a complex movie plot in just one short sentence – you lose too much information. Core Idea: Test-Time Training (TTT) Layers So, we need the efficiency of RNNs but the expressiveness that approaches Transformers. How? We introduced Test-Time Training (TTT) Layers. Here\u0026rsquo;s the intuition:\nRNNs Revisited: Think of any RNN layer. It takes an input (xt) and its current \u0026ldquo;memory\u0026rdquo; or hidden state (Wt-1), updates that memory (Wt), and produces an output (zt). Making the Memory Smarter: What if, instead of the hidden state (Wt) being just a passive matrix of numbers, it was itself a small neural network (f)? A mini-brain, if you will. Learning On-The-Fly: Now, here\u0026rsquo;s the key TTT part: As each new piece of the video sequence (xt) comes in during generation (at \u0026ldquo;test time\u0026rdquo;), we actually perform a tiny bit of training on this mini-brain hidden state. We update the weights (W) of this internal network f based on the input xt it just saw (using a quick gradient descent step, Equation 1). Why \u0026ldquo;Test-Time Training\u0026rdquo;? Because this training/updating of the hidden state (W) happens while the model is generating the video (test time), not just during the initial, large-scale training phase.\n4.1 The Benefit : Because the hidden state is now an active, learning neural network (they used a simple 2-layer MLP, see Section 2.3), it can potentially store much richer, more complex information than a static matrix. It adapts and learns specifically for the sequence it\u0026rsquo;s currently processing. This gives it a better chance to remember long-range dependencies and handle complex scene changes. It\u0026rsquo;s like having a smarter, adaptive summary that actively learns as the story unfolds. Putting TTT into a Video Model (Section 3): Base Model: They didn\u0026rsquo;t start from scratch. We took a powerful pre-trained video generation model (CogVideo-X 5B) which was good at generating short (3-second) clips. The Problem: CogVideo-X uses standard Transformer attention, which is too slow for one minute. The Approach (Figure 3): Local Attention: We keep the original Transformer self-attention layers but make them local. They only look at frames within their own 3-second segment. This keeps them fast. Global TTT: We insert our TTT layers. Since TTT layers are efficient (linear cost), we run them globally across the entire one-minute sequence. They act as the connective tissue, passing information between the 3-second segments that the local attention layers can\u0026rsquo;t see. Gating (Eq 6): When you add new layers (like TTT) to a pre-trained model, you don\u0026rsquo;t want them to mess things up initially. We use a \u0026ldquo;gate\u0026rdquo; (a learnable multiplier tanh(α)) that starts close to zero. This means, at first, the TTT layer\u0026rsquo;s output is mostly ignored. As the model fine-tunes, it can learn to \u0026ldquo;open the gate\u0026rdquo; (α increases) if the TTT layer proves helpful. Bi-direction (Eq 7): Video models often need to understand context from both past and future frames. Standard RNNs/TTT only look backward. So, we use a standard trick: run one TTT layer forward, and another identical TTT layer (TTT\u0026rsquo;) backward over the sequence, then combine their results. This gives each position information flowing from both directions. Segmentation: We process the video in 3-second chunks for the local attention, but the TTT layers see the whole concatenated sequence of chunks. Text prompts are also associated with these chunks (Format 3 storyboard). Training: We fine-tuned this modified architecture on our Tom and Jerry dataset, starting with short clips and gradually increasing the length (multi-stage context extension, Section 3.3). Evaluation and Results (Section 4): They compared our TTT-MLP approach against strong baselines like Mamba 2, Gated DeltaNet (state-of-the-art RNNs), sliding-window attention, and a simpler TTT-Linear (where the hidden state f is just a linear layer, not an MLP). Key Finding: For long (one-minute) videos with complex stories, TTT-MLP performed significantly better than the baselines, especially in \u0026ldquo;Temporal Consistency\u0026rdquo; (making sense over time) and \u0026ldquo;Motion Naturalness\u0026rdquo; (realistic movement). It won by 34 Elo points on average in human evaluations (Table 1), which is a meaningful difference. Short Videos: Interestingly, for shorter 18-second videos (Table 3 in Appendix), Gated DeltaNet was slightly better, suggesting that the richer hidden state of TTT-MLP really shines when the context gets very long and complex. Limitations (Sec 4.4): It\u0026rsquo;s not perfect! Videos still have artifacts (weird physics, morphing objects - see Fig 7). It\u0026rsquo;s also computationally slower than Mamba or DeltaNet (Fig 6). The quality is also limited by the base CogVideo-X model. ","permalink":"https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/","tags":null,"title":"One-Minute Video Generation with Test-Time Training"},{"categories":["GenAI"],"content":"Overview This is a brief summary of the example project, highlighting its main purpose or key feature. It\u0026rsquo;s used on the card display.\nDetails Here you can write a full description of the project.\nExplain the goals. Describe the technologies used. Discuss challenges faced and solutions implemented. Include code snippets or further images if relevant using Markdown. {{/* Example code snippet */}} ","permalink":"https://deepskandpal.github.io/creations/animal-gpt/","tags":["Web dev","RAG","LLMs"],"title":"Animal GPT"},{"categories":["AI"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-llm/","tags":null,"title":"Hands-On Large Language Models: Language Understanding and Generation "},{"categories":["AI"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/the-lm-book/","tags":null,"title":"The Hundred-Page Language Models Book"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-2/","tags":null,"title":"Harry Potter and the Chamber of Secrets"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-7/","tags":null,"title":"Harry Potter and the Deathly Hallows"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-6/","tags":null,"title":"Harry Potter and the Half-Blood Prince"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-5/","tags":null,"title":"Harry Potter and the Order of the Phoenix"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-3/","tags":null,"title":"Harry Potter and the Prisoner of Azkaban"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-4/","tags":null,"title":"Harry Potter and the Prisoner of Azkaban"},{"categories":["Biographies"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/elon-musk/","tags":null,"title":"Elon Musk: Tesla, SpaceX, and the Quest for a Fantastic Future"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-1/","tags":null,"title":"Harry Potter and the Philosopher's StoneS"},{"categories":["Biographies"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/shoe-dog/","tags":null,"title":"Shoe Dog"},{"categories":["Biographies"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/steve-jobs/","tags":null,"title":"STEVE JOBS"},{"categories":["Non Fiction"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/accelerating-india/","tags":null,"title":"Accelerating India's Development: A State-Led Roadmap for Effective Governance"},{"categories":["Business"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/good-to-great/","tags":null,"title":"Good To Great"},{"categories":["AI"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/interpretable-machine-learning/","tags":null,"title":"Interpretable Machine Learning"},{"categories":["Business"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/priceless/","tags":null,"title":"Priceless: the myth of fair value (and how to take advantage of it)"},{"categories":["Non Fiction"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/begin-infinity/","tags":null,"title":"The Beginning of Infinity"}]