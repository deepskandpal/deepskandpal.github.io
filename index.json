[{"categories":null,"content":"This isn\u0026rsquo;t fluff. These three words—Reliability, Scalability, and Maintainability—are the pillars of any serious system. In a system design interview, these are the top-level concerns you must address. Everything else is an implementation detail that serves these goals.\nWhat are we ultimately trying to achieve here? We\u0026rsquo;re trying to build systems that work, can grow, and don\u0026rsquo;t become a nightmare to manage. Let\u0026rsquo;s break that down.\n1. Reliability: It Works, Even When Things Go Wrong Reliability means the system continues to work correctly, performing its function at the desired level of performance, even in the face of adversity.\nThe key idea is Faults vs. Failures.\nA fault is when one component of the system deviates from its spec (e.g., a server crashes, a network link is slow). A failure is when the system as a whole stops providing the required service to the user. You cannot build a system with zero faults. It\u0026rsquo;s impossible. Your job is to design a fault-tolerant system that prevents faults from causing failures.\nTypes of Faults:\nHardware Faults: Disks crash, RAM goes bad, someone unplugs the wrong cable.\nTimeless: Redundancy. RAID for disks, dual power supplies. Post-2024 Era: We now live in the cloud. We don\u0026rsquo;t just plan for a disk to fail; we plan for the entire virtual machine to disappear without warning. We build resilience at the software layer, not just the hardware layer. This is a fundamental shift from the old on-premise world. Think of Netflix\u0026rsquo;s famous Chaos Monkey—it deliberately introduces faults to ensure the system is resilient. That\u0026rsquo;s the mindset. Software Errors: Bugs that cause a system-wide issue. A bad input crashes every server instance. These are often harder to deal with than random hardware faults.\nHuman Errors: Operators make mistakes. Configuration errors are a leading cause of outages.\nIn a system design interview, when you say \u0026ldquo;reliability,\u0026rdquo; you should be thinking about specific fault scenarios and your plan to mitigate them. How do you handle a node going down? A network partition between data centers?\n2. Scalability: Having a Plan for Growth Scalability isn\u0026rsquo;t a magic label you slap on a system. Saying \u0026ldquo;my system is scalable\u0026rdquo; is meaningless. Scalability is about answering the question: \u0026ldquo;As the system grows in a particular way, what\u0026rsquo;s our plan for coping with that growth?\u0026rdquo;\nFirst, you have to Describe Load. You can\u0026rsquo;t talk about scaling if you don\u0026rsquo;t know what you\u0026rsquo;re scaling for. Use specific load parameters.\nRequests per second? Read-to-write ratio? Simultaneously active users? Cache hit rate? Case Study: Twitter Timeline (DDIA Figure 1-2, 1-3) This is a classic. The problem isn\u0026rsquo;t just \u0026ldquo;lots of tweets.\u0026rdquo; It\u0026rsquo;s the fan-out. A celebrity with 30 million followers tweets once. How do you deliver that tweet to all 30 million timelines?\nThere are two naive approaches:\nRead-time Fan-out (Pull): A user requests their timeline. The system looks up everyone they follow, gets the recent tweets for each, and merges them. Write-time Fan-out (Push): A user posts a tweet. The system looks up everyone who follows them and inserts the new tweet into each of their timeline \u0026ldquo;inboxes\u0026rdquo; (caches). graph TD subgraph \u0026#34;Approach 1: Read-time Fan-out (Slow Reads)\u0026#34; User[User Requests Timeline] --\u0026gt; A{Find all followed users} A --\u0026gt; B[For each followed user...] B --\u0026gt; C[Fetch their recent tweets] C --\u0026gt; D{Merge all tweets} D --\u0026gt; Result[Show Timeline] end subgraph \u0026#34;Approach 2: Write-time Fan-out (Slow Writes)\u0026#34; Tweet[User Posts Tweet] --\u0026gt; E{Find all followers} E --\u0026gt; F[For each follower...] F --\u0026gt; G[Insert tweet into their timeline cache] G --\u0026gt; H[Done] end Twitter started with Approach 1, but timeline reads were too slow. They switched to Approach 2. This is a perfect example of a scalability trade-off: they made writes more expensive to make reads cheaper. The real system is a hybrid: most users are \u0026ldquo;push,\u0026rdquo; but for celebrities, it\u0026rsquo;s \u0026ldquo;pull\u0026rdquo; to avoid a single tweet overwhelming the system.\nNext, you Describe Performance.\nDon\u0026rsquo;t talk about \u0026ldquo;average response time.\u0026rdquo; It\u0026rsquo;s a mostly useless metric. Talk about percentiles.\nMedian (p50): Half your users get this response time or faster. The other half are getting a worse experience. 95th, 99th, 99.9th percentiles (p95, p99, p999): These are your tail latencies. They represent the experience of your unluckiest users. Why do tail latencies matter? Amazon found that the customers with the slowest requests were often their most valuable—the ones with long purchase histories. Making the p999 fast makes your best customers happy.\nIn modern systems (microservices, ML inference pipelines), you get tail latency amplification.\ngraph LR UserRequest --\u0026gt; API_Gateway subgraph Backend Services API_Gateway --\u0026gt; ServiceA API_Gateway --\u0026gt; ServiceB API_Gateway --\u0026gt; ServiceC API_Gateway --\u0026gt; ServiceD end ServiceA --\u0026gt; API_Gateway ServiceB --\u0026gt; API_Gateway ServiceC --\u0026gt; API_Gateway ServiceD --\u0026gt; API_Gateway API_Gateway --\u0026gt; UserResponse style ServiceC fill:#f9f,stroke:#333,stroke-width:2px If a user request requires calling 5 backend services in parallel, and each service has a 1% chance of being slow (p99), the chance that the user sees a slow response is much higher than 1%. It only takes one slow service to delay the entire request. This is why focusing on p99 and p999 is critical.\n3. Maintainability: Avoiding a Future Mess This is the most overlooked aspect by junior engineers, but it\u0026rsquo;s where most of the cost of software lies. The goal is to design a system that future engineers (including you in 6 months) can work on productively.\nIt boils down to three principles:\nOperability: Make it easy for operations teams to keep the system running smoothly. This means good monitoring, good automation, standard tools, and predictable behavior. No magic. Simplicity: Manage complexity. This isn\u0026rsquo;t about dumbing things down. It\u0026rsquo;s about finding the right abstractions that hide a great deal of implementation detail behind a clean interface. A well-designed database is a great abstraction. Evolvability (or Plasticity): Make it easy to change the system in the future. Business needs change. You\u0026rsquo;ll need to add features. How easy is it to \u0026ldquo;refactor\u0026rdquo; your architecture, like Twitter did with its timeline? Summary for System Design Interviews\nWhen you get a design question, these three concepts are your high-level checklist. For any component you propose, ask:\nReliability: What are its failure modes? How will we make it fault-tolerant? Scalability: What are the load parameters? Where are the bottlenecks? What is our strategy for handling 10x or 100x the load? How will we measure performance? Maintainability: Is this design easy to understand? Can a new engineer get up to speed quickly? How will we evolve it when requirements change? This chapter gives us the vocabulary and the framework. In the next chapter, we\u0026rsquo;ll dive into the first major design choice that impacts all three of these: the Data Model.\n","permalink":"https://deepskandpal.github.io/bookshelf/design-data-intensive/chapter-1/","tags":null,"title":"Chapter 1: Reliable, Scalable, and Maintainable Applications"},{"categories":null,"content":"This chapter introduces the fundamentals of designing machine learning systems at scale. It covers the key differences between traditional software systems and ML systems, and outlines the unique challenges faced when building production ML systems.\nKey Concepts ML System vs Traditional Systems: Understanding the differences in architecture, requirements, and challenges Scale Considerations: How scale affects ML system design decisions Production ML Pipeline: Components of a typical ML production pipeline Main Topics Covered What makes ML systems different Key components of ML systems Common challenges in ML system design Framework for approaching ML system design interviews Interview Tips Always start with clarifying requirements Think about data flow and system components Consider scalability and performance trade-offs Don\u0026rsquo;t forget about monitoring and maintenance (Your detailed notes for Chapter 1 go here\u0026hellip;)\nExcellent question. This is exactly the mindset a senior candidate needs to have – not just knowing the material, but understanding how it fits into the current landscape. You\u0026rsquo;ve got 5 days, so we need to be efficient and focus on high-impact areas.\nFirst, let\u0026rsquo;s get straight to your main question:\nYes, you can and absolutely should use this book. The 2022 \u0026ldquo;Machine Learning Design Interview\u0026rdquo; by Khang Pham is a fantastic resource. The fundamental principles it covers – the \u0026ldquo;why\u0026rdquo; behind system design choices – are timeless. Things like the two-stage (candidate/ranking) architecture, feature engineering for sparse data (hashing, embeddings), and handling data at scale are the bedrock of ML systems.\nHowever, you\u0026rsquo;re right that the field moves incredibly fast. What a senior staff engineer expects from a senior MLE candidate in 2024 has evolved. We\u0026rsquo;re not just looking for knowledge of these patterns, but also for an understanding of the new class of problems and tools that have emerged, primarily around Large Language Models (LLMs) and Generative AI.\nThink of this book as your solid foundation. We\u0026rsquo;re going to build a modern extension on top of it.\nPart 1: What to ADD (The 2024+ Lens) Here are the key concepts that have become critical since 2022. You need to be able to discuss these intelligently.\n1. Retrieval-Augmented Generation (RAG) This is the single most important ML system design pattern to emerge in the last two years. It\u0026rsquo;s how you make LLMs factual, current, and context-aware. The book talks about retrieval for recommendations (two-tower models), but RAG applies it to conversational AI and generation tasks.\nSimple Intuition: An LLM knows a lot, but it doesn\u0026rsquo;t know about your company\u0026rsquo;s private data, yesterday\u0026rsquo;s news, or a specific user\u0026rsquo;s profile. RAG is the process of:\nReceiving a user\u0026rsquo;s query. Using that query to retrieve relevant documents from a knowledge base. Stuffing those documents into the LLM\u0026rsquo;s prompt along with the original query. Asking the LLM to generate an answer based only on the provided context. Here is a simple system diagram for RAG:\nflowchart TD subgraph offline[\u0026#34;Offline: Data Indexing\u0026#34;] A[Unstructured Data\u0026lt;br/\u0026gt;Docs, PDFs, etc.] --\u0026gt; B[Chunking Service] B --\u0026gt; C[Embedding Model\u0026lt;br/\u0026gt;text-embedding-ada-002] C --\u0026gt; D[Vector Database\u0026lt;br/\u0026gt;Pinecone, Qdrant] end subgraph online[\u0026#34;Online: Inference\u0026#34;] U[User Query] --\u0026gt; Q[Query Embedding] Q --\u0026gt; S{Vector Search} S --\u0026gt; R[Retrieved Context] U --\u0026gt; P[Prompt Engineering] R --\u0026gt; P P --\u0026gt; LLM[Large Language Model\u0026lt;br/\u0026gt;GPT-4, Llama 3] LLM --\u0026gt; F[Final Answer] end D -.-\u0026gt; S style D fill:#cde4ff,stroke:#333,stroke-width:2px 2. Vector Databases The book mentions FAISS, which is a library. The modern discussion is about managed Vector Databases as a core infrastructure component. They are purpose-built to do fast Approximate Nearest Neighbor (ANN) search on billions of embeddings.\nKey discussion points for a senior role:\nTrade-offs: Latency vs. recall vs. cost. How do you choose an index type (e.g., HNSW vs. IVF)? Metadata Filtering: How do you retrieve vectors that not only are semantically similar but also match specific criteria (e.g., user_id = '123' AND is_public = true)? This is a crucial production feature. Scaling: How do these databases scale for reads and writes? 3. LLMOps \u0026amp; Foundation Model Serving Deploying a 5GB BERT model is different from deploying a 70B parameter Llama model.\nQuantization: Techniques like GPTQ or GGML/GGUF to shrink model size to run on cheaper hardware. Specialized Serving Frameworks: vLLM, TensorRT-LLM. These use techniques like PagedAttention to dramatically increase throughput for LLM inference. Fine-tuning vs. Prompt Engineering vs. RAG: When do you use each? (Hint: Start with Prompting/RAG. Fine-tuning is expensive and only for changing a model\u0026rsquo;s style or teaching it a new skill, not for adding knowledge). 4. Real-time Feature Stores The book mentions the concept, but their importance has solidified. A feature store is the source of truth that solves the training-serving skew problem.\nModern Take: Feature stores now often have streaming capabilities (e.g., using Flink or Spark Streaming) to compute real-time features (e.g., \u0026ldquo;user\u0026rsquo;s clicks in the last 5 minutes\u0026rdquo;) and make them available for inference with low latency. flowchart TD subgraph \u0026#34;Real-time Feature Engineering\u0026#34; direction TB A[\u0026#34;Event Stream\\n(Kafka, Kinesis)\u0026#34;] --\u0026gt; B{\u0026#34;Stream Processor\\n(Flink, Spark Streaming)\u0026#34;} B --\u0026gt; C[\u0026#34;Real-time Feature Store\\n(Redis, DynamoDB)\u0026#34;] end subgraph \u0026#34;Serving\u0026#34; direction TB D[\u0026#34;Prediction Service\u0026#34;] --\u0026gt; C end subgraph \u0026#34;Training\u0026#34; direction TB B --\u0026gt; E[\u0026#34;Batch Storage\\n(S3, BigQuery)\u0026#34;] F[\u0026#34;Model Training\u0026#34;] --\u0026gt; E end Part 2: What to DEPRECATE (or De-emphasize) Technology moves on. While the principles are good, don\u0026rsquo;t spend your limited time on the implementation details of these.\nPure Hadoop/MapReduce: The book mentions MapReduce jobs. Today, the de-facto standard for large-scale batch processing is Apache Spark, and for streaming, it\u0026rsquo;s Spark Streaming or Apache Flink. If you say \u0026ldquo;MapReduce job,\u0026rdquo; it will sound dated. Frame everything in terms of Spark or Flink.\nElaborate, from-scratch feature engineering for text/images: For many problems, you no longer build TF-IDF vectors or train a Word2Vec model from scratch on day one.\nThe 2024 Way: You start with a powerful pre-trained foundation model (e.g., a BERT variant for embeddings, or even an LLM). Your \u0026ldquo;feature engineering\u0026rdquo; is now about choosing the right model and deciding on an embedding strategy. The core idea of \u0026ldquo;semantic representation\u0026rdquo; is the same, but the tool has become much more powerful. Over-indexing on specific architectures (like DCNv2): It\u0026rsquo;s great to know why a model like Deep \u0026amp; Cross Network (DCN) exists – to explicitly learn feature interactions. But a senior staff engineer is more interested in you recognizing the problem (the need for both memorization and generalization) than memorizing the exact architecture of DCNv2. You could say: \u0026ldquo;For a system with many categorical features like this, we need to handle both memorization of specific feature pairs and generalization. Historically, models like Wide \u0026amp; Deep or DCN addressed this. Today, we might handle this with a powerful embedding model that captures these interactions implicitly, or by using an LLM to reason over the features.\u0026rdquo;\nYour 5-Day High-Intensity Action Plan Days 1-2: Master the Book\u0026rsquo;s Fundamentals\nRead the book cover-to-cover. Don\u0026rsquo;t skip. For every component (e.g., One-Hot Encoding, Two-Tower Model), force yourself to answer: \u0026ldquo;What business or system problem does this solve?\u0026rdquo; (e.g., Two-tower model solves the problem of efficiently retrieving a small set of candidates from a massive corpus). Focus on Chapters 1 (Primer), 2 (RecSys Components), and then skim the end-to-end examples. Day 3: Layer on the 2024 Concepts\nDeep-dive into RAG. Watch 2-3 YouTube videos explaining it. Understand the diagram I drew above. Read about Vector Databases (e.g., read the intro docs for Pinecone or Weaviate). Understand the difference between real-time, near-real-time, and batch features. Day 4: Practice Designing\nTake a classic problem from the book, like \u0026ldquo;YouTube Video Recommendations.\u0026rdquo; First, design it the \u0026ldquo;classic\u0026rdquo; way as described in the book (two-tower candidate gen, sophisticated ranker). Then, ask yourself: \u0026ldquo;How would I redesign this in 2024?\u0026rdquo; Could I add a conversational search feature using RAG? How would my video embeddings be stored and retrieved? (Vector DB!) Could an LLM re-rank the final candidates by providing natural language \u0026ldquo;reasons\u0026rdquo; for why a user might like a video? Draw the new architecture. This is your key senior-level exercise. Day 5: Review \u0026amp; Communication\nPractice verbalizing your designs. Use a framework: Clarify: Ask about scope, scale, latency, and business goals. Metrics: Define offline (e.g., NDCG, Precision) and online (e.g., CTR, user session time) metrics. High-Level Design: Draw the big boxes. Start simple. Deep Dive: Go into the components (data, model, serving). This is where you\u0026rsquo;ll use the book\u0026rsquo;s content and the 2024 additions. Scale \u0026amp; Reliability: Discuss bottlenecks, caching, and failure modes. You have a solid resource. Your goal in the next 5 days is not to become an expert in LLMOps, but to be able to confidently place the timeless principles from the book into the context of modern AI systems.\nGood luck. You\u0026rsquo;ve got this.\n","permalink":"https://deepskandpal.github.io/bookshelf/ml-design-interview/chapter-0/","tags":null,"title":"Chapter 0: ML Design Primer"},{"categories":null,"content":"This chapter introduces the fundamentals of designing machine learning systems at scale. It covers the key differences between traditional software systems and ML systems, and outlines the unique challenges faced when building production ML systems.\nChapter 1: Machine Learning Primer This chapter covers the building blocks. Mastering these fundamentals is non-negotiable. An interviewer will expect you to be fluent in this language.\nTopic: Feature Selection and Feature Engineering 1. One-Hot Encoding (OHE) Book\u0026rsquo;s Core Idea (Timeless): OHE is used for low-cardinality categorical features (e.g., device_type = [\u0026lsquo;mobile\u0026rsquo;, \u0026lsquo;desktop\u0026rsquo;, \u0026rsquo;tablet\u0026rsquo;]). It prevents the model from assuming a false numerical order (i.e., that tablet(2) is somehow \u0026ldquo;more\u0026rdquo; than mobile(0)). Its major drawback is creating very wide, sparse vectors for high-cardinality features, which is memory-intensive. The 2024+ Perspective: The principle is still valid, but its use is more limited. For any feature with more than a handful of categories (like user_id, product_id), OHE is a non-starter. The industry has almost completely moved to embeddings for these cases. Interview Focus \u0026amp; Phrasing: Good: \u0026ldquo;For a low-cardinality feature like \u0026lsquo;day_of_week\u0026rsquo;, we can use one-hot encoding.\u0026rdquo; Senior Level: \u0026ldquo;We\u0026rsquo;ll need to handle our categorical features. For low-cardinality ones like country_code, one-hot encoding is a simple and effective baseline. However, for high-cardinality features like user_id, OHE would lead to extreme sparsity and high dimensionality. We\u0026rsquo;ll use embeddings for those instead.\u0026rdquo; 2. Mean Encoding (or Target Encoding) Book\u0026rsquo;s Core Idea (Timeless): Replace a category with the average value of the target variable for that category (e.g., replace \u0026lsquo;San Francisco\u0026rsquo; with the average click-through rate for all users from SF). It\u0026rsquo;s a very powerful technique for tree-based models (like XGBoost) because it packs a lot of signal into a single numerical feature. The book\u0026rsquo;s warning about label leakage is its most critical point. The 2024+ Perspective: Still a highly relevant and powerful technique, especially in tabular data competitions (Kaggle) and for boosting models where latency isn\u0026rsquo;t the primary concern. Modern feature stores sometimes automate the robust calculation of these encodings (e.g., using out-of-fold calculations to prevent leakage). Interview Focus \u0026amp; Phrasing: Mention this as a strong option for tree-based models, but immediately bring up the risks. \u0026ldquo;We could consider target encoding for features like city, as it can provide a strong signal to a tree-based model. However, we must be extremely careful about data leakage. To mitigate this, we\u0026rsquo;d compute the encoding on a separate dataset or use an out-of-fold strategy during training to ensure the encoding for a given row doesn\u0026rsquo;t use its own label.\u0026rdquo; 3. Feature Hashing (\u0026ldquo;The Hashing Trick\u0026rdquo;) Book\u0026rsquo;s Core Idea (Timeless): A technique to handle extremely high-cardinality features by hashing them into a fixed-size vector. It\u0026rsquo;s memory-efficient because you pre-define the vector size. The main drawback is hash collisions, where different features get mapped to the same index, which can hurt model performance. The 2024+ Perspective (Outdated): Feature hashing for model features is now largely a legacy technique. While the idea is clever, the performance loss from collisions is often not worth the memory savings, especially when compared to embeddings. Modern hardware and frameworks can handle large embedding tables more gracefully. Interview Focus \u0026amp; Phrasing: Acknowledge it historically. \u0026ldquo;In the past, for systems with millions of features like in AdTech, feature hashing was a common way to manage memory. It\u0026rsquo;s a trade-off between memory and performance due to collisions. Today, we\u0026rsquo;d almost always prefer to learn a dedicated embedding layer, as it captures semantic relationships and avoids collisions, leading to better model performance.\u0026rdquo; 4. Cross Features Book\u0026rsquo;s Core Idea (Timeless): Explicitly combining two or more categorical features to capture their interaction (e.g., feature_A=\u0026lsquo;USA\u0026rsquo; + feature_B=\u0026lsquo;iPhone\u0026rsquo; -\u0026gt; crossed_feature=\u0026lsquo;USA_iPhone\u0026rsquo;). This helps linear models and shallow networks learn relationships they otherwise couldn\u0026rsquo;t. The book\u0026rsquo;s reference to Wide \u0026amp; Deep is the classic example. The 2024+ Perspective: The concept of capturing feature interactions is more important than ever. The method has evolved. While Wide \u0026amp; Deep is still a valid pattern, modern architectures like Transformers (with their self-attention mechanism) are exceptionally good at learning these interactions implicitly from the raw feature embeddings. You don\u0026rsquo;t always need to manually define the crosses. Interview Focus \u0026amp; Phrasing: Focus on the \u0026ldquo;why.\u0026rdquo; \u0026ldquo;A key challenge in this problem is capturing the interaction between features, for example, a user\u0026rsquo;s country and their device type. A classic approach is the Wide \u0026amp; Deep model, where we\u0026rsquo;d manually create cross-product transformations for the \u0026lsquo;wide\u0026rsquo; part. A more modern approach would be to feed the embeddings for all features into a deep network, perhaps with a cross-network layer like DCN or an attention mechanism, which can learn these interactions automatically.\u0026rdquo; 5. Embedding Book\u0026rsquo;s Core Idea (Timeless): This is the most important feature engineering technique in the book and in modern ML. An embedding is a dense, low-dimensional, learned vector representation of a categorical feature. The book correctly explains the two main ways to get them: Co-trained: Learning the embedding as part of your main model (e.g., an embedding layer in Keras/PyTorch). Pre-trained: Learning the embedding separately on a different task (e.g., Word2Vec on text, or node2vec on a graph) and then using it as a static feature. The 2024+ Perspective: This is the heart of modern ML. The Rise of Foundation Models: The \u0026ldquo;pre-trained\u0026rdquo; paradigm now dominates. You don\u0026rsquo;t just use Word2Vec; you use embeddings from massive, powerful foundation models (e.g., OpenAI\u0026rsquo;s text-embedding-3-large, Sentence-BERT, or image embeddings from CLIP). Your job is less about designing the pre-training task and more about choosing the right foundation model. The Two-Tower Model: The book\u0026rsquo;s description of the two-tower model for retrieval (e.g., at YouTube) is still State-of-the-Art for recommendation and search. This is a critical pattern to know inside and out. Interview Focus \u0026amp; Phrasing: This should be your default answer for high-cardinality features. \u0026ldquo;For user IDs and item IDs, we will learn dense embedding vectors. This allows the model to capture semantic similarities—for instance, users who buy similar products will have similar vectors.\u0026rdquo; For a retrieval system, you must bring up the two-tower model. \u0026ldquo;We\u0026rsquo;ll design a two-tower retrieval model. The query tower will ingest user features and produce a query embedding. The candidate tower will ingest item features and produce an item embedding. At training time, we\u0026rsquo;ll optimize these towers using a contrastive loss so that the dot product between a user and a relevant item is high. For serving, we can pre-compute all item embeddings and put them into a vector index for efficient retrieval.\u0026rdquo; Here is the diagram for the Two-Tower Model, which is central to this discussion.\ngraph TD subgraph \u0026#34;Query Tower\u0026#34; U_Features[User Features\u0026lt;br/\u0026gt;context, history] --\u0026gt; U_L1(Dense Layer) U_L1 --\u0026gt; U_L2(Dense Layer) U_L2 --\u0026gt; QueryEmbedding[Query Embedding] end subgraph \u0026#34;Candidate Tower\u0026#34; I_Features[Item Features\u0026lt;br/\u0026gt;metadata, text] --\u0026gt; I_L1(Dense Layer) I_L1 --\u0026gt; I_L2(Dense Layer) I_L2 --\u0026gt; ItemEmbedding[Item Embedding] end subgraph \u0026#34;Training Objective\u0026#34; QueryEmbedding -- Dot Product --\u0026gt; Score ItemEmbedding -- Similarity --\u0026gt; Score Score --\u0026gt; Loss(Contrastive Loss\u0026lt;br/\u0026gt;e.g., Softmax, Hinge) end subgraph \u0026#34;Serving / Inference\u0026#34; direction LR ItemEmbedding --\u0026gt; AllItemEmbeddings[Pre-computed\u0026lt;br/\u0026gt;Item Embeddings] AllItemEmbeddings --\u0026gt; ANN_Index[(Vector Database / FAISS\u0026lt;br/\u0026gt;Approximate Nearest Neighbor)] QueryEmbedding --\u0026gt; ANN_Index ANN_Index --\u0026gt; Top_K_Candidates[Top K Candidates] end style QueryEmbedding fill:#cde4ff style ItemEmbedding fill:#dff0d8 Excellent. Let\u0026rsquo;s dive into the next section of Chapter 1. We\u0026rsquo;ll cover Training Pipelines, Handling Imbalance, and Data Generation/Splitting. These topics are less about the model itself and more about the crucial data infrastructure that makes modeling possible at scale.\nTopic: Training Pipeline \u0026amp; Data Partitioning (Page 33) Book\u0026rsquo;s Core Idea (Timeless): To train models on massive datasets (terabytes or petabytes), you can\u0026rsquo;t just load a CSV file. The book correctly identifies two key strategies:\nColumnar Storage (Parquet, ORC): Instead of storing data row-by-row like in a CSV, columnar formats store it column-by-column. Why this matters: ML training jobs often only need a subset of columns. Columnar formats let you read only the columns you need, dramatically reducing I/O and speeding up data loading. This is a fundamental concept in big data. Data Partitioning: Breaking up the data into a logical directory structure, almost always by date (e.g., /year=2024/month=05/day=21/). Why this matters: It allows the query engine to completely skip reading data from partitions that aren\u0026rsquo;t relevant to your query (e.g., \u0026ldquo;give me the last 7 days of data\u0026rdquo;). This is called \u0026ldquo;predicate pushdown\u0026rdquo; and it\u0026rsquo;s a massive performance win. The 2024+ Perspective (Modern Augmentation): The principles are solid, but the ecosystem built on top has become standard.\nThe Rise of Table Formats (Iceberg/Delta Lake): The biggest change is the widespread adoption of open table formats like Apache Iceberg and Delta Lake. These are layers that sit on top of your Parquet files in the data lake. They solve critical problems that raw Parquet files don\u0026rsquo;t: ACID Transactions: They prevent you from reading corrupted or incomplete data if a write job fails midway. This is huge for data reliability. Schema Evolution: They make it safe to add/remove/rename columns without breaking downstream jobs. Time Travel: They allow you to query the state of your data at a specific point in time, which is invaluable for debugging and reproducing experiments. Orchestration: Training pipelines aren\u0026rsquo;t run manually. They are scheduled and managed by orchestrators. Airflow is the industry veteran, but newer tools like Dagster and Prefect are gaining traction by offering better data awareness and local development experiences. Interview Focus \u0026amp; Phrasing:\nGood: \u0026ldquo;We will store our training data in a data lake like S3, using the Parquet format and partitioning by date.\u0026rdquo; Senior Level: \u0026ldquo;We\u0026rsquo;ll design a daily batch training pipeline orchestrated by Airflow. The pipeline\u0026rsquo;s source data will reside in our data lake (S3) and be managed by an open table format like Apache Iceberg. This gives us transactional guarantees and schema safety. The training job itself, running on a Spark cluster, will read the last 90 days of data, leveraging Iceberg\u0026rsquo;s partition pruning for efficiency, compute the necessary features, and then train the model.\u0026rdquo; Here\u0026rsquo;s a diagram illustrating this modern training pipeline:\ngraph TD subgraph datasources[\u0026#34;Data Sources\u0026#34;] Logs[Log Files] DB[(Production DB)] end subgraph pipeline[\u0026#34;Daily Batch Pipeline\u0026#34;] direction LR Ingest[Ingestion Job Spark] --\u0026gt; Lake[Data Lake with\u0026lt;br/\u0026gt;Apache Iceberg Tables] Lake -- \u0026#34;Reads last N days\u0026#34; --\u0026gt; FeatEng[Feature Engineering Job Spark] FeatEng --\u0026gt; TrainJob[Model Training Job\u0026lt;br/\u0026gt;PyTorch/XGBoost on Spark] TrainJob --\u0026gt; ModelRegistry[(Model Registry\u0026lt;br/\u0026gt;MLflow, SageMaker)] end Logs --\u0026gt; Ingest DB --\u0026gt; Ingest style Lake fill:#cde4ff,stroke:#333,stroke-width:2px Mapping Generic Concepts to AWS Services Let\u0026rsquo;s break down the AWS equivalent of that senior-level answer and build a concrete diagram around a common use case: Training a daily model for a Recommendation System.\nGeneric Concept AWS Primary Service(s) Role / \u0026ldquo;Why\u0026rdquo; you use it Data Lake Storage Amazon S3 (Simple Storage Service) The foundation. It\u0026rsquo;s the cheap, durable, and scalable object store where you dump all your raw and processed data (like Parquet files). Table Format AWS Glue Data Catalog (as the metastore) + Apache Iceberg/Hudi Glue Data Catalog acts as the central \u0026ldquo;address book\u0026rdquo; for your data lake. It stores the metadata (schema, location, partitions) for your tables. Iceberg (or Hudi) manages the actual data files within S3, providing the ACID transactions, time travel, and schema evolution we discussed. Data Querying / Ad-hoc Analysis Amazon Athena A serverless query engine that lets you run standard SQL directly on your data in S3 using the Glue Data Catalog. This is how data scientists explore the data without spinning up a cluster. ETL / Feature Engineering AWS Glue (for serverless Spark jobs) or Amazon EMR (for managed Spark/Hadoop clusters) Glue is great for simpler, serverless ETL jobs. EMR is for heavy-duty, long-running Spark jobs where you need more control over the cluster configuration. Both are used for the heavy lifting of data transformation. Pipeline Orchestration Amazon MWAA (Managed Workflows for Apache Airflow) This is the AWS-managed version of Airflow. It\u0026rsquo;s the \u0026ldquo;conductor\u0026rdquo; that defines the dependencies between your jobs (e.g., \u0026ldquo;Run the feature engineering job only after the daily data ingestion job succeeds\u0026rdquo;) and schedules them. Model Training Amazon SageMaker Training The service for running training jobs at scale. It handles spinning up the necessary compute instances (with GPUs if needed), running your training script (e.g., PyTorch, TensorFlow), and saving the final model artifact back to S3. Model Registry Amazon SageMaker Model Registry A centralized repository to version, approve, and manage your trained models before they are deployed. It\u0026rsquo;s crucial for governance and reproducibility. What about Snowflake? Snowflake is a cloud data warehouse, not a data lake tool in this context. While it can connect to S3, it represents a different architectural pattern. The \u0026ldquo;modern data stack\u0026rdquo; we\u0026rsquo;re describing here is centered on an open data lakehouse architecture (S3 + Iceberg + Spark), which is more common for large-scale ML training pipelines because it\u0026rsquo;s more flexible and cost-effective for unstructured and semi-structured data. Mentioning Snowflake is fine, but the S3/Glue/Spark stack is the more direct AWS equivalent.\nUse Case Diagram: Daily Re-training of a \u0026ldquo;Products You May Like\u0026rdquo; Model on AWS Here is the diagram showing how these services fit together in a production pipeline.\ngraph TD subgraph \u0026#34;Data Sources\u0026#34; A[App \u0026amp; Web Logs\u0026lt;br/\u0026gt;via Kinesis Firehose] B[S3 Raw Data Bucket] C[Production DB\u0026lt;br/\u0026gt;e.g., RDS, DynamoDB] D[Daily DB Snapshots] end subgraph \u0026#34;Orchestration Layer: Amazon MWAA Airflow\u0026#34; P1(DAG Starts\u0026lt;br/\u0026gt;Daily @ 1 AM) end subgraph \u0026#34;ETL \u0026amp; Feature Engineering Layer\u0026#34; S3_Lakehouse[S3 Data Lakehouse\u0026lt;br/\u0026gt;Storage: Parquet Files\u0026lt;br/\u0026gt;Format: Apache Iceberg\u0026lt;br/\u0026gt;Metastore: AWS Glue Data Catalog] Glue_Job[AWS Glue ETL Job\u0026lt;br/\u0026gt;Spark] end subgraph \u0026#34;Model Training \u0026amp; Registration Layer\u0026#34; SageMaker_Train[SageMaker Training Job\u0026lt;br/\u0026gt;PyTorch] SageMaker_Registry[(SageMaker Model Registry)] end subgraph \u0026#34;Analytics \u0026amp; Exploration\u0026#34; Athena[Amazon Athena] end A --\u0026gt; B C --\u0026gt; D D --\u0026gt; B B --\u0026gt; Glue_Job Glue_Job --\u0026gt; S3_Lakehouse S3_Lakehouse --\u0026gt; SageMaker_Train SageMaker_Train --\u0026gt; SageMaker_Registry Athena --\u0026gt; S3_Lakehouse P1 --\u0026gt; Glue_Job P1 --\u0026gt; SageMaker_Train P1 --\u0026gt; SageMaker_Registry style S3_Lakehouse fill:#cde4ff,stroke:#333,stroke-width:2px style Glue_Job fill:#fff0b3,stroke:#333,stroke-width:2px style SageMaker_Train fill:#dff0d8,stroke:#333,stroke-width:2px style Athena fill:#f5c6cb,stroke:#333,stroke-width:2px Explaining the Diagram in an Interview \u0026ldquo;Here\u0026rsquo;s how I would architect the daily training pipeline on AWS for our recommendation model.\nOrchestration: The entire process would be managed by a DAG in Amazon MWAA (Airflow), scheduled to run daily.\nIngestion: Raw data, like clickstream logs from Kinesis and daily snapshots from our production RDS database, lands in a dedicated S3 bucket.\nETL \u0026amp; Feature Store Creation: Our first Airflow task kicks off an AWS Glue ETL job. This serverless Spark job reads the raw data, cleans and transforms it, computes our features, and writes the output as partitioned Parquet files back to our main S3 data lake. We\u0026rsquo;ll use the AWS Glue Data Catalog as a metastore with the Apache Iceberg format. This gives us ACID compliance, so downstream jobs never read partial data, and allows our data science team to query the data easily with Athena.\nModel Training: Once the Glue job succeeds, Airflow triggers the next task: a SageMaker Training Job. SageMaker pulls our training script, provisions the necessary compute (e.g., GPU instances), reads the clean feature data directly from our S3 data lakehouse, and trains the model.\nModel Registration: After training, the final model artifact is versioned and saved to the SageMaker Model Registry. This creates a golden copy, tracks its performance metrics, and puts it in a \u0026lsquo;pending approval\u0026rsquo; state before it can be deployed to production. This CI/CD for ML approach is crucial for safe and reproducible deployments.\u0026rdquo;\nTopic: Handle Imbalance Class Distribution (Page 34) Book\u0026rsquo;s Core Idea (Timeless): This is a classic problem in domains with rare events (ad clicks, fraud). The book gives the three canonical solutions:\nClass Weighting: The simplest and often best first step. Penalize errors on the minority class more heavily in your loss function. Downsampling (Majority Class): When you have massive data (e.g., billions of non-clicks), you can throw away a large, random portion to make the training problem more manageable. This is the standard in big tech. Oversampling (Minority Class) / SMOTE: For smaller datasets where every positive example is precious. SMOTE creates synthetic minority class examples. Less common in large-scale systems. The book\u0026rsquo;s most important warning is to NEVER resample your validation/test set. They must reflect reality. The 2024+ Perspective (Modern Augmentation):\nThe Calibration Imperative: This is the critical piece that senior candidates must discuss. When you downsample, you change the baseline probability in your training data (e.g., from 1% clicks to 50% clicks). The model\u0026rsquo;s raw output p will be biased high. You need to correct for this before the prediction can be used for anything quantitative (like bidding in an ad auction). The formula is essential: calibrated_p = p / (p + (1-p) / w) where w is the downsampling rate (e.g., if you kept 10% of negatives, w=0.1). Focal Loss: As mentioned before, Focal Loss is a more advanced alternative to simple class weighting. It\u0026rsquo;s an excellent talking point to show you know modern techniques. Interview Focus \u0026amp; Phrasing:\nGood: \u0026ldquo;This is an imbalanced problem, so I\u0026rsquo;ll downsample the negative class in the training set.\u0026rdquo; Senior Level: \u0026ldquo;This is a classic class imbalance problem. We have several strategies. I\u0026rsquo;d start with class weighting, as it\u0026rsquo;s non-invasive. If performance is still an issue, we can move to downsampling the negative class. This is computationally efficient, but it requires a crucial post-processing step: we must calibrate the model\u0026rsquo;s outputs to correct for the artificial sampling rate before they are used downstream. Our validation set will, of course, maintain the original, un-sampled distribution to give us a true measure of performance.\u0026rdquo; Topic: Data Generation Strategy \u0026amp; How to Split Train/Test Data (Pages 36, 39) Book\u0026rsquo;s Core Idea (Timeless):\nGetting Labels: How do you get initial labels? The book gives great examples: use chronological feeds, use existing user actions (\u0026ldquo;likes\u0026rdquo; are positive, items they saw but skipped are negative), or use proxy signals (LinkedIn using skills to recommend courses). Splitting Data: The most important rule is that for any time-series problem (forecasting, user behavior models), you must not split randomly. A random split would cause data leakage by using future information to train a model that predicts the past. You must use a time-based split (e.g., train on January, validate on February). The book\u0026rsquo;s examples of Sliding Window and Expanding Window for backtesting are perfect. The 2024+ Perspective (Modern Augmentation):\nNegative Sampling is a Design Choice: The book treats \u0026ldquo;negative labels\u0026rdquo; as something you just find. In modern recommender systems, particularly with two-tower models, the choice of which negatives to use during training is a key modeling decision. Easy Negatives: Randomly sampled items. Hard Negatives: Items that are \u0026ldquo;close\u0026rdquo; to the positive item in the embedding space but the user didn\u0026rsquo;t click. Training on these helps the model learn finer distinctions. YouTube\u0026rsquo;s papers talk extensively about the importance of hard negative mining. User-based Splitting: For personalization problems, another important splitting strategy is to split by user. You hold out a set of users entirely to test how the model performs on users it has never seen before (the \u0026ldquo;cold start\u0026rdquo; problem). Interview Focus \u0026amp; Phrasing:\nGood: \u0026ldquo;We\u0026rsquo;ll split our data by time. We\u0026rsquo;ll train on one month and test on the next day.\u0026rdquo; Senior Level: \u0026ldquo;Our splitting strategy is critical to avoid leakage. We will use a strict temporal split, for example, training on data up to day T and validating on day T+1. For our two-tower retrieval model, we need to be thoughtful about negative sampling. We\u0026rsquo;ll start with random in-batch negatives, but to improve performance, we would implement a strategy for hard negative mining to help the model learn the subtle differences between similar items. To evaluate cold-start performance, we\u0026rsquo;ll also maintain a holdout set of users.\u0026rdquo; Of course. Let\u0026rsquo;s slow down and unpack these concepts properly. This is the core of what separates a junior from a senior MLE. We will go deep into each one, with the goal of you being able to explain it with the same level of clarity and intuition.\nTopic : Loss Functions - The \u0026ldquo;Why\u0026rdquo; Behind Optimization A loss function is the mathematical objective your model minimizes during training. It\u0026rsquo;s the most direct way you, the engineer, tell the model what \u0026ldquo;good\u0026rdquo; looks like.\nDeep Dive 1: Regression Loss: How wrong is our number? A. MSE vs. MAE: The Outlier Dilemma Let\u0026rsquo;s imagine we\u0026rsquo;re predicting house prices.\nTarget Price: $300k Model A Prediction: $310k (Error: $10k) Model B Prediction: $350k (Error: $50k) Now, let\u0026rsquo;s see how MSE and MAE treat these errors.\nMean Absolute Error (MAE): The penalty is linear with the error.\nEquation: MAE = (1/n) * Σ |y_true - y_pred| Intuition: It answers \u0026ldquo;On average, how far off are our predictions in dollars?\u0026rdquo; Calculation: Model A\u0026rsquo;s MAE contribution: |$300k - $310k| = $10k Model B\u0026rsquo;s MAE contribution: |$300k - $350k| = $50k Conclusion: The error from Model B ($50k) is exactly 5 times worse than the error from Model A ($10k). This feels intuitive. Mean Squared Error (MSE): The penalty grows quadratically with the error.\nEquation: MSE = (1/n) * Σ (y_true - y_pred)² Intuition: It answers \u0026ldquo;What\u0026rsquo;s the average of the squared errors?\u0026rdquo; It doesn\u0026rsquo;t have an intuitive unit, which is why we often use RMSE (Root Mean Squared Error = √MSE) to bring it back to the original units (dollars). Calculation: Model A\u0026rsquo;s MSE contribution: ($10k)² = 100,000,000 Model B\u0026rsquo;s MSE contribution: ($50k)² = 2,500,000,000 Conclusion: The error from Model B is 25 times worse than the error from Model A. MSE despises large errors and will aggressively tune the model to avoid them, even at the cost of being slightly worse on other, smaller errors. graph TD subgraph \u0026#34;MAE (Linear Penalty)\u0026#34; A[Error of $10k] --\u0026gt; B(Penalty = 10k) C[Error of $50k] --\u0026gt; D(Penalty = 50k) end subgraph \u0026#34;MSE (Quadratic Penalty)\u0026#34; E[Error of $10k] --\u0026gt; F(Penalty = 100M) G[Error of $50k] --\u0026gt; H(Penalty = 2,500M) end style D fill:#f5c6cb style H fill:#f5c6cb Senior Takeaway: Choose MSE when outliers are \u0026ldquo;real\u0026rdquo; errors you must avoid (e.g., predicting stress on a bridge beam). Choose MAE when outliers might be measurement noise and you don\u0026rsquo;t want them to dominate the model\u0026rsquo;s training.\nB. Quantile Loss: When Over vs. Under Matters Imagine we\u0026rsquo;re DoorDash, estimating food delivery time.\nCase 1 (Over-prediction): We predict 30 mins, food arrives in 25 mins. Result: Happy, pleasantly surprised customer. Case 2 (Under-prediction): We predict 30 mins, food arrives in 35 mins. Result: Unhappy, frustrated customer who might not order again. Clearly, under-prediction is more costly. We want our model to \u0026ldquo;know\u0026rdquo; this. This is where Quantile Loss comes in.\nEquation: Quantile Loss = Σ q * |y_true - y_pred| (if error is positive, i.e., under-prediction) + Σ (1-q) * |y_true - y_pred| (if error is negative, i.e., over-prediction). Here q is the quantile. Intuition: It\u0026rsquo;s just a weighted MAE. You choose the weights. Example (q = 0.8): We want 80% of our predictions to be over-estimates or correct. If we predict 30 mins and it arrives in 35 (under-prediction), the error of 5 mins is multiplied by q = 0.8. Loss = 5 * 0.8 = 4.0. If we predict 30 mins and it arrives in 25 (over-prediction), the error of -5 mins is multiplied by (1-q) = 0.2. Loss = 5 * 0.2 = 1.0. The model now learns it\u0026rsquo;s 4 times more painful to be late than to be early. It will naturally start \u0026ldquo;padding\u0026rdquo; its estimates to avoid the larger penalty. Deep Dive 2: Classification Loss: How wrong is our label? A. Contrastive Loss: Learning by Comparison This is the key to all modern retrieval systems (search, recommendation, RAG). Forget predicting a single score for a moment. The goal is to produce an embedding (a vector) for the user and for every item, such that the vectors of \u0026ldquo;good\u0026rdquo; pairs are close together.\nThe Setup: Imagine a mini-batch of training data for a YouTube recommender. It contains one \u0026ldquo;positive\u0026rdquo; pair: (User A, Video_SciFi_1) because User A watched it. It also contains several \u0026ldquo;negative\u0026rdquo; pairs, which are just the other videos in the batch: (User A, Video_Cooking_2), (User A, Video_Sports_3).\nThe Goal: We want to make the similarity score (often dot product) of the positive pair much higher than the similarity scores of the negative pairs.\nSim(User_A_vec, Video_SciFi_1_vec) should be \u0026raquo; Sim(User_A_vec, Video_Cooking_2_vec) Sim(User_A_vec, Video_SciFi_1_vec) should be \u0026raquo; Sim(User_A_vec, Video_Sports_3_vec) How it Works (In-batch Softmax, a common contrastive loss):\nCalculate the similarity score s_i for every user-item pair in the batch. Treat it like a classification problem! Apply a Softmax function across all the scores. The loss is simply the Cross-Entropy loss of trying to \u0026ldquo;classify\u0026rdquo; the positive pair as the correct one. Equation (Conceptual): Loss = -log( exp(s_positive) / (exp(s_positive) + Σ exp(s_negative)) )\nWhy it\u0026rsquo;s genius: By minimizing this loss, you are implicitly pushing the positive score up and all the negative scores down, achieving the goal of separating them in the embedding space. This is how the two-tower model is trained.\ngraph LR subgraph \u0026#34;Embedding Space\u0026#34; direction BT U(User) P((Positive\u0026lt;br/\u0026gt;Item)) N1((Negative\u0026lt;br/\u0026gt;Item)) N2((Negative\u0026lt;br/\u0026gt;Item)) U -- \u0026#34;Pull Closer\u0026#34; --\u0026gt; P U -- \u0026#34;Push Away\u0026#34; --\u0026gt; N1 U -- \u0026#34;Push Away\u0026#34; --\u0026gt; N2 end CL(Contrastive Loss) CL --\u0026gt; U CL --\u0026gt; P CL --\u0026gt; N1 CL --\u0026gt; N2 style P fill:#dff0d8 style N1 fill:#f5c6cb style N2 fill:#f5c6cb Senior Takeaway: When designing a system with a retrieval step, you must talk about using a contrastive loss to train the embedding models (the two towers).\nDeep Dive 3: Huber Loss: The Robust \u0026ldquo;Best of Both Worlds\u0026rdquo; Book\u0026rsquo;s Core Idea (Timeless): Huber Loss is designed as a composite function to get the best attributes of both MSE and MAE. For small errors, it uses a quadratic function (like MSE), which is strongly convex and leads to stable convergence. For large errors, it switches to a linear function (like MAE), which prevents outliers from dominating the gradient and pulling the model too far off course.\nIntuition \u0026amp; Equation: It\u0026rsquo;s defined by a threshold hyperparameter, δ (delta). You, the engineer, decide what constitutes a \u0026ldquo;small\u0026rdquo; vs. \u0026ldquo;large\u0026rdquo; error.\nThe loss is defined piecewise:\nIf |y_true - y_pred| \u0026lt;= δ: Loss = 1/2 * (y_true - y_pred)² (It\u0026rsquo;s MSE) If |y_true - y_pred| \u0026gt; δ: Loss = δ * |y_true - y_pred| - 1/2 * δ² (It\u0026rsquo;s MAE, with a constant adjustment to make the function smooth at the δ boundary) Example: Let\u0026rsquo;s set δ = 1.0. We\u0026rsquo;re predicting a value of 10.\nPrediction 1 = 10.5 (Error = 0.5): Since 0.5 \u0026lt;= δ, we use the MSE part. Loss = 0.5 * (0.5)² = 0.125. Prediction 2 = 13.0 (Error = 3.0): Since 3.0 \u0026gt; δ, we use the MAE part. Loss = 1.0 * 3.0 - 0.5 * (1.0)² = 2.5. Notice that a pure MSE would have given a loss of (3.0)² = 9.0. Huber loss provides a much smaller, more reasonable penalty. The 2024+ Perspective: Huber Loss remains a fantastic, practical choice for many regression problems. Its main drawback is simply the need to tune the δ hyperparameter. In libraries like XGBoost and LightGBM, it\u0026rsquo;s often available as a built-in objective function. It signals that you are thinking about the robustness of your training process.\nInterview Phrasing: \u0026ldquo;For this regression task, I\u0026rsquo;d start with Huber loss instead of a simple MSE. This would make our training process more robust to potential outliers in the data, which are common in real-world datasets. We would need to tune the delta hyperparameter, likely via cross-validation, to define the point at which we treat an error as an outlier.\u0026rdquo;\ngraph TD subgraph \u0026#34;Error Size\u0026#34; A[Small Error\u0026lt;br/\u0026gt;error \u0026lt;= δ] B[Large Error\u0026lt;br/\u0026gt;error \u0026gt; δ] end subgraph \u0026#34;Huber Loss Behavior\u0026#34; C[Quadratic Penalty\u0026lt;br/\u0026gt;Like MSE] D[Linear Penalty\u0026lt;br/\u0026gt;Like MAE] end A --\u0026gt; C B --\u0026gt; D C --\u0026gt; E[Result: Stable convergence\u0026lt;br/\u0026gt;for common cases] D --\u0026gt; F[Result: Robust to outliers] style C fill:#dff0d8 style D fill:#f5c6cb Deep Dive 4: How Facebook Uses Normalized Cross Entropy (NCE): The \u0026ldquo;Fair Comparison\u0026rdquo; Metric Book\u0026rsquo;s Core Idea (Timeless): The book presents a crucial real-world problem: how do you compare the performance of two models that operate in different environments? Normal Log Loss is not a fair comparison.\nProblem: Imagine you have Model A for US ads (base CTR = 2%) and Model B for Indian ads (base CTR = 0.5%). Model A achieves a log loss of 0.10. Model B achieves a log loss of 0.04. Is Model B better? Not necessarily! It\u0026rsquo;s much easier to get a low log loss on a dataset with very low base probability. Intuition \u0026amp; Equation: NCE re-frames the question from \u0026ldquo;What is the model\u0026rsquo;s log loss?\u0026rdquo; to \u0026ldquo;How much better is the model than a dumb baseline that always predicts the average?\u0026rdquo;\nNCE = LogLoss(model) / LogLoss(background_rate)\nWhere LogLoss(background_rate) is the cross-entropy you\u0026rsquo;d get if your model just predicted the average CTR for every single example.\nInterpretation: NCE \u0026lt; 1.0: Your model is smarter than the baseline. The lower, the better. NCE = 1.0: Your model is exactly as good as the baseline. NCE \u0026gt; 1.0: Your model is actively worse than just predicting the average. Example (from the book):\nModel 1 (Fixed Prediction) has a log loss of 0.36 on data with 10% CTR. The baseline log loss is 0.325. NCE = 0.36 / 0.325 = 1.11. This model is worse than just guessing the average. Model 2 (Fancy Model) has a log loss of 0.65 on data with 50% CTR. The baseline log loss is 0.693. NCE = 0.65 / 0.693 = 0.945. This model is better than guessing the average. Conclusion: The fancy model is the \u0026ldquo;smarter\u0026rdquo; model, even though its absolute log loss was higher. The 2024+ Perspective: This concept is a hallmark of a mature ML practice. It\u0026rsquo;s used for comparing models across different countries, platforms (iOS vs. Android), or any segment with a different underlying target distribution. It\u0026rsquo;s a fantastic point to bring up in a discussion about offline evaluation.\nInterview Phrasing: \u0026ldquo;When evaluating our click prediction model across different regions, simply comparing log loss can be misleading due to varying background CTRs. I would implement Normalized Cross-Entropy (NCE) as a key offline metric. This would allow us to measure the \u0026rsquo;lift over baseline\u0026rsquo; for each model in its respective domain, giving us a fair and comparable measure of model intelligence.\u0026rdquo;\nDeep Dive 4: Forecast Metrics: MAPE and SMAPE Book\u0026rsquo;s Core Idea (Timeless): These metrics are used when you care about relative error (percentage) rather than absolute error. This is useful for comparing forecast accuracy on items with different scales (e.g., forecasting sales of a $1000 TV vs. a $10 pack of batteries). Mean Absolute Percentage Error (MAPE): Equation: MAPE = (100%/n) * Σ |(Actual - Forecast) / Actual| The Big Flaw (Asymmetry): MAPE has a different range for over-forecasting vs. under-forecasting. If Actual = 100, Forecast = 50 -\u0026gt; |50/100| = 50% error. If Actual = 100, Forecast = 150 -\u0026gt; |-50/100| = 50% error. If Actual = 100, Forecast = 0 (max under-forecast) -\u0026gt; |100/100| = 100% error. If Actual = 100, Forecast = 300 -\u0026gt; |-200/100| = 200% error. The penalty for over-forecasting is unbounded, while the penalty for under-forecasting is capped at 100%. This means a model trained to minimize MAPE will learn to be biased towards under-forecasting. It also blows up if Actual is zero. Symmetric Absolute Percentage Error (SMAPE): Equation: SMAPE = (100%/n) * Σ |Forecast - Actual| / ((|Actual| + |Forecast|)/2) The Fix: It attempts to fix the asymmetry by normalizing by the average of the actual and forecast values. This bounds the metric between 0% and 200%. The New Problem: The interpretation is less intuitive, and it can still be unstable if both Actual and Forecast are very close to zero. The 2024+ Perspective: While commonly found in business reports due to their intuitive nature, these metrics are often avoided as direct loss functions for training because of their instability. A more common approach in serious forecasting systems is to: Transform the data: Take the log(1 + target). Train the model: Use a stable loss like MAE or Huber on the transformed data. Inverse transform: Exponentiate the model\u0026rsquo;s predictions to get back to the original scale. Report: Calculate MAPE/SMAPE on the final predictions for business stakeholders. Interview Phrasing: \u0026ldquo;For reporting forecast accuracy to our business partners, MAPE is a good choice because percentage error is easy to understand. However, for the model\u0026rsquo;s objective function, I would avoid MAPE due to its known asymmetry and instability. A more robust approach is to train on the log-transformed sales data using a simple MAE loss, and then report MAPE on the final, inverse-transformed predictions.\u0026rdquo; Deep Dive 5: Focal Loss: Focusing on the Hard Cases Book\u0026rsquo;s Core Idea (Timeless): Focal Loss is an improvement on standard Cross-Entropy for cases with extreme class imbalance, especially when there are many \u0026ldquo;easy\u0026rdquo; examples that dominate the loss. Its canonical use case is object detection, where the vast majority of pixels in an image are \u0026ldquo;easy background.\u0026rdquo;\nIntuition \u0026amp; Equation: It adds a modulating factor (1 - p_t)^γ to the cross-entropy loss.\nFocal Loss = - (1 - p_t)^γ * log(p_t)\np_t: The model\u0026rsquo;s predicted probability for the ground-truth class. γ (gamma): The focusing parameter (e.g., γ = 2). Example:\nEasy Example: The model is very confident about a correct prediction, p_t = 0.99. The modulating factor is (1 - 0.99)² = 0.0001. The loss for this example is almost zeroed out. Hard Example: The model is very unsure, p_t = 0.1. The modulating factor is (1 - 0.1)² = 0.81. The loss for this example is only slightly reduced. The Result: The model is freed from wasting its capacity on perfecting its confidence for easy examples and can focus its updates on learning the hard ones.\ngraph TD subgraph \u0026#34;Standard Cross-Entropy\u0026#34; Easy[Easy Example\u0026lt;br/\u0026gt;p=0.99] --\u0026gt; Loss_Easy_CE[Loss = 0.01] Hard[Hard Example\u0026lt;br/\u0026gt;p=0.1] --\u0026gt; Loss_Hard_CE[Loss = 2.3] Loss_Hard_CE -- \u0026#34;230x larger\u0026#34; --\u0026gt; Loss_Easy_CE end subgraph \u0026#34;Focal Loss (γ=2)\u0026#34; Easy2[Easy Example\u0026lt;br/\u0026gt;p=0.99] --\u0026gt; Loss_Easy_FL[Loss ≈ 0.0001] Hard2[Hard Example\u0026lt;br/\u0026gt;p=0.1] --\u0026gt; Loss_Hard_FL[Loss ≈ 1.86] Loss_Hard_FL -- \u0026#34;~18,600x larger\u0026#34; --\u0026gt; Loss_Easy_FL end Title1[Total loss is dominated by\u0026lt;br/\u0026gt;a huge number of easy examples] Title2[Total loss is dominated by\u0026lt;br/\u0026gt;the few, hard examples] The 2024+ Perspective: Focal loss remains a very important and widely recognized technique. Mentioning it shows you are familiar with advanced solutions to the common class imbalance problem. Interview Phrasing: \u0026ldquo;Given the extreme imbalance between clicked and non-clicked ads, a simple weighted cross-entropy might not be enough. The loss could be dominated by the millions of easily-classified non-clicks. I would experiment with Focal Loss. By setting a gamma value, we can dynamically down-weight the loss for these easy examples and force the model to focus its capacity on the much smaller and harder-to-predict set of positive clicks.\u0026rdquo; Deep Dive 6: Hinge Loss: Learning with a Margin Book\u0026rsquo;s Core Idea (Timeless): Hinge loss is primarily associated with Support Vector Machines (SVMs). Its goal is not just to get the classification right, but to get it right with a high degree of confidence. It introduces the concept of a \u0026ldquo;margin.\u0026rdquo;\nIntuition \u0026amp; Equation: It operates on raw scores (not probabilities) and uses labels of {-1, +1}.\nLoss = max(0, 1 - y_true * y_pred)\nExample: The margin is at +1 for positive classes and -1 for negative classes.\nCase 1 (Correct, Confident): y_true = 1, y_pred = 1.5. Loss = max(0, 1 - 1*1.5) = 0. No penalty. The point is outside the margin. Case 2 (Correct, Not Confident): y_true = 1, y_pred = 0.6. Loss = max(0, 1 - 1*0.6) = 0.4. It\u0026rsquo;s penalized because it\u0026rsquo;s correct but falls inside the margin. Case 3 (Incorrect): y_true = 1, y_pred = -0.5. Loss = max(0, 1 - 1*(-0.5)) = 1.5. Heavily penalized. The 2024+ Perspective: While classic SVMs are less used, Hinge Loss is very relevant today as a form of contrastive loss. It\u0026rsquo;s excellent for training retrieval models. You can formulate a Triplet Loss using it: Loss = max(0, margin - Sim(anchor, positive) + Sim(anchor, negative)). This loss function directly optimizes to ensure the similarity of a positive pair is greater than the similarity of a negative pair by at least margin.\nInterview Phrasing: \u0026ldquo;While Hinge Loss is classically from SVMs, its \u0026lsquo;margin\u0026rsquo; concept is very useful in modern systems. For our two-tower retrieval model, we could use a hinge-based triplet loss. This would train the model to ensure that the dot product of a user with a relevant item is greater than the dot product with an irrelevant item by a specific, pre-defined margin, leading to a more robust separation in the embedding space.\u0026rdquo;\nTopic: Model Evaluation Metrics Area Under the Curve (AUC): The \u0026ldquo;Better than Chance?\u0026rdquo; Metric Book\u0026rsquo;s Core Idea (Timeless): AUC, specifically for the Receiver Operating Characteristic (ROC) curve, is one of the most fundamental metrics for binary classification. It answers a single, powerful question: \u0026ldquo;If I randomly pick one positive example and one negative example, what is the probability that my model assigned a higher score to the positive one?\u0026rdquo; Intuition \u0026amp; How it\u0026rsquo;s built: Your model doesn\u0026rsquo;t output 0s and 1s. It outputs a continuous score (e.g., 0.0 to 1.0). To make a decision, you need a threshold. (e.g., \u0026ldquo;If score \u0026gt; 0.5, classify as 1\u0026rdquo;). The ROC curve is generated by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) for every possible threshold. TPR (Recall): True Positives / (True Positives + False Negatives) -\u0026gt; \u0026ldquo;Of all the actual positives, how many did we find?\u0026rdquo; FPR: False Positives / (False Positives + True Negatives) -\u0026gt; \u0026ldquo;Of all the actual negatives, how many did we incorrectly label as positive?\u0026rdquo; AUC is the literal area under this curve. graph TD subgraph \u0026#34;ROC Curve\u0026#34; direction LR A((0,0)) -- \u0026#34;Threshold=1.0\u0026lt;br/\u0026gt;Catch nothing\u0026#34; --\u0026gt; B(Model\u0026#39;s Curve) B -- \u0026#34;Threshold=0.0\u0026lt;br/\u0026gt;Catch everything\u0026#34; --\u0026gt; C((1,1)) A -- \u0026#34;Random Guessing\u0026lt;br/\u0026gt;AUC = 0.5\u0026#34; --\u0026gt; C D((0,1)) style D fill:#dff0d8,stroke-width:4px,stroke:green X_AXIS --- Y_AXIS end subgraph Legend Ideal[\u0026#34;Perfect Model\u0026lt;br/\u0026gt;Point (0,1)\u0026#34;] Good[\u0026#34;Good Model\u0026lt;br/\u0026gt;Curve bows to top-left\u0026#34;] Random[\u0026#34;Random Model\u0026lt;br/\u0026gt;Diagonal Line\u0026#34;] end Y_AXIS[True Positive Rate] X_AXIS[False Positive Rate] Interpretation: AUC = 1.0: Perfect classifier. AUC = 0.5: Useless classifier, equivalent to a random guess. AUC = 0.0: Perfectly wrong classifier (it\u0026rsquo;s always predicting the opposite). The 2024+ Perspective: AUC\u0026rsquo;s main strength is that it\u0026rsquo;s threshold-independent. It evaluates the quality of your model\u0026rsquo;s scoring without you having to first pick a decision threshold. However, its weakness is that it can be misleading on highly imbalanced datasets. For a dataset with 99.9% negatives, a model can get a very high AUC by just being good at identifying negatives. This is why Precision-Recall AUC (PR-AUC) is often preferred in these imbalanced cases. Interview Phrasing: \u0026ldquo;To get a high-level view of my classifier\u0026rsquo;s discriminative power, I would start by measuring the ROC AUC. This gives us a threshold-independent measure of separability. However, since this is a highly imbalanced ad-click problem, I would also closely monitor the Precision-Recall AUC, as it provides a better picture of performance on the rare positive class, which is our primary business interest.\u0026rdquo; Mean Average Recall at K (MAR@K): The \u0026ldquo;Did we find everything?\u0026rdquo; Metric Book\u0026rsquo;s Core Idea (Timeless): As we discussed, this is for the retrieval/candidate generation stage. But what if you have a list of users? MAR@K is simply the average of the Recall@K values across all your users. Equation: MAR@K = (1/|U|) * Σ (for each user u in U) Recall@K(u) Where |U| is the total number of users. Example: User 1: We found 7 of their 10 relevant items. Recall@K = 0.7. User 2: We found 4 of their 5 relevant items. Recall@K = 0.8. User 3: We found 9 of their 12 relevant items. Recall@K = 0.75. MAR@K = (0.7 + 0.8 + 0.75) / 3 = 0.75 The 2024+ Perspective: It\u0026rsquo;s a solid metric. The senior-level extension is to ask, \u0026ldquo;What is the distribution of recall?\u0026rdquo; An average of 75% could be great, but it could also hide that the model has 99% recall for US users and only 10% recall for new users from Brazil. You must investigate the variance and look at different percentiles (e.g., p50, p90, p99) of the recall scores, not just the mean. Interview Phrasing: \u0026ldquo;Our primary metric for the candidate generator will be Mean Average Recall at 100. But I won\u0026rsquo;t stop at the mean. I will also analyze the full distribution of the per-user recall scores to ensure the model is performing equitably across different user segments and not just optimizing for the average.\u0026rdquo; Mean Average Precision (MAP) \u0026amp; Mean Reciprocal Rank (MRR): \u0026ldquo;Is the first right answer high up?\u0026rdquo; Book\u0026rsquo;s Core Idea (Timeless): These are ranking metrics. They care about the order of the results.\nMean Reciprocal Rank (MRR): The simpler of the two. It\u0026rsquo;s only useful when you care about a single right answer.\nQuestion it answers: \u0026ldquo;On average, how far down the list do I have to go to find the first relevant item?\u0026rdquo; Equation: MRR = (1/|Q|) * Σ (for each query q in Q) (1 / rank_of_first_relevant_item) Example: Query 1: First correct answer is at position 3. Score = 1/3. Query 2: First correct answer is at position 1. Score = 1/1. Query 3: First correct answer is at position 8. Score = 1/8. MRR = (1/3 + 1 + 1/8) / 3. Use Case: Fact-based question answering (\u0026ldquo;Who is the CEO of Apple?\u0026rdquo;). There\u0026rsquo;s only one right answer. Mean Average Precision (MAP): The more sophisticated one. It\u0026rsquo;s used when there can be multiple relevant documents.\nQuestion it answers: It\u0026rsquo;s a bit complex, but intuitively it\u0026rsquo;s the mean of the \u0026ldquo;Precision@K\u0026rdquo; scores, calculated only at the positions where a relevant document was found. It rewards both finding many relevant documents and finding them early. Use Case: A web search for \u0026ldquo;deep learning tutorials.\u0026rdquo; There are many good results. The 2024+ Perspective (Outdated): While historically important, MAP and MRR have been largely superseded by NDCG in modern ML systems. Why? Because NDCG can handle graded relevance (e.g., a tutorial from a top university is more relevant than a random blog post), while MAP/MRR cannot (an item is just relevant or not). In an interview, it\u0026rsquo;s good to know what they are, but you should recommend NDCG.\nInterview Phrasing: \u0026ldquo;For this search ranking task, we could use metrics like MAP or MRR. However, since not all relevant results are equally good, I would propose using NDCG. It\u0026rsquo;s a more flexible and powerful metric because it allows us to assign different relevance scores to the results—for example, a direct booking link could have a higher relevance score than a link to a review site.\u0026rdquo;\nNormalized Discounted Cumulative Gain (NDCG) \u0026amp; Cumulative Gain (CG) Book\u0026rsquo;s Core Idea (Timeless): We already did a deep dive on NDCG, but let\u0026rsquo;s formalize it and its precursor, CG. Cumulative Gain (CG): The dumbest version. It just sums up the relevance scores of the items in your list, ignoring order. Equation: CG@k = Σ (from i=1 to k) rel_i (where rel_i is the relevance of the item at position i). Example: Rankings [Rel=3, Rel=2, Rel=1] and [Rel=1, Rel=2, Rel=3] both have a CG@3 of 6. This is obviously wrong, as the first ranking is much better. CG is useless in practice. Discounted Cumulative Gain (DCG): The smarter version that fixes CG\u0026rsquo;s problem by introducing a positional penalty (the logarithmic discount). We covered this. Normalized Discounted Cumulative Gain (NDCG): The best version that fixes DCG\u0026rsquo;s problem by normalizing by the score of the perfect ranking. We covered this. The 2024+ Perspective: NDCG is the gold standard for ranking evaluation in any major tech company. If the problem involves ranking, you must talk about NDCG. Full stop. Online Metrics: Click-Through Rate, Time Spent Book\u0026rsquo;s Core Idea (Timeless): These are the business metrics you measure in an A/B test. Click-Through Rate (CTR): (Clicks / Impressions). The classic engagement metric for anything \u0026ldquo;clickable.\u0026rdquo; Time Spent: The total time a user spends on a page, watching a video, etc. This is often a better proxy for user satisfaction than CTR. A user might click on a clickbait title (high CTR) but leave after 2 seconds (low time spent), which is a bad outcome. The 2024+ Perspective: The key evolution here is understanding complex, long-term trade-offs and designing metrics to capture them. Example: YouTube Shorts vs. Long-form video. Shorts have an incredibly high CTR and \u0026ldquo;session starts\u0026rdquo; per hour. Long-form videos have lower CTR but generate much more total watch time and ad revenue per stream. If you only optimize for CTR, the algorithm will only show Shorts. If you only optimize for total watch time, it might only show 2-hour documentaries. The Solution: You need a composite or weighted primary metric for your A/B test. This is a business decision, not just an ML one. It might be something like Primary_Metric = w1 * CTR + w2 * Total_Watch_Time + w3 * Subscribes. Deciding on these weights is a key product/engineering collaboration. Interview Phrasing: \u0026ldquo;For our online A/B test, CTR is a good starting metric. However, it could be susceptible to clickbait and might not capture true user satisfaction. I would propose we track \u0026lsquo;Time Spent on Content\u0026rsquo; as a more robust proxy for engagement. Even better, we should work with the product team to define a composite primary metric that balances multiple business goals, such as engagement and creator follows, to prevent our model from over-optimizing for a single, narrow objective.\u0026rdquo; Topic: Common Sampling Techniques Random Sampling: The Simplest Baseline Book\u0026rsquo;s Core Idea (Timeless): This is the most straightforward technique: select items from a population where each item has an equal probability of being chosen. Intuition: Think of drawing names out of a hat. Use Case \u0026amp; Why it\u0026rsquo;s important: Its main application in ML design is for negative sampling in recommendation systems, especially in the early stages. Example (YouTube): You have a user\u0026rsquo;s watch history (positive examples). To train a classifier or a two-tower model, you need negative examples. The simplest way is to randomly sample a few hundred videos from the entire corpus of billions of videos. The 2024+ Perspective: While simple, \u0026ldquo;uniform random\u0026rdquo; negative sampling is often considered a weak baseline. The model learns very quickly to distinguish a user\u0026rsquo;s niche interest (e.g., \u0026ldquo;vintage fountain pen restoration\u0026rdquo;) from a completely random video (\u0026ldquo;how to bake a cake\u0026rdquo;). To get better performance, you need to show it \u0026ldquo;harder\u0026rdquo; negatives, which leads to other sampling strategies. Interview Phrasing: \u0026ldquo;For the initial version of our two-tower recommender, we will generate training pairs by matching each positive user-item interaction with N randomly sampled negative items from the entire catalog. This establishes a strong baseline. In V2, we would explore more sophisticated hard negative sampling strategies to improve the model\u0026rsquo;s ability to make fine-grained distinctions.\u0026rdquo; graph TD subgraph \u0026#34;Video Corpus\u0026#34; V1(Video 1) V2(Video 2) V3(...) VN(Video N) end U(User A Watched V1) -- Positive Example --\u0026gt; M(Model Training) subgraph \u0026#34;Negative Sampling\u0026#34; V2 -- Randomly Picked --\u0026gt; N1(Negative Example) VN -- Randomly Picked --\u0026gt; N2(Negative Example) end N1 --\u0026gt; M N2 --\u0026gt; M style U fill:#dff0d8 style V2 fill:#f5c6cb style VN fill:#f5c6cb Rejection Sampling: Sampling from a \u0026ldquo;Difficult\u0026rdquo; Distribution Book\u0026rsquo;s Core Idea (Timeless): Use this when you want to sample from a complex target distribution p(z), but you don\u0026rsquo;t know how to do it directly. However, you do know how to sample from a simpler proposal distribution q(z) that \u0026ldquo;envelopes\u0026rdquo; p(z). Intuition (The Dartboard Analogy): Imagine you want to sample points uniformly from within a circle. You don\u0026rsquo;t know how to do that directly, but you know how to sample uniformly from the square that contains the circle. Propose: Throw a dart at a random point (x, y) inside the square (sampling from q(z)). Accept/Reject: Check if the point (x, y) is also inside the circle (evaluating p(z)). If it is, you accept the sample. If it\u0026rsquo;s outside the circle but inside the square, you reject it and try again. Diagram: The book\u0026rsquo;s rand7() from rand10() example is good, but a graphical one is more intuitive. graph TD subgraph \u0026#34;Rejection Sampling Process\u0026#34; A[Start] --\u0026gt; B{Sample z from\u0026lt;br/\u0026gt;Proposal Dist. q} B --\u0026gt; C{Sample u from\u0026lt;br/\u0026gt;Uniform 0 to M*q} C --\u0026gt; D{Is u \u0026lt;= p z?} D -- Yes --\u0026gt; E[Accept z as sample] D -- No --\u0026gt; B end subgraph \u0026#34;Graphical View\u0026#34; direction LR Wrapper(Proposal Distribution q) Target(Target Distribution p) subgraph Wrapper P1(Sample 1) P2(Sample 2) P3(Sample 3) end subgraph Target P1 end style Target fill:#cde4ff,stroke:#333 style P2 fill:red style P3 fill:red end Note1[P1 is under p so ACCEPT] --\u0026gt; P1 Note2[P2 is outside p so REJECT] --\u0026gt; P2 Note3[P3 is outside p so REJECT] --\u0026gt; P3 The 2024+ Perspective: While fundamental in statistics, Rejection Sampling is less common in day-to-day large-scale ML design compared to Importance Sampling. Its main drawback is that it can be very inefficient if the proposal distribution q(z) is not a good fit for p(z), leading to a very high rejection rate. Interview Phrasing: It\u0026rsquo;s a good concept to know as a sign of a well-rounded education. \u0026ldquo;If we needed to sample from a complex, non-standard distribution, one approach could be rejection sampling, where we would sample from a simpler, enveloping distribution like a Gaussian and then use a probabilistic check to accept or reject the sample.\u0026rdquo; Importance Sampling: Correcting for the \u0026ldquo;Wrong\u0026rdquo; Distribution Book\u0026rsquo;s Core Idea (Timeless): This is the inverse of Rejection Sampling. Instead of rejecting samples, we keep all of them but assign them a weight to correct for the fact that we sampled from the \u0026ldquo;wrong\u0026rdquo; distribution. This is used to estimate an expected value.\nIntuition: Imagine you want to find the average height of people in a city, but you only have data from a basketball convention. Your samples are heavily biased towards tall people. To get an unbiased estimate, you need to down-weight the tall people in your calculation. The weight is the correction factor.\nEquation: The goal is to estimate E_p[f(x)], the expected value of f(x) under distribution p. We sample from distribution q.\nE_p[f(x)] ≈ (1/N) * Σ [ f(x_i) * ( p(x_i) / q(x_i) ) ]\nThe term w(x_i) = p(x_i) / q(x_i) is the importance weight. It corrects for the bias.\nUse Case (Off-Policy Evaluation): This is the killer app for Importance Sampling in modern ML.\nProblem: You have a new recommendation model (Policy B) you want to evaluate, but you don\u0026rsquo;t want to run a risky A/B test. You only have log data from the old model that is currently in production (Policy A). Can you estimate how well Policy B would have performed using Policy A\u0026rsquo;s data? Solution: Yes. You look at the actions taken by Policy A. For each action, you re-weight the outcome (e.g., whether the user clicked) by the ratio of probabilities: p(action|Policy_B) / p(action|Policy_A). This gives you an unbiased estimate of Policy B\u0026rsquo;s performance without ever deploying it. Diagram:\ngraph TD subgraph \u0026#34;Data from Production Policy A\u0026#34; A[User Context] --\u0026gt; PA[Policy A\u0026lt;br/\u0026gt;Production Model] PA --\u0026gt; Rec1(Recommendation 1) Rec1 --\u0026gt; R1[Reward: 1 Click] end subgraph \u0026#34;Offline What If Evaluation\u0026#34; A --\u0026gt; PB[Policy B\u0026lt;br/\u0026gt;New Candidate Model] PB --\u0026gt; Rec1 W[Importance Weight\u0026lt;br/\u0026gt;w = P Rec1 Policy B / P Rec1 Policy A] R1 --\u0026gt; Est(Weighted Reward) W --\u0026gt; Est Est --\u0026gt; Final[Estimated Performance\u0026lt;br/\u0026gt;of Policy B] end style W fill:#fff0b3 The 2024+ Perspective: This is a very active area of research and a critical concept for safe, iterative deployment in reinforcement learning and large-scale recommender systems. Interview Phrasing: \u0026ldquo;To evaluate a new ranking model without a risky online experiment, we can use off-policy evaluation. We\u0026rsquo;ll use the logs from our current production model and apply importance sampling to re-weight the observed outcomes (like clicks). This will give us an unbiased estimate of the new model\u0026rsquo;s performance, allowing us to iterate much more quickly and safely.\u0026rdquo; Stratified Sampling: Fair Representation Book\u0026rsquo;s Core Idea (Timeless): Use this when your population has distinct subgroups (strata) and you want to ensure that your sample accurately reflects the proportions of these subgroups. Intuition: You\u0026rsquo;re polling for an election. The country is 52% female and 48% male. A simple random sample might accidentally give you 60% males, skewing your results. With stratified sampling, you would first divide the population by gender, and then draw random samples from each group in the correct proportion. Use Case: Building a robust test set. Problem: You\u0026rsquo;re building a fraud detection model. Your user base is 70% from the US, 20% from Europe, and 10% from Asia. If you create your test set with pure random sampling, you might get very few examples from Asia, and you won\u0026rsquo;t have a reliable measure of how well your model performs for that critical segment. Solution: You use stratified sampling to build a test set that has the exact 70/20/10 split, guaranteeing a fair evaluation across all geographies. The 2024+ Perspective: The principle is more important than ever, especially in the context of Responsible AI and fairness. You must be able to evaluate your model not just on an overall metric, but on its performance across sensitive attributes (age, gender, race, geography). Stratified sampling is the mechanism to create the evaluation datasets that allow you to do this. Interview Phrasing: \u0026ldquo;To ensure our final model performance is robust and fair, we won\u0026rsquo;t use a simple random split for our test set. Instead, we\u0026rsquo;ll create it using stratified sampling based on user country. This guarantees that our test set maintains the same geographic distribution as our production traffic, allowing us to confidently measure and report performance for each key region.\u0026rdquo; Reservoir Sampling: Sampling from a Stream Book\u0026rsquo;s Core Idea (Timeless): How do you get a uniform random sample of k items from a dataset so large you can\u0026rsquo;t fit it in memory and don\u0026rsquo;t even know its total size (N) in advance? This is the classic \u0026ldquo;sampling from a stream\u0026rdquo; problem. Intuition \u0026amp; Algorithm (Algorithm R): Fill the reservoir: Take the first k items from the stream and put them in your reservoir (an array of size k). Process the rest: For each subsequent item i (from k+1 to N): Generate a random integer j from 1 to i. If j is between 1 and k, swap the i-th item from the stream with the item at the j-th position in your reservoir. Otherwise, do nothing (let the i-th item flow past). Why it works: It\u0026rsquo;s a bit of mathematical magic, but it ensures that at the end of the stream, every item that has ever passed through has had an equal k/N probability of being in the final reservoir. Diagram: graph TD subgraph \u0026#34;Stream of Unknown Length N\u0026#34; I1 --\u0026gt; I2 --\u0026gt; I3 --\u0026gt; I4 --\u0026gt; I5 --\u0026gt; I6 --\u0026gt; I7 --\u0026gt; Etc end subgraph \u0026#34;Reservoir size k=3\u0026#34; R[Reservoir Array] R --\u0026gt; RI1 R --\u0026gt; RI2 R --\u0026gt; RI3 end subgraph \u0026#34;Step 1: Fill Reservoir\u0026#34; I1 --\u0026gt; RI1(Item 1) I2 --\u0026gt; RI2(Item 2) I3 --\u0026gt; RI3(Item 3) end subgraph \u0026#34;Step 2: Process Item i=4\u0026#34; I4 --\u0026gt; C1[j = random 1 to 4] C1 --\u0026gt; Swap[Swap I4 with\u0026lt;br/\u0026gt;item at R j] C1 --\u0026gt; Skip1(Do Nothing) end subgraph \u0026#34;Step 3: Process Item i=5\u0026#34; I5 --\u0026gt; C2[j = random 1 to 5] C2 --\u0026gt; Swap2[Swap I5 with\u0026lt;br/\u0026gt;item at R j] C2 --\u0026gt; Skip2(Do Nothing) end Etc --\u0026gt; FinalState(Final Reservoir\u0026lt;br/\u0026gt;A random sample of 3 items) The 2024+ Perspective: This is a classic algorithm that comes up frequently in coding rounds for data-heavy roles. It\u0026rsquo;s also used in large-scale data processing systems (like in Spark or Flink) for approximating analytics (e.g., \u0026ldquo;get me a random sample of 1000 user IDs who performed this action\u0026rdquo;) without having to do a full, expensive shuffle of the data. Interview Phrasing: \u0026ldquo;If we need to generate a random sample of user sessions from our live Kafka event stream for analysis, we can\u0026rsquo;t load the whole stream. This is a perfect use case for Reservoir Sampling. We can implement a streaming job that maintains a reservoir of size k, and for each new event, it probabilistically decides whether to swap it into the reservoir. This gives us a statistically valid, uniform random sample at any point in time without knowing the total size of the stream.\u0026rdquo; Topic: Common Deep Learning Model Architectures 1. Wide and Deep Architecture: The Power of Memorization and Generalization Book\u0026rsquo;s Core Idea (Timeless): This is a seminal architecture from Google that elegantly addresses a fundamental tension in recommendation systems: Memorization: The system needs to learn and exploit simple, direct rules from the data. For example: \u0026ldquo;People who searched for \u0026lsquo;iPhone case\u0026rsquo; often buy the \u0026lsquo;Apple official silicone case\u0026rsquo;.\u0026rdquo; This is about recommending highly related items. Generalization: The system needs to explore and discover new, less obvious connections. For example: \u0026ldquo;People who bought an iPhone case might also be interested in a wireless charger, even if they\u0026rsquo;ve never shown interest before.\u0026rdquo; This is about recommending novel items. Intuition \u0026amp; Architecture: The Wide \u0026amp; Deep model combines two distinct parts that are joined only at the very end. The \u0026ldquo;Wide\u0026rdquo; Part (Memorization): This is a simple, linear model (like logistic regression). Its job is to learn the direct, one-to-one feature interactions. It takes in raw features and, most importantly, cross-product features. A cross-product feature is a feature created by combining two others (e.g., AND(user_country='USA', item_category='gardening')). This allows the linear model to explicitly memorize the impact of this specific combination. The \u0026ldquo;Deep\u0026rdquo; Part (Generalization): This is a standard feed-forward neural network (MLP). Its job is to take low-dimensional embeddings of the features and discover complex, non-linear, and unseen feature combinations through its hidden layers. It can learn that user_country='USA' and item_category='gardening' are correlated with item_category='bbq_grills' even if very few users have exhibited all three behaviors. graph TD subgraph \u0026#34;Input Features\u0026#34; Cat_Features[Categorical Features\u0026lt;br/\u0026gt;user_id, item_id, country] Num_Features[Numerical Features\u0026lt;br/\u0026gt;price, age] end subgraph \u0026#34;Wide Part Memorization\u0026#34; direction LR Cross[Cross-Product\u0026lt;br/\u0026gt;Transform] Wide_Model(Simple Linear Model) Raw_Features[Raw Features] Raw_Features --\u0026gt; Cross Cross --\u0026gt; Wide_Model end subgraph \u0026#34;Deep Part Generalization\u0026#34; direction LR Embedding(Embedding Layer) Deep_Model(Deep Neural Network) Embedding --\u0026gt; Deep_Model end Cat_Features --\u0026gt; Embedding Num_Features --\u0026gt; Deep_Model subgraph \u0026#34;Output\u0026#34; Combine(+) Final_Output(Final Prediction\u0026lt;br/\u0026gt;e.g., Sigmoid for p click) Combine --\u0026gt; Final_Output end Wide_Model --\u0026gt; Combine Deep_Model --\u0026gt; Combine style Cross fill:#fff0b3 style Embedding fill:#cde4ff The 2024+ Perspective: The principle of combining memorization and generalization is more relevant than ever. The Wide \u0026amp; Deep architecture itself is still a very strong baseline. However, the \u0026ldquo;Deep\u0026rdquo; side has become more sophisticated (e.g., using attention layers), and the \u0026ldquo;Wide\u0026rdquo; side is sometimes replaced or augmented by different explicit feature interaction layers. This leads directly to our next topic\u0026hellip; Interview Phrasing: \u0026ldquo;For a search ranking problem with both sparse categorical features and dense numerical features, a Wide \u0026amp; Deep architecture is a great starting point. The \u0026lsquo;wide\u0026rsquo; component would allow us to explicitly learn the memorized interactions between key query and item features using cross-product transformations. The \u0026lsquo;deep\u0026rsquo; component, operating on learned embeddings, would handle generalization and discover less obvious correlations. This hybrid approach ensures our model is both relevant and capable of novel recommendations.\u0026rdquo; 2. Two-Tower Architecture: The Scalable Retrieval Champion Book\u0026rsquo;s Core Idea (Timeless): We\u0026rsquo;ve touched on this before, but let\u0026rsquo;s formalize it. This architecture is the industry standard for the candidate generation/retrieval stage of any large-scale recommendation or search system (YouTube, Facebook, Pinterest, etc.). The Problem it Solves: How do you efficiently find a few hundred \u0026ldquo;pretty good\u0026rdquo; candidates from a corpus of billions of items in real-time (under 50ms)? You cannot score every item against the user query. Intuition \u0026amp; Architecture: It decouples the query and candidate models. Query Tower: Takes in all information about the user and their context (user profile, recent history, time of day) and crunches it down into a single embedding vector (e.g., a 128-dimensional vector). Candidate/Item Tower: Takes in all information about an item (title, description, category) and crunches it down into a single embedding vector of the same dimension. The Magic (Decoupling): During serving, you can pre-compute the embedding for EVERY item in your corpus and store them in an Approximate Nearest Neighbor (ANN) index like FAISS or a Vector Database. When a user request comes in, you only need to run the lightweight Query Tower to get the user\u0026rsquo;s embedding. Then, you use that vector to query the ANN index, which very quickly finds the k item vectors with the highest dot product similarity. Diagram: graph TD subgraph \u0026#34;Query Tower\u0026#34; U_Features[User Features\u0026lt;br/\u0026gt;context, history] --\u0026gt; U_L1(Dense Layer) U_L1 --\u0026gt; U_L2(Dense Layer) U_L2 --\u0026gt; QueryEmbedding[Query Embedding] end subgraph \u0026#34;Candidate Tower\u0026#34; I_Features[Item Features\u0026lt;br/\u0026gt;metadata, text] --\u0026gt; I_L1(Dense Layer) I_L1 --\u0026gt; I_L2(Dense Layer) I_L2 --\u0026gt; ItemEmbedding[Item Embedding] end subgraph \u0026#34;Training Objective\u0026#34; QueryEmbedding -- Dot Product --\u0026gt; Score ItemEmbedding -- Similarity --\u0026gt; Score Score --\u0026gt; Loss(Contrastive Loss\u0026lt;br/\u0026gt;e.g., Softmax, Hinge) end subgraph \u0026#34;Serving / Inference\u0026#34; direction LR ItemEmbedding --\u0026gt; AllItemEmbeddings[Pre-computed\u0026lt;br/\u0026gt;Item Embeddings] AllItemEmbeddings --\u0026gt; ANN_Index[(Vector Database / FAISS\u0026lt;br/\u0026gt;Approximate Nearest Neighbor)] QueryEmbedding --\u0026gt; ANN_Index ANN_Index --\u0026gt; Top_K_Candidates[Top K Candidates] end style QueryEmbedding fill:#cde4ff style ItemEmbedding fill:#dff0d8 The 2024+ Perspective: This pattern is now everywhere, not just in classic recommendations. It\u0026rsquo;s the foundation of Retrieval-Augmented Generation (RAG). The \u0026ldquo;query tower\u0026rdquo; is the model that embeds the user\u0026rsquo;s search query, and the \u0026ldquo;candidate tower\u0026rdquo; is the model that embeds the documents in your knowledge base. The principles are identical. Interview Phrasing: \u0026ldquo;To solve the candidate generation problem at scale, I\u0026rsquo;d design a two-tower retrieval model. We\u0026rsquo;d have a user/query tower and an item tower, trained jointly with a contrastive loss to map relevant pairs close in the embedding space. The key advantage of this architecture is its serving efficiency. We can pre-compute all item embeddings offline and load them into a vector index. At inference time, we only need to compute the user embedding on-the-fly and query the index to retrieve the top-k candidates in milliseconds.\u0026rdquo; 3. Deep Cross Network (DCN): Explicit and Bounded Feature Crossing Book\u0026rsquo;s Core Idea (Timeless): DCN is an evolution of the Wide \u0026amp; Deep concept. It asks: \u0026ldquo;What if we could get the benefits of the \u0026lsquo;wide\u0026rsquo; side\u0026rsquo;s feature crossing, but do it more automatically and efficiently than manually defining cross-products?\u0026rdquo; Intuition \u0026amp; Architecture: DCN replaces the \u0026ldquo;wide\u0026rdquo; part with a series of special \u0026ldquo;cross layers.\u0026rdquo; Each cross layer takes the output from the previous layer x_i and explicitly calculates its interaction with the original input x_0. Equation of a cross layer: x_{i+1} = x_0 * (w_i^T * x_i) + b_i + x_i What this means: It\u0026rsquo;s a very specific mathematical formula designed to create feature crosses of increasing complexity with each layer. Layer 1 creates 2nd-degree interactions, Layer 2 creates 3rd-degree, and so on. The + x_i is a residual connection that helps with training stability. Diagram: The book\u0026rsquo;s diagram showing the stacked cross layers is a good representation. graph TD Input --\u0026gt; Embedding subgraph \u0026#34;Deep Part (MLP)\u0026#34; D1(Dense) --\u0026gt; D2(Dense) --\u0026gt; D_Out end subgraph \u0026#34;Cross Part (DCN)\u0026#34; C1(Cross Layer 1) --\u0026gt; C2(Cross Layer 2) --\u0026gt; C_Out end Embedding --\u0026gt; D1 Embedding --\u0026gt; C1 D_Out --\u0026gt; Combine --\u0026gt; Output C_Out --\u0026gt; Combine --\u0026gt; Output style C1 fill:#fff0b3 style C2 fill:#fff0b3 The 2024+ Perspective: DCN is a very well-respected architecture and a great example to bring up. It shows you know about solutions beyond the original Wide \u0026amp; Deep paper. Its main benefit is creating explicit and bounded-degree feature interactions, which can be more efficient and interpretable than letting a giant MLP learn them implicitly. The latest version, DCNv2, improves the cross-layer to be even more expressive. Interview Phrasing: \u0026ldquo;To improve upon a standard Wide \u0026amp; Deep model, we could explore using a Deep Cross Network (DCN). Instead of manually crafting cross-features for the wide part, DCN uses specialized cross-layers to learn explicit feature interactions of increasing degree automatically. This can be more effective and require less manual feature engineering than the traditional approach.\u0026rdquo; 4. Multitask Learning: Learning More with Less Book\u0026rsquo;s Core Idea (Timeless): Instead of training separate models to predict different things, can we train a single model to predict all of them at once? Intuition \u0026amp; Architecture: The most common pattern is \u0026ldquo;Shared Bottom\u0026rdquo; architecture. The model has a large, shared set of bottom layers (e.g., the user and item embedding layers and a few shared dense layers). This is where the model learns a general representation of the input. The model then splits into multiple small, separate \u0026ldquo;towers\u0026rdquo; or \u0026ldquo;heads,\u0026rdquo; one for each task. Each tower learns the specific nuances for its own prediction. Use Case (YouTube - from the book): A video recommendation model needs to predict multiple outcomes: Will the user click on the video? (predict p(click)) Will the user like the video? (predict p(like)) Will the user subscribe to the channel? (predict p(subscribe)) How much watch time will the user generate? (predict expected_watch_time) Diagram: graph TD Input[Shared Input Features] --\u0026gt; Shared_Bottom(Shared Layers\u0026lt;br/\u0026gt;e.g., Embeddings, MLP) subgraph \u0026#34;Task-Specific Heads\u0026#34; Tower_Click(Click Tower) Tower_Like(Like Tower) Tower_Watch(Watch Time Tower) P_Click[Predict p click] P_Like[Predict p like] P_Time[Predict Watch Time] Shared_Bottom --\u0026gt; Tower_Click Shared_Bottom --\u0026gt; Tower_Like Shared_Bottom --\u0026gt; Tower_Watch Tower_Click --\u0026gt; P_Click Tower_Like --\u0026gt; P_Like Tower_Watch --\u0026gt; P_Time end subgraph \u0026#34;Total Loss Sum of individual losses\u0026#34; L_Click(LogLoss) L_Like(LogLoss) L_Time(MSE Loss) Total_Loss(Combined Loss) P_Click --\u0026gt; L_Click P_Like --\u0026gt; L_Like P_Time --\u0026gt; L_Time L_Click --\u0026gt; Total_Loss L_Like --\u0026gt; Total_Loss L_Time --\u0026gt; Total_Loss end Why it\u0026rsquo;s powerful: Implicit Regularization: Forcing the model to learn a shared representation that is good for multiple tasks prevents it from overfitting to the idiosyncrasies of any single task. This often leads to better generalization. Data Sparsity: It allows tasks with very little data (e.g., \u0026ldquo;subscribes\u0026rdquo;) to benefit from the large amount of data available for other tasks (e.g., \u0026ldquo;clicks\u0026rdquo;). The shared layers learn from all the data. The 2024+ Perspective: This is a core technique in any large-scale system. Modern variants like MMoE (Multi-gate Mixture-of-Experts) improve on the shared-bottom by learning to route information through different \u0026ldquo;expert\u0026rdquo; sub-networks based on the input, which is even more powerful. Mentioning MMoE is a strong senior signal. Interview Phrasing: \u0026ldquo;Instead of training separate models to predict clicks and likes, I would build a multi-task learning model. It would have a shared bottom to learn a common user/item representation from all interactions, with separate task-specific heads for predicting p(click) and p(like). This approach acts as a form of regularization and allows the like prediction task, which has sparser data, to benefit from the rich data of the click task, likely improving overall model performance.\u0026rdquo; Topic: A/B Testing \u0026amp; Experimentation 1. A/B Testing Fundamentals \u0026amp; Budget-Splitting A/B Testing Book\u0026rsquo;s Core Idea (Timeless):\nStandard A/B Testing: You split users into two groups. Group A (Control) sees the old model. Group B (Treatment) sees the new model. You compare metrics (CTR, conversions, etc.) between the two groups to declare a winner. This is the gold standard for measuring impact. The Problem in Marketplaces (Ads, Search): In a system with a shared, limited resource (like an ad budget or the top spot on a search page), a standard A/B test can be misleading. A \u0026ldquo;better\u0026rdquo; model might not be better at all; it might just be more aggressive and \u0026ldquo;win\u0026rdquo; the budget from the control group\u0026rsquo;s model. The overall system revenue might not increase. Budget-Splitting A/B Testing: This is LinkedIn\u0026rsquo;s clever solution to the marketplace problem. Instead of splitting users, you split the budget for each ad campaign. 50% of the budget can only be spent on control group users, and 50% can only be spent on treatment group users. This creates two independent mini-auctions, and you can now fairly compare which model generates more revenue from its allocated budget. Intuition \u0026amp; Diagram:\ngraph TD subgraph \u0026#34;Standard A/B Test (User Split)\u0026#34; U(User Arrives) --\u0026gt; USplit{Is User in Group A or B?} USplit -- A --\u0026gt; M_A(Model A) USplit -- B --\u0026gt; M_B(Model B) M_A \u0026amp; M_B --\u0026gt; Auction{Single Auction\u0026lt;br\u0026gt;Models compete for budget} Result_A[Problem: Model B might win\u0026lt;br\u0026gt;just by being more aggressive,\u0026lt;br\u0026gt;not by being better overall.] end subgraph \u0026#34;Budget-Splitting A/B Test\u0026#34; AdCampaign[Ad Campaign\u0026lt;br\u0026gt;Total Budget: $1000] --\u0026gt; BSplit{Split Budget} BSplit --\u0026gt; Budget_A(Budget for Control\u0026lt;br\u0026gt;$500) BSplit --\u0026gt; Budget_B(Budget for Treatment\u0026lt;br\u0026gt;$500) U_A(Control User Arrives) --\u0026gt; M_A2(Model A) M_A2 --\u0026gt; Auction_A{Auction A\u0026lt;br\u0026gt;Can only use Budget A} U_B(Treatment User Arrives) --\u0026gt; M_B2(Model B) M_B2 --\u0026gt; Auction_B{Auction B\u0026lt;br\u0026gt;Can only use Budget B} Result_B[Solution: Fair comparison.\u0026lt;br\u0026gt;Does Model B generate more\u0026lt;br\u0026gt;revenue from its $500?] end style Budget_A fill:#cde4ff style Budget_B fill:#dff0d8 The 2024+ Perspective: The core principles are solid. The modern conversation has expanded to include more sophisticated experimental designs.\nInterleaving: For ranking problems, instead of showing a user a full list from Model A or Model B, you can show them a single, blended list containing results from both. You then measure which model\u0026rsquo;s results the user clicks on more within that blended list. This can be a much faster and more sensitive way to detect ranking improvements. Multi-Armed Bandits (Thompson Sampling): For problems where you need to balance exploration (trying new things) vs. exploitation (using the current best option), bandits are key. Instead of a fixed 50/50 traffic split, a bandit algorithm will dynamically allocate more traffic to the better-performing model over time, minimizing regret (lost revenue) during the experiment. Interview Phrasing: \u0026ldquo;To test our new ranking model, a standard A/B test would be the first step. However, given this is an ad marketplace, I\u0026rsquo;d be concerned about budget contention between the control and treatment groups. A more robust approach would be a budget-splitting A/B test to ensure a fair comparison. For faster iteration on ranking quality, we could also consider interleaving, which can detect superior models with less traffic and time. If minimizing revenue loss during the experiment is critical, a multi-armed bandit approach could dynamically allocate traffic to the winning model.\u0026rdquo;\nTopic: Common Deployment Patterns 1. Aggregator / Broker Service for Imbalanced Workload Book\u0026rsquo;s Core Idea (Timeless): You have a complex request that requires scoring a large number of candidates (e.g., 10,000 ads). A single inference server might be too slow to meet the latency requirement (e.g., \u0026lt; 100ms). Intuition \u0026amp; Architecture: The Aggregator (or Broker) pattern is a simple \u0026ldquo;divide and conquer\u0026rdquo; strategy. An Aggregator Service receives the initial request with all 10,000 candidates. It splits the list of candidates into smaller chunks (e.g., 10 chunks of 1,000 candidates each). It sends these chunks in parallel to multiple Worker Services (the actual model inference servers). It waits for all the workers to return their scored chunks. It combines the results, does a final sort, and returns the top N candidates. graph TD Client --\u0026gt; Aggregator{Aggregator Service} subgraph \u0026#34;Worker Pool (Model Inference)\u0026#34; W1(Worker 1) W2(Worker 2) W3(...) WN(Worker N) end Aggregator -- \u0026#34;Candidates 1-1000\u0026#34; --\u0026gt; W1 Aggregator -- \u0026#34;Candidates 1001-2000\u0026#34; --\u0026gt; W2 Aggregator -- \u0026#34;...\u0026#34; --\u0026gt; WN W1 -- \u0026#34;Scored chunk\u0026#34; --\u0026gt; Aggregator W2 -- \u0026#34;Scored chunk\u0026#34; --\u0026gt; Aggregator WN -- \u0026#34;Scored chunk\u0026#34; --\u0026gt; Aggregator Aggregator --\u0026gt; FinalResponse(Final Sorted List) The 2024+ Perspective: This pattern is fundamental and widely used. It\u0026rsquo;s essentially a backend-specific load balancer. In a modern Kubernetes environment, the \u0026ldquo;Worker Pool\u0026rdquo; would be a set of pods that can be auto-scaled based on the request volume. The key design consideration is handling failures: what if one worker is slow or fails? A robust aggregator needs to have timeouts and potentially a retry mechanism or the ability to return a partial result. Interview Phrasing: \u0026ldquo;A single prediction request involves ranking 10,000 candidates, which would likely exceed our 100ms latency budget if handled by one server. I would implement an aggregator pattern. A front-end service would receive the request, shard the candidate list, and fan out the scoring requests in parallel to a pool of stateless model inference workers. The aggregator would then collect the scored lists, merge them, and return the final ranking. This allows us to scale the number of workers horizontally to meet our latency SLA.\u0026rdquo; 2. Serving Logic and Multiple Models Book\u0026rsquo;s Core Idea (Timeless): One model doesn\u0026rsquo;t fit all. You often need business logic to decide which model to use for a given request. Example (from the book): An ad serving system might have different models for different user device types. If user_device == 'iOS', route to Model_iOS. Else, route to Model_Generic. The 2024+ Perspective: This is the foundation of more complex systems. Model Ensembles: Instead of just choosing one model, you might route the request to multiple models and then combine their outputs (e.g., by averaging their scores). Cascade of Models: For latency-critical funnels, you use a cascade. A very fast, simple model first filters out the 90% worst candidates. The remaining 10% are then passed to a more complex, slower, but more accurate model for the final ranking. This provides the best trade-off between accuracy and speed. Diagram (Cascade Example): graph TD Request[\u0026#34;Request w/ 100k candidates\u0026#34;] --\u0026gt; M1{\u0026#34;Model 1: Fast and Simple\u0026lt;br/\u0026gt;(e.g., Logistic Regression)\u0026#34;} M1 -- \u0026#34;Filters down to 5k candidates\u0026#34; --\u0026gt; M2{\u0026#34;Model 2: Slow and Complex\u0026lt;br/\u0026gt;(e.g., Large Transformer)\u0026#34;} M2 -- \u0026#34;Final ranking of top 10\u0026#34; --\u0026gt; Response style M1 fill:#cde4ff style M2 fill:#dff0d8 Interview Phrasing: \u0026ldquo;We could improve performance by implementing a model cascade. A fast, lightweight first-pass model, perhaps a simple logistic regression on basic features, could filter the initial 100,000 candidates down to the top 5,000. Only these promising candidates would then be passed to our larger, more computationally expensive deep learning model for the final re-ranking. This preserves overall accuracy while dramatically reducing the end-to-end latency.\u0026rdquo; 3. Serving Embeddings Book\u0026rsquo;s Core Idea (Timeless): This section synthesizes many of the previous points, showing how LinkedIn serves its two-tower model. The key distinction is between offline and nearline serving. Offline Serving (Batch): For things that don\u0026rsquo;t change often. For example, a user\u0026rsquo;s embedding based on their long-term profile. This can be pre-computed in a daily or weekly batch job and stored in a feature store. Nearline Serving (Streaming): For things that change quickly. For example, a new job posting is created. This event triggers a streaming process that immediately computes the job\u0026rsquo;s embedding and makes it available for retrieval. The 2024+ Perspective: This is the essence of a modern feature platform. Offline (Batch Features): Features computed on a slow cadence (hourly/daily). user_lifetime_clicks, item_average_rating. Online (Real-time Features): Features computed on-the-fly at request time, or from a stream processor. user_clicks_in_last_5_minutes, current_popularity_of_item. The challenge is joining these features together at inference time with low latency. This is why a high-performance Feature Store is so critical. It\u0026rsquo;s the system that provides a unified view of both batch and real-time features to the model. Diagram (Feature Store in action): graph TD subgraph \u0026#34;Serving Path\u0026#34; Request --\u0026gt; PredictionService{Prediction Service} PredictionService --\u0026gt; FeatureStore[(Feature Store)] FeatureStore --\u0026gt; Model(ML Model) Model --\u0026gt; PredictionService end subgraph \u0026#34;Batch Pipeline (Offline)\u0026#34; DataLake[Data Lake] --\u0026gt; BatchETL{Spark Job} BatchETL -- \u0026#34;user_age\u0026lt;br\u0026gt;user_country\u0026#34; --\u0026gt; FeatureStore end subgraph \u0026#34;Streaming Pipeline (Nearline)\u0026#34; EventStream[Kafka Stream] --\u0026gt; StreamProcessor{Flink Job} StreamProcessor -- \u0026#34;user_clicks_last_5m\u0026#34; --\u0026gt; FeatureStore end style FeatureStore fill:#cde4ff,stroke-width:2px,stroke:#333 Interview Phrasing: \u0026ldquo;Our model will require both static and dynamic features. Static features, like user profile embeddings, will be pre-computed via a daily batch job and loaded into our online feature store. Dynamic features, like a user\u0026rsquo;s recent activity in this session, will be computed by a streaming pipeline and also pushed to the feature store. At inference time, our prediction service will query the feature store to assemble the final feature vector from both sources with low latency.\u0026rdquo; The Training-Serving Skew Problem (Spotify Case Study) Book\u0026rsquo;s Core Idea (Timeless): The case study describes a scenario where a model performs well offline but fails silently in production. The root cause was a subtle difference in how features were processed during training versus during online serving.\nThe Problem, Deconstructed: Let\u0026rsquo;s imagine a concrete feature: user_podcast_listening_history.\nTraining Pipeline (Offline): A data scientist using a Python script in a Jupyter notebook writes code to process the listening history. Let\u0026rsquo;s say the logic is: \u0026ldquo;Take the last 50 podcasts the user listened to, embed their IDs, and average the embeddings.\u0026rdquo; This pipeline runs on a Spark cluster, reads from the data lake, and produces a training dataset. The model trains on this data and gets great offline metrics. Serving System (Online): A different team of backend engineers is responsible for the production service that recommends podcasts. This service is written in Java for low latency. They are given the same instructions: \u0026ldquo;Take the last 50 podcasts, embed, and average.\u0026rdquo; They re-implement this feature transformation logic in Java. The \u0026ldquo;Simple Mistake\u0026rdquo;: The Python split() function might handle a missing history differently than the Java split() function. Or maybe the Java service, in a rare edge case, only has access to the last 49 podcasts due to a cache refresh issue. Or maybe the Python script defaults to zero-padding while the Java service doesn\u0026rsquo;t. The Result: The live production model is consistently receiving feature vectors that are calculated slightly differently than the vectors it was trained on. The model is making predictions on data that it doesn\u0026rsquo;t quite understand, leading to poor (but not catastrophically bad) recommendations. Because the failure isn\u0026rsquo;t a crash, it goes undetected for months, silently hurting user experience. This is training-serving skew. Diagram of the Failure Mode:\ngraph TD subgraph \u0026#34;Training World (Python/Spark)\u0026#34; A[Data Lake] --\u0026gt; B{Feature Transform v1\u0026lt;br\u0026gt;\u0026lt;span style=\u0026#39;color: blue;\u0026#39;\u0026gt;Python Logic\u0026lt;/span\u0026gt;} B --\u0026gt; C[Training Data] C --\u0026gt; D(Model Training) D --\u0026gt; E[(Saved Model)] end subgraph \u0026#34;Serving World (Java/Real-time)\u0026#34; F[Live User Request] --\u0026gt; G{Feature Transform v2\u0026lt;br\u0026gt;\u0026lt;span style=\u0026#39;color: red;\u0026#39;\u0026gt;Java Logic\u0026lt;/span\u0026gt;} G --\u0026gt; H[Inference Feature Vector] E --\u0026gt; I{Model Inference} H --\u0026gt; I I --\u0026gt; J(Recommendation) end subgraph \u0026#34;The Problem\u0026#34; Skew(\u0026#34;Training-Serving Skew\u0026lt;br\u0026gt;Transform v1 ≠ Transform v2\u0026#34;) end style B fill:#cde4ff style G fill:#f5c6cb Solutions to Training-Serving Skew The book highlights Spotify\u0026rsquo;s solution, which is excellent. Let\u0026rsquo;s break down the modern best practices for solving this, which have become industry standard.\nSolution 1: The Feature Store (The \u0026ldquo;Source of Truth\u0026rdquo; Approach) This is the most robust, architectural solution and the one we discussed previously.\nHow it solves the problem: You create a single, canonical feature engineering pipeline. This pipeline computes features and writes them to both an offline store (for training) and an online store (for serving). The model training job reads directly from the offline store. The model serving service reads directly from the online store. There is no re-implementation of feature logic. The logic is defined once. This completely eliminates the possibility of implementation drift between training and serving. Diagram of the Solution: graph TD subgraph \u0026#34;Canonical Feature Pipeline (Defined ONCE)\u0026#34; A[\u0026#34;Raw Data Source\u0026#34;] --\u0026gt; B{\u0026#34;Feature Transform Logic\u0026lt;br/\u0026gt;(Spark/Flink)\u0026#34;} B -- \u0026#34;Writes to\u0026#34; --\u0026gt; C[(\u0026#34;Online Feature Store\u0026lt;br/\u0026gt;Redis, DynamoDB\u0026#34;)] B -- \u0026#34;Writes to\u0026#34; --\u0026gt; D[(\u0026#34;Offline Feature Store\u0026lt;br/\u0026gt;S3/Iceberg\u0026#34;)] end subgraph \u0026#34;Training World\u0026#34; D -- \u0026#34;Reads clean features\u0026#34; --\u0026gt; Model_Training(\u0026#34;Model Training\u0026#34;) end subgraph \u0026#34;Serving World\u0026#34; C -- \u0026#34;Reads clean features\u0026#34; --\u0026gt; Model_Serving(\u0026#34;Model Serving\u0026#34;) end style B fill:#dff0d8 Solution 2: The Shared Transformation Library Approach This is a lighter-weight solution if a full feature store is overkill.\nHow it solves the problem: You encapsulate all your feature transformation logic into a dedicated library that can be used by both the training and serving environments. Example (TensorFlow Transform - TFX): You define your feature preprocessing steps (normalization, vocabulary creation, etc.) using TFX. TFX analyzes your entire training dataset to compute the necessary statistics (e.g., the mean and variance for normalization). It then exports two things: The transformed data for training. A transform_fn graph that contains all the logic and the computed statistics. Your serving system simply loads this transform_fn graph and applies it to the raw incoming data. There\u0026rsquo;s no re-implementation. The exact same means, standard deviations, and vocabularies are used in both worlds. Solution 3: The Logging and Monitoring Approach This is not a preventative solution, but a detection and alerting solution. You must do this even if you have one of the above.\nHow it works:\nLog Predictions and Features: Your online serving system logs every feature vector it uses to make a prediction, along with the prediction itself, to a central data lake. Statistical Monitoring: You set up a separate pipeline that runs periodically (e.g., hourly) and compares the statistical distribution of the features from the production logs against the distribution of the features from your training data. Tools: Libraries like TensorFlow Data Validation (TFDV) or open-source tools like WhyLogs/whylogs are purpose-built for this. They can automatically generate schemas and detect anomalies. Alerting: If the monitoring pipeline detects a feature drift or concept drift (e.g., the mean of user_age in production suddenly shifts significantly from the training data, or the distribution of predicted scores changes), it triggers an alert for the on-call engineer. The book\u0026rsquo;s mention of the Chebyshev distance metric is a great specific detail to include here.\nInterview Phrasing: \u0026ldquo;Training-serving skew is one of the most insidious failure modes in production ML. My primary strategy to prevent it would be to use a Feature Store, which provides a single source of truth for both training and serving features. This architecturally eliminates the need to re-implement feature logic. As a crucial second layer of defense, I would implement a monitoring and alerting pipeline. The production service would log all its input feature vectors, and a separate job using a tool like TFDV would compare the statistical distributions of these live features against our training set\u0026rsquo;s features. If any significant drift is detected, it would trigger an on-call alert, allowing us to catch issues like the one Spotify faced in hours, not months.\u0026rdquo;\n","permalink":"https://deepskandpal.github.io/bookshelf/ml-design-interview/chapter-1/","tags":null,"title":"Chapter 1: Introduction to ML System Design"},{"categories":null,"content":"1. GenAI Overview: The Big Picture Before we build anything, we need to know what we\u0026rsquo;re working with.\nWhat are we trying to achieve? We\u0026rsquo;re trying to categorize our problem. Are we trying to classify something that already exists, or are we trying to create something new? This is the most fundamental decision you\u0026rsquo;ll make. The book correctly breaks ML models into two camps: Discriminative and Generative.\nLet\u0026rsquo;s break down these two categories of ML models. This is a classic interview question, and you need to nail it.\n1. Discriminative Models: The Art Critic\nWhat they do: They learn the boundary or the difference between different classes of data. The formal definition: They learn the conditional probability P(Y | X). Intuitive Explanation: Think of an art critic. You show them a painting (X), and they tell you the probability that it\u0026rsquo;s a Picasso (Y=Picasso) or a Monet (Y=Monet). The critic doesn\u0026rsquo;t know how to paint a Picasso; they only know how to distinguish a Picasso from other paintings based on features like brush strokes, color palette, and subject matter. Core Task: Classification (Is this email spam or not?) and Regression (What is the price of this house?). Examples: Logistic Regression, SVMs, the \u0026ldquo;classifier\u0026rdquo; part of a GAN. 2. Generative Models: The Artist\nWhat they do: They learn the underlying distribution of the data itself. They learn what makes a Picasso a Picasso. The formal definition: They model the distribution P(X) or the joint distribution P(X, Y). Intuitive Explanation: This is the artist. You can ask them, \u0026ldquo;Create a new painting in the style of Picasso.\u0026rdquo; Because they have learned the essence of what a Picasso painting looks like (P(X) where X is the distribution of Picasso paintings), they can generate a brand new, never-before-seen example. If you ask for a \u0026ldquo;Picasso-style painting of a cat\u0026rdquo; (Y=\u0026quot;cat\u0026quot;), they are modeling the joint distribution P(X, Y). Core Task: Creating new data (text, images, audio, etc.). Examples: VAEs, GANs, Diffusion Models, Autoregressive models (like GPT). The key takeaway is: \u0026ldquo;While discriminative algorithms can predict a target variable from input features, most of them lack the capability to learn the underlying data distribution needed to generate new data instances. For that, we turn to generative models.\u0026rdquo;\nLet\u0026rsquo;s visualize this relationship.\n%%{init: {\u0026#39;flowchart\u0026#39;: {\u0026#39;nodeSpacing\u0026#39;: 50, \u0026#39;rankSpacing\u0026#39;: 80}}}%% graph TD A[🤖 Artificial Intelligence] --\u0026gt; B[🧠 Machine Learning] B --\u0026gt; C{🔀 Model Type} C --\u0026gt; D[⚖️ Discriminative\u0026lt;br/\u0026gt;The Judge] C --\u0026gt; E[🎨 Generative\u0026lt;br/\u0026gt;The Artist] D --\u0026gt; D1[📊 Classification:\u0026lt;br/\u0026gt;Is this a cat?] D --\u0026gt; D2[💡 Recommendation:\u0026lt;br/\u0026gt;Will you like this movie?] E --\u0026gt; E1[🖼️ Image Generation:\u0026lt;br/\u0026gt;Create a picture of a cat] E --\u0026gt; E2[📝 Text Generation:\u0026lt;br/\u0026gt;Write a story about a cat] subgraph Core[\u0026#34;🎯 Core Task\u0026#34;] F[🚪 Learns the boundary\u0026lt;br/\u0026gt;between data] G[📈 Learns the distribution\u0026lt;br/\u0026gt;of data] end D -.-\u0026gt; F E -.-\u0026gt; G style A fill:#dae8fc,stroke:#6c8ebf,stroke-width:3px style B fill:#d5e8d4,stroke:#82b366,stroke-width:3px style C fill:#f9f9f9,stroke:#333,stroke-width:2px style D fill:#f8cecc,stroke:#b85450,stroke-width:2px style E fill:#e1d5e7,stroke:#9673a6,stroke-width:2px style F fill:#fff2cc,stroke:#d6b656,stroke-width:2px style G fill:#fff2cc,stroke:#d6b656,stroke-width:2px Here are popular tasks for each type, which helps clarify the distinction:\n%%{init: {\u0026#39;flowchart\u0026#39;: {\u0026#39;nodeSpacing\u0026#39;: 40, \u0026#39;rankSpacing\u0026#39;: 70}}}%% graph TD A[🤖 ML-powered Tasks] --\u0026gt; B[⚖️ Discriminative] A --\u0026gt; C[🎨 Generative] B --\u0026gt; B1[🖼️ Image Segmentation] B --\u0026gt; B2[👁️ Object Detection] B --\u0026gt; B3[😊 Sentiment Analysis] B --\u0026gt; B4[🏷️ Named Entity Recognition] B --\u0026gt; B5[💡 Recommendation Systems] B --\u0026gt; B6[🔍 Visual Search] C --\u0026gt; C1[💬 Chatbots] C --\u0026gt; C2[📄 Summarization] C --\u0026gt; C3[🖼️📝 Image Captioning] C --\u0026gt; C4[📝➡️🖼️ Text-to-Image] C --\u0026gt; C5[👤 Face Generation] C --\u0026gt; C6[🎵 Audio Synthesis] style A fill:#f0f0f0,stroke:#666,stroke-width:3px style B fill:#f8cecc,stroke:#b85450,stroke-width:2px style C fill:#e1d5e7,stroke:#9673a6,stroke-width:2px 2. Why is GenAI So Powerful? The Three Pillars The book nails the three key drivers. In an interview, you can frame this as the \u0026ldquo;perfect storm\u0026rdquo; that made GenAI possible.\nWhat are we trying to achieve? We\u0026rsquo;re explaining the fundamental enablers. This shows you understand the context of the current AI boom. Data (The Library of Alexandria): Traditional ML needed meticulously labeled data (e.g., \u0026ldquo;this image is a cat,\u0026rdquo; \u0026ldquo;this one is a dog\u0026rdquo;). This is slow and expensive. The breakthrough for GenAI was self-supervised learning. We can now feed models the entire internet—unlabeled text from Wikipedia, books, code from GitHub. The model creates its own learning tasks (e.g., \u0026ldquo;predict the next word\u0026rdquo;). The sheer volume and diversity of this data allow models to learn nuanced patterns about language, reasoning, and the world.\nModel Capacity (The Brain Size):\nParameters: Think of these as the knobs or synapses in the model\u0026rsquo;s \u0026ldquo;brain.\u0026rdquo; The more parameters (e.g., GPT-3 with 175B, PaLM with 540B), the more information and complex patterns it can learn and store. FLOPs (Floating Point Operations): This isn\u0026rsquo;t about size, but about computational cost. How much \u0026ldquo;thinking\u0026rdquo; does it take to get an answer? A model can have fewer parameters but a more complex architecture (like dense connections) that requires more FLOPs. Understanding the difference between model size (parameters) and computational complexity (FLOPs) is a sign of a senior engineer. Compute (The Engine): You can have a giant brain and a massive library, but if you can only read one word per second, it\u0026rsquo;s useless. Specialized hardware like GPUs (NVIDIA\u0026rsquo;s A100, H100) and TPUs (Google\u0026rsquo;s custom chips) are the powerful engines that can perform the trillions of calculations needed to train these massive models in a feasible amount of time (weeks instead of centuries).\n3. Scaling Laws: The Recipe for Success This is a critical, FAANG-level concept. In an interview, discussing scaling laws shows you\u0026rsquo;re thinking about the science and economics of training, not just throwing compute at a problem.\nWhat are we trying to achieve? We want to train the best possible model without wasting money. Given a fixed budget for computation (the total FLOPs we can afford), what\u0026rsquo;s the best way to spend it? Should we make the model bigger (more parameters, N) or train it on more data (more tokens, D)? Before scaling laws, it was a bit of a guessing game: \u0026ldquo;For my next model, should I double the data or double the model size?\u0026rdquo;\nThe OpenAI (2020) Finding: They found that performance scales predictably as a power-law with model size, dataset size, and compute. Crucially, \u0026ldquo;the impact of scaling on model performance is significantly more pronounced than the influence of architectural variations.\u0026rdquo; This means that for a while, making models bigger was more important than making them architecturally cleverer.\nThe DeepMind (Chinchilla, 2022) Refinement: DeepMind found that many existing LLMs were undertrained. We were building huge models but not feeding them enough data. They proposed that for optimal performance, model size and training dataset size should be scaled linearly together. This led to models like \u0026ldquo;Chinchilla,\u0026rdquo; which was smaller than Gopher but trained on much more data, and outperformed it.\nWhy this matters for an interview: It shows you understand the economics. If a VP gives you a $50 million budget to train a new model, you can use scaling laws to propose a plan: \u0026ldquo;Based on Chinchilla\u0026rsquo;s scaling laws, to get the optimal performance for this budget, we should aim for a model with X parameters and train it on Y trillion tokens.\u0026rdquo; This is a data-driven engineering decision.\nScaling laws gave us a predictable recipe. They demonstrated that for a given compute budget, there\u0026rsquo;s an optimal ratio between model size (number of parameters) and the amount of training data (number of tokens). This turned model training from a \u0026ldquo;dark art\u0026rdquo; into a more predictable engineering discipline.\n4. The Framework for ML System Design Interviews This is the absolute core of the chapter and your roadmap for any design interview. This is what the interviewer is evaluating you on.\nWhat are we trying to achieve? We\u0026rsquo;re trying to demonstrate a structured, logical, and comprehensive approach to solving a complex, open-ended problem. We need to show we think about the entire system, not just the model. Let\u0026rsquo;s recreate their flowchart. This is your mental checklist.\n%%{init: {\u0026#39;flowchart\u0026#39;: {\u0026#39;nodeSpacing\u0026#39;: 60, \u0026#39;rankSpacing\u0026#39;: 90}}}%% graph TD A[📋 1. Clarify Requirements] --\u0026gt; B[🎯 2. Frame as ML Task] B --\u0026gt; C[📊 3. Data Preparation] C --\u0026gt; D[🛠️ 4. Model Development] D --\u0026gt; E[📈 5. Evaluation] E --\u0026gt; F[🏗️ 6. Overall System Design] F --\u0026gt; G[🚀 7. Deployment \u0026amp; Monitoring] style A fill:#e6f3ff,stroke:#0066cc,stroke-width:3px style B fill:#fff2e6,stroke:#cc6600,stroke-width:3px style C fill:#e6ffe6,stroke:#00cc00,stroke-width:3px style D fill:#ffe6f3,stroke:#cc0066,stroke-width:3px style E fill:#f3e6ff,stroke:#6600cc,stroke-width:3px style F fill:#ffe6e6,stroke:#cc0000,stroke-width:3px style G fill:#f0f0f0,stroke:#666666,stroke-width:3px Let\u0026rsquo;s walk through the first few steps from this part of the book.\nStep 1: Clarifying Requirements Before you write a single line of code or draw a single box, you must understand the problem. This is where you distinguish yourself as a senior engineer. You\u0026rsquo;re not just a code monkey; you\u0026rsquo;re a problem solver.\nFunctional Requirements: What should the system do?\nExample: \u0026ldquo;Generate an image and customize its style based on the user\u0026rsquo;s prompt.\u0026rdquo; My advice: Be specific. For a customer service chatbot, is it just for answering FAQs, or should it be able to process returns? Does it need to remember conversation history? Non-Functional Requirements: How should the system perform? This is where the real engineering challenges lie. Ask about:\nBusiness Objective: Why are we building this? To reduce customer support costs? To increase user engagement? This North Star dictates your trade-offs. System Features: Does the user rate the outputs? Can they edit the generated image? These are features that create feedback loops. Data: What data can we use? Is it sensitive (PII, medical)? Is it labeled? Constraints: Will it run on-device (on a phone) or in the cloud? This has massive implications for model size and latency. Scale: Are we serving 100 users or 100 million users? This impacts everything from database choice to inference architecture. Performance: What is the acceptable latency? Does a user expect an image in 2 seconds or 20 seconds? Is quality or speed more important? In an interview, spend a solid 5 minutes here. Ask these questions. The interviewer often has a more detailed scenario in mind, and you need to extract it from them.\nStep 2: Framing the Problem as an ML Task Now you translate the requirements into a machine learning problem.\nSpecify Input and Output: This seems simple, but it\u0026rsquo;s crucial. Example for Text-to-Image: Input = text prompt (string), maybe some style parameters. Output = Image (e.g., a 1024x1024 pixel grid). Example for Chatbot: Input = user\u0026rsquo;s text query. Output = system\u0026rsquo;s text response. Example for Video generation: Input = text prompt, maybe a starting image. Output = sequence of frames (video). Choose a Suitable ML Approach: This is where you make your first major architectural decision. Here\u0026rsquo;s the logic, simplified:\nDiscriminative vs. Generative? Look at the output. Are you predicting a label from a fixed set (discriminative) or creating new content (generative)? For GenAI problems, the answer is almost always generative.\nIdentify the Task Type: What kind of content are you generating? Text? Image? Audio? Video? This narrows down your model choices significantly.\nChoose a Suitable Algorithm: Now you get into the specific families of models. For image generation, the main contenders are GANs, VAEs, Autoregressive models, and Diffusion models. You should be able to briefly discuss the trade-offs:\nGANs: Fast to generate, but can be unstable to train and may have \u0026ldquo;mode collapse\u0026rdquo; (lack diversity). VAEs: Stable to train, produce diverse outputs, but can sometimes be blurrier than GANs. Diffusion Models: Produce state-of-the-art, high-quality images, but are computationally expensive and slow to sample from (though this is improving). Autoregressive Models: Generate pixel by pixel. Can be very high quality but are extremely slow. In an interview, you\u0026rsquo;d say: \u0026ldquo;This is a text-to-image generation task. The key requirements are high-quality output and the ability to follow complex prompts. Given this, a Diffusion model is a strong candidate. While they are historically slow for inference, recent advancements like Latent Diffusion and improved sampling schedules have made them practical. I\u0026rsquo;d choose this over a GAN because training stability and output quality are paramount for this product.\u0026rdquo;\nStep 3: Data Preparation Great models are built on great data. Garbage in, garbage out. The book makes a key distinction between data prep for traditional ML vs. GenAI.\nTraditional ML (Structured Data): The focus is on Feature Engineering. You have tables of data (e.g., customer purchase history) and you spend your time creating clever features (e.g., \u0026ldquo;days since last purchase,\u0026rdquo; \u0026ldquo;average transaction value\u0026rdquo;). %%{init: {\u0026#39;flowchart\u0026#39;: {\u0026#39;nodeSpacing\u0026#39;: 50, \u0026#39;rankSpacing\u0026#39;: 60}}}%% graph LR A[📊 Data Sources] --\u0026gt; B(🔧 Data Engineering ETL) B --\u0026gt; C(⚙️ Feature Engineering) C --\u0026gt; D[✨ Prepared Features] style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px style B fill:#fff2e6,stroke:#cc6600,stroke-width:2px style C fill:#e6ffe6,stroke:#00cc00,stroke-width:2px style D fill:#f3e6ff,stroke:#6600cc,stroke-width:2px GenAI (Unstructured Data): The focus shifts. Since we\u0026rsquo;re using massive internet-scale datasets, feature engineering is less of a thing. The model learns the features itself. The challenges are different: %%{init: {\u0026#39;flowchart\u0026#39;: {\u0026#39;nodeSpacing\u0026#39;: 50, \u0026#39;rankSpacing\u0026#39;: 60}}}%% graph TD A[🌐 Various Data Sources\u0026lt;br/\u0026gt;Internet, Books, Code] --\u0026gt; B(🕷️ Data Collection\u0026lt;br/\u0026gt;Web Scraping) B --\u0026gt; C[📄 Collected Raw Data] C --\u0026gt; D(🧽 Data Cleaning) D --\u0026gt; E[✨ Clean Data] style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px style B fill:#fff2e6,stroke:#cc6600,stroke-width:2px style C fill:#f0f0f0,stroke:#666,stroke-width:2px style D fill:#e6ffe6,stroke:#00cc00,stroke-width:2px style E fill:#f3e6ff,stroke:#6600cc,stroke-width:2px Key Steps in GenAI Data Prep:\nData Collection: How do you get 15 trillion tokens of text? You build scrapers for web pages (like the Common Crawl dataset), digitize books, and pull from sources like GitHub.\nData Cleaning: This is arguably the most important, underrated step. The internet is a messy place. You need to:\nRemove Harmful/Toxic/NSFW content. Deduplicate: You don\u0026rsquo;t want the model to see the exact same text 10,000 times. It can bias the model and is an inefficient use of compute. Filter Low-Quality Content: Remove boilerplate text, machine-translated garbage, etc. You might even use another model to assign a quality score. Balance Data: Ensure you have a diverse mix of data types (prose, dialogue, code, different languages) to create a well-rounded model. Data Efficiency (Storage and Retrieval): When you have 50 Terabytes of data, you can\u0026rsquo;t just load it into memory. You need:\nEfficient Storage: Use distributed file systems (HDFS, S3) and efficient formats (Parquet, ORC). Efficient Retrieval: Use techniques like sharding (splitting data across machines) and indexing to quickly access the data needed for training batches. A Hot Topic: Synthetic Data\nPros: Can improve diversity and scale up your dataset, especially for niche topics where real data is scarce. Cons: Quality depends on the original model. You risk creating a model that just copies the biases and errors of its predecessor, or misses the complexity of the real world. This is a very active area of research. Step 4: Model Development This is the largest and most technical step. It covers three phases: Architecture, Training, and Sampling.\nA. Model Architecture\nHere you zoom in on your chosen model family. You need to talk about the specific components.\nExample: U-Net for Diffusion Models If you chose a Diffusion model for image generation, the backbone is often a U-Net. You should be able to describe it. %%{init: {\u0026#39;flowchart\u0026#39;: {\u0026#39;nodeSpacing\u0026#39;: 40, \u0026#39;rankSpacing\u0026#39;: 60}}}%% graph TD Input[🖼️ Image] --\u0026gt; D1(📉 Downsampling Block 1) D1 --\u0026gt; D2(📉 Downsampling Block 2) D2 --\u0026gt; D3(📉 Downsampling Block 3) D3 --\u0026gt; U1(📈 Upsampling Block 1) U1 --\u0026gt; U2(📈 Upsampling Block 2) U2 --\u0026gt; U3(📈 Upsampling Block 3) U3 --\u0026gt; Output[🔮 Predicted Noise] D3 -.-\u0026gt;|Skip Connection| U1 D2 -.-\u0026gt;|Skip Connection| U2 D1 -.-\u0026gt;|Skip Connection| U3 style Input fill:#e6f3ff,stroke:#0066cc,stroke-width:2px style Output fill:#f3e6ff,stroke:#6600cc,stroke-width:2px style D1 fill:#fff2e6,stroke:#cc6600,stroke-width:2px style D2 fill:#fff2e6,stroke:#cc6600,stroke-width:2px style D3 fill:#fff2e6,stroke:#cc6600,stroke-width:2px style U1 fill:#e6ffe6,stroke:#00cc00,stroke-width:2px style U2 fill:#e6ffe6,stroke:#00cc00,stroke-width:2px style U3 fill:#e6ffe6,stroke:#00cc00,stroke-width:2px Your explanation: \u0026ldquo;The U-Net architecture consists of an encoder (downsampling path) that captures contextual information, and a decoder (upsampling path) that reconstructs the image. The critical feature is the skip connections, which connect layers from the encoder directly to corresponding layers in the decoder. This allows the model to reuse low-level feature information (like fine textures and edges) during reconstruction, which is essential for generating sharp, detailed images.\u0026rdquo;\nDeep Dive: Transformer\u0026rsquo;s Self-Attention (The heart of LLMs) This is the most important architectural concept in modern GenAI. You MUST understand it intuitively.\nThe Goal: For any given word in a sentence, we want to understand how it relates to all other words in that sentence to get its true contextual meaning. The word \u0026ldquo;bank\u0026rdquo; means something different in \u0026ldquo;river bank\u0026rdquo; vs. \u0026ldquo;money bank\u0026rdquo;.\nThe Q, K, V Analogy:\nQuery (Q): From the perspective of the current word, this is a question: \u0026ldquo;What am I, and what context do I need?\u0026rdquo; Key (K): From the perspective of every other word, this is a label: \u0026ldquo;Here\u0026rsquo;s the kind of information I have.\u0026rdquo; Value (V): From the perspective of every other word, this is the actual content: \u0026ldquo;Here is my information.\u0026rdquo; The Mechanism (Scaled Dot-Product Attention):\nYou take the Query of your current word and compute the dot-product with the Key of every other word. This gives you a compatibility score. You scale these scores (divide by the square root of the dimension) to keep gradients stable during training. You run these scores through a Softmax function. This turns the scores into weights that sum to 1. It\u0026rsquo;s like distributing 100% of your \u0026ldquo;attention\u0026rdquo; across all other words. You multiply these attention weights by the Value of each word and sum them up. The result is a new representation for your current word, blended with contextual information from all other words it paid attention to. Here\u0026rsquo;s the Scaled Dot-Product Attention:\n%%{init: {\u0026#39;flowchart\u0026#39;: {\u0026#39;nodeSpacing\u0026#39;: 40, \u0026#39;rankSpacing\u0026#39;: 50}}}%% graph TD Q[🤔 Query] --\u0026gt; MatMul1[× Matrix Multiply] K[🔑 Key] --\u0026gt; MatMul1 MatMul1 --\u0026gt; Scale[📏 Scale] Scale --\u0026gt; Mask[🎭 Mask Optional] Mask --\u0026gt; SoftMax[🧮 SoftMax] SoftMax --\u0026gt; MatMul2[× Matrix Multiply] V[💎 Value] --\u0026gt; MatMul2 MatMul2 --\u0026gt; Output[✨ Attention Output] style Q fill:#e6f3ff,stroke:#0066cc,stroke-width:2px style K fill:#fff2e6,stroke:#cc6600,stroke-width:2px style V fill:#e6ffe6,stroke:#00cc00,stroke-width:2px style Output fill:#f3e6ff,stroke:#6600cc,stroke-width:2px Multi-Head Attention: The Problem: Just one set of Q, K, V matrices might only learn one type of relationship (e.g., just grammatical relationships). The Solution: Do the whole attention process multiple times in parallel, each with different, learned Q, K, V weight matrices. Each \u0026ldquo;head\u0026rdquo; can specialize in learning a different type of relationship (e.g., one head for semantic meaning, one for syntax, one for long-distance dependencies). You then concatenate the outputs from all heads and pass them through a final linear layer to combine the knowledge. B. Model Training\nOnce you have the architecture, you need to train it.\nTraining Methodology:\nThis is model-specific. For Diffusion, it\u0026rsquo;s a process of adding noise and then training the U-Net to predict and remove that noise. For GANs, it\u0026rsquo;s an adversarial process between a generator and a discriminator. For LLMs, it\u0026rsquo;s often a multi-stage process: Pre-training: On massive, general data (the internet). The goal is to learn language, facts, and reasoning. The objective is often next-token prediction. Supervised Fine-Tuning (SFT): On a smaller, high-quality dataset of instruction-response pairs to teach the model how to follow commands. Alignment (e.g., RLHF): Using reinforcement learning from human feedback to make the model more helpful, harmless, and honest. ML Objective and Loss Function:\nThe objective is what you want the model to do (e.g., \u0026ldquo;predict the next token\u0026rdquo;). The loss function is the mathematical formula that measures how far the model\u0026rsquo;s prediction is from the truth. The entire training process is about minimizing this loss. This covers the essence of the first half of the chapter. We\u0026rsquo;ve established the landscape and have a clear, structured plan to tackle the design.\n5. Deep Dive into Model Training: The Heavy Lifting The book talks about techniques for training large-scale models. These are your bread and butter as a senior engineer at FAANG.\nWhat are we trying to achieve? We\u0026rsquo;re trying to train a model that is too big to fit in one GPU\u0026rsquo;s memory and on a dataset that is too large to process quickly on one machine. We need to be efficient with memory, time, and money. Here are the key optimization techniques:\nGradient Checkpointing:\nIntuition: During training, you need to store intermediate values (activations) to calculate the gradients for backpropagation. This consumes a ton of memory. Gradient checkpointing is a clever trade-off: it throws away most of these intermediate values to save memory, and then re-computes them on the fly during the backward pass. Trade-off: You use less memory, but it increases your training time (more compute). It\u0026rsquo;s perfect for when you want to train a massive model on GPUs that don\u0026rsquo;t have enough VRAM. Mixed Precision Training:\nIntuition: By default, calculations are done in 32-bit floating point (FP32). But do we need that much precision for everything? Mixed precision uses faster, less memory-intensive 16-bit floats (FP16) for most of the calculations, while keeping critical parts (like weight updates) in FP32 to maintain stability. Benefit: On modern GPUs with Tensor Cores, this can speed up training by 2-3x and cut memory usage in half. Frameworks like PyTorch (with torch.amp) make this almost automatic. Distributed Training (The Teamwork): This is how you train on hundreds or thousands of GPUs at once. The book correctly identifies the main types of parallelism.\n%%{init: {\u0026#39;flowchart\u0026#39;: {\u0026#39;nodeSpacing\u0026#39;: 50, \u0026#39;rankSpacing\u0026#39;: 80}}}%% graph LR P[Parallelism] --\u0026gt; DP[Data Parallelism] P --\u0026gt; MP[Model Parallelism] DP --\u0026gt; DP_Desc[Split the DATA\u0026lt;br/\u0026gt;Each GPU gets a full\u0026lt;br/\u0026gt;copy of the MODEL] MP --\u0026gt; PP[Pipeline Parallelism\u0026lt;br/\u0026gt;Inter-layer] MP --\u0026gt; TP[Tensor Parallelism\u0026lt;br/\u0026gt;Intra-layer] PP --\u0026gt; PP_Desc[Split the MODEL LAYERS\u0026lt;br/\u0026gt;GPU_0: Layers 1-8\u0026lt;br/\u0026gt;GPU_1: Layers 9-16] TP --\u0026gt; TP_Desc[Split SINGLE LAYER ops\u0026lt;br/\u0026gt;GPU_0: half matrix\u0026lt;br/\u0026gt;GPU_1: other half] style P fill:#f9f9f9,stroke:#333,stroke-width:3px style DP fill:#e8f4fd,stroke:#1f77b4,stroke-width:2px style MP fill:#fff2e8,stroke:#ff7f0e,stroke-width:2px style DP_Desc fill:#d5e8d4,stroke:#82b366,stroke-width:2px style PP_Desc fill:#e1d5e7,stroke:#9673a6,stroke-width:2px style TP_Desc fill:#e1d5e7,stroke:#9673a6,stroke-width:2px Data Parallelism: The simplest and most common. You have a giant dataset. You give a small chunk (a mini-batch) to each GPU. Each GPU calculates the gradients for its batch, and then they all sync up their results via a Parameter Server or an all-reduce operation. Model Parallelism: You use this when the model itself is too big for a single GPU. Pipeline Parallelism: You put different layers of the model on different GPUs. GPU 0 computes layers 1-8 and passes its output to GPU 1, which computes layers 9-16, etc. It\u0026rsquo;s like an assembly line. The main challenge is keeping all GPUs busy (the \u0026ldquo;pipeline bubble\u0026rdquo;). Tensor Parallelism: You use this when a single layer is too big. You split the actual matrix multiplications of that layer across multiple GPUs. This is more complex but essential for the massive layers in models like GPT. A state-of-the-art training setup (like what\u0026rsquo;s used for Llama 3) uses a hybrid approach, combining all of these techniques (Data, Pipeline, and Tensor parallelism) to work efficiently at massive scale. This is what frameworks like FSDP (Fully Sharded Data Parallel) help manage.\nC. Model Sampling (Inference)\nAfter the model is trained, how do you generate output? This is sampling.\nGreedy Search: At each step, pick the single most likely next word/pixel. Fast, but boring, repetitive, and deterministic. Beam Search: Keep track of the k most likely sequences at each step. Better than greedy, but can still suppress creativity. Stochastic Sampling (Top-k, Top-p): This is what\u0026rsquo;s used in modern chatbots. Top-k Sampling: Consider only the k most likely next words, and then sample from that smaller set. Top-p (Nucleus) Sampling: Consider the smallest set of words whose cumulative probability is greater than p. This is adaptive; if the model is very certain, the set is small. If it\u0026rsquo;s uncertain, the set is larger. This often gives the best balance of coherence and creativity. Step 5: Evaluation How do you know if your model is any good?\nOffline Evaluation (Using a test dataset):\nDiscriminative Metrics: Easy. Accuracy, Precision, Recall, F1 score. You have a ground truth. Generative Metrics (Harder!): There\u0026rsquo;s no single \u0026ldquo;right\u0026rdquo; answer. Text: BLEU, ROUGE (compare generated text to references), Perplexity (how surprised is the model by the text). Images: FID, KID (measure how similar the distribution of generated images is to real images), CLIPScore (measures how well an image matches a text prompt). The Interview Point: You need to show you understand that for generative models, no single metric is enough. You need a suite of metrics to measure different aspects: quality, diversity, alignment to the prompt, etc. And ultimately\u0026hellip; Online Evaluation (In production, with real users):\nThis is about business impact. Metrics: Click-Through Rate (CTR), Conversion Rate, User Retention, Latency, User Satisfaction surveys. You\u0026rsquo;ll often use A/B testing to compare a new model against the old one on these business metrics. Human Evaluation: For creative tasks, you can\u0026rsquo;t escape it. You need human raters to score outputs on dimensions like \u0026ldquo;creativity,\u0026rdquo; \u0026ldquo;coherence,\u0026rdquo; \u0026ldquo;factuality,\u0026rdquo; and \u0026ldquo;harmlessness.\u0026rdquo; This is a core part of the RLHF process.\nThe Golden Rule: Online and offline metrics don\u0026rsquo;t always align. A model might get a great ROUGE score but generate text that users find unhelpful. The ultimate test is how it performs with real users and impacts your business goals.\nStep 6: Overall ML System Design Now, zoom out from just the model and draw the whole system. This is where you integrate everything. You need to think about more than just the model.predict() call.\nSystem Components: A request comes in. What happens? Input Preprocessing: The raw prompt is cleaned. Safety \u0026amp; Moderation (Pre-filter): Check the input prompt for harmful content. Core Model Inference: Call your generative model. Post-processing: Convert model output to a user-friendly format. Safety \u0026amp; Moderation (Post-filter): Check the generated output for harmful content, bias, or PII leakage. User Feedback Loop: Log the output, the prompt, and any user feedback (thumbs up/down) for future retraining. Scalability: How do you serve 100 million users? You\u0026rsquo;ll have a load balancer distributing requests to a fleet of inference servers. You might use model parallelism for inference if the model is huge. Security \u0026amp; Bias: How do you prevent misuse? How do you detect and filter biased outputs? This is a huge area and showing awareness is critical. Here is a simple, intuitive diagram of what a production GenAI system looks like.\n%%{init: {\u0026#39;flowchart\u0026#39;: {\u0026#39;nodeSpacing\u0026#39;: 60, \u0026#39;rankSpacing\u0026#39;: 100, \u0026#39;curve\u0026#39;: \u0026#39;basis\u0026#39;}}}%% graph TD User[👤 User] --\u0026gt;|1. Request| API[🌐 API Gateway /\u0026lt;br/\u0026gt;Load Balancer] API --\u0026gt;|2. Forward| Pre[🛡️ Preprocessing \u0026amp;\u0026lt;br/\u0026gt;Safety] Pre --\u0026gt;|3. Cleaned Prompt| Inference[🧠 Model Inference\u0026lt;br/\u0026gt;Cluster GPUs] Inference --\u0026gt;|4. Raw Output| Post[🛡️ Postprocessing \u0026amp;\u0026lt;br/\u0026gt;Safety] Post --\u0026gt;|5. Final Response| API API --\u0026gt;|6. Generated Image| User Pre -.-\u0026gt;|Check violations| P_Filter[🚫 Policy Filter] Post -.-\u0026gt;|Scan output| O_Filter[🚫 Output Filter] User -.-\u0026gt;|👍👎 Feedback| Feedback[🔄 Feedback Loop\u0026lt;br/\u0026gt;RLHF] Feedback -.-\u0026gt; Retraining[🔧 Model Retraining\u0026lt;br/\u0026gt;Pipeline] Retraining -.-\u0026gt; Inference style User fill:#e6f3ff,stroke:#0066cc,stroke-width:3px style API fill:#fff2e6,stroke:#cc6600,stroke-width:2px style Inference fill:#d5e8d4,stroke:#82b366,stroke-width:3px style Pre fill:#f8cecc,stroke:#b85450,stroke-width:2px style Post fill:#f8cecc,stroke:#b85450,stroke-width:2px style P_Filter fill:#fff2cc,stroke:#d6b656,stroke-width:2px style O_Filter fill:#fff2cc,stroke:#d6b656,stroke-width:2px style Feedback fill:#e1d5e7,stroke:#9673a6,stroke-width:2px style Retraining fill:#e1d5e7,stroke:#9673a6,stroke-width:2px Key talking points for this diagram:\nSystem Components: It\u0026rsquo;s not just the model. There are safety filters before and after the model, load balancers, caching layers, and monitoring services. Safety Mechanisms: You absolutely must talk about this. How do you prevent users from generating harmful content? You need input filters (prompt filtering) and output filters (content moderation classifiers). User Feedback \u0026amp; Continuous Learning (RLHF): The thumbs up/down buttons aren\u0026rsquo;t just for show. That data is collected and used to continuously fine-tune the model to better align with user preferences. This is a critical feedback loop. Scalability: How do you serve millions of users? You use load balancers to distribute traffic to a cluster of GPU machines for inference. You use techniques like model parallelism within that cluster if the model is huge. Monitoring: The final step. You log everything. Latency, error rates, GPU utilization, metric scores from your safety classifiers. If something breaks, you need to know immediately. Step 7: Deployment and Monitoring Deployment: How do you get the model into production? You might have a CI/CD pipeline for models. Monitoring: What are you tracking? System Metrics: Latency, error rates, GPU utilization. Model Metrics: Monitor the distribution of inputs and outputs. Is there a \u0026ldquo;drift\u0026rdquo; over time? Is the model\u0026rsquo;s performance degrading? This is crucial for knowing when you need to retrain. Summary This framework is your bible for a GenAI system design interview. Start with the user and the business problem, frame it as an ML task, go deep on the data and model, evaluate your results, design the end-to-end production system, and think about what happens after it\u0026rsquo;s live.\nBy walking through this chapter, you\u0026rsquo;ve built a powerful mental model for GenAI system design:\nFrame the Problem: Start broad (AI vs. ML), then narrow down (Discriminative vs. Generative). Understand the \u0026ldquo;why\u0026rdquo; (Data, Model, Compute). Follow the Roadmap: Use the 7-step framework as your guide. It prevents you from getting lost and ensures you cover all your bases. Think Like an Engineer, Not Just a Scientist: Don\u0026rsquo;t just talk about the model. Talk about the data pipelines, the training optimizations (parallelism!), the evaluation metrics (offline AND online), and the full production system with its safety and feedback loops. Know Your Architectures: Be able to explain U-Net for diffusion models and self-attention for transformers at an intuitive level. Understand the Economics: Scaling laws help you make data-driven decisions about compute budgets. Safety and Ethics First: Always discuss content moderation, bias detection, and user feedback loops. If you can walk an interviewer through these steps, providing the \u0026ldquo;why\u0026rdquo; behind your choices and discussing the trade-offs at each stage, you\u0026rsquo;re not just answering the question—you\u0026rsquo;re demonstrating the strategic thinking of a senior staff engineer at a top company.\nThis approach will make you appear structured, thorough, and deeply knowledgeable—exactly what a FAANG interviewer is looking for.\n","permalink":"https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/chapter-1/","tags":null,"title":"Chapter 1: Introduction and Overview"},{"categories":null,"content":"Chapter 1: Machine Learning Basics Even for seasoned practitioners, a quick refresher on the fundamentals, especially as they pertain to language models, is key.\nAI and Machine Learning: We start with a brief history – from the early days of AI with concepts like the Perceptron and ELIZA, through AI winters, to the rise of modern ML with deep learning. Model: At its heart, a model y = f(x). We explore the simple linear model wx + b, understand parameters (weights and bias), and the crucial concept of a loss function (like Mean Squared Error for regression) to quantify error. Four-Step ML Process: This is the core loop: Collect a dataset. Define the model\u0026rsquo;s structure. Define the loss function. Minimize the loss (often using derivatives). Vector and Matrix: We then move to representing data and parameters using vectors (feature vectors, dot products, norms, cosine similarity) and matrices (matrix multiplication, transpose). This is vital for understanding how neural networks process data efficiently. Neural Network: We introduce non-linearity with activation functions (ReLU, sigmoid, tanh). We look at feedforward neural networks (FNNs), multilayer perceptrons (MLPs), and how layers combine hierarchically. Gradient Descent: Since analytical solutions for minimizing loss in complex NNs are often infeasible, we rely on gradient descent. I walk through an example of binary classification using logistic regression, introducing binary cross-entropy loss. Automatic Differentiation (Autograd): Manually deriving gradients is impractical. Modern frameworks like PyTorch automate this with autograd. We see a practical PyTorch example, understanding the forward and backward passes, and the role of tensors. Chapter 2: Language Modeling Basics This is where we start tailoring our ML knowledge to text.\nBag of Words (BoW): One of the simplest ways to convert text to numbers for tasks like classification. We discuss corpus, vocabulary, tokenization (words and subwords), and vectorization (document-term matrix). For multi-class classification, we introduce the softmax activation and cross-entropy loss. A PyTorch example shows how to build a simple text classifier. Word Embeddings: BoW has limitations (sparsity, no semantic understanding). Word embeddings (like those from word2vec\u0026rsquo;s skip-gram model) represent words as dense vectors where similar words have similar vectors. This allows for capturing semantic similarity and dimensionality reduction. Byte-Pair Encoding (BPE): A common subword tokenization algorithm. It helps manage vocabulary size and handle out-of-vocabulary words by breaking words into smaller, frequently occurring units. Language Model Definition: Formally, a language model predicts the next token in a sequence given previous tokens, P(next_token | context). We focus on autoregressive (or causal) language models. Count-Based Language Models: Before neural nets, n-gram models were standard. We look at how they estimate probabilities (e.g., trigram probability P(w3 | w1, w2) based on counts). We discuss challenges like zero probabilities for unseen n-grams and solutions like backoff and Laplace (add-one) smoothing. Evaluating Language Models: Perplexity: A core metric. Lower is better, indicating the model is less \u0026ldquo;surprised\u0026rdquo; by the test data. It\u0026rsquo;s derived from the average negative log-likelihood. ROUGE: For evaluating models on tasks like summarization, comparing model output to reference (ground truth) texts. We touch on ROUGE-N and ROUGE-L. Human Evaluation: Essential for qualities automated metrics miss. We discuss Likert scales and pairwise comparisons with Elo ratings. Chapter 3: Recurrent Neural Network (RNN) RNNs were a breakthrough for sequential data like text.\nElman RNN: The basic RNN structure, where the output at a given step depends on the current input and the hidden state from the previous step. This \u0026ldquo;memory\u0026rdquo; allows RNNs to capture sequential dependencies. Mini-Batch Gradient Descent: A practical necessity for training large models, processing data in small batches. Programming an RNN: We\u0026rsquo;d build an Elman RNN from scratch in PyTorch, understanding how hidden states are passed and updated. RNN as a Language Model: How to use the RNN architecture to predict the next token. Embedding Layer: PyTorch\u0026rsquo;s nn.Embedding layer, which is essentially a lookup table for token embeddings, often trained as part of the model. Dataset and DataLoader: PyTorch utilities for efficiently loading and batching data for training. Chapter 4: Transformer This is the architecture that powers modern LLMs.\nDecoder-Only Architecture: We focus on this variant, common for autoregressive LMs. Key Innovations: Self-Attention: Allows the model to weigh the importance of different tokens in the input sequence when processing a particular token. We\u0026rsquo;d cover query, key, and value matrices. Positional Encoding: Since transformers process tokens in parallel (unlike RNNs), they need a way to incorporate word order. Rotary Position Embedding (RoPE) is a key technique here. Decoder Block Components: Masked Multi-Head Self-Attention (with RoPE) Position-Wise Multilayer Perceptron (MLP) Residual Connections (Skip Connections): Crucial for training deep networks by mitigating the vanishing gradient problem. Layer Normalization (specifically, RMSNorm): Stabilizes training. Key-Value Caching: An optimization for faster inference by caching past key and value states. Python Implementation: We build a decoder-only Transformer in PyTorch, piece by piece. Chapter 5: Large Language Model (LLM) Here, we scale up and look at practical applications.\nWhy Larger is Better: The impact of scale – more parameters, larger context windows, vast training data, and massive compute – leads to emergent capabilities. Supervised Finetuning (SFT): Pretrained LLMs predict the next token. SFT trains them to follow instructions, answer questions, or engage in dialogue using high-quality instruction-response pairs. We\u0026rsquo;d compare a base pretrained model with its instruction-tuned version. Finetuning a Pretrained Model: Practical steps using a model like GPT-2. We\u0026rsquo;d cover formatting data for tasks like emotion generation (text-to-label) or instruction following (using formats like ChatML). Sampling Strategies: Beyond greedy decoding. Temperature: Controls randomness. Top-k sampling: Limits selection to k most probable tokens. Top-p (Nucleus) sampling: Selects from the smallest set of tokens whose cumulative probability exceeds p. Penalties: Frequency and presence penalties to discourage repetition. Low-Rank Adaptation (LoRA): A parameter-efficient finetuning (PEFT) technique. Instead of finetuning all model weights, LoRA adds and trains small \u0026ldquo;adapter\u0026rdquo; matrices, significantly reducing computational cost. LLM as a Classifier: Attaching a classification head to an LLM. Prompt Engineering: Crafting effective prompts (situation, role, task, format, constraints, examples, call to action). Discussing few-shot prompting and followup strategies. Hallucinations: Why they happen (models optimize for coherence, not truth) and how to mitigate them (e.g., Retrieval-Augmented Generation - RAG, domain-specific pretraining). Copyright and Ethics: Critical considerations around training data and generated content. Chapter 6: Further Reading Pointers to more advanced topics for continued learning.\nMixture of Experts (MoE): Increases model capacity efficiently by routing tokens to specialized sub-networks. Model Merging: Combining multiple pretrained models. Model Compression: Techniques like quantization (e.g., QLoRA) and pruning. Preference-Based Alignment: Methods like RLHF (Reinforcement Learning from Human Feedback) and Constitutional AI to align LLMs with user values. Advanced Reasoning: Chain-of-Thought (CoT), Tree of Thought (ToT), ReAct. Security: Jailbreak attacks, prompt injection. Vision Language Models (VLMs): Models that understand both text and images. Preventing Overfitting: Regularization, dropout, early stopping, validation sets. ","permalink":"https://deepskandpal.github.io/bookshelf/100-page-lm-book/chapter-1/","tags":null,"title":"Chapter 1: Machine Learning Basics"},{"categories":null,"content":"Chapter 1: Introduction - Reliable ML Engineering Notes Core Concept: The ML Loop ML is Iterative, Not Linear: ML applications are rarely \u0026ldquo;done.\u0026rdquo; They exist in a continuous cycle of development, deployment, evaluation, and improvement. Why a Loop? If a model underperforms: Teams (DS, Business, MLE) collaborate to improve it (features, data, architecture). If a model performs well: Organizations want more sophistication and broader application, leading to further development. The first model is just a starting point. The ML Lifecycle Stages (The \u0026ldquo;Pit Stops\u0026rdquo; in the Loop) (See Figure 1-1 for visual representation) 1. Data Collection and Analysis Goal: Understand available data, identify needs, prioritize uses, collect, and process. Involves: Business/Product (for priorities), Data Engineers (pipelines), SREs (reliability of pipelines), DS/MLEs (data utility). Key: Proper data management is foundational. Business context drives data needs. 2. ML Training Pipelines Goal: Consume processed data and produce trained models using ML algorithms. Involves: Data Engineers, DS, MLEs, SREs. Tech: TensorFlow, PyTorch, XGBoost, etc. Critical: Training pipelines are production systems. Treat with rigor. Common Failures: Data issues (lack, format), bugs, misconfigurations, resource shortages, hardware/distributed system failures. Silent Failures: ML models can also fail due to subtle issues like data distribution shifts. 3. Build and Validate Applications Goal: Integrate the model into a customer-facing system to deliver value. Involves: Product/Business (specs), MLEs/SDEs (implementation), QA (oversight). Action: Log user interactions and model outputs for future improvement. 4. Quality and Performance Evaluation Goal: Assess if the model \u0026ldquo;works\u0026rdquo; and how well, before and during initial launch. Methods: Offline Evaluation: Test on historical/curated data. Live Launch: Model sees live traffic (monitor closely). Dark Launch: Model sees live traffic, logs predictions, but doesn\u0026rsquo;t affect user experience (tests integration). Fractional Launch (Canary/A/B): Model exposed to a subset of users (tests quality and integration). Purpose: Gain confidence for wider rollout, establish baselines. It\u0026rsquo;s a validation checkpoint. 5. Defining and Measuring SLOs (Service-Level Objectives) Goal: Define and track thresholds (SLIs - Indicators) for system performance according to requirements. Involves: SREs, PMs, DS, MLEs, SDEs. Challenge in ML: Subtle data/world changes can degrade ML performance significantly. Types of SLOs: System: Latency, error rates, throughput (serving \u0026amp; training). Application: # of recommendations, successful model calls. ML Performance/Business: Click-through rates, revenue from model (often sliced by user segments). Key: Business must define tolerable SLOs. 6. Launch Goal: Ship the ML-enhanced application to users. Involves: Product SDEs, MLEs, SREs. ML-Specific Concerns: Models as Code: New models can break systems like bad code. Launch Slowly: Progressive rollouts to limit damage. Isolate Rollouts at Data Layer: Critical to avoid data format incompatibilities during rollbacks (see \u0026ldquo;Progressive Rollouts in a Stateful System\u0026rdquo; story). Release, Not Refactor: Minimize changes during a release. Measure SLOs During Launch: Monitor dashboards. Review the Rollout: Manual or automated oversight. 7. Monitoring and Feedback Loops Goal: Continuously observe system health and effectiveness post-launch, and gather data for future improvements. Signals to Monitor: System Health (Golden Signals): Latency, traffic, errors, saturation. Basic Model Health: Model size, load errors (context-free checks). Model Quality (Domain-Specific): Business metrics (CTR, conversion), performance drift over time. Hardest but most crucial. Feedback Collection: Log user interactions, predictions, and context to fuel the next cycle. Purpose: Ensure sustained performance, detect issues, provide input for the next iteration. Key Differences: Evaluation vs. Monitoring Quality \u0026amp; Performance Evaluation: A checkpoint before/during initial launch to validate and decide on wider rollout. Monitoring \u0026amp; Feedback Loops: Continuous vigilance post-launch to maintain health, detect drift, and gather data for future iterations. Overall Lessons from the Loop Data is King: ML begins and ends with data. Cyclical Process: No single order; stages are revisited. Holistic View Required: Understand the entire loop and organization. Risk \u0026amp; Experimentation: Not all ML ideas work. Approach as continual experimentation. Organizational Readiness Matters. ","permalink":"https://deepskandpal.github.io/bookshelf/reliable-ml/chapter-1/","tags":null,"title":"Chapter 1: Introduction"},{"categories":null,"content":"Prompt\nThe chapter opens with a great example: Google Translate\u0026rsquo;s multilingual neural machine translation system back in 2016. This was a landmark – deep learning making a massive, tangible impact at scale. It showed the world the power of modern ML.\nSince then, as the book says, ML has \u0026ldquo;found its way into almost every aspect of our lives.\u0026rdquo; This is true. At FAANG, we see this daily – from the recommendations you get, to how your photos are enhanced, to how we optimize our data centers.\nBut here\u0026rsquo;s the first crucial takeaway for any ML design interview, and indeed, for your day-to-day work:\n\u0026ldquo;Many people, when they hear \u0026lsquo;machine learning system,\u0026rsquo; think of just the ML algorithms\u0026hellip; However, the algorithm is only a small part of an ML system in production.\u0026rdquo;\nThis is HUGE. I can\u0026rsquo;t stress this enough. In interviews, if you only talk about model architectures and training loops, you\u0026rsquo;re missing 90% of the picture. The system includes:\nBusiness Requirements: Why are we even building this? What problem does it solve? User Interface: How do users interact with it? How do developers interact with it? Data Stack: Ingestion, storage, processing, versioning – the lifeblood. Development, Monitoring, Updating Logic: This is MLOps territory. Infrastructure: The compute, storage, and networking that makes it all run. Figure 1-1 (which we\u0026rsquo;ll see on the next page) visually breaks this down and shows how the book will cover these components. We\u0026rsquo;ll keep coming back to this idea of the holistic system.\nSelf-Correction/Interview Tip: When an interviewer asks you to design an ML system, your first thoughts should be about the business problem, the users, and the data, not immediately about whether to use a Transformer or a ResNet.\nPage 2: MLOps vs. ML Systems Design \u0026amp; The Book\u0026rsquo;s Philosophy This page introduces two key terms:\nMLOps: This comes from DevOps. It\u0026rsquo;s about operationalizing ML – deploying, monitoring, maintaining. It\u0026rsquo;s a set of tools and best practices. Think CI/CD for ML, model registries, monitoring dashboards. ML Systems Design: This is the approach the book (and this cohort) takes. It\u0026rsquo;s a holistic view of MLOps, considering all components and stakeholders to meet objectives. It\u0026rsquo;s the \u0026ldquo;architecture\u0026rdquo; level thinking. Figure 1-1 is really important here. You see ML system users and business requirements feeding in (Chapters 1 \u0026amp; 2). You see ML system developers and the entire book dedicated to them. The \u0026ldquo;ML system\u0026rdquo; box itself has:\nData (Chapters 3 \u0026amp; 4) Feature Engineering (Chapter 5) ML Algorithms (Chapter 6 – notice it\u0026rsquo;s just one chapter!) Evaluation (also Chapter 6) Deployment, monitoring, updating of logics (Chapters 7, 8 \u0026amp; 9) Infrastructure (Chapter 10) And Chapter 11 covers the overarching aspects like stakeholders, ethics, etc. (though ethics is woven throughout).\nThe book explicitly states it won\u0026rsquo;t cover specific algorithms in detail. Why?\nBecause algorithms change, they get outdated. The framework for building robust, scalable, maintainable ML systems – that\u0026rsquo;s timeless.\nThis is exactly what FAANG companies look for: people who can think in terms of systems and principles, not just the latest hot model.\nFAANG Perspective: At places like Google, Meta, Amazon, we have dedicated MLOps platform teams building tools, but product-focused ML engineers are the ones using these tools within a systems design framework to solve specific business problems. They need to understand the whole lifecycle.\nPages 3-8: When to Use Machine Learning (The Litmus Test) This is arguably the most critical section for any ML project\u0026rsquo;s inception and a very common starting point for ML design interviews. Before you design anything, you must ask: Is ML necessary or cost-effective?\nThe book offers a fantastic five-part definition: \u0026ldquo;Machine learning is an approach to:\n(1) Learn: The system must have the capacity to learn. A relational database isn\u0026rsquo;t an ML system because you explicitly define relationships. ML systems infer them, usually from data. For supervised learning (e.g., predicting Airbnb prices), you need input-output pairs.\nInterview Relevance: \u0026ldquo;How does your system learn from new data?\u0026rdquo; (2) Complex patterns: ML shines when patterns are too complex to hand-code. Predicting if a fair die roll is a 6? No pattern. Predicting stock prices? Complex patterns (hopefully!). Sorting listings by state based on zip code? Lookup table. Predicting Airbnb rental price from many features? ML! This is Andrej Karpathy\u0026rsquo;s \u0026ldquo;Software 2.0\u0026rdquo; idea – you provide data, the system writes the \u0026ldquo;code\u0026rdquo; (the learned patterns).\nFAANG Nuance: We often deal with problems where the rules are unknown or too numerous to list, like ranking search results or newsfeed items. This is prime ML territory. (3) Existing data: ML needs data to learn from. No data, no ML (usually). The book mentions predicting taxes without tax data – impossible.\nZero-shot learning is an interesting case: it makes predictions for tasks it wasn\u0026rsquo;t directly trained on, but it was trained on related tasks/data. So, it still needs some data. Fake-it-til-you-make-it: Launch with humans making predictions, collect that data, then train an ML model. This is a common bootstrapping strategy for new products. Interview Relevance: \u0026ldquo;What data would you use? How would you collect it? What if you don\u0026rsquo;t have labeled data initially?\u0026rdquo; (4) Predictions: ML solves problems requiring predictive answers. \u0026ldquo;What will the weather be?\u0026rdquo; \u0026ldquo;What movie will a user watch next?\u0026rdquo; Even compute-intensive problems can be reframed: \u0026ldquo;What would the outcome of this complex rendering process look like?\u0026rdquo; (approximate it with ML).\n(5) Unseen data: The patterns learned must generalize to new, unseen data. If your model trained on 2008 app download data (Koi Pond!) tries to predict 2020 downloads, it\u0026rsquo;ll fail. Technically, training and unseen data should come from similar distributions. If not, your model will perform poorly (hello, monitoring and retraining, Chapters 8 \u0026amp; 9!).\nInterview Relevance: This leads directly to discussions of data drift, concept drift, and the importance of robust evaluation and monitoring. The book then lists additional characteristics where ML solutions shine:\nIt\u0026rsquo;s repetitive: ML algorithms (especially deep learning) often need many examples. Repetitive tasks provide these. The cost of wrong predictions is cheap (usually): A bad movie recommendation? User ignores it. A self-driving car makes a wrong turn? Catastrophic. This influences your choice of problem and acceptable error rates. However, even for high-stakes, if ML on average outperforms humans (like statistically safer self-driving cars), it can be viable. It\u0026rsquo;s at scale: ML often needs significant upfront investment (data, compute, talent). This is justified if you\u0026rsquo;re making many predictions (e.g., sorting millions of emails, routing thousands of support tickets). \u0026ldquo;At scale\u0026rdquo; also implies lots of data for training. The patterns are constantly changing: Spam emails evolve. Fashion trends change. ML models can be updated with new data to adapt, unlike hardcoded rules. This is where \u0026ldquo;Continual Learning\u0026rdquo; (page 264, Chapter 9) comes in. When NOT to use ML:\nIt\u0026rsquo;s unethical: Automated grader biases (page 341) – we\u0026rsquo;ll cover ethics in detail. Simpler solutions do the trick: (Chapter 6) ALWAYS start with a non-ML baseline. If a heuristic works, great! FAANG Mantra: \u0026ldquo;Start simple.\u0026rdquo; A common failure mode for junior engineers is over-engineering. It\u0026rsquo;s not cost-effective. A crucial caution: Don\u0026rsquo;t dismiss new tech just because it\u0026rsquo;s not cost-effective now. Early adoption can be a competitive advantage later. And sometimes, you can break a big problem into smaller pieces, using ML for just one part.\nInterview Gold: The \u0026ldquo;When to use ML\u0026rdquo; checklist is your first filter for any ML system design question. Always discuss these trade-offs. If you propose an ML solution, be ready to defend why ML is appropriate over a simpler, non-ML approach.\nPages 9-11: Machine Learning Use Cases This section is a tour of where ML is making an impact. It\u0026rsquo;s good for broadening your understanding of the applications.\nConsumer Applications:\nSearch engines (Google) Recommender systems (Netflix, Amazon) Predictive typing (your phone\u0026rsquo;s keyboard) Photo enhancement Biometric authentication (fingerprint, face ID) Machine translation (the author\u0026rsquo;s personal anecdote about parents using Google Translate is lovely) Smart assistants (Alexa, Google Assistant) Smart security cameras (pet detection, uninvited guests) At-home health monitoring (fall detection) Enterprise Applications: The book rightly states these are the majority of ML use cases.\nOften have stricter accuracy requirements but can be more forgiving on latency compared to consumer apps (e.g., 0.1% improvement in resource allocation for Google can save millions, even if the system takes a few seconds to run). Figure 1-3 (Algorithmia 2020 survey): Shows diverse enterprise uses: Internal: Reducing costs (38%), generating customer insights (37%), internal processing automation (30%). External: Improving customer experience (34%), retaining customers (29%), interacting with customers (28%). Specific examples: Fraud detection: Anomaly detection on transactions. (Very common, one of the oldest ML uses in enterprise). Price optimization: Dynamic pricing for ads, flights, ride-sharing. Maximize margin/revenue. Demand forecasting: For inventory, resource allocation. Customer acquisition: Identifying potential customers, targeted ads, optimizing discounts. Churn prediction: Predicting when customers (or employees!) might leave. Self-Correction/Interview Tip: Having a few diverse use cases in your back pocket is useful. If an interviewer asks, \u0026ldquo;Give me an example of an ML system you find interesting,\u0026rdquo; you can draw from these. It also helps you think about the types of problems ML can solve (classification, regression, anomaly detection, etc.).\nPage 12: More Enterprise Use Cases \u0026amp; Intro to Understanding ML Systems More enterprise examples:\nAutomated support ticket classification: Route tickets to the right department faster. Brand monitoring: Sentiment analysis on brand mentions. Healthcare: Skin cancer detection, diabetes diagnosis (often through a provider, due to accuracy/privacy needs). This page then transitions to the next major section: Understanding Machine Learning Systems. The key motivation here is that ML systems are different from:\nML in research (or academia). Traditional software. And these differences necessitate the kind of thinking this book promotes.\nPages 12-21: Machine Learning in Research Versus in Production This is a CRITICAL section, especially for those coming from academia or who have mostly worked on Kaggle-like projects. ML in the wild is a different beast. Table 1-1 summarizes key differences: Requirements Research: State-of-the-art model performance on benchmarks Production: Different stakeholders, often conflicting requirements Computational Priority Research: Fast training, high throughput Production: Fast inference, low latency Data Research: Static, benchmark datasets Production: Constantly shifting, messy, real-world data Fairness Research: Often not a focus Production: Must be considered Interpretability Research: Often not a focus Production: Must be considered (often a requirement) Let\u0026rsquo;s break these down:\nDifferent stakeholders and requirements (Page 13-14): Research: Usually a single objective – e.g., SOTA on a benchmark. Researchers might use complex techniques for marginal gains. Production: Multiple stakeholders with conflicting needs. The restaurant recommender example is classic: ML Engineers: Want best model for user clicks (maybe complex, needs more data). Sales Team: Want model recommending expensive restaurants (more service fees). Product Team: Want low latency (\u0026lt;100ms), as latency drops orders. ML Platform Team: Worried about scaling, want to pause updates to improve platform. Manager: Wants to maximize margin (maybe cut the ML team if costs are too high!). The book mentions decoupling objectives (page 41) – we\u0026rsquo;ll get there. For now, understand that production ML is about trade-offs. Is a 100ms latency a must-have or a nice-to-have? This determines if Model A or B is even viable.\nEnsembling: Popular in competitions (Netflix Prize), but often too complex, slow, or hard to interpret for production. A small performance lift might not justify the operational overhead.\nCriticism of ML Leaderboards (Page 15): Hard steps (data collection, problem formulation) are often done for you. Multiple hypothesis testing: With many teams, some might get good results by chance. Misaligned incentives: Drive for accuracy at the expense of compactness, fairness, energy efficiency (Ethayarajh \u0026amp; Jurafsky). FAANG Perspective: This stakeholder wrangling is daily life. Product Managers, Engineering Managers, ML Engineers, and sometimes legal/policy teams all have a say. Clear communication and defining priorities are key.\nComputational priorities (Page 15-18): Research: Focus on fast training and high throughput (samples/sec during training). Production: Focus on fast inference and low latency (time from query to result). Terminology Clash (Page 16): The book uses \u0026ldquo;latency\u0026rdquo; to mean \u0026ldquo;response time\u0026rdquo; (what the client sees, including network/queueing delays). This is common ML community usage.\nLatency vs. Throughput (Figure 1-4, Page 17):\nSingle query processing: Higher latency = lower throughput. Batched query processing: Can increase throughput, but also individual query latency (waiting for a batch to fill). This is a common trade-off. Impact of Latency: Real-world examples (Akamai: 100ms delay = 7% conversion drop; Booking.com: 30% latency increase = 0.5% conversion cost; Google: \u0026gt;3s load time, users leave). Users are impatient!\nLatency is a distribution (Page 18): Don\u0026rsquo;t just use average! Averages hide outliers. Use percentiles:\np50 (median): 50% of requests are faster/slower. p90, p95, p99: Show tail latencies. These outliers might affect your most valuable customers (e.g., Amazon customers with large purchase histories). Product requirements are often \u0026ldquo;p99 latency \u0026lt; X ms.\u0026rdquo; Interview Relevance: Be ready to discuss latency/throughput trade-offs, batching strategies, and how you\u0026rsquo;d measure and set SLOs for latency (e.g., \u0026ldquo;p95 latency for recommendations must be under 200ms\u0026rdquo;).\nData (Page 18-19): Research: Often clean, well-formatted, static benchmark datasets. Known quirks, public scripts for processing. Production: Data is MESSY! Noisy, unstructured, constantly shifting. Biased (and you might not know how). Labels can be sparse, imbalanced, incorrect. Changing requirements mean updating labels. Privacy and regulatory concerns (GDPR, CCPA). Data is constantly generated by users, systems, third parties. Figure 1-5 (Karpathy\u0026rsquo;s \u0026ldquo;Amount of sleep lost over\u0026hellip;\u0026rdquo; graphic): In PhD, sleep lost over models/algorithms. At Tesla (production), sleep lost over datasets. This is profoundly true.\nFAANG Perspective: Data engineering, data quality, data governance, and data privacy are massive efforts. Often, more engineering time is spent on the data pipeline than on the model itself.\nFairness (Page 19-20): Research: Often an afterthought. Production: CRITICAL. ML models encode past biases from data. Deployed at scale, they can discriminate at scale. Examples: Loan applications biased by zip code, resume ranking biased by name spelling, mortgage rates based on biased credit scores. Cathy O\u0026rsquo;Neil\u0026rsquo;s \u0026ldquo;Weapons of Math Destruction\u0026rdquo; is a must-read. Misclassifying minority groups might have a small impact on overall accuracy metrics but a huge impact on those individuals/groups. McKinsey study (2019): Only 13% of large companies mitigating algorithmic bias. This is changing, but slowly. We\u0026rsquo;ll cover Responsible AI in Chapter 11. Interview Relevance: Expect questions on fairness. \u0026ldquo;How would you detect bias in your model? How would you mitigate it?\u0026rdquo; This is table stakes now.\nInterpretability (Page 20-21): Geoffrey Hinton\u0026rsquo;s AI surgeon dilemma: 90% cure rate AI surgeon (black box) vs. 80% human surgeon (explainable). Who do you choose? (When the author asked execs, it was 50/50). Research: Often not incentivized if focus is pure performance. Production: Often a requirement. For users/business leaders: To trust the model, detect biases. For developers: To debug and improve the model. Legal: \u0026ldquo;Right to explanation\u0026rdquo; in some jurisdictions (e.g., GDPR). 2019 Stanford HAI report: Only 19% of large companies working on explainability. Again, this is improving. FAANG Nuance: For critical systems (e.g., fraud, content moderation, medical), interpretability is non-negotiable. Techniques like SHAP, LIME are used, but simpler, inherently interpretable models are often preferred if performance is comparable.\nDiscussion (Page 21): Why production focus matters? Most companies can\u0026rsquo;t afford pure research without business application. The \u0026ldquo;bigger, better\u0026rdquo; ML models (e.g., large language models) require massive data and compute, often tens of millions of dollars. As ML becomes more accessible, demand for productionizing ML grows. The vast majority of ML-related jobs are in productionizing ML. This is why you\u0026rsquo;re here! Pages 22-23: Machine Learning Systems Versus Traditional Software If ML is part of software engineering (SWE), why not just use existing SWE best practices? Good idea! ML production would be better if ML experts were also strong software engineers. Many SWE tools are useful.\nHowever, ML has unique challenges:\nCode + Data + Artifacts: SWE: Assumes code and data are separate. Focus on modularity. ML: Systems are tightly coupled: code (training scripts, inference logic), data (features, labels), and artifacts (trained models). Trend: \u0026ldquo;Best data wins\u0026rdquo; over \u0026ldquo;best algorithm.\u0026rdquo; So, focus shifts to improving data. Data changes quickly =\u0026gt; ML apps need to adapt quickly =\u0026gt; faster dev/deploy cycles. Testing and Versioning: SWE: Test and version code. ML: Must test and version code and data. Versioning large datasets is hard. How to know if a data sample is \u0026ldquo;good\u0026rdquo; or \u0026ldquo;bad\u0026rdquo;? Not all data samples are equal: A scan of a cancerous lung is more valuable if you have 1M normal lung scans and only 1k cancerous ones. Indiscriminate data acceptance can hurt performance or lead to data poisoning attacks (footnote 31). Model Size: As of 2022, models with billions of parameters are common (e.g., LLMs). Require GBs of RAM. This might seem quaint in the future (like the 32MB RAM of the Apollo moon computer). Deploying large models, especially on edge devices (Chapter 7), is a massive engineering challenge. Speed (Inference Latency): How to run these large models fast enough? An autocompletion model slower than typing is useless. Monitoring and Debugging: Non-trivial for complex, black-box models. Hard to know what went wrong or get alerted quickly. The good news (Page 23): These challenges are being tackled. BERT (2018) was initially seen as too big/slow (340M params, 1.35GB). By 2020, it was in \u0026ldquo;almost every English search on Google.\u0026rdquo; Progress is rapid.\nInterview Relevance: Understanding these unique challenges of ML (data-centricity, versioning data+model, model size/latency, monitoring complexity) distinguishes a candidate who has thought about production issues from one who has only trained models in a lab.\nPage 23: Summary This opening chapter gives you the lay of the land:\nML is widely used, especially in enterprise. Knowing when and when not to use ML is crucial. ML in production is very different from ML in research (stakeholders, compute, data, fairness, interpretability). ML systems have unique challenges compared to traditional software (data entanglement, versioning, model size, monitoring). The key theme of the book, and this cohort, is a holistic system approach. We don\u0026rsquo;t just look at algorithms; we look at all components working together.\n","permalink":"https://deepskandpal.github.io/bookshelf/design-ml-system/chapter-1/","tags":null,"title":"Chapter 1: Overview of Machine Learning Systems"},{"categories":null,"content":"Notes for Chapter 1 When you hear \u0026ldquo;Machine Learning,\u0026rdquo; what pops into your head? Robots? Terminators? Maybe a friendly butler? The book nails it – it\u0026rsquo;s not just sci-fi; it\u0026rsquo;s already here. Think about the spam filter. That was one of the first really big ML applications that touched millions. It learned, from examples of spam and non-spam (or \u0026ldquo;ham,\u0026rdquo; as we call it), to tell the difference. And it got so good that we barely notice it anymore. That\u0026rsquo;s the hallmark of good ML – it just works.\nThis chapter aims to clarify what ML is, why it\u0026rsquo;s useful, and give you a \u0026ldquo;map\u0026rdquo; of the ML continent: supervised vs. unsupervised, online vs. batch, instance-based vs. model-based. We\u0026rsquo;ll also touch on the typical project workflow and some common challenges.\n(Page 2: What Is Machine Learning?) Alright, so what is it? The book gives a great, simple definition:\n\u0026ldquo;Machine Learning is the science (and art) of programming computers so they can learn from data.\u0026rdquo;\nThe key here is \u0026ldquo;learn from data.\u0026rdquo; Instead of you, the programmer, writing explicit rules for every single scenario, you show the computer a bunch of examples, and it figures out the patterns itself.\nArthur Samuel, a pioneer back in 1959, said it\u0026rsquo;s the \u0026ldquo;field of study that gives computers the ability to learn without being explicitly programmed.\u0026rdquo; Think about that – without explicit programming. That\u0026rsquo;s the magic.\nThen there\u0026rsquo;s Tom Mitchell\u0026rsquo;s more engineering-focused definition from 1997, which is super useful for grounding this:\n\u0026ldquo;A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\u0026rdquo;\nLet\u0026rsquo;s break that down with our spam filter:\nTask T: Flagging spam emails. Experience E: The training data – thousands of example emails, each labeled as \u0026ldquo;spam\u0026rdquo; or \u0026ldquo;ham.\u0026rdquo; Performance Measure P: How well does it do the task? Maybe it\u0026rsquo;s accuracy – the percentage of emails it correctly classifies. So, if our spam filter gets better at correctly identifying spam (higher accuracy P) after being shown more examples of emails (more experience E), then it\u0026rsquo;s learning!\nAnd the book rightly points out: downloading all of Wikipedia doesn\u0026rsquo;t make your computer \u0026ldquo;learn\u0026rdquo; in the ML sense. It has more data, sure, but it\u0026rsquo;s not suddenly better at, say, translating languages or identifying cats in pictures, unless you use that data to train it for a specific task.\n(Page 3-4: Why Use Machine Learning?) So, why bother? Why not just write the rules, like we\u0026rsquo;ve always done in traditional programming? The book uses the spam filter example (Figure 1-1 vs 1-2), and it\u0026rsquo;s perfect.\nProblems too complex for traditional rules: Imagine trying to write rules for spam. \u0026ldquo;If email contains \u0026lsquo;4U\u0026rsquo;, \u0026lsquo;credit card\u0026rsquo;, \u0026lsquo;free\u0026rsquo;, \u0026lsquo;amazing\u0026rsquo;\u0026hellip;\u0026rdquo; Okay, a start. But spammers get smart. They start writing \u0026ldquo;For U\u0026rdquo; or using images. Your list of rules would become a monster – thousands, maybe millions of lines long, and a nightmare to maintain (Figure 1-1). An ML spam filter, on the other hand, learns which words and phrases are good predictors by looking at frequencies in spam vs. ham (Figure 1-2). It\u0026rsquo;s often shorter, easier to maintain, and more accurate. Adapting to changing environments: When spammers change tactics (\u0026ldquo;For U\u0026rdquo; instead of \u0026ldquo;4U\u0026rdquo;), a traditional filter needs you to manually update the rules. An ML system, especially an online learning one (we\u0026rsquo;ll get to that), can see these new patterns emerging in user-flagged spam and automatically adapt (Figure 1-3). It keeps learning! No known algorithm: Think about speech recognition. How would you even begin to write rules to distinguish \u0026ldquo;one\u0026rdquo; from \u0026ldquo;two\u0026rdquo; for every voice, accent, in noisy environments, across dozens of languages? It\u0026rsquo;s incredibly hard. But give an ML algorithm enough recordings of people saying \u0026ldquo;one\u0026rdquo; and \u0026ldquo;two,\u0026rdquo; and it can learn to distinguish them. Helping humans learn (Data Mining): This is a fascinating one (Figure 1-4). Sometimes, we train an ML model, and then we can peek inside (though it\u0026rsquo;s tricky for some complex models) to see what it learned. A spam filter might reveal surprising combinations of words that are highly predictive of spam. This can give us new insights into complex problems by finding patterns we wouldn\u0026rsquo;t have spotted. (Page 5-6: Examples of Applications) The book lists a ton, and this really shows the breadth of ML:\nImage Classification (CNNs): Identifying products on a production line, detecting tumors in brain scans (this is more semantic segmentation – classifying every pixel). Natural Language Processing (NLP): Classifying news articles, flagging offensive comments, summarizing documents, chatbots (NLU, question-answering). These often use RNNs, CNNs, or more recently, Transformers. Regression (predicting values): Forecasting company revenue. This can use Linear Regression, SVMs, Random Forests, Neural Networks. Speech Recognition: Making your app react to voice commands. Anomaly Detection: Detecting credit card fraud. Clustering (Unsupervised): Segmenting customers based on purchases for targeted marketing. Data Visualization/Dimensionality Reduction: Taking high-dimensional data and making it understandable in 2D or 3D. Recommender Systems: Suggesting products you might like. Reinforcement Learning (RL): Building intelligent bots for games, like AlphaGo that beat the world Go champion. This isn\u0026rsquo;t exhaustive, but it gives you a taste of the sheer power and versatility.\n(Page 7-9: Types of Machine Learning Systems - The Big Picture) Okay, now for the \u0026ldquo;map of the ML continent.\u0026rdquo; We can categorize ML systems based on a few key criteria. These aren\u0026rsquo;t mutually exclusive; a system can be a mix.\n1. Based on Human Supervision during Training: (Page 8) Supervised Learning: This is probably the most common. The \u0026ldquo;supervision\u0026rdquo; comes from the fact that your training data includes the desired solutions, called labels (Figure 1-5). You show the system an email AND tell it \u0026ldquo;this is spam.\u0026rdquo; You show it a picture of a cat AND tell it \u0026ldquo;this is a cat.\u0026rdquo;\nClassification: The task is to predict a category. Spam or ham? Cat or dog? Figure 1-5 (spam classification) is a classic example. Regression: The task is to predict a numerical value. What\u0026rsquo;s the price of this car given its mileage, age, brand (these are features or predictors)? (Figure 1-6). The \u0026ldquo;label\u0026rdquo; here is the actual price. A quick note on terminology (page 8): An attribute is a data type (e.g., \u0026ldquo;mileage\u0026rdquo;). A feature is often an attribute plus its value (e.g., \u0026ldquo;mileage = 15,000\u0026rdquo;). People often use them interchangeably, but it\u0026rsquo;s good to know the nuance. The book lists some key supervised algorithms we\u0026rsquo;ll cover: k-Nearest Neighbors, Linear Regression, Logistic Regression (often used for classification despite \u0026ldquo;regression\u0026rdquo; in its name!), SVMs, Decision Trees, Random Forests, and Neural Networks. (Page 9-12) Unsupervised Learning: Here, the training data is unlabeled (Figure 1-7). There\u0026rsquo;s no \u0026ldquo;teacher\u0026rdquo; providing answers. The system tries to find patterns and structure in the data on its own.\n(Page 10) Clustering: Trying to find natural groupings in the data. For example, grouping your blog visitors into different segments based on their behavior (Figure 1-8). You don\u0026rsquo;t tell it the groups beforehand; it discovers them. Algorithms include K-Means, DBSCAN, Hierarchical Cluster Analysis (HCA). (Page 11) Visualization \u0026amp; Dimensionality Reduction: Taking complex, high-dimensional data and creating a 2D or 3D plot (Figure 1-9, t-SNE example). This helps us understand the data. Dimensionality reduction aims to simplify the data by merging correlated features (e.g., car mileage and age into \u0026ldquo;wear and tear\u0026rdquo;) or dropping less important ones, without losing too much information. This is called feature extraction. It can make subsequent learning faster and sometimes better. PCA, Kernel PCA, LLE are examples. (Page 12) Anomaly Detection / Novelty Detection: Spotting unusual instances. Anomaly detection is about finding things that look different from most of the data (e.g., fraud detection, Figure 1-10). Novelty detection is similar but assumes your training data is \u0026ldquo;clean\u0026rdquo; and you want to find things different from anything seen in training. (Page 13) Association Rule Learning: Discovering relationships between attributes in large datasets. E.g., people who buy barbecue sauce and potato chips also tend to buy steak. (Page 13) Semisupervised Learning: This is a middle ground. You have a lot of unlabeled data and a little bit of labeled data (Figure 1-11). The system uses both. Think of Google Photos: it clusters faces (unsupervised), then you label a few faces (\u0026ldquo;That\u0026rsquo;s Aunt May\u0026rdquo;), and it can then label Aunt May in many other photos (supervised part). Deep Belief Networks (DBNs) using Restricted Boltzmann Machines (RBMs) are an example.\n(Page 14) Reinforcement Learning (RL): This is a different beast altogether! The learning system, called an agent, observes an environment, selects and performs actions, and gets rewards (or penalties) in return (Figure 1-12). It learns the best strategy, called a policy, to maximize its cumulative reward over time. Think training a robot to walk, or AlphaGo learning to play Go. It learns by trial and error, essentially.\n2. Based on Incremental Learning (On-the-fly): (Page 15) Batch Learning (Offline Learning): The system is trained using all available data at once. It takes time and resources. Once trained, it\u0026rsquo;s launched and doesn\u0026rsquo;t learn anymore; it just applies what it learned. If you want it to learn about new data (e.g., new types of spam), you have to retrain it from scratch on the full dataset (old + new). This can be automated (as in Figure 1-3), but it\u0026rsquo;s still a full retrain. This is fine for many things, but not if you need to adapt rapidly or have massive datasets.\n(Page 15-16) Online Learning (Incremental Learning): The system is trained incrementally by feeding it data instances sequentially, either individually or in small groups called mini-batches (Figure 1-13). Each learning step is fast and cheap. This is great for:\nSystems needing rapid adaptation (e.g., stock price prediction). Systems with limited computing resources (once it learns from a data instance, it might not need to store it anymore). Handling huge datasets that can\u0026rsquo;t fit in memory (called out-of-core learning, Figure 1-14). It loads a chunk, trains, loads the next chunk, trains, etc. The book notes, \u0026ldquo;Think of it as incremental learning,\u0026rdquo; which is a good way to avoid confusion, as out-of-core is often done offline. A key parameter here is the learning rate: how quickly it adapts. Too high, and it forgets old patterns quickly. Too low, and it learns slowly and might be less sensitive to noise or outliers. 3. Based on How They Generalize: This is about how systems make predictions on new, unseen data.\n(Page 17-18) Instance-Based Learning: The system learns the training examples by heart. Then, when it sees a new instance, it compares it to the stored examples using a similarity measure and makes a prediction based on the most similar known instances. For example, in Figure 1-15, the new instance (cross) is classified as a triangle because most of its nearest neighbors are triangles. k-Nearest Neighbors is a classic example.\n(Page 18-22) Model-Based Learning: The system builds a model from the training examples and then uses that model to make predictions. This is like a scientist observing data and building a theory. The book uses a great example: predicting life satisfaction based on GDP per capita (Table 1-1, Figure 1-17).\nYou select a type of model – say, a linear model (Equation 1-1: life_satisfaction = θ₀ + θ₁ × GDP_per_capita). This is model selection. This model has parameters (θ₀ and θ₁ – theta-zero and theta-one). By tweaking these, you get different lines (Figure 1-18). How do you find the best parameters? You need a performance measure. For linear regression, it\u0026rsquo;s often a cost function that measures how far the model\u0026rsquo;s predictions are from the training examples. The goal is to minimize this cost. The learning algorithm (e.g., Linear Regression algorithm) takes your training data and finds the parameter values (θ₀, θ₁) that make the model best fit the data. This is called training the model (Figure 1-19). Once trained (e.g., θ₀ = 4.85, θ₁ = 4.91 × 10⁻⁵), you can use the model to make predictions on new data (e.g., Cyprus\u0026rsquo;s life satisfaction, page 21). The code snippet on page 21-22 shows how you\u0026rsquo;d do this with Scikit-Learn. And then it shows how simple it is to swap in an instance-based algorithm like k-Nearest Neighbors! (Page 23-29: Main Challenges of Machine Learning) So, you select an algorithm and train it. What can go wrong? The book says: \u0026ldquo;bad algorithm\u0026rdquo; and \u0026ldquo;bad data.\u0026rdquo;\nLet\u0026rsquo;s start with \u0026ldquo;Bad Data\u0026rdquo;:\n(Page 23) Insufficient Quantity of Training Data: Most ML algorithms need a lot of data to work well. Thousands for simple problems, millions for complex ones like image recognition. The \u0026ldquo;Unreasonable Effectiveness of Data\u0026rdquo; paper (page 24, Figure 1-20) showed that even simple algorithms can perform incredibly well if given enough data. Data often trumps a fancy algorithm, but getting more data isn\u0026rsquo;t always cheap or easy. (Page 25) Nonrepresentative Training Data: Your training data must be representative of the new cases you want to generalize to. If you train a model on life satisfaction vs. GDP using only rich countries, it won\u0026rsquo;t predict well for poor countries (Figure 1-21). This can happen due to sampling noise (if your sample is too small and just happens to be unrepresentative by chance) or sampling bias (if your sampling method is flawed). The Literary Digest poll example (page 26) is a classic case of sampling bias – they polled wealthier people, who leaned Landon, but Roosevelt won. And nonresponse bias – only certain types of people responded. Want to build a funk music video recognizer by searching YouTube? Your results will be biased towards popular artists. (Page 26) Poor-Quality Data: Errors, outliers, noise. \u0026ldquo;Garbage in, garbage out.\u0026rdquo; Data cleaning is a huge part of a data scientist\u0026rsquo;s job! You might discard outliers, fix errors, or decide how to handle missing feature values (ignore the feature, ignore the instance, fill it in – e.g., with the median). (Page 27) Irrelevant Features: If your training data has too many irrelevant features (and not enough relevant ones), the system will struggle. Feature engineering is critical: Feature selection: Choosing the most useful features. Feature extraction: Combining existing features into a more useful one (like we saw with dimensionality reduction). Creating new features: Sometimes you need to gather new data or derive new features. Now for \u0026ldquo;Bad Algorithm\u0026rdquo; (or more accurately, issues with the model itself):\n(Page 27-29) Overfitting the Training Data: This is a HUGE one. The model performs great on the training data but poorly on new, unseen data. It\u0026rsquo;s like it memorized the training data, including its noise and quirks, instead of learning the underlying general pattern (Figure 1-22).\nImagine your life satisfaction model learns that countries with a \u0026lsquo;W\u0026rsquo; in their name are happier based on your training data (New Zealand, Norway, Sweden). This is just a chance pattern in your data, not a real rule! Solutions (page 28): Simplify the model: Choose one with fewer parameters (e.g., linear instead of high-degree polynomial), reduce features, or constrain the model. Gather more training data. Reduce noise in the training data (fix errors, remove outliers). Regularization (page 28-29): Constraining a model to make it simpler and reduce overfitting. For our linear model, if we force the slope (θ₁) to be small, it makes the line flatter and less likely to chase noise (Figure 1-23). The amount of regularization is controlled by a hyperparameter. Crucial distinction (page 29): A model parameter (like θ₀, θ₁) is something the learning algorithm tunes. A hyperparameter is a parameter of the learning algorithm itself (e.g., the amount of regularization to apply). You set hyperparameters before training. (Page 29) Underfitting the Training Data: The opposite of overfitting. Your model is too simple to learn the underlying structure of the data. A linear model for life satisfaction might underfit because reality is more complex (Figure 1-21 showed this too – the linear model wasn\u0026rsquo;t great for very rich or very poor countries).\nSolutions: Select a more powerful model (more parameters). Feed better features to the algorithm (feature engineering). Reduce constraints on the model (e.g., reduce the regularization hyperparameter). (Page 30-33: Stepping Back, Testing and Validating) Phew! That was a lot. The book summarizes it well on page 30:\nML is about machines learning from data, not explicit rules. Many types: supervised/unsupervised, batch/online, instance/model-based. Typical project: Gather data, feed to algorithm. Model-based learns parameters to fit. Instance-based learns by heart. Pitfalls: Bad data (too little, non-representative, noisy, irrelevant features) or bad model (overfitting/underfitting). So, you\u0026rsquo;ve trained a model. How do you know if it will generalize to new cases? You can\u0026rsquo;t just \u0026ldquo;hope\u0026rdquo;!\nTraining Set and Test Set (Page 30): Split your data. You train on the training set. You evaluate on the test set (data the model has never seen). The error rate on the test set is called the generalization error (or out-of-sample error). This tells you how well it will likely do in the real world.\nIf training error is low but generalization error is high, you\u0026rsquo;re overfitting! Common split: 80% train, 20% test (but depends on dataset size, as page 31 notes). (Page 31) Hyperparameter Tuning and Model Selection:\nWhat if you\u0026rsquo;re choosing between a linear model and a polynomial model? Or trying to find the best regularization hyperparameter? You can\u0026rsquo;t just try them all on the test set and pick the best. Why? Because then you\u0026rsquo;ve tuned your model and hyperparameters to that specific test set. It might not perform well on other new data. You\u0026rsquo;ve essentially \u0026ldquo;used up\u0026rdquo; your test set. Holdout Validation: The solution! Split your original training data further. Keep some aside as a validation set (or dev set). Train various models (with different hyperparameters, or different model types) on the reduced training set (full training set - validation set). Evaluate them on the validation set. Pick the best one. Now, train your best model (with its best hyperparameters) on the full original training set (including the validation set). This is your final model. Finally, evaluate this final model on the test set to get an estimate of its true generalization error. Cross-validation is mentioned as a way to deal with small validation sets by using many small validation sets. More robust, but takes longer. (Page 32) Data Mismatch: What if your training data (e.g., flower pictures from the web) isn\u0026rsquo;t perfectly representative of your production data (e.g., flower pictures taken by your mobile app)?\nCrucial rule: Your validation set and test set must be as representative of the production data as possible. So, they should come from the app pictures. If performance on the validation set is bad after training on web pictures, is it overfitting or data mismatch? Andrew Ng suggests a train-dev set: a subset of the web (training) pictures held out. If model does poorly on train-dev: it overfit the training web pictures. Simplify/regularize. If model does well on train-dev but poorly on (app-based) validation set: it\u0026rsquo;s data mismatch. Try to make web images look more like app images (preprocessing). (Page 33) No Free Lunch (NFL) Theorem: A humbling but important concept. There\u0026rsquo;s no single model that is a priori guaranteed to work best on all problems. If you make no assumptions about your data, any model is as good as any other.\nA linear model might be best for one dataset, a neural network for another. The only way to know for sure is to try them all (impossible!). In practice, we make reasonable assumptions about the data and try a few suitable models. (Page 33-34: Exercises) The chapter ends with a great set of exercises. I strongly encourage you to go through them. If you can answer these, you\u0026rsquo;ve got a solid grasp of this foundational material.\nOkay! We\u0026rsquo;ve covered a massive amount of ground – the entire landscape of Machine Learning, really. We\u0026rsquo;ve defined what it is, why it\u0026rsquo;s a game-changer, explored the main types of systems, the process of building models, and the pitfalls to watch out for.\nThe key takeaways:\nML is about learning patterns from data. The type of learning (supervised, unsupervised, etc.) depends on your data and your goal. Data is king, but it needs to be good quality and representative. Overfitting and underfitting are constant battles. Always test your model on unseen data! This chapter sets the stage. From here on out, we\u0026rsquo;ll be diving deeper into these concepts, with more math, more code, and more hands-on examples from the book.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-1/","tags":null,"title":"Chapter 1: The Machine Learning Landscape"},{"categories":null,"content":"Chapter 5 Overview: This chapter covers the basic properties of arrays (specifically Python lists), common manipulation techniques, and classic array-based problems. It heavily emphasizes thinking about space complexity and optimizing operations.\nThink of arrays like this: Imagine a row of numbered boxes, right next to each other in memory.\nContiguous: The boxes are physically adjacent. This is crucial for performance. Indexed: You can instantly jump to any box if you know its number (index). This is O(1) access. Homogeneous (usually): Typically, all boxes hold the same type of item (like all integers or all strings), though Python lists offer more flexibility. Why Arrays Are Important (Especially for Interviews): Efficiency: Direct access by index is super fast (O(1)). Memory Locality: Because elements are stored together, accessing sequential elements is often cache-friendly, leading to good practical performance. Building Blocks: Many other data structures (like hash maps, heaps, stacks, queues) are often implemented using arrays underneath. In-Place Operations: Interviewers love problems where you modify the array directly without using significant extra memory (O(1) space). This often involves clever use of pointers or swapping elements. Array Boot Camp: Reordering Array Entries (Page 1) The boot camp problem is a fantastic introduction to in-place array manipulation using multiple pointers.\nThe Problem: Given an array of integers, rearrange it so that all the even numbers appear before all the odd numbers.\nThe order within the evens or odds doesn\u0026rsquo;t matter.\nCrucially, do this in-place, meaning using only O(1) extra space (you can\u0026rsquo;t just create two new lists and combine them).\nExample: Input [3, 1, 2, 4, 5, 6] could become [2, 4, 6, 1, 5, 3] or [6, 2, 4, 3, 1, 5], etc.\nThe Boot Camp Code Explained (even_odd): def even_odd(A): next_even, next_odd = 0, len(A) - 1 # Pointers start at opposite ends while next_even \u0026lt; next_odd: # Stop when pointers meet or cross if A[next_even] % 2 == 0: # A[next_even] is even. It\u0026#39;s in the correct partition. # Leave it there and advance the \u0026#39;even\u0026#39; boundary. next_even += 1 else: # A[next_even] is odd. It needs to go to the \u0026#39;odd\u0026#39; section. # Swap it with the element at A[next_odd]. A[next_even], A[next_odd] = A[next_odd], A[next_even] # The element now at A[next_odd] is confirmed odd (or hasn\u0026#39;t been checked), # so shrink the \u0026#39;odd\u0026#39; boundary. next_odd -= 1 # No return needed as A is modified in-place Intuition (Partitioning): Think of the array being divided into three sections as you process it:\nA[0 ... next_even - 1]: Even Numbers (Correctly placed) A[next_even ... next_odd]: Unclassified Numbers (Yet to be examined) A[next_odd + 1 ... len(A) - 1]: Odd Numbers (Correctly placed) The while loop processes the Unclassified section.\nThe next_even pointer expands the Even section. The next_odd pointer shrinks the Unclassified section from the right (effectively expanding the Odd section). When next_even finds an odd number, it swaps it with A[next_odd], placing the odd number in the correct final partition, and then shrinks the next_odd boundary. The element that gets swapped into A[next_even] still needs to be checked in the next iteration of the loop, which is why next_even doesn\u0026rsquo;t advance in the else block.\nComplexity: Time: O(n). Each element is examined at most once by next_even, and involved in at most one swap. Space: O(1). We only use a couple of pointer variables and potentially one temporary variable for the swap (though Python handles the tuple swap elegantly without explicit temp). Key Takeaway: This boot camp demonstrates a powerful pattern: using two (or sometimes more) pointers to partition an array in-place based on some property.\nTop Tips for Arrays (Page 2) This section lists crucial strategies for array problems:\nUse the array itself for space: Cleverly reuse parts of the array or use indices/values to store information, avoiding extra lists/dicts. Fill from the back: When merging or writing results, starting from the end of the allocated space can avoid overwriting needed data and reduce shifting (e.g., merging two sorted arrays into one). Overwrite, don\u0026rsquo;t delete: If you need to remove elements and order doesn\u0026rsquo;t matter for the \u0026ldquo;removed\u0026rdquo; part, just overwrite them with elements you want to keep. Much faster than shifting everything left. Process integer digits: For arrays representing large numbers, process from the LSB (rightmost/last element) for arithmetic. Or reverse the array first if processing LSB-first simplifies logic. Subarrays: Be comfortable working with slices or pointer ranges (A[start:end]). Off-by-one errors: Double-check loop conditions (\u0026lt; vs \u0026lt;=), index calculations (i+1, i-1), and empty array edge cases. Don\u0026rsquo;t preserve properties prematurely: It\u0026rsquo;s often fine to temporarily violate sortedness or other properties during intermediate steps if you restore them by the end. Arrays as direct maps: If values are in a known, limited range (like 0-99 or ASCII), use a boolean or count array indexed by the value itself (e.g., seen[value] = True). Parallel logic for 2D arrays: Often, the logic for rows (iterating i) and columns (iterating j) is very similar; exploit this symmetry. Simulation vs. Formula: For generating sequences (like spiral order), sometimes writing code that directly simulates the generation process step-by-step is easier and less error-prone than deriving a complex mathematical formula for the k-th element. Know Your Array Libraries (Python list) (Page 2-3) Python\u0026rsquo;s list is your go-to array. It\u0026rsquo;s dynamic (grows/shrinks) and versatile.\nInstantiation: my_list = [] nums = [1, 6, 3] zeros = [0] * 10 copy_list = list(original_list) List comprehensions are powerful: squares = [x*x for x in range(10)] Basic Ops: len(A) A.append(val) A.insert(idx, val) A.remove(val) (removes first occurrence) val in A (O(n) search) 2D Arrays: matrix = [[0] * cols for _ in range(rows)] BEWARE: matrix = [[0] * cols] * rows creates shallow copies of the inner list - modifying one row modifies all! Copying: B = A: B is just another reference to A. Changes to B affect A. B = list(A) or B = A[:] or B = copy.copy(A): Shallow copy. Creates a new list B, but elements inside B are still references to the same objects as in A. If A contains mutable objects (like other lists), changing them via A will reflect in B, and vice-versa. B = copy.deepcopy(A): Deep copy. Creates a fully independent copy, including copies of any nested objects. Use this when you need total separation. Key Methods: min(A), max(A) bisect module (bisect_left, bisect_right, insort): For efficient searching/insertion in sorted lists (O(log n) search, O(n) insertion). A.reverse(): Reverses in-place. reversed(A): Returns an iterator for reverse order (doesn\u0026rsquo;t modify A, doesn\u0026rsquo;t create a full new list immediately). A.sort(): Sorts in-place. sorted(A): Returns a new sorted list (doesn\u0026rsquo;t modify A). del A[i]: Deletes element at index i. del A[i:j]: Deletes slice from i up to (not including) j. Slicing (A[start:stop:step]): Super powerful! A[i:j] gives subarray. A[:j] from start up to j. A[i:] from i to end. A[:] shallow copy. A[::-1] reverses the list (returns a new reversed list). A[k:] + A[:k] rotates left by k. List Comprehensions: [expression for item in iterable if condition] Concise way to build lists. Often clearer than map/filter. Can nest: [val for row in matrix for val in row] flattens 2D list. Avoid excessive nesting (more than 2 levels often becomes hard to read). Section 5.1: The Dutch National Flag Problem (Pages 3-6) This is a classic partitioning problem, essential for understanding quicksort and in-place array manipulation.\nThe Problem: Given an array A and an index i (whose value A[i] is the \u0026ldquo;pivot\u0026rdquo;), rearrange A such that:\nAll elements less than the pivot come first. Then, all elements equal to the pivot. Finally, all elements greater than the pivot. Do this in-place (O(1) space). The relative order within the \u0026lt;, ==, \u0026gt; groups doesn\u0026rsquo;t matter.\nExample: A = [0, 1, 2, 0, 2, 1, 1], pivot_index = 3 (pivot value = 0). A valid result: [0, 0, 1, 2, 2, 1, 1] or [0, 0, 1, 1, 1, 2, 2] (after sorting groups). But the simplest valid result might be [0, 0, 1, 2, 2, 1, 1].\nApproach 1: Two Passes (Less efficient, conceptually simpler) First Pass (Less Than): Iterate through A. Keep a pointer smaller (initially 0). If A[current_index] \u0026lt; pivot, swap A[current_index] with A[smaller] and increment smaller. This brings all smaller elements to the front. Second Pass (Greater Than): Iterate backwards through A. Keep a pointer larger (initially n-1). If A[current_index] \u0026gt; pivot, swap A[current_index] with A[larger] and decrement larger. This pushes all larger elements to the end. After these two passes, elements equal to the pivot will naturally be left in the middle.\nTime: O(n) + O(n) = O(n). Space: O(1). Approach 2: Single Pass (The Standard, Efficient Solution) This is the implementation shown in the book, often preferred in interviews.\nIntuition: Divide the array into four regions using three pointers:\nA[0 ... smaller - 1]: Bottom (Known to be \u0026lt; pivot) A[smaller ... equal - 1]: Middle (Known to be == pivot) A[equal ... larger - 1]: Unclassified (Elements we haven\u0026rsquo;t processed yet) A[larger ... n - 1]: Top (Known to be \u0026gt; pivot) Pointers:\nsmaller: Boundary between Bottom and Middle. equal: Boundary between Middle and Unclassified (the current element being processed). larger: Boundary between Unclassified and Top. Algorithm:\nInitialize smaller = 0, equal = 0, larger = len(A). Get pivot = A[pivot_index]. Loop while equal \u0026lt; larger: (While there are unclassified elements) Examine A[equal]. Case 1: A[equal] \u0026lt; pivot Swap A[equal] with A[smaller]. Increment smaller. Increment equal. Case 2: A[equal] == pivot Increment equal. (Element is in the right place relative to smaller). Case 3: A[equal] \u0026gt; pivot Decrement larger. (Shrink unclassified region from the right). Swap A[equal] with A[larger]. Crucially: Do NOT increment equal here. The element just swapped into A[equal] came from the unprocessed larger region and still needs to be classified in the next loop iteration. Let\u0026rsquo;s revisit the requirements:\nInput: An array A (e.g., [0, 1, 2, 0, 2, 1, 1]) and an index i (e.g., pivot_index = 3). Pivot Value: The value at that index, pivot = A[pivot_index] (so pivot = 0 in our example). Goal: Rearrange A so it looks like this: [ \u0026lt;pivot | ==pivot | \u0026gt;pivot ] Specifically: [ All elements LESS THAN pivot ] - These come first. [ All elements EQUAL TO pivot ] - These come next. [ All elements GREATER THAN pivot ] - These come last. Why three categories? This is extremely useful as a subroutine in Quicksort. If an array has many duplicate elements equal to the pivot, standard Quicksort (which partitions into just \u0026lt; pivot and \u0026gt;= pivot) can perform poorly (O(n^2)). By putting all elements equal to the pivot into their own group in the middle, you exclude them from the recursive calls on the \u0026lt; pivot and \u0026gt; pivot subarrays. This makes Quicksort much more efficient (O(n log n) on average, O(n) if all elements are equal) when duplicates are present.\n\u0026ldquo;Do this in-place (O(1) space)\u0026rdquo;: This means you cannot create new helper arrays to store the \u0026lt;, ==, and \u0026gt; elements temporarily. You must achieve the final arrangement by only swapping elements within the original array A, using only a constant amount of extra memory (like variables for the pivot value and your pointers smaller, equal, larger). This is the main challenge and why the pointer techniques are important.\n\u0026ldquo;The relative order within the \u0026lt;, ==, \u0026gt; groups doesn\u0026rsquo;t matter\u0026rdquo;: This is a relaxation that makes the problem easier. Look at the example A = [0, 1, 2, 0, 2, 1, 1] with pivot = 0.\nThe \u0026lt; pivot group is empty. The == pivot group contains [0, 0]. The \u0026gt; pivot group contains [1, 2, 2, 1, 1]. A valid final arrangement could be [0, 0, | 1, 2, 2, 1, 1]. Another could be [0, 0, | 2, 1, 1, 2, 1]. As long as all the 0s come first, followed by all the numbers greater than 0, it\u0026rsquo;s correct. We don\u0026rsquo;t need to keep the original relative order of the 1s and 2s within the \u0026gt; pivot group. This freedom allows us to perform swaps more easily. If we had to maintain relative order (a \u0026ldquo;stable partition\u0026rdquo;), the O(1) space solution would be much harder.\nIn essence: The Dutch Flag problem is just a more detailed partitioning than even_odd. Instead of two bins, you have three (\u0026lt;, ==, \u0026gt;). The challenge lies in achieving this three-way split efficiently using only the existing array.\nSection 5.2: Increment an Arbitrary-Precision Integer (Page 7 / PDF Page 43) The Concept: Standard integer types in many languages (like int in C++ or Java) have a fixed size (e.g., 32 or 64 bits) and thus a maximum value. Python handles this automatically with its arbitrary-precision integers, but interview problems often ask you to simulate this behavior, typically by storing the digits of a very large number in an array (or list in Python).\nRepresentation: An array A represents a non-negative integer D. Each element A[i] holds a single digit. The most significant digit is usually at A[0].\nExample: D = 129 is represented as A = [1, 2, 9]. Example: D = 99 is represented as A = [9, 9]. The Problem: Write a function that takes such an array A representing integer D and updates it in-place to represent D + 1. Handle potential carries, including the case where the number of digits increases (like 99 + 1 = 100).\nBrute-Force (and why it\u0026rsquo;s often not allowed/intended):\nConvert the array [1, 2, 9] into the integer 129. Add 1: 129 + 1 = 130. Convert 130 back into an array [1, 3, 0]. Limitation: This fails if the integer D is larger than the maximum value the language\u0026rsquo;s built-in integer type can hold (this isn\u0026rsquo;t an issue for Python\u0026rsquo;s runtime integers, but the problem setup often simulates fixed-precision constraints or asks you to avoid this conversion). It also doesn\u0026rsquo;t modify the array in-place directly.\nThe Grade-School Algorithm Approach: Think about how you add 1 to a number on paper, like 129 + 1:\nStart from the rightmost digit (Least Significant Digit - LSB). Add 1 to the last digit: 9 + 1 = 10. Write down the 0, carry-over the 1. Move to the next digit to the left (2). Add the carry: 2 + 1 = 3. Write down the 3. No carry-over this time (carry is 0). Move to the next digit to the left (1). Add the carry: 1 + 0 = 1. Write down the 1. No carry-over. No more digits, result is 130. Now consider 99 + 1:\nStart rightmost: 9 + 1 = 10. Write 0, carry 1. Next digit: 9 + 1 (carry) = 10. Write 0, carry 1. No more digits, but we still have a carry. This means the result needs an extra digit. The result is 100. Simulating on the Array:\nWe operate directly on the array A. Increment the last element: A[n-1] += 1. Iterate from right-to-left (from n-1 down to 1). If A[i] == 10 (meaning we had a carry into this position): Set A[i] = 0. Increment the digit to the left: A[i-1] += 1. If A[i] is not 10 after adding the potential carry, then the carry propagation stops, and we can break the loop early. Special Case: After the loop, check if the first digit A[0] became 10. If it did, it means we had a carry out of the most significant digit. Set A[0] = 1. Append a 0 to the end of the array to accommodate the new least significant digit. (The book uses a slightly different but equivalent way: set A[0]=1, A.append(0)). Section 5.3: Multiply Two Arbitrary-Precision Integers (Page 7-8 / PDF Pages 43-44) The Problem: Now, instead of adding 1, multiply two non-negative integers num1 and num2, represented as arrays of digits. Return their product, also as an array of digits. Handle potential negative numbers indicated by a negative sign in the first element (e.g., [-7, 6, 1] represents -761).\nExample: [1, 2, 3] (123) * [9, 8, 7] (987) = [1, 2, 1, 4, 0, 1] (121401)\nAgain, Brute-Force Conversion Fails: Converting to built-in integers and multiplying won\u0026rsquo;t work for numbers exceeding the language\u0026rsquo;s fixed precision limits.\nGrade-School Multiplication Algorithm: Think about 123 * 987:\n123 x 987 ------ 861 (= 7 * 123) 984 (= 8 * 123, shifted left 1 place) 1107 (= 9 * 123, shifted left 2 places) ------ 121401 (Sum of the above) Simulating on Arrays:\nHandle Sign:\nDetermine the sign of the result (-1 if one input is negative, 1 otherwise). Make both input arrays positive for the multiplication step (e.g., using abs() on the first element). Result Array Size:\nThe product of an n-digit number and an m-digit number can have at most n + m digits. Create a result array of size n + m, initialized to zeros. Core Multiplication Loop: Iterate through num1 from right-to-left (index i) and num2 from right-to-left (index j).\nFor each pair num1[i] and num2[j], calculate the product num1[i] * num2[j]. This product contributes to the position result[i + j + 1] (think about the place values: 10a * 10b = 10(a+b)). Add this product to result[i + j + 1]. Handle the carry: The carry from this addition goes into result[i + j]. result[i + j] += result[i + j + 1] // 10. Keep the digit: result[i + j + 1] %= 10. This directly adds the partial products into the correct positions in the result array, simulating the addition step of grade-school multiplication implicitly. Remove Leading Zeros:\nThe result array might have leading zeros (e.g., if multiplying small numbers like [1]*[1]). Find the first non-zero digit and return the slice from that point onwards. Handle the case where the result is 0 (return [0]). Apply Sign:\nIf the overall sign calculated in step 1 was negative, negate the first element of the final result array. Complexity: Time: O(n * m), where n and m are the number of digits in the two input numbers. This comes from the nested loops iterating through all pairs of digits. Space: O(n + m) for the result array. Section 5.4: Advancing Through an Array (Page 8 / PDF Page 44) The Problem: Imagine a board game where you\u0026rsquo;re on a sequence of positions represented by an array A. Each A[i] contains a non-negative integer telling you the maximum number of steps you can take forward from position i. You start at index 0. Can you reach the last index of the array?\nExample 1: A = [3, 3, 1, 0, 2, 0, 1]\nStart at index 0 (A[0]=3). Can move 1, 2, or 3 steps. Option 1: Move 1 step to index 1 (A[1]=3). From index 1, can move 1, 2, or 3 steps. Let\u0026rsquo;s move 3 steps to index 4 (A[4]=2). From index 4, can move 1 or 2 steps. Let\u0026rsquo;s move 2 steps to index 6 (A[6]=1). Index 6 is the last index. Yes, we can reach the end. Example 2: A = [3, 2, 0, 0, 2, 0, 1]\nStart at index 0 (A[0]=3). Can move 1, 2, or 3 steps. If we move 1 step -\u0026gt; index 1 (A[1]=2). Can reach index 2 or 3. If we move 2 steps -\u0026gt; index 2 (A[2]=0). Stuck. If we move 3 steps -\u0026gt; index 3 (A[3]=0). Stuck. From index 1, max reach is 1+2=3. Both indices 2 and 3 have 0s. We cannot get past index 3. No, we cannot reach the end. Initial Thoughts \u0026amp; Why Simple Greedy Fails: You might think: \u0026ldquo;Always take the maximum jump possible from the current position.\u0026rdquo; This doesn\u0026rsquo;t work. Consider A = [2, 4, 1, 1, 0, 2, 3]. Max jump from index 0 takes you to index 2 (A[2]=1). From there you get stuck. But taking just 1 step from index 0 to index 1 (A[1]=4) lets you jump much further and potentially reach the end.\nThe Efficient Approach: Track Maximum Reach The key idea is to iterate through the array and keep track of the furthest index we know we can possibly reach so far.\nAlgorithm:\nInitialize max_reach = 0. This variable will store the furthest index reachable from the positions we\u0026rsquo;ve visited. Iterate through the array with an index i from 0 up to len(A) - 1. Crucial Check: In each iteration i, first check if i \u0026gt; max_reach. If i is greater than max_reach, it means we could never have reached the current index i from any previous position. Therefore, we definitely cannot reach the end. Return False. Update the maximum reach: Calculate the furthest we could jump from the current position i. This is i + A[i]. Update max_reach = max(max_reach, i + A[i]). Check for early success: If at any point max_reach becomes greater than or equal to the last index (len(A) - 1), we know we can reach the end. Return True. If the loop completes without returning False (meaning we could always reach the current i) and without having reached the end yet (which implies max_reach might have landed exactly on the last index in the final iteration), we still need a final check after the loop, but the logic shown below incorporates this cleanly. Note: The book\u0026rsquo;s code slightly reformulates the loop condition and final check, but the core logic of tracking max_reach is the same.\nIntuition Walkthrough (Example 1: A = [3, 3, 1, 0, 2, 0, 1]) i=0: max_reach=0. i \u0026lt;= max_reach is true. max_reach = max(0, 0+3) = 3. i becomes 1. i=1: max_reach=3. i \u0026lt;= max_reach is true. max_reach = max(3, 1+3) = 4. i becomes 2. i=2: max_reach=4. i \u0026lt;= max_reach is true. max_reach = max(4, 2+1) = 4. i becomes 3. i=3: max_reach=4. i \u0026lt;= max_reach is true. max_reach = max(4, 3+0) = 4. i becomes 4. i=4: max_reach=4. i \u0026lt;= max_reach is true. max_reach = max(4, 4+2) = 6. i becomes 5. Loop condition: i \u0026lt;= max_reach (5 \u0026lt;= 6) is true, but max_reach \u0026lt; last_index (6 \u0026lt; 6) is false. Loop terminates. Return max_reach \u0026gt;= last_index (6 \u0026gt;= 6) -\u0026gt; True. Intuition Walkthrough (Example 2: A = [3, 2, 0, 0, 2, 0, 1]) i=0: max_reach=0. i \u0026lt;= max_reach is true. max_reach = max(0, 0+3) = 3. i becomes 1. i=1: max_reach=3. i \u0026lt;= max_reach is true. max_reach = max(3, 1+2) = 3. i becomes 2. i=2: max_reach=3. i \u0026lt;= max_reach is true. max_reach = max(3, 2+0) = 3. i becomes 3. i=3: max_reach=3. i \u0026lt;= max_reach is true. max_reach = max(3, 3+0) = 3. i becomes 4. i=4: max_reach=3. i \u0026lt;= max_reach (4 \u0026lt;= 3) is false. Loop terminates earlier. Return max_reach \u0026gt;= last_index (3 \u0026gt;= 6) -\u0026gt; False. Analogy: Hopping Stones Across a River Imagine the array indices 0, 1, 2, ... n-1 are stones lined up across a river.\nYou start at stone 0. The number A[i] on each stone tells you the maximum length of a single jump you can take from that stone. So, from stone i, you can land on any stone j where i \u0026lt; j \u0026lt;= i + A[i]. Goal: Can you reach the last stone (stone n-1)? Why simple jumping doesn\u0026rsquo;t work: If you just stand on stone i and decide which jump to take next, you might make a bad choice. A shorter jump might land you on a stone with a much bigger A[j] value that lets you cross the river, while the longest jump from i might land you somewhere useless.\nThe max_reach Idea: Tracking Your Potential Why this \u0026ldquo;tracking the potential\u0026rdquo; works:\nIt\u0026rsquo;s safe: We only advance our current position i if it\u0026rsquo;s within the known max_reach. If we ever encounter an i that\u0026rsquo;s beyond max_reach, it means no combination of previous jumps could have gotten us there. It\u0026rsquo;s sufficient: We don\u0026rsquo;t need to know the exact path. We only need to know if the potential to reach the end exists. By always updating max_reach to the maximum possible distance (max(current_max_reach, i + A[i])), we are essentially keeping track of the \u0026ldquo;frontier\u0026rdquo; of reachable stones. If that frontier ever touches or crosses the last stone, we know a path must exist. Think of it like this: max_reach is your \u0026ldquo;optimistic outlook\u0026rdquo;. At each step i you can actually reach, you check if the jump from i gives you a more optimistic outlook than you already had. If your current position i ever falls behind your optimistic outlook (i \u0026gt; max_reach), then your optimism was unfounded, and you\u0026rsquo;re stuck.\nInstead of deciding which specific jump to take, we focus on a simpler question: \u0026ldquo;What is the absolute furthest stone I could possibly reach, given the stones I\u0026rsquo;ve visited so far?\u0026rdquo;\nComplexity: Time: O(n). We iterate through the array at most once. Space: O(1). We only use a few variables (max_reach, i, last_index). Variant: Minimum Steps The variant asks for the minimum number of steps to reach the end. This smells like a Breadth-First Search (BFS).\nThink of array indices as nodes in a graph. There\u0026rsquo;s an edge from i to j if j is reachable from i (i.e., i \u0026lt; j \u0026lt;= i + A[i]). You want the shortest path from node 0 to node n-1.\nA standard BFS exploring reachable indices level by level will find the minimum number of jumps. You\u0026rsquo;d need a way to keep track of visited indices to avoid cycles/redundant work and a queue to manage the BFS levels. The state in the queue could be (index, num_jumps). This would likely take O(n) time and O(n) space (for the queue and visited set/array).\nSection 5.5: Delete Duplicates from a Sorted Array (Page 9 / PDF Page 45) The Problem: You are given a sorted array A. Your task is to remove duplicate elements so that each unique element appears only once. You should do this in-place, modifying the original array A. The elements that are kept should be shifted to the beginning of the array. You need to return the number of valid, unique elements remaining in the array. What\u0026rsquo;s stored in the array beyond the last valid element doesn\u0026rsquo;t matter.\nExample:\nInput: A = [2, 3, 5, 5, 7, 11, 11, 11, 13] Output: The function should return 6. After the function runs, the array A should be modified so its beginning looks like: [2, 3, 5, 7, 11, 13, ... ]. The values in the positions after index 5 are irrelevant (e.g., [2, 3, 5, 7, 11, 13, 11, 11, 13] would be a valid state for A). Why \u0026ldquo;Sorted\u0026rdquo; is Key: Because the array is sorted, all duplicate elements are guaranteed to be grouped together consecutively. This makes identifying them much easier than if the array were unsorted.\nCommon Pitfalls:\nUsing O(n) space: It\u0026rsquo;s easy to iterate through A, store unique elements in a set or a new list, and then copy them back. But the problem often requires O(1) extra space (in-place modification). Shifting elements repeatedly: You could find a duplicate A[i] == A[i-1], and then shift all elements from A[i+1] onwards one step to the left. This works, but if you have many duplicates (e.g., [2, 2, 2, 2, 3]), you end up doing a lot of shifting, leading to O(n^2) time complexity in the worst case. The Efficient In-Place Approach: Two Pointers (Read \u0026amp; Write) This is a very common and useful pattern for modifying arrays in-place when you want to keep only certain elements.\nThe Strategy:\nUse one pointer (read_index or just the loop index i in the code below) to iterate through the original array from the second element onwards. Use a second pointer (write_index) to keep track of the next position in the array where a unique element should be written. Initialize write_index = 1 (since the very first element A[0] is always unique by definition, assuming the array isn\u0026rsquo;t empty). Compare the element being read (A[i]) with the last unique element written (A[write_index - 1]). If A[i] is different from A[write_index - 1], it means we\u0026rsquo;ve found a new unique element. Copy this unique element A[i] to the next available write position: A[write_index] = A[i]. Advance the write pointer: write_index += 1. If A[i] is the same as A[write_index - 1], it\u0026rsquo;s a duplicate. We simply ignore it and move the read pointer (i) forward, without advancing write_index. This effectively overwrites duplicates later when a new unique element is found. After iterating through the entire array, write_index will hold the count of unique elements, and the subarray A[0 ... write_index - 1] will contain those unique elements. Section 5.6: Buy and Sell a Stock Once (Page 10 / PDF Page 46) The Problem: You\u0026rsquo;re given an array prices where prices[i] is the price of a given stock on day i. You want to find the maximum profit you can achieve by buying the stock on one day and selling it on a later day. If no profit can be made (prices always decrease), the maximum profit is 0. You must buy before you sell.\nExample: prices = [310, 315, 275, 295, 260, 270, 290, 230, 255, 250]\nBuy at 260 (day 4, index 4), Sell at 290 (day 6, index 6). Profit = 290 - 260 = 30. This is the maximum possible profit. Note: minimum price is 230, max is 315, but 315 occurs before 230, so you can\u0026rsquo;t buy at 230 and sell at 315. Brute-Force: Try every possible pair of (buy_day, sell_day) where sell_day \u0026gt; buy_day. Calculate the profit for each pair and keep track of the maximum.\nUse nested loops: outer loop for buy_day from 0 to n-2, inner loop for sell_day from buy_day + 1 to n-1. Time Complexity: O(n^2). Space Complexity: O(1). (Too slow for large inputs). Divide and Conquer (Mentioned in Book Introduction, not here): Split the array in half, find the max profit in the left half, find the max profit in the right half, and find the max profit that crosses the midpoint (buy in left, sell in right). The crossing part requires finding the min price in the left and max price in the right. This leads to O(n log n) time. Better, but we can do even better.\nThe Efficient Linear Scan Approach:\nKey Idea: As you iterate through the prices day by day, what information do you need to know from the past to decide the best potential profit if you sell today? You need to know the minimum price encountered before today.\nAlgorithm:\nInitialize min_price_so_far to a very large value (or prices[0] if the array is not empty). Initialize max_profit to 0.0. Iterate through the prices array, starting from the first price. Let the current price be price. Calculate potential profit if selling today: potential_profit = price - min_price_so_far. Update maximum profit: max_profit = max(max_profit, potential_profit). Update minimum price seen: min_price_so_far = min(min_price_so_far, price). After iterating through all prices, max_profit will hold the maximum profit achievable. Intuition Walkthrough (prices = [310, 315, 275, 295, 260, 270, 290, 230, 255, 250]): price min_price_so_far (Before Update) profit_sell_today max_profit (After Update) min_price_so_far (After Update) 310 inf -inf 0 310 315 310 5 5 310 275 310 -35 5 275 295 275 20 20 275 260 275 -15 20 260 270 260 10 20 260 290 260 30 30 260 230 260 -30 30 230 255 230 25 30 230 250 230 20 30 230 Final max_profit = 30.\nComplexity: Time: O(n). We iterate through the array exactly once. Space: O(1). We only use two variables (min_price_so_far, max_profit). Variant: Longest Equal Subarray Find the length of the longest contiguous subarray where all entries are equal.\nIterate through the array, keeping track of the current_length of the equal sequence and the max_length found so far.\nIf A[i] == A[i-1], increment current_length. If A[i] != A[i-1], reset current_length to 1. Update max_length = max(max_length, current_length) in each iteration. O(n) time, O(1) space.\nIntuition Walkthrough (prices = [310, 315, 275, 295, 260, 270, 290, 230, 255, 250]): price min_price_so_far (Before Update) profit_sell_today max_profit (After Update) min_price_so_far (After Update) 310 inf -inf 0 310 315 310 5 5 310 275 310 -35 5 275 295 275 20 20 275 260 275 -15 20 260 270 260 10 20 260 290 260 30 30 260 230 260 -30 30 230 255 230 25 30 230 250 230 20 30 230 Final max_profit = 30.\nComplexity: Time: O(n). We iterate through the array exactly once. Space: O(1). We only use two variables (min_price_so_far, max_profit). Variant: Longest Equal Subarray Find the length of the longest contiguous subarray where all entries are equal.\nIterate through the array, keeping track of the current_length of the equal sequence and the max_length found so far.\nIf A[i] == A[i-1], increment current_length. If A[i] != A[i-1], reset current_length to 1. Update max_length = max(max_length, current_length) in each iteration. O(n) time, O(1) space.\nSection 5.7: Buy and Sell a Stock Twice (Page 11 / PDF Page 47) The Problem: Same setup as before, but now you can buy and sell at most twice. The second buy must happen after the first sell. Find the maximum profit.\nExample: prices = [12, 11, 13, 9, 12, 8, 14, 13, 15]\nFirst Tx: Buy at 9, Sell at 12 (Profit 3) Second Tx: Buy at 8, Sell at 15 (Profit 7) Total Profit = 3 + 7 = 10. This is the maximum. Brute-Force: Try all combinations of two disjoint buy-sell intervals. O(n4) (choose 4 days). Too slow.\nImproved Brute-Force: Iterate through all possible split points i (day the first sale happens or second buy happens). Find the max profit for one transaction in prices[0...i] and the max profit for one transaction in prices[i...n-1]. Add them up. Keep track of the maximum sum.\nUsing the O(n) algorithm from Section 5.6 for each subarray takes O(n) time. Doing this for n possible split points gives O(n2) total time. Space is O(1). Better, but still not optimal. The Efficient Dynamic Programming Approach:\nThe Core Difficulty: How do you decide the four days (buy1, sell1, buy2, sell2) optimally? Trying all combinations is too slow (O(n4)).\nThe Key Insight (Decomposition): Imagine you complete your first transaction by selling on day i.\nThis means you bought on some day b1 \u0026lt;= i and sold on day i. The best you could have done for this first transaction ending on day i is MaxProfit(prices[0...i]). Let\u0026rsquo;s call this Profit1_ending_at_i. Now, you need to make your second transaction using only the days after day i.\nThis means you buy on some day b2 \u0026gt; i and sell on some day s2 \u0026gt;= b2. The best you could do for this second transaction happening after day i is MaxProfit(prices[i+1...n-1]). Let\u0026rsquo;s call this Profit2_starting_after_i. If we split the entire process at day i (meaning the first transaction finishes by day i, and the second starts after day i), the total profit for this specific split point i would be: TotalProfit(split at i) = Profit1_ending_at_i + Profit2_starting_after_i\nThe Problem: We don\u0026rsquo;t know the best day i to split the timeline. So, we need to calculate this TotalProfit(split at i) for every possible split day i (from i=0 to i=n-1) and take the maximum among them.\nHow to Calculate Efficiently? Calculating Profit1_ending_at_i and Profit2_starting_after_i from scratch for every i is slow (O(n2)). We need to precompute or compute iteratively.\nStep 1: Calculate all Profit1_ending_at_i values.\nNotice that Profit1_ending_at_i is exactly the maximum profit you can make with a single transaction within the range prices[0...i]. We already know how to calculate this efficiently! It\u0026rsquo;s the logic from the \u0026ldquo;Buy and Sell Once\u0026rdquo; problem (Section 5.6). We can run that algorithm once, and as we iterate through the prices array, we store the maximum profit found up to that point in a new array. Let\u0026rsquo;s call this array F (for Forward). F[i] = Maximum profit from one transaction using prices from day 0 to day i. This takes one pass, O(n) time, and O(n) space for the F array. Step 2: Calculate all Profit2_starting_after_i values.\nThis is the maximum profit from one transaction using only prices from day i+1 to day n-1. How can we calculate this efficiently for all i? We can run the \u0026ldquo;Buy and Sell Once\u0026rdquo; logic backwards! Imagine iterating from the end of the prices array (n-1) down to 0. Keep track of the max_price_seen_so_far (during the backward scan). The best profit if you buy on day j (during this backward scan) is max_price_seen_so_far - prices[j]. Let B[j] be the maximum profit from one transaction using prices from day j to day n-1. We can compute all B[j] values in a single backward pass (O(n) time). We could store this in another array B. Step 3: Combine the Results.\nNow we have F[i] (best profit ending by day i) and B[j] (best profit starting from day j). The total profit for a split right after day i is F[i] + B[i+1]. We need to calculate F[i] + B[i+1] for all i from 0 to n-2. The final answer is the maximum value found in these sums. We also need to consider the possibility that the best profit comes from only one transaction (e.g., if prices always go down after the first transaction). The best single-transaction profit over the whole period is simply F[n-1]. So the overall maximum is max(F[n-1], max(F[i] + B[i+1] for i in 0..n-2)). The Book\u0026rsquo;s Space Optimization: The book realizes we don\u0026rsquo;t actually need to store the entire B array from Step 2. We can combine Step 2 and Step 3.\nCalculate and store the F array (forward pass) as described in Step 1. Perform the backward pass (like Step 2). As you iterate backwards with index i (from n-1 down to 1): Calculate the maximum profit for a single transaction starting at or after day i. Let\u0026rsquo;s call this profit_second_tx. (This is conceptually B[i]). You do this by keeping track of max_price_so_far encountered during the backward scan. profit_second_tx = max(profit_if_buy_at_i, profit_if_buy_later). Immediately combine: Now that you know the best profit starting after day i-1 (which is profit_second_tx you just calculated for index i), you can add it to the best profit ending before day i (which is F[i-1], already calculated in the forward pass). total_profit_for_split_at_i-1 = F[i-1] + profit_second_tx. Keep track of the overall maximum total profit seen across all split points considered so far. Let\u0026rsquo;s retry the Intuition Walkthrough (prices = [12, 11, 13, 9, 12, 8, 14, 13, 15]): Forward Pass: Compute F = [0, 0, 2, 2, 3, 3, 6, 6, 7].\nmax_total_profit initially is the best single transaction profit, so max_total_profit = F[8] = 7. Backward Pass \u0026amp; Combine:\ni = 8 (price prices[8]=15): max_price (for range [8..8]) = 15. profit_second_tx (for range [8..8]) is max(0, 15-15) = 0. total = F[7](6) + 0 = 6. max_total_profit = max(7, 6) = 7. i = 7 (price prices[7]=13): max_price (for range [7..8]) = max(15, 13) = 15. profit_second_tx (for range [7..8]) is max(profit_starting_at_8=0, profit_buy_at_7=15-13=2) = 2. total = F[6](6) + 2 = 8. max_total_profit = max(7, 8) = 8. i = 6 (price prices[6]=14): max_price (for range [6..8]) = max(15, 14) = 15. profit_second_tx (for range [6..8]) is max(profit_starting_at_7=2, profit_buy_at_6=15-14=1) = 2. total = F[5](3) + 2 = 5. max_total_profit = max(8, 5) = 8. i = 5 (price prices[5]=8): max_price (for range [5..8]) = max(15, 8) = 15. profit_second_tx (for range [5..8]) is max(profit_starting_at_6=2, profit_buy_at_5=15-8=7) = 7. total = F[4](3) + 7 = 10. max_total_profit = max(8, 10) = 10. \u0026hellip; and so on. The backward pass calculates the best profit for the second transaction on the fly and immediately combines it with the pre-calculated best profit for the first transaction ending just before it.\nThink of the backward loop index i as defining the start day for the potential second transaction. profit_second_tx calculated at step i represents the best possible profit if your second transaction happens entirely within prices[i...n-1]. We add this to F[i-1] (best profit from first transaction ending before day i) to get the total profit for that specific way of splitting the two transactions. We do this for all possible start days i for the second transaction and find the overall maximum.\nSection 5.8: Computing an Alternation (Page 12 / PDF Page 48) The Problem: You are given an array A of numbers. Rearrange its elements in-place to create a new array (or modify A directly) such that it follows an alternating pattern: A[0] \u0026lt;= A[1] \u0026gt;= A[2] \u0026lt;= A[3] \u0026gt;= A[4] \u0026lt;= A[5] ... (Less-than-or-equal followed by greater-than-or-equal, repeating).\nExample:\nInput: A = [3, 1, 4, 1, 5, 9, 2, 6] A valid output: [1, 4, 1, 5, 2, 9, 3, 6] (Check: 1\u0026lt;=4\u0026gt;=1\u0026lt;=5\u0026gt;=2\u0026lt;=9\u0026gt;=3\u0026lt;=6) - Note: 3\u0026lt;=6 is satisfied at the end. Another valid output: [1, 3, 1, 4, 2, 6, 5, 9] Approach 1: Sorting (Simple but Suboptimal) Sort the array A. Example: [1, 1, 2, 3, 4, 5, 6, 9] Swap adjacent pairs starting from the second element. Swap A[1] and A[2], then A[3] and A[4], then A[5] and A[6], etc. Swap A[1](1) and A[2](2) -\u0026gt; [1, 2, 1, 3, 4, 5, 6, 9] Swap A[3](3) and A[4](4) -\u0026gt; [1, 2, 1, 4, 3, 5, 6, 9] Swap A[5](5) and A[6](6) -\u0026gt; [1, 2, 1, 4, 3, 6, 5, 9] Check the result: 1\u0026lt;=2\u0026gt;=1\u0026lt;=4\u0026gt;=3\u0026lt;=6\u0026gt;=5\u0026lt;=9. It works! Complexity: Dominated by the sort step. Time: O(n log n). Space: O(1) if using an in-place sort, or O(n) otherwise. Approach 2: Median Finding (Mentioned, but not fully explored) Find the median element of the array A (using an O(n) median-of-medians algorithm, like in Problem 11.8). Partition the array around the median (like one step of Quicksort or using Dutch Flag) so elements smaller than the median are first, then elements equal to the median, then elements larger. This takes O(n) time. Interleave elements from the lower half and the upper half, similar to the swapping pattern in Approach 1. Complexity: Dominated by median finding and partitioning. Time: O(n). Space: O(1). This is theoretically optimal time, but the median-of-medians algorithm is complex to implement. Approach 3: The Clever Local Swap (Efficient and Simple) Key Insight: The required property (A[i-1] \u0026lt;= A[i] \u0026gt;= A[i+1] or A[i-1] \u0026gt;= A[i] \u0026lt;= A[i+1]) is very local. It only involves adjacent elements. We don\u0026rsquo;t necessarily need the global order provided by sorting or median finding.\nThe Goal: Arrange the array so it goes \u0026ldquo;up, down, up, down\u0026hellip;\u0026rdquo; like this: Small \u0026lt;= Big \u0026gt;= Small \u0026lt;= Big \u0026gt;= Small ...\nThe Rule We Need:\nAt every even index i, we need A[i] \u0026lt;= A[i+1]. At every odd index i, we need A[i] \u0026gt;= A[i+1]. The Local Swap Algorithm: We walk through the array, looking at pairs (A[i], A[i+1]). We force the local rule to be true for that pair.\nLook at (A[0], A[1]) (i=0, which is even):\nRule: We need A[0] \u0026lt;= A[1]. If it\u0026rsquo;s already true, great, do nothing. If A[0] \u0026gt; A[1], swap them. Now A[0] \u0026lt;= A[1] is true for this pair. Look at (A[1], A[2]) (i=1, which is odd):\nRule: We need A[1] \u0026gt;= A[2]. If it\u0026rsquo;s true, do nothing. If A[1] \u0026lt; A[2], swap them. Now A[1] \u0026gt;= A[2] is true for this pair. Look at (A[2], A[3]) (i=2, which is even):\nRule: We need A[2] \u0026lt;= A[3]. If A[2] \u0026gt; A[3], swap them. Now A[2] \u0026lt;= A[3] is true. \u0026hellip;and so on. Why Doesn\u0026rsquo;t Enforcing the Rule at Step i Break the Rule We Just Fixed at Step i-1? This is the core confusion. Let\u0026rsquo;s trace carefully. We need to check the condition involving A[i] in both the step i-1 comparison and the step i comparison.\nConsider index i. It\u0026rsquo;s involved in two checks:\nCheck at step i-1: Compares (A[i-1], A[i]). Check at step i: Compares (A[i], A[i+1]). Let\u0026rsquo;s analyze the transition at index i when we process step i.\nCase 1: i is EVEN.\nThe rule we enforce at step i: We ensure A[i] \u0026lt;= A[i+1] (by swapping if needed). What about the rule from step i-1? Step i-1 was ODD. The rule enforced there was A[i-1] \u0026gt;= A[i]. Does enforcing A[i] \u0026lt;= A[i+1] break A[i-1] \u0026gt;= A[i]? If we didn\u0026rsquo;t swap at step i (because A[i] \u0026lt;= A[i+1] was already true): No problem, the previous condition A[i-1] \u0026gt;= A[i] still holds (we didn\u0026rsquo;t change A[i] or A[i-1]). If we did swap at step i (because A[i] \u0026gt; A[i+1] initially): Let the original values be Old_A[i] and Old_A[i+1]. We know Old_A[i] \u0026gt; Old_A[i+1]. The new value at A[i] is New_A[i] = Old_A[i+1]. The condition from step i-1 was A[i-1] \u0026gt;= Old_A[i]. Since Old_A[i] \u0026gt; Old_A[i+1] = New_A[i], it follows that A[i-1] \u0026gt; New_A[i]. So, the required condition A[i-1] \u0026gt;= New_A[i] still holds! Swapping in a smaller value at A[i] cannot break the previous A[i-1] \u0026gt;= A[i] condition. Case 2: i is ODD.\nThe rule we enforce at step i: We ensure A[i] \u0026gt;= A[i+1] (by swapping if needed). What about the rule from step i-1? Step i-1 was EVEN. The rule enforced there was A[i-1] \u0026lt;= A[i]. Does enforcing A[i] \u0026gt;= A[i+1] break A[i-1] \u0026lt;= A[i]? If we didn\u0026rsquo;t swap at step i (because A[i] \u0026gt;= A[i+1] was already true): No problem. If we did swap at step i (because A[i] \u0026lt; A[i+1] initially): Let the original values be Old_A[i] and Old_A[i+1]. We know Old_A[i] \u0026lt; Old_A[i+1]. The new value at A[i] is New_A[i] = Old_A[i+1]. The condition from step i-1 was A[i-1] \u0026lt;= Old_A[i]. Since Old_A[i] \u0026lt; Old_A[i+1] = New_A[i], it follows that A[i-1] \u0026lt; New_A[i]. So, the required condition A[i-1] \u0026lt;= New_A[i] still holds! Swapping in a larger value at A[i] cannot break the previous A[i-1] \u0026lt;= A[i] condition. Conclusion: Enforcing the rule for the pair (A[i], A[i+1]) preserves the rule that was already established for the pair (A[i-1], A[i]) in the previous step. Since we process the array from left to right, by the time we reach the end, all adjacent pairs satisfy their respective conditions (\u0026lt;= for even i, \u0026gt;= for odd i), fulfilling the overall alternation requirement.\nAnalogy: Smoothing Out Bumps Imagine the numbers are heights. You want a profile like low \u0026lt;= high \u0026gt;= low \u0026lt;= high ...\nYou walk along the profile, looking at two adjacent points i and i+1.\nIf i should be a \u0026ldquo;low\u0026rdquo; point (i is even), but A[i] is higher than A[i+1], you swap them to push the high point to i+1. This makes A[i] lower, which doesn\u0026rsquo;t violate any preceding \u0026ldquo;high \u0026gt;= low\u0026rdquo; condition. If i should be a \u0026ldquo;high\u0026rdquo; point (i is odd), but A[i] is lower than A[i+1], you swap them to pull the high point to i. This makes A[i] higher, which doesn\u0026rsquo;t violate any preceding \u0026ldquo;low \u0026lt;= high\u0026rdquo; condition. Section 5.9: Enumerate All Primes to n (Page 13 / PDF Page 49) The Problem: Given an integer n, return a list of all prime numbers between 1 and n (inclusive).\nPrime Number Definition: A natural number p is prime if it is greater than 1 and has no divisors other than 1 and itself. (2, 3, 5, 7, 11, 13, 17, 19\u0026hellip; are primes). 1 is not prime.\nExample:\nInput: n = 18 Output: [2, 3, 5, 7, 11, 13, 17] Approach 1: Trial Division (Brute-Force) Iterate through each number i from 2 up to n. For each i, check if it\u0026rsquo;s prime. How to check if i is prime? Try dividing i by every number d from 2 up to sqrt(i). If you find any d that divides i evenly (i % d == 0), then i is not prime. Stop checking for this i and move to the next i. If you check all d up to sqrt(i) and find no divisors, then i is prime. If i is found to be prime, add it to your results list. Why only check up to sqrt(i)? If i has a divisor d larger than sqrt(i), then i = d * k. For this equation to hold, k must be smaller than sqrt(i). So, if i has any divisor other than 1 and itself, it must have at least one divisor less than or equal to its square root.\nComplexity: Checking if a single number i is prime takes roughly O(sqrt(i)) time. Doing this for all numbers from 2 to n gives a total time complexity around O(n * sqrt(n)) or O(n1.5). Space is O(p) where p is the number of primes found, roughly O(n/log n).\nApproach 2: Sieve of Eratosthenes (Much More Efficient) This is the classic algorithm for finding all primes up to n. It avoids redundant checks by eliminating multiples.\nThe Idea:\nStart with a list (or boolean array) representing all numbers from 0 up to n. Initially, mark all numbers from 2 to n as potentially prime (e.g., True). Mark 0 and 1 as not prime (False). Find the first number p in the list (starting from 2) that is still marked as potentially prime. This number p must be prime (because if it had a smaller divisor, that divisor would have been found earlier, and p would have already been marked as not prime). Add p to your list of primes. Sieve: Mark all multiples of p (i.e., 2*p, 3*p, 4*p, \u0026hellip;) up to n as not prime (False) in your boolean array. They cannot be prime because they have p as a divisor. Repeat from step 2: Find the next number in the list greater than p that is still marked as potentially prime, and repeat the sieving process. Continue until you have checked numbers up to n. Sieve Optimization 1: When sieving multiples of prime p, you can start marking from p*p (p2). Why? Any smaller multiple k*p where k \u0026lt; p would have already been marked as not prime when you processed the prime factors of k.\nSieve Optimization 2: You only need to find primes p up to sqrt(n) to perform the sieving. Any composite number c \u0026lt;= n must have a prime factor less than or equal to sqrt(n). If c hasn\u0026rsquo;t been marked off by the time you reach sqrt(n), it must be prime itself.\nThe Old Way (Trial Division): Is 2 prime? Yes. Is 3 prime? Yes. Is 4 prime? No (divisible by 2). Is 5 prime? Yes. Is 6 prime? No (divisible by 2). Is 7 prime? Yes. Is 8 prime? No (divisible by 2). Is 9 prime? No (divisible by 3). \u0026hellip; This is slow because we check 9 for divisibility by 3 even though we already knew 3 was prime and could have somehow used that information. The Sieve Idea: Be Proactive! Instead of checking each number individually, when we find a prime number, let\u0026rsquo;s immediately cross out all of its multiples because we know they cannot be prime.\nSteps for n = 20:\nList all numbers: Write down all numbers from 2 up to 20. Imagine we have little flags (or boolean values) next to them, initially all set to \u0026ldquo;potentially prime\u0026rdquo; (True). [ (2,T), (3,T), (4,T), (5,T), (6,T), (7,T), (8,T), (9,T), (10,T), (11,T), (12,T), (13,T), (14,T), (15,T), (16,T), (17,T), (18,T), (19,T), (20,T) ]\nFind the first number p that is marked True: Start from the beginning. The first number marked True is p = 2.\nAction: We know 2 is prime (it\u0026rsquo;s the first one we found). Add 2 to our prime list: Primes = [2]. Sieve Action: Now, cross out (mark as False) all multiples of 2 in our list (4, 6, 8, 10, 12, 14, 16, 18, 20). List becomes: [ (2,T), (3,T), (4,F), (5,T), (6,F), (7,T), (8,F), (9,T), (10,F), (11,T), (12,F), (13,T), (14,F), (15,T), (16,F), (17,T), (18,F), (19,T), (20,F) ] Find the next number p (after the previous p=2) that is marked True: Looking through the list, the next True is at p = 3.\nAction: We know 3 must be prime (because if it had a smaller prime factor, it would have been crossed out in a previous step - the only smaller prime was 2, and 3 isn\u0026rsquo;t a multiple of 2). Add 3 to our prime list: Primes = [2, 3]. Sieve Action: Cross out all multiples of 3 (starting from 3*2=6, though 6 is already False. Next is 3*3=9, then 3*4=12(F), 3*5=15, 3*6=18(F)). List becomes: [ (2,T), (3,T), (4,F), (5,T), (6,F), (7,T), (8,F), (9,F), (10,F), (11,T), (12,F), (13,T), (14,F), (15,F), (16,F), (17,T), (18,F), (19,T), (20,F) ] Find the next number p (after p=3) that is marked True: The next True is at p = 5.\nAction: 5 must be prime. Add 5 to list: Primes = [2, 3, 5]. Sieve Action: Cross out multiples of 5 (5*2=10(F), 5*3=15(F), 5*4=20(F)). No changes needed as they were already False. List remains the same. Find the next number p (after p=5) that is marked True: The next True is at p = 7.\nAction: 7 must be prime. Add 7 to list: Primes = [2, 3, 5, 7]. Sieve Action: Cross out multiples of 7 (7*2=14(F)). No changes. List remains the same. Find the next number p (after p=7) that is marked True: The next True is at p = 11.\nAction: 11 must be prime. Add 11 to list: Primes = [2, 3, 5, 7, 11]. Sieve Action: Cross out multiples of 11 (11*2=22, which is \u0026gt; 20). No changes. List remains the same. Find the next number p (after p=11) that is marked True: The next True is at p = 13.\nAction: 13 must be prime. Add 13: Primes = [2, 3, 5, 7, 11, 13]. Sieve Action: Multiples are \u0026gt; 20. Find the next number p (after p=13) that is marked True: The next True is at p = 17.\nAction: 17 must be prime. Add 17: Primes = [2, 3, 5, 7, 11, 13, 17]. Sieve Action: Multiples are \u0026gt; 20. Find the next number p (after p=17) that is marked True: The next True is at p = 19.\nAction: 19 must be prime. Add 19: Primes = [2, 3, 5, 7, 11, 13, 17, 19]. Sieve Action: Multiples are \u0026gt; 20. Find the next number p (after p=19) that is marked True: There are none left up to n=20. We are done.\nThe Final Prime List: [2, 3, 5, 7, 11, 13, 17, 19].\nWhy is this efficient?\nWe only \u0026ldquo;process\u0026rdquo; (find multiples of) the actual prime numbers (2, 3, 5, 7, etc.). We touch each composite number (like 4, 6, 8, 9, 10, 12\u0026hellip;) only when we cross it out as a multiple of one of its prime factors. We don\u0026rsquo;t waste time trying to see if 12 is divisible by 4, 5, 6, etc., because it was already crossed out when we processed prime 2 (and again for prime 3). The Optimization (Sieving from p*p) Let\u0026rsquo;s look at step 3 again, when p=3. We crossed out 6, 9, 12, 15, 18. Notice that 6 was already crossed out when we processed p=2. Why? Because 6 = 2 * 3. The factor k=2 is less than p=3. In general, when processing prime p, any multiple k * p where k \u0026lt; p has already been crossed out by the prime factors of k (which are all smaller than p). So, the first multiple of p that we really need to worry about crossing out is p * p.\nFor p=3, we start crossing out from 3*3 = 9. For p=5, we start crossing out from 5*5 = 25 (which is \u0026gt; 20, so we do nothing). For p=7, we start from 7*7=49 (\u0026gt; 20). This optimization saves some redundant work. The Boolean Array (is_prime) In the code, instead of a list of pairs like (2, T), we use a simple boolean array (a list of True/False values). The index of the array represents the number. is_prime = [False, False, True, True, True, ... True] (up to index n)\nWhen we process p=2, we loop for i in range(p*p, n+1, p): is_prime[i] = False. When we process p=3, we loop for i in range(p*p, n+1, p): is_prime[i] = False. And so on. Section 5.10: Permute the Elements of an Array (Page 14 / PDF Page 50) The Problem: You are given an array A of n elements, and a permutation array P of length n. The permutation array P describes how to rearrange A: for each index i, the element currently at A[i] should be moved to position P[i]. You need to apply this permutation to array A in-place (O(1) extra space).\nExample:\nA = [a, b, c, d] P = [2, 0, 1, 3] Interpretation of P:\nElement at index 0 (a) moves to index P[0]=2. Element at index 1 (b) moves to index P[1]=0. Element at index 2 (c) moves to index P[2]=1. Element at index 3 (d) moves to index P[3]=3. Desired Result: A should become [b, c, a, d].\nApproach 1: Using Extra Space (Simple but Not In-Place) Create a new array B of the same size as A. Iterate through A from i = 0 to n-1. For each element A[i], place it in its target position in B: B[P[i]] = A[i]. Copy the contents of B back into A. Complexity: Time: O(n) (for the iteration and copy). Space: O(n) (for the auxiliary array B). Issue: Doesn\u0026rsquo;t meet the O(1) space requirement. Approach 2: In-Place Permutation using Cycles Key Insight: Any permutation can be broken down into one or more disjoint cycles.\nA cycle means that element at index i0 moves to i1, element at i1 moves to i2, \u0026hellip;, element at ik-1 moves to ik, and element at ik moves back to i0. In our example P = [2, 0, 1, 3]:\nStart at index 0: Element a moves to index 2 (P[0]=2). Look at index 2: Element c moves to index 1 (P[2]=1). Look at index 1: Element b moves to index 0 (P[1]=0). We\u0026rsquo;re back at index 0. This forms the cycle 0 -\u0026gt; 2 -\u0026gt; 1 -\u0026gt; 0. Start at index 3 (the next unvisited index): Element d moves to index 3 (P[3]=3). This is a cycle of length 1: 3 -\u0026gt; 3. The permutation P consists of two cycles: (0 2 1) and (3). Algorithm Idea: If we can process one cycle at a time, we can do it in place.\nIterate through the array A from i = 0 to n-1. For each index i, check if the element at this position has already been moved as part of a previous cycle. If it has, skip it. If the element at i hasn\u0026rsquo;t been moved, we\u0026rsquo;ve found the start of a new cycle. Process this cycle: Remember the starting index start = i and the value temp = A[start]. Easier cycle traversal: Store the element you\u0026rsquo;re about to overwrite: value_to_move = A[i]. Follow the cycle: curr = i. Loop: next_idx = P[curr]. Save the element at the destination: next_value = A[next_idx]. Move the value_to_move to the destination: A[next_idx] = value_to_move. Update curr = next_idx. Update value_to_move = next_value. Mark the position curr (or maybe P[curr]) as visited so we don\u0026rsquo;t process this cycle again. Stop when curr gets back to the starting index i. How to Mark Visited Cycles without O(n) Space? This is the clever trick. We can modify the permutation array P itself to mark elements that have been processed. The book suggests subtracting n (or using the sign bit if possible) from P[i] after its corresponding element A[i] has been placed correctly.\nThe Cycle Idea: Follow One Element All the Way Let\u0026rsquo;s pick an element and follow its journey until it lands in its final spot, and see where the element it displaced needs to go, and so on.\nLet\u0026rsquo;s trace starting with index i = 0:\nElement: A[0] is \u0026lsquo;a\u0026rsquo;. Destination: Where should \u0026lsquo;a\u0026rsquo; go? P[0] tells us it goes to index 2. Problem: Index 2 currently holds \u0026lsquo;c\u0026rsquo;. We can\u0026rsquo;t just overwrite \u0026lsquo;c\u0026rsquo;. Solution: Before moving \u0026lsquo;a\u0026rsquo; to index 2, let\u0026rsquo;s save \u0026lsquo;c\u0026rsquo; somewhere temporarily. Let temp = 'c'. Move \u0026lsquo;a\u0026rsquo;: Now, put \u0026lsquo;a\u0026rsquo; into A[2]. The array A is now ['a', 'b', 'a', 'd']. (We still have \u0026lsquo;c\u0026rsquo; saved in temp). Where does \u0026lsquo;c\u0026rsquo; go? \u0026lsquo;c\u0026rsquo; originally came from index 2. According to P, the element from index 2 should go to index P[2] = 1. Problem: Index 1 currently holds \u0026lsquo;b\u0026rsquo;. Solution: Save \u0026lsquo;b\u0026rsquo;. Let temp = 'b'. Move \u0026lsquo;c\u0026rsquo;: Put \u0026lsquo;c\u0026rsquo; (which we saved way back in step 4) into A[1]. Array A is now ['a', 'c', 'a', 'd']. (We have \u0026lsquo;b\u0026rsquo; saved in temp). Where does \u0026lsquo;b\u0026rsquo; go? \u0026lsquo;b\u0026rsquo; originally came from index 1. According to P, the element from index 1 should go to index P[1] = 0. Problem: Index 0 currently holds \u0026lsquo;a\u0026rsquo;. Solution: Save \u0026lsquo;a\u0026rsquo;. Let temp = 'a'. Move \u0026lsquo;b\u0026rsquo;: Put \u0026lsquo;b\u0026rsquo; (saved in step 8) into A[0]. Array A is now ['b', 'c', 'a', 'd']. (We have \u0026lsquo;a\u0026rsquo; saved in temp). Where does \u0026lsquo;a\u0026rsquo; go? This \u0026lsquo;a\u0026rsquo; originally came from index 0. We followed the path 0 -\u0026gt; 2 -\u0026gt; 1 -\u0026gt; 0. We are back where we started! This \u0026lsquo;a\u0026rsquo; we saved in step 12 is the one we started with. Since we\u0026rsquo;re back at the start, the cycle is complete. We don\u0026rsquo;t need to place this saved \u0026lsquo;a\u0026rsquo;; it\u0026rsquo;s already implicitly handled because the element now at index 0 (\u0026lsquo;b\u0026rsquo;) is correct. We\u0026rsquo;ve just processed the cycle 0 -\u0026gt; 2 -\u0026gt; 1 -\u0026gt; 0! The array A is now ['b', 'c', 'a', 'd'].\nHow do we know we are done? We need to check other starting points.\nLet\u0026rsquo;s trace starting with index i = 1: We need a way to know if index 1 was already handled. If we followed the cycle correctly above, index 1 was involved. We need a mechanism to remember this. Let\u0026rsquo;s trace starting with index i = 3: Assume index 3 hasn\u0026rsquo;t been handled yet. Element: A[3] is \u0026rsquo;d\u0026rsquo;. Destination: P[3] tells us it goes to index 3. It\u0026rsquo;s already there! This is a cycle of length 1: 3 -\u0026gt; 3. We don\u0026rsquo;t need to do anything. Putting it Together \u0026amp; The \u0026ldquo;Marking\u0026rdquo; Trick: The process above shows how a cycle works with temporary variables. The challenge is doing it without explicitly using lots of temp variables (just one is needed at a time) and knowing which cycles have been completed. This is where modifying the perm array comes in (or using a visited array). We mark perm[k] by making it negative after we\u0026rsquo;ve moved the element originally at A[k] to its correct spot.\nMy previous manual trace using the book\u0026rsquo;s code swap logic was slightly off. The core idea of the book\u0026rsquo;s code is to complete a cycle correctly.\nCorrected Trace (Using Book\u0026rsquo;s Code Logic - A[next_idx], A[target_idx] = A[target_idx], A[next_idx]):\nInitial State: A = ['a', 'b', 'c', 'd'], perm = [2, 0, 1, 3]\nOuter loop: i = 0\nnext_idx = 0. perm[0] (2) \u0026gt;= 0. Enter while. target_idx = perm[0] = 2. Swap A[0] (\u0026lsquo;a\u0026rsquo;) and A[target_idx] (A[2] which is \u0026lsquo;c\u0026rsquo;). A becomes ['c', 'b', 'a', 'd']. temp = perm[0] (which is 2). Mark perm[0]: perm[0] = 2 - 4 = -2. perm is now [-2, 0, 1, 3]. next_idx = temp = 2. perm[2] (1) \u0026gt;= 0. Continue while. target_idx = perm[2] = 1. Swap A[next_idx] (A[2] which is \u0026lsquo;a\u0026rsquo;) and A[target_idx] (A[1] which is \u0026lsquo;b\u0026rsquo;). A becomes ['c', 'a', 'b', 'd']. temp = perm[2] (which is 1). Mark perm[2]: perm[2] = 1 - 4 = -3. perm is now [-2, 0, -3, 3]. next_idx = temp = 1. perm[1] (0) \u0026gt;= 0. Continue while. target_idx = perm[1] = 0. Swap A[next_idx] (A[1] which is \u0026lsquo;a\u0026rsquo;) and A[target_idx] (A[0] which is \u0026lsquo;c\u0026rsquo;). A becomes ['a', 'c', 'b', 'd']. temp = perm[1] (which is 0). Mark perm[1]: perm[1] = 0 - 4 = -4. perm is now [-2, -4, -3, 3]. next_idx = temp = 0. perm[0] (-2) \u0026lt; 0. Exit while loop. Outer loop: i = 1\nperm[1] (-4) is negative. Skip while loop. (This index was part of the cycle we just processed). Outer loop: i = 2\nperm[2] (-3) is negative. Skip while loop. Outer loop: i = 3\nnext_idx = 3. perm[3] (3) \u0026gt;= 0. Enter while. target_idx = perm[3] = 3. Swap A[3] (\u0026rsquo;d\u0026rsquo;) and A[3] (\u0026rsquo;d\u0026rsquo;). A is unchanged: ['a', 'c', 'b', 'd']. temp = perm[3] (which is 3). Mark perm[3]: perm[3] = 3 - 4 = -1. perm is now [-2, -4, -3, -1]. next_idx = temp = 3. perm[3] (-1) \u0026lt; 0. Exit while loop. Outer loop finishes.\nFinal state for A is ['a', 'c', 'b', 'd']. This trace still gives the incorrect result.\nThe confusion arises from how the swaps in the book\u0026rsquo;s code achieve the \u0026ldquo;following an element\u0026rdquo; logic. Let\u0026rsquo;s use the \u0026ldquo;pick up and carry\u0026rdquo; mental model which is more direct.\nCycle Following - Simpler View (Corrected to match desired outcome):\nThis approach correctly gets the desired result by focusing on placing the element currently \u0026ldquo;held\u0026rdquo; (value) to its destination, and then picking up the element that was at that destination.\nOuter loop i = 0 to n-1: If perm[i] is negative, this element\u0026rsquo;s cycle is already processed. Skip. Start a new cycle: current_cycle_idx = i value_to_place = A[i] (the element we are \u0026ldquo;holding\u0026rdquo;) Inner do-while loop (or while True with a break): next_target_pos = perm[current_cycle_idx] (where value_to_place should go). value_at_target = A[next_target_pos] (save the element currently at the target). A[next_target_pos] = value_to_place (place our \u0026ldquo;held\u0026rdquo; value). Mark perm[current_cycle_idx] as processed (e.g., perm[current_cycle_idx] -= len(perm)). current_cycle_idx = next_target_pos (this is where value_at_target came from). value_to_place = value_at_target (this is the new value we are \u0026ldquo;holding\u0026rdquo;). If current_cycle_idx == i (we\u0026rsquo;ve returned to the start of the cycle), break the inner loop. Let\u0026rsquo;s try this view on A = ['a', 'b', 'c', 'd'], P = [2, 0, 1, 3]\nOuter loop i = 0:\nperm[0] (2) is not negative. Start cycle. current_cycle_idx = 0. value_to_place = A[0] = 'a'. Inner Loop 1: next_target_pos = perm[0] = 2. value_at_target = A[2] = 'c'. A[2] = 'a'. A is ['a', 'b', 'a', 'd']. perm[0] -= 4 \\implies -2. perm is [-2, 0, 1, 3]. current_cycle_idx = 2. value_to_place = 'c'. current_cycle_idx (2) != i (0). Continue. Inner Loop 2: next_target_pos = perm[2] = 1. value_at_target = A[1] = 'b'. A[1] = 'c'. A is ['a', 'c', 'a', 'd']. perm[2] -= 4 \\implies -3. perm is [-2, 0, -3, 3]. current_cycle_idx = 1. value_to_place = 'b'. current_cycle_idx (1) != i (0). Continue. Inner Loop 3: next_target_pos = perm[1] = 0. value_at_target = A[0] = 'a'. A[0] = 'b'. A is ['b', 'c', 'a', 'd']. perm[1] -= 4 \\implies -4. perm is [-2, -4, -3, 3]. current_cycle_idx = 0. value_to_place = 'a'. current_cycle_idx (0) == i (0). Break inner loop. Cycle for i=0 is complete. Outer loop i = 1:\nperm[1] (-4) is negative. Skip. Outer loop i = 2:\nperm[2] (-3) is negative. Skip. Outer loop i = 3:\nperm[3] (3) is not negative. Start cycle. current_cycle_idx = 3. value_to_place = A[3] = 'd'. Inner Loop 1: next_target_pos = perm[3] = 3. value_at_target = A[3] = 'd'. A[3] = 'd'. A is ['b', 'c', 'a', 'd']. perm[3] -= 4 \\implies -1. perm is [-2, -4, -3, -1]. current_cycle_idx = 3. value_to_place = 'd'. current_cycle_idx (3) == i (3). Break inner loop. Cycle for i=3 is complete. Done. Final A is ['b', 'c', 'a', 'd']. This matches the desired result! The key is correctly following the element you \u0026ldquo;picked up\u0026rdquo; and placing it, then picking up the displaced element and continuing, while marking the origin index in P as complete for that step of placing its element. The book\u0026rsquo;s code\u0026rsquo;s swap A[next_idx], A[target_idx] = A[target_idx], A[next_idx] is actually performing this \u0026ldquo;pick up and carry\u0026rdquo; implicitly. The element at target_idx is moved to next_idx, and the old A[next_idx] is moved to target_idx. The next_idx for the next iteration then becomes the original target_idx. This is a common way to implement cycle sort.\nSection 5.11: Compute the Next Permutation (Page 16 / PDF Page 52) The Problem: Given an array perm representing a permutation of numbers (e.g., [1, 0, 3, 2]), find the next permutation in dictionary (lexicographical) order. If the input is already the largest possible permutation (i.e., sorted in descending order, like [3, 2, 1, 0]), return an empty array or list.\nDictionary Order: Think of the permutations as words. [0, 1, 2] comes before [0, 2, 1], which comes before [1, 0, 2], etc. We want the permutation that comes immediately after the input one in this sorted list of all permutations.\nExample:\nInput: [1, 0, 3, 2] -\u0026gt; Output: [1, 2, 0, 3] Input: [1, 2, 0, 3] -\u0026gt; Output: [1, 2, 3, 0] Input: [3, 2, 1, 0] -\u0026gt; Output: [] (or indication of none) The Key Insight: Minimizing the Change We want to find the next permutation, which means we want to increase the sequence by the smallest possible amount while still making it larger.\nWhere to Make the Change? To make the smallest possible increase, we want to change the sequence as far to the right as possible.\nConsider [1, 0, 3, 2]:\nIf we only change [3, 2]? The only other arrangement is [2, 3]. This gives [1, 0, 2, 3]. Is this larger than the original [1, 0, 3, 2]? Yes (because at index 2, 2 \u0026lt; 3). If we only change [0, 3, 2]? We could rearrange these to [2, 0, 3], [2, 3, 0], [3, 0, 2], [3, 2, 0]. Combined with the 1 at the start, these give [1, 2, 0, 3], [1, 2, 3, 0], [1, 3, 0, 2], [1, 3, 2, 0]. All of these are larger than the original. Which one is the smallest? [1, 2, 0, 3]. Notice that [1, 2, 0, 3] seems to be the smallest possible increase we found. It involved changing the 0 at index 1.\nFinding the \u0026ldquo;Pivot\u0026rdquo; - Where the Increase Must Happen Let\u0026rsquo;s look at the sequence from right to left. perm = [1, 0, 3, 2]\n2: Nothing to compare. 3, 2: 3 \u0026gt; 2. The sequence is decreasing here. 0, 3, 2: 0 \u0026lt; 3. The decreasing trend is broken! This 0 at index k=1 is important. Why? Because any rearrangement only involving the suffix [3, 2] will keep the [1, 0] prefix the same. To get a permutation larger than [1, 0, 3, 2] but potentially smaller than [1, 0, 2, 3] (which we found by rearranging only the suffix), we must change something at or before index 1. To make the smallest change, we should change the rightmost possible element, which is perm[1]=0. This leads to:\nAlgorithm Steps:\nFind the \u0026ldquo;pivot\u0026rdquo; (k):\nScan the permutation perm from right to left. Find the first index k such that perm[k] \u0026lt; perm[k+1]. This perm[k] is the rightmost element we can increase while potentially making the resulting permutation larger than the original but as small as possible. If no such k exists (the entire array is in descending order), it\u0026rsquo;s the last permutation. Return an empty list or signal no next permutation. For [1, 0, 3, 2], k=1 (where perm[k]=0). The suffix is [3, 2]. Find the \u0026ldquo;successor\u0026rdquo; (l) to swap with the pivot:\nWe need to replace perm[k] with something larger than it. To make the overall permutation increase by the least amount, we should replace perm[k] with the smallest possible value from the suffix perm[k+1:] that is still greater than perm[k]. Scan the suffix perm[k+1:] from right to left. Find the first element perm[l] that is greater than perm[k]. For [1, 0, 3, 2], perm[k]=0. Suffix is [3, 2]. Scan from right: perm[3] = 2. Is 2 \u0026gt; 0? Yes. Found it! l=3. The successor value is 2. (Why scan from the right for the successor? Because the suffix perm[k+1:] was identified because k was the first break in a descending sequence from the right. This means the suffix perm[k+1:] itself must be in descending order. Scanning a descending sequence from the right guarantees that the first element we find that\u0026rsquo;s greater than the pivot is also the smallest element in that suffix that\u0026rsquo;s greater than the pivot.) Swap pivot and successor:\nSwap perm[k] and perm[l]. Swap perm[1](0) and perm[3](2). perm becomes [1, 2, 3, 0]. Make the rest of the sequence (suffix after k) as small as possible:\nWe\u0026rsquo;ve now made the prefix up to index k (e.g., [1, 2]) the smallest possible larger prefix. Now, we need to make the suffix (the part after index k, which is now [3, 0]) as small as possible in dictionary order to ensure this is the very next permutation. How do you make a sequence of numbers as small as possible in dictionary order? You sort it in ascending order. The suffix after index k (perm[k+1:]) was originally in descending order (or became so after the swap, as perm[l] was replaced by perm[k], which is smaller than other elements in the original suffix). To sort a descending sequence into ascending order efficiently, just reverse it! The suffix after k=1 is [3, 0]. Reversing it gives [0, 3]. Attach this to the prefix: [1, 2] + [0, 3] = [1, 2, 0, 3]. This is the final answer. Let\u0026rsquo;s try perm = [6, 2, 1, 5, 4, 3, 0] again:\nFind k: Scan right-to-left.\n0 3 \u0026gt; 0 4 \u0026gt; 3 5 \u0026gt; 4 1 \u0026lt; 5. Break! k=2 (pivot perm[k]=1). Suffix is [5, 4, 3, 0]. Find l: Scan suffix [5, 4, 3, 0] right-to-left for first element \u0026gt; perm[k](1).\n0 \u0026gt; 1? No. 3 \u0026gt; 1? Yes. Found it! l=5 (index in original array, successor perm[l]=3). Swap: Swap perm[k](1) and perm[l](3).\nperm becomes [6, 2, 3, 5, 4, 1, 0]. Reverse suffix: Reverse the part after k=2, which is [5, 4, 1, 0].\nReversing gives [0, 1, 4, 5]. Final Result: [6, 2, 3] + [0, 1, 4, 5] = [6, 2, 3, 0, 1, 4, 5].\nThis algorithm correctly finds the immediate next permutation by making the smallest necessary change to the prefix and then making the suffix as small as possible.\nSampling Algorithms and their implementation Problem: You have a bag (array A) with n distinct items. You want to pull out exactly k items such that every possible group of k items has the same chance of being selected. You do this by modifying the original array A so the chosen k items end up in the first k positions (A[0] to A[k-1]).\nAnalogy: Picking Players for a Team You have n players lined up (A[0] to A[n-1]). You need to form a team of k players. You want the selection process to be completely fair – any group of k players is equally likely. A Naive (But Fair) Way - Drawing Names from a Hat:\nWrite each of the n player names on a slip of paper. Put all n slips into a hat. Shake the hat well. Draw out k slips one by one without putting them back. This is perfectly fair. Every group of k names has an equal chance. How can we simulate this \u0026ldquo;drawing from a hat\u0026rdquo; using the array A directly? This is what the Offline Sampling algorithm (random_sampling) does. Let\u0026rsquo;s trace it with A = ['A', 'B', 'C', 'D', 'E'] and k = 3. We want to pick 3 players fairly and put them in A[0], A[1], A[2].\nThe Algorithm Step-by-Step: Goal: Fill A[0], then A[1], then A[2] with fairly chosen players.\nStep 1: Fill A[0] (i=0)\nIdea: We need to pick one player randomly out of the entire group (A[0] to A[4]) to put into the first team slot (A[0]). Action: Choose a random index r between 0 and 4 (inclusive). Let\u0026rsquo;s say random.randrange(0, 5) gives r = 3. Swap the player currently at A[0] (which is \u0026lsquo;A\u0026rsquo;) with the player at the random index A[r] (which is A[3], \u0026lsquo;D\u0026rsquo;). A becomes ['D', 'B', 'C', 'A', 'E']. Result: A[0] now contains \u0026lsquo;D\u0026rsquo;. \u0026lsquo;D\u0026rsquo; was chosen uniformly from all 5 original players. We have fairly selected the first team member and placed them in the first slot. The remaining players (['B', 'C', 'A', 'E']) are now in slots A[1] to A[4]. Step 2: Fill A[1] (i=1)\nIdea: We need to pick the second team member. This member must be chosen fairly from the players who haven\u0026rsquo;t been picked yet. Where are the unpicked players? They are currently in the array from index i=1 to the end (A[1] to A[4]). There are n-i = 5-1 = 4 players remaining (\u0026lsquo;B\u0026rsquo;, \u0026lsquo;C\u0026rsquo;, \u0026lsquo;A\u0026rsquo;, \u0026lsquo;E\u0026rsquo;). Action: Choose a random index r between 1 and 4 (inclusive). Let\u0026rsquo;s say random.randrange(1, 5) gives r = 1. Swap the player currently at A[i] (which is A[1], \u0026lsquo;B\u0026rsquo;) with the player at the random index A[r] (which is A[1], \u0026lsquo;B\u0026rsquo;). A remains ['D', 'B', 'C', 'A', 'E']. (Swapping with itself). Result: A[1] now contains \u0026lsquo;B\u0026rsquo;. \u0026lsquo;B\u0026rsquo; was chosen uniformly from the 4 players available (B, C, A, E) after \u0026lsquo;D\u0026rsquo; was picked. We have fairly selected the second team member. The remaining 3 players (['C', 'A', 'E']) are now in slots A[2] to A[4]. Step 3: Fill A[2] (i=2)\nIdea: Pick the third team member fairly from the remaining players. The available players are at indices i=2 to the end (A[2] to A[4]). There are n-i = 5-2 = 3 players remaining (\u0026lsquo;C\u0026rsquo;, \u0026lsquo;A\u0026rsquo;, \u0026lsquo;E\u0026rsquo;). Action: Choose a random index r between 2 and 4 (inclusive). Let\u0026rsquo;s say random.randrange(2, 5) gives r = 4. Swap the player currently at A[i] (which is A[2], \u0026lsquo;C\u0026rsquo;) with the player at the random index A[r] (which is A[4], \u0026lsquo;E\u0026rsquo;). A becomes ['D', 'B', 'E', 'A', 'C']. Result: A[2] now contains \u0026lsquo;E\u0026rsquo;. \u0026lsquo;E\u0026rsquo; was chosen uniformly from the 3 players available (C, A, E) after \u0026lsquo;D\u0026rsquo; and \u0026lsquo;B\u0026rsquo; were picked. We have fairly selected the third team member. Stop: We needed k=3 players, and we have filled A[0], A[1], A[2].\nFinal State: The array is ['D', 'B', 'E', 'A', 'C']. The Sample: The first k=3 elements are ['D', 'B', 'E']. This is our uniformly random sample.\nWhy is this like drawing from a hat? At step i, choosing a random index r from i to n-1 is like drawing one name randomly from the n-i names still left in the hat. Swapping A[i] with A[r] is like taking the drawn name (A[r]) and putting it aside as the i-th selected player (by placing it at A[i]), while the name that was at A[i] is effectively put back into the \u0026ldquo;remaining players\u0026rdquo; part of the array (at index r) to potentially be drawn later. This process ensures that at each step i, every available element has an equal chance of being selected for position i. Over k steps, this builds up a sample where every possible group of k had an equal shot.\nLet\u0026rsquo;s try a different random sequence for k=2, A=['a','b','c','d']:\ni=0: n=4. Pick r from [0, 1, 2, 3]. Say r=2. Swap A[0](\u0026lsquo;a\u0026rsquo;) and A[2](\u0026lsquo;c\u0026rsquo;). A = ['c', 'b', 'a', 'd']. A[0] is fairly chosen. i=1: n=4. Pick r from [1, 2, 3]. Say r=3. Swap A[1](\u0026lsquo;b\u0026rsquo;) and A[3](\u0026rsquo;d\u0026rsquo;). A = ['c', 'd', 'a', 'b']. A[1] is fairly chosen from the rest. Stop. k=2. Sample is A[0..1] = ['c', 'd']. Think about filling slots 0 to k-1. For slot i, you randomly pick one element from all elements currently at or after index i and swap it into place.\nSection 5.12: Sample Offline Data (Page 18 / PDF Page 54) The Scenario: Imagine you have a large dataset (like all your customers, or all sensor readings from a day) stored in an array A. You want to select a smaller, random sample of size k from this dataset (e.g., to send a survey to, or to analyze more closely). The key requirement is that every possible subset of size k must have an equal chance of being selected. This is called uniform random sampling. \u0026ldquo;Offline\u0026rdquo; means you have the entire dataset A available before you start sampling.\nThe Problem: Implement an algorithm that takes an array A of n distinct elements and an integer k (where k \u0026lt;= n). It should rearrange A such that the first k elements (A[0] to A[k-1]) hold a uniformly random subset of the original elements of size k.\nExample:\nInput: A = [3, 7, 5, 11], k = 3 Possible outputs (in the first k=3 slots): [3, 7, 5], [3, 5, 11], [7, 3, 11], [11, 5, 7], etc. Each combination of 3 elements should appear with probability 1/(4 choose 3) = 1/4. The order within the first k elements doesn\u0026rsquo;t strictly matter for the subset definition, but the algorithm naturally produces a random permutation within the subset. Why Not Just Pick Random Indices? If you just pick k random indices between 0 and n-1, you might pick the same index multiple times. If you discard duplicates and retry, the process becomes complex and potentially slow (related to the Coupon Collector\u0026rsquo;s problem), especially if k is close to n.\nThe Efficient In-Place Algorithm (Fisher-Yates Shuffle Adaptation): This algorithm is elegant and efficient. It builds the random subset in the first k positions of the array iteratively.\nAlgorithm:\nIterate with an index i from 0 up to k-1. This loop runs exactly k times. In each iteration i: We need to select a random element from the portion of the array that hasn\u0026rsquo;t been considered for the sample yet. This \u0026ldquo;unseen\u0026rdquo; portion is from index i to n-1 (inclusive). Generate a random integer r such that i \u0026lt;= r \u0026lt; n. (Pick a random index from the current position i to the end). Swap the element at the current index i with the element at the randomly chosen index r: A[i], A[r] = A[r], A[i]. After the loop finishes (after k swaps), the subarray A[0...k-1] contains the uniformly random sample of size k. Why does this work? (Intuition)\nIteration i=0: We randomly pick an index r from 0 to n-1. We swap A[0] with A[r]. Now A[0] holds a uniformly random element chosen from the entire original array. Iteration i=1: We randomly pick an index r from 1 to n-1. This picks uniformly from all elements except the one we already placed at A[0]. We swap A[1] with A[r]. Now A[1] holds a uniformly random element chosen from the remaining n-1 elements. A[0] and A[1] together now form a random pair. Iteration i: We pick a random index r from i to n-1. This selects uniformly from the n-i elements that haven\u0026rsquo;t yet been fixed into the sample positions 0 to i-1. We swap A[i] with A[r]. This places a uniformly chosen element (from the remaining available ones) into position i. By induction, after k steps, A[0...k-1] holds a uniformly chosen subset of size k. Each element had an equal chance of ending up in each position of the sample.\nSection 5.13: Sample Online Data (Page 19 / PDF Page 55) The Scenario: Now imagine data isn\u0026rsquo;t available all at once (\u0026ldquo;offline\u0026rdquo;). Instead, you\u0026rsquo;re receiving a stream of data items (like packets in a network session, or log entries as they happen). You don\u0026rsquo;t know how many items will be in the stream in total beforehand. You want to maintain a random sample of size k from the items seen so far. As each new item arrives, you update your sample. The sample must always be a uniformly random subset of the items processed up to that point.\nThe Problem: Design an algorithm that takes an input size k and reads a sequence of data items (an iterator or stream). It should continuously maintain a subset of size k of the items read so far. This subset must be a uniformly random sample.\nWhy this is Tricky:\nYou don\u0026rsquo;t know n (the total number of items) in advance. When the (m+1)-th item arrives, you need to decide if it should be in your sample of size k (replacing one of the existing k items) or if it should be discarded. This decision must ensure that after processing m+1 items, any k items from the m+1 seen so far have an equal chance of being in your sample. The Algorithm: Reservoir Sampling This is a classic algorithm for this problem.\nAlgorithm:\nInitialization: Read the first k items from the stream and store them directly into your sample (let\u0026rsquo;s call it sample_array of size k). At this point, sample_array is trivially a uniform random sample of the first k items. Process Subsequent Items: For each subsequent item x that arrives (from item k+1 onwards): Let num_items_seen_so_far be the total number of items processed so far (including the current item x). So, when processing the (k+1)-th item, num_items_seen_so_far = k+1. When processing the (k+2)-th, num_items_seen_so_far = k+2, and so on. Generate a random integer r between 0 and num_items_seen_so_far - 1 (inclusive). (Or 1 to num_items_seen_so_far and adjust). Decision: If r \u0026lt; k (i.e., if the random number falls within the range 0 to k-1), then replace the element sample_array[r] with the current item x. If r \u0026gt;= k, do nothing; discard the current item x. After processing all items in the stream, sample_array will hold a uniformly random sample of size k from the entire stream. Why does this work? (Proof by Induction - High Level)\nBase Case: After seeing k items, the sample_array contains all k items. This is a uniform random sample (the only possible sample of size k from k items).\nInductive Hypothesis: Assume that after seeing m items (where m \u0026gt;= k), the sample_array (of size k) is a uniformly random sample of size k from those m items. This means any specific item y from the first m items has a probability of k/m of being in the sample_array.\nInductive Step: Now, the (m+1)-th item, let\u0026rsquo;s call it x_m+1, arrives.\nWhat is the probability that x_m+1 enters the sample? It enters if we pick a random number r (from 0 to m) and r \u0026lt; k. The probability of this is k / (m+1). What is the probability that an old item y (which was among the first m items and was in the sample) remains in the sample? Prob(y was in sample after m items) = k/m. Prob(y is not kicked out when x_m+1 arrives) = Prob(x_m+1 is not chosen to replace anything) + Prob(x_m+1 is chosen, but y is not the one picked for replacement). This is (1 - k/(m+1)) (if x_m+1 doesn\u0026rsquo;t replace anything) + (k/(m+1)) * ((k-1)/k) (if x_m+1 replaces, but it replaces one of the other k-1 items). = (m+1-k)/(m+1) + (k-1)/(m+1) = (m+1-k+k-1)/(m+1) = m/(m+1). So, Prob(y remains in sample) = (Prob(y was in sample)) * (Prob(y not kicked out)) = (k/m) * (m/(m+1)) = k/(m+1). Conclusion: After m+1 items, both the new item x_m+1 and any old item y (that was previously in the sample) now have a probability of k/(m+1) of being in the sample. This holds true for all items, thus maintaining the uniform random property.\nsampling Algorithms Recap Recall the Offline Sampling (Fisher-Yates) Logic: At step i (filling the i-th slot of our sample, 0-indexed), we do:\nr = random.randrange(i, n) A[i], A[r] = A[r], A[i] The key principle ensuring fairness here is: At step i, we choose uniformly from all items not yet placed in the sample (those in positions i through n-1) and place the chosen item into position i.\nNow, the Online/Reservoir Sampling Challenge: When the m-th item arrives (m \u0026gt; k), we don\u0026rsquo;t know n. We cannot simply pick a random index r from i to n-1. We only know the items seen so far (m items). We have a reservoir R of size k which (by induction) is a fair sample of the first m-1 items.\nGoal: Update the reservoir R using the m-th item (x_m) such that R becomes a fair sample of all m items. Fairness means every item x_1, ..., x_m must have a k/m probability of being in R.\nReservoir Sampling Step for item m:\nGenerate r = random.randrange(m) (an index from 0 to m-1). if r \u0026lt; k: R[r] = x_m (replace element at index r in the reservoir with the new item). How does this achieve the k/m probability goal?\nLet\u0026rsquo;s connect this to the \u0026ldquo;drawing from a hat\u0026rdquo; idea.\nImagine a Hypothetical Hat: Conceptually, after m-1 items, our reservoir R holds k items drawn fairly from a hat containing the first m-1 items. Item m Arrives: It\u0026rsquo;s like adding the m-th item\u0026rsquo;s name (x_m) to the hat. Now the hat contains m items. Fair Draw Needed: To get a fair sample of size k from this hat of m items, each item in the hat should have a k/m chance of being selected if we were to redraw completely. Can we achieve this without redrawing?\nLet\u0026rsquo;s analyze the chances for the new item x_m:\nThe algorithm gives x_m a chance to get into the reservoir if r \u0026lt; k, where r is chosen from 0 to m-1. There are k \u0026ldquo;winning\u0026rdquo; values for r (0 to k-1) out of m total possibilities. So, Prob(x_m enters the reservoir) = k/m. This is exactly the fairness probability we need for the new item! Now, let\u0026rsquo;s analyze the chances for an old item Y that was already in the reservoir R before x_m arrived.\nBy our assumption (inductive hypothesis), Y was in the reservoir with probability k / (m-1) after m-1 items.\nFor Y to still be in the reservoir after item m is processed, one of two things must happen:\nThe random number r was not less than k (i.e., r \u0026gt;= k). This happens with probability (m-k)/m. In this case, x_m is ignored, and Y definitely stays. The random number r was less than k (prob k/m), AND the slot chosen for replacement was not Y\u0026rsquo;s slot. Since r is chosen uniformly from 0 to k-1 in this case to be the replacement index, the chance that Y\u0026rsquo;s specific slot is not picked is (k-1)/k. So, the probability of this combined event is (k/m) * (k-1)/k = (k-1)/m. Total probability Y stays in the reservoir = Prob(Case 1) + Prob(Case 2) = (m-k)/m + (k-1)/m = (m-1)/m.\nNow, the final probability that Y is in the reservoir after processing item m is: Prob(Y was in after m-1) * Prob(Y stays when m arrives) = [k / (m-1)] * [(m-1) / m] = k/m.\nThe Connection: Both algorithms achieve fairness, but through different mechanisms constrained by what information is available:\nOffline (Fisher-Yates): Knows n. Can directly simulate picking from the remaining available items at each step i by choosing a random index r in the future part of the array (i to n-1) and swapping into position i. It ensures fairness by giving every available item an equal chance (1/(n-i)) to be put into slot i. Online (Reservoir): Doesn\u0026rsquo;t know n. Cannot pick from future indices. Instead, it uses probabilities based on the count m seen so far. It gives the new item x_m the exact target probability k/m of being included. If included, the random replacement maintains the correct probabilities for the older items, ensuring they also end up with probability k/m. It achieves the same final probability distribution as Offline Sampling, but through an incremental, probabilistic update rather than direct selection and placement. Think of Reservoir Sampling as cleverly calculating how often the new item should replace an old one to maintain the correct k/m probability for everyone in the long run, even without knowing the total number of items m will eventually reach.\nSection 5.15: Compute a Random Subset (Page 21 / PDF Page 57) The Problem: Given a positive integer n and a size k \u0026lt;= n, return a size-k subset of the set {0, 1, 2, ..., n-1}. The subset should be represented as an array. All (n choose k) possible subsets should be equally likely. Additionally, all permutations of elements within the returned array should be equally likely (though this is often a natural byproduct of algorithms that produce uniform subsets).\nHow is this different from Offline Sampling (Section 5.12)?\nOffline Sampling (5.12): Input is an existing array A. Output is a modification of A where A[0...k-1] holds the sample. It operates on given elements. This Problem (5.15): Input is just integers n and k. The \u0026ldquo;elements\u0026rdquo; are implicitly the integers from 0 to n-1. We need to construct an array containing a random subset of these integers. Solution Approach 1: Mimic Offline Sampling (Fisher-Yates) Create an array elements = [0, 1, ..., n-1]. Apply the Offline Sampling algorithm (from Section 5.12, also known as Fisher-Yates shuffle) to this elements array to pick k items. Return the first k elements of the shuffled elements array (elements[0...k-1]). Complexity: Time: O(n) to create the initial elements array + O(k) for sampling = O(n). Space: O(n) for the elements array. Can we do better if k is much smaller than n? (e.g., sample 3 items from 1 million) Creating an array of 1 million elements just to pick 3 seems wasteful.\nSolution Approach 2: Using a Hash Table (for k \u0026laquo; n) This approach avoids creating the full array of n elements if k is small. It directly simulates the Fisher-Yates logic but only keeps track of the elements that would have been moved from their original positions.\nThe Idea: Imagine you\u0026rsquo;re running the Fisher-Yates shuffle on the conceptual array [0, 1, ..., n-1].\nThe array initially has A[j] = j. When you iterate for the i-th element of your sample (from i=0 to k-1): You pick a random index rand_idx from i to n-1. You swap A[i] and A[rand_idx]. If k is small, most of the conceptual array A will remain A[j]=j. Only a few elements get swapped. We can use a hash table (dictionary in Python) to store only those elements whose values are not equal to their indices (i.e., the elements that have been moved). Algorithm with Hash Table:\nInitialize an empty dictionary changed_elements. This dictionary will store index: value pairs for any position where the value is different from what it would be in the identity permutation [0, 1, ..., n-1] after our simulated swaps. Iterate i from 0 to k-1 (to select k elements for our subset). Generate a random index rand_idx in the range [i, n-1]. Get current values: value_at_i = changed_elements.get(i, i) (If i is in the dictionary, use its stored value; otherwise, its value is i). value_at_rand_idx = changed_elements.get(rand_idx, rand_idx) (Similarly for rand_idx). Perform the swap (conceptually, updating the dictionary): Store value_at_i at rand_idx: changed_elements[rand_idx] = value_at_i. Store value_at_rand_idx at i: changed_elements[i] = value_at_rand_idx. After the loop, the desired subset consists of the values that are conceptually in A[0]...A[k-1]. We retrieve these from our changed_elements dictionary (defaulting to i if i is not a key). result_subset = [changed_elements.get(j, j) for j in range(k)]. When is the Hash Table approach better?\nWhen k is significantly smaller than n.\nFisher-Yates on full array (Approach 1): Time: O(n) Space: O(n) (for the initial array [0, ..., n-1]) Hash Table method (Approach 2): Time: O(k) (k iterations, each with dictionary operations which are O(1) on average) Space: O(k) (at most 2k entries in the dictionary if i and rand_idx are always different and neither was previously in the dictionary) So, if k \u0026lt;\u0026lt; n (k is much much less than n), the hash table method wins on both time and space by avoiding the creation and full scan of the n-element array.\nIf k is close to n (e.g., k = n/2 or k = n-1), the O(n) approach (Approach 1) is fine and might even be slightly faster in practice due to less overhead compared to dictionary get/set operations, even though their asymptotic complexities might suggest otherwise for k ~ O(n). The constant factors matter.\nSection 5.16: Generate Nonuniform Random Numbers (Page 22 / PDF Page 58) The Problem: You are given:\nA list of n numbers (values): T = [t_0, t_1, ..., t_{n-1}]. A list of n corresponding probabilities: P = [p_0, p_1, ..., p_{n-1}]. These probabilities sum up to 1. You need to write a function that generates one of the numbers from T according to its specified probability p_i. You have access to a standard uniform random number generator that produces values in the range [0.0, 1.0) (i.e., 0 inclusive, 1 exclusive).\nExample:\nNumbers T = [3, 5, 7, 11] Probabilities P = [9/18, 6/18, 2/18, 1/18] (which are [0.5, 0.333..., 0.111..., 0.055...]) If you call your function many times:\nAbout 50% of the time, it should return 3. About 33.3% of the time, it should return 5. About 11.1% of the time, it should return 7. About 5.5% of the time, it should return 11. The Core Idea: Mapping the [0,1) Interval The uniform random number generator gives us a value r in [0.0, 1.0). We need to divide this [0.0, 1.0) interval into n segments, where the length of each segment corresponds to the probability of picking the associated number.\nSegment 0: Length p_0. Corresponds to t_0. Range [0, p_0). Segment 1: Length p_1. Corresponds to t_1. Range [p_0, p_0 + p_1). Segment 2: Length p_2. Corresponds to t_2. Range [p_0 + p_1, p_0 + p_1 + p_2). \u0026hellip;and so on. Segment n-1: Length p_{n-1}. Corresponds to t_{n-1}. Range [sum(p_0 to p_{n-2}), sum(p_0 to p_{n-1}) = 1.0) Analogy: A Dartboard with Unequal Slices Imagine you have a circular dartboard. Instead of all slices being equal (like in a standard dartboard), the slices have different sizes.\nYou have n numbers you want to pick from: t_0, t_1, t_2, ..., t_{n-1}. Each number t_i has a probability p_i of being picked. The slice on the dartboard for t_0 takes up p_0 fraction of the dartboard\u0026rsquo;s area. The slice for t_1 takes up p_1 fraction of the area. And so on. All p_i fractions add up to 1 (the whole dartboard). Example:\nNumbers T = [Red, Green, Blue] Probabilities P = [0.5 (for Red), 0.3 (for Green), 0.2 (for Blue)] Red slice covers 50% of the dartboard. Green slice covers 30%. Blue slice covers 20%. How to Pick Fairly?\nThrow a dart completely randomly at the dartboard. \u0026ldquo;Completely randomly\u0026rdquo; means any point on the board is equally likely to be hit. See which slice your dart lands in. That\u0026rsquo;s the number you pick. Since the Red slice is bigger, you\u0026rsquo;re more likely to hit Red. This simulates the probabilities. Connecting to random.random() which gives a number in [0.0, 1.0) Instead of a circular dartboard, let\u0026rsquo;s imagine a straight line segment of length 1. Think of it like a ruler from 0.0 to 1.0. We\u0026rsquo;re going to divide this ruler into sections, where the length of each section is proportional to the probability.\nFor T = [Red, Green, Blue] and P = [0.5, 0.3, 0.2]:\nSection for Red: Length 0.5. It goes from 0.0 up to 0.5. (Range: [0.0, 0.5)) Section for Green: Length 0.3. It starts where Red left off, so it goes from 0.5 up to 0.5 + 0.3 = 0.8. (Range: [0.5, 0.8)) Section for Blue: Length 0.2. It starts where Green left off, so it goes from 0.8 up to 0.8 + 0.2 = 1.0. (Range: [0.8, 1.0)) Visually:\n0.0 0.5 0.8 1.0 \u0026lt;\u0026mdash; Red \u0026mdash;\u0026gt; \u0026lt;\u0026ndash; Green \u0026ndash;\u0026gt; \u0026lt;\u0026ndash; Blue \u0026ndash;\u0026gt; (length 0.5) (length 0.3) (length 0.2) The Algorithm using this \u0026ldquo;Ruler\u0026rdquo;: Prepare the Ruler Divisions (these are the \u0026ldquo;cumulative probabilities\u0026rdquo;):\nRed ends at: 0.5 Green ends at: 0.5 + 0.3 = 0.8 Blue ends at: 0.5 + 0.3 + 0.2 = 1.0 Let\u0026rsquo;s call these end-points: Endpoints = [0.5, 0.8, 1.0] \u0026ldquo;Throw a Dart\u0026rdquo;: Get a random number from your computer, rand_val = random.random(). This rand_val will be somewhere between 0.0 and 1.0 (but not including 1.0).\nSee Which Section it Lands In:\nIf 0.0 \u0026lt;= rand_val \u0026lt; Endpoints[0] (i.e., rand_val \u0026lt; 0.5), it landed in Red\u0026rsquo;s section. Output: Red. Else if Endpoints[0] \u0026lt;= rand_val \u0026lt; Endpoints[1] (i.e., 0.5 \u0026lt;= rand_val \u0026lt; 0.8), it landed in Green\u0026rsquo;s section. Output: Green. Else if Endpoints[1] \u0026lt;= rand_val \u0026lt; Endpoints[2] (i.e., 0.8 \u0026lt;= rand_val \u0026lt; 1.0), it landed in Blue\u0026rsquo;s section. Output: Blue. Why does this work? Because the length of each section on our \u0026ldquo;ruler\u0026rdquo; is exactly its probability, a uniformly random rand_val is proportionally more likely to fall into longer sections.\nExample from the Book:\nNumbers T = [3, 5, 7, 11] Probabilities P = [9/18, 6/18, 2/18, 1/18] P_decimal = [0.5, 0.333..., 0.111..., 0.055...] Ruler Divisions (Cumulative Probabilities):\ndiv[0] (for number 3): 0.5 div[1] (for number 5): 0.5 + 0.333... = 0.833... div[2] (for number 7): 0.833... + 0.111... = 0.944... div[3] (for number 11): 0.944... + 0.055... = 1.0 So, Endpoints = [0.5, 0.833..., 0.944..., 1.0] Throw a Dart: rand_val = random.random(). Let\u0026rsquo;s say rand_val = 0.75.\nSee Where it Lands:\nIs 0.75 \u0026lt; Endpoints[0] (0.5)? No. Is 0.75 \u0026lt; Endpoints[1] (0.833\u0026hellip;)? Yes. Aha! It landed between Endpoints[0] and Endpoints[1]. This corresponds to the second number in our original list T, which is 5. So, we output 5. What if rand_val = 0.2?\nIs 0.2 \u0026lt; Endpoints[0] (0.5)? Yes. It landed before the first endpoint. This corresponds to the first number, 3. Output 3. The \u0026ldquo;Binary Search\u0026rdquo; Part (e.g., bisect_left) When we have many sections (many numbers t_i), checking rand_val against each endpoint one by one (if rand_val \u0026lt; E[0]\u0026hellip; else if rand_val \u0026lt; E[1]\u0026hellip;) is slow (O(n)). Since the Endpoints array [0.5, 0.833, 0.944, 1.0] is sorted, we can use binary search to quickly find which \u0026ldquo;bucket\u0026rdquo; rand_val falls into. bisect_left(Endpoints, rand_val) efficiently finds the index idx such that all Endpoints[j] for j \u0026lt; idx are less than rand_val, and Endpoints[idx] is the first endpoint that is greater than or equal to rand_val. This idx is precisely the index of the chosen t_i in our original list T.\nSummary in Simple Terms: Line up all your probabilities end-to-end on a ruler of length 1. Mark where each probability \u0026ldquo;ends\u0026rdquo;. Generate a random point on that ruler (a random number between 0 and 1). See which probability\u0026rsquo;s segment your random point landed in. Pick the number associated with that segment. (Binary search is just a fast way to do step 3 if you have lots of segments). Section 5.17: The Sudoku Checker Problem (Page 24 / PDF Page 60) The Problem: You are given a 9x9 2D array (a grid) representing a partially or fully completed Sudoku puzzle. The grid contains integers from 0 to 9. A value of 0 indicates an empty cell. Values from 1 to 9 are filled-in digits.\nYour task is to determine if the current state of the Sudoku grid is valid according to Sudoku rules:\nEach row must contain unique digits from 1 to 9 (ignoring 0s). Each column must contain unique digits from 1 to 9 (ignoring 0s). Each of the nine 3x3 subgrids (also called \u0026ldquo;boxes\u0026rdquo; or \u0026ldquo;regions\u0026rdquo;) must contain unique digits from 1 to 9 (ignoring 0s). The problem asks only to check for validity (no duplicate 1-9 digits in any row, column, or 3x3 box). It does not ask you to solve the Sudoku.\nExample:\nFigure 5.2(a) on page 60 is a partially filled valid Sudoku. Figure 5.2(b) is a complete, valid solution. An invalid example: A row [5, 3, 4, 6, 7, 8, 9, 1, 5] is invalid because 5 appears twice. Core Implementation Idea (Directly Testing Constraints): The most straightforward approach is to check each of the three types of constraints one by one. For checking uniqueness within a row, column, or 3x3 box, a common technique is to use a helper data structure (like a hash set or a boolean array) to keep track of the digits seen so far within that unit.\nAlgorithm Outline:\nFunction to Check a Unit (Row, Column, or Box):\nCreate a helper function, say has_duplicate(unit_array), that takes a 1D array (representing a row, column, or flattened 3x3 box) and returns True if it contains duplicate digits from 1-9, False otherwise. Inside has_duplicate(unit_array): Initialize a seen_digits = set() (or a boolean array seen = [False]*10). Iterate through each digit in unit_array: If digit == 0, ignore it (empty cell). If digit != 0: If digit is already in seen_digits (or seen[digit] is True), then a duplicate is found. Return True immediately. Otherwise, add digit to seen_digits (or set seen[digit] = True). If the loop finishes without finding duplicates, return False. Check Row Constraints:\nIterate i from 0 to 8 (for each row). Extract row i from the Sudoku grid: row_data = grid[i]. If has_duplicate(row_data) is True, then the Sudoku is invalid. Return False. Check Column Constraints:\nIterate j from 0 to 8 (for each column). Extract column j: col_data = [grid[i][j] for i in range(9)]. If has_duplicate(col_data) is True, then the Sudoku is invalid. Return False. Check 3x3 Subgrid Constraints:\nIterate through the 9 subgrids. A common way is to use nested loops for the top-left corner of each subgrid: Iterate box_row_start from 0 to 6, with a step of 3 (i.e., 0, 3, 6). Iterate box_col_start from 0 to 6, with a step of 3 (i.e., 0, 3, 6). For each (box_row_start, box_col_start): (The next step would be to extract the 3x3 subgrid data and call has_duplicate on it. If it returns True, the Sudoku is invalid and we return False from the main function.) All Checks Passed: If all the above checks pass without returning False, then the Sudoku grid is valid. Return True.\nSimplified Implementation Steps:\nHelper function is_unit_valid(list_of_9_cells):\nTakes a list (a row, a column, or the 9 cells of a box). Uses a seen_numbers = set(). For each cell in list_of_9_cells: If cell == 0, continue. If cell is in seen_numbers, return False (duplicate found!). Add cell to seen_numbers. If loop finishes, return True (no duplicates). Main function is_sudoku_valid(grid):\nRows: For each row r from 0 to 8: If not is_unit_valid(grid[r]), then return False. Columns: For each column c from 0 to 8: Create column_list = [grid[r][c] for r in range(9)]. If not is_unit_valid(column_list), then return False. Boxes: For each box (e.g., box_row_start from 0,3,6 and box_col_start from 0,3,6): Create box_list by picking the 9 cells. If not is_unit_valid(box_list), then return False. If you get through all of this, return True. Section 5.18: Compute the Spiral Ordering of a 2D Array (Page 25 / PDF Page 61) What: Given an n x n 2D array, return a 1D list of its elements in \u0026ldquo;spiral order\u0026rdquo; (outside-in, clockwise).\nWhy (Interview): This is a common 2D array traversal problem. It tests:\nAbility to manage 2D array indices carefully. Handling boundary conditions. Devising a systematic way to cover all elements without repetition. State management (current position, current direction, visited cells). Underlying Pattern: Systematic Traversal with State Management. The two main patterns are \u0026ldquo;Layer Peeling\u0026rdquo; or \u0026ldquo;Path Simulation.\u0026rdquo;\nSimplified Implementation Intuition (Simulation Approach - \u0026ldquo;Robot Walk\u0026rdquo;): Imagine a robot starting at matrix[0][0]. It needs to walk in a spiral and report the numbers it steps on.\nSetup:\nresult_list = [] (to store the spiral numbers). current_row = 0, current_col = 0. directions = [[0, 1], [1, 0], [0, -1], [-1, 0]] (These are [row_change, col_change] for Right, Down, Left, Up). current_direction_idx = 0 (Start by going Right). num_elements = n * n. A way to mark visited cells: Let\u0026rsquo;s assume we can modify the input matrix. We\u0026rsquo;ll set matrix[row][col] = None (or some other special marker) after visiting. (If not, use a separate visited[n][n] boolean array). The Walk (Loop num_elements times):\nCollect: Add matrix[current_row][current_col] to result_list. Mark: matrix[current_row][current_col] = None (or visited[current_row][current_col] = True). Try to move in current direction: next_row_change, next_col_change = directions[current_direction_idx]. potential_next_row = current_row + next_row_change. potential_next_col = current_col + next_col_change. Check if turn is needed: Is (potential_next_row, potential_next_col) OFF the grid (e.g., potential_next_row \u0026lt; 0 or \u0026gt;= n, or potential_next_col \u0026lt; 0 or \u0026gt;= n)? OR, is matrix[potential_next_row][potential_next_col] ALREADY visited (e.g., == None)? If YES (turn needed): Change direction: current_direction_idx = (current_direction_idx + 1) % 4 (turn clockwise). Recalculate actual next step with new direction: next_row_change, next_col_change = directions[current_direction_idx]. current_row += next_row_change. current_col += next_col_change. If NO (can continue in current direction): current_row = potential_next_row. current_col = potential_next_col. Return result_list. Example Trace Snippet (3x3): [[1,2,3],[4,5,6],[7,8,9]] res=[], (r,c)=(0,0), dir=0 (Right)\nAdd 1. res=[1]. matrix[0][0]=None. Try (0,1) (Right). Valid. (r,c)=(0,1). Add 2. res=[1,2]. matrix[0][1]=None. Try (0,2) (Right). Valid. (r,c)=(0,2). Add 3. res=[1,2,3]. matrix[0][2]=None. Try (0,3) (Right). INVALID (off grid). Turn! dir=(0+1)%4 = 1 (Down). New try from (0,2) with Down: (1,2). Valid. (r,c)=(1,2). Add 6. res=[1,2,3,6]. matrix[1][2]=None. Try (2,2) (Down). Valid. (r,c)=(2,2). Add 9. res=[1,2,3,6,9]. matrix[2][2]=None. Try (3,2) (Down). INVALID (off grid). Turn! dir=(1+1)%4 = 2 (Left). New try from (2,2) with Left: (2,1). Valid. (r,c)=(2,1). \u0026hellip;and so on. Managing Direction: We need two things:\nA way to represent the current direction. A way to change the direction systematically (usually clockwise). 1. Representing Direction: We can assign numbers to directions:\n0: Right 1: Down 2: Left 3: Up We store the robot\u0026rsquo;s current direction in a variable, say current_direction_idx (initialized to 0 for Right). 2. Changing Direction (Turning Clockwise): When the robot hits a wall or an already visited cell, it needs to turn right (clockwise).\nIf current direction is Right (0), next is Down (1). If current direction is Down (1), next is Left (2). If current direction is Left (2), next is Up (3). If current direction is Up (3), next is Right (0). This pattern can be achieved with a simple calculation: current_direction_idx = (current_direction_idx + 1) % 4 (% 4 (modulo 4) ensures that if current_direction_idx becomes 4, it wraps around back to 0). (0 + 1) % 4 = 1 (1 + 1) % 4 = 2 (2 + 1) % 4 = 3 (3 + 1) % 4 = 0 3. How Direction Affects Movement (Delta Row, Delta Column): Once we know the current_direction_idx, how do we know how to change the row and col to move one step? We can use a lookup table (an array or list of lists/tuples) that stores the change in row (dr) and change in column (dc) for each direction index.\nLet SHIFT = [[0, 1], # Direction 0 (Right): row doesn't change, col increases by 1 [1, 0], # Direction 1 (Down): row increases by 1, col doesn't change [0, -1], # Direction 2 (Left): row doesn't change, col decreases by 1 [-1, 0]] # Direction 3 (Up): row decreases by 1, col doesn't change\nWhen the robot wants to move:\nGet the changes for the current direction: dr = SHIFT[current_direction_idx][0] dc = SHIFT[current_direction_idx][1] Calculate the potential next position: potential_next_row = current_row + dr potential_next_col = current_col + dc Section 5.19: Rotate a 2D Array (Page 28 / PDF Page 64) The Problem: You are given an n x n 2D array (a square matrix). You need to rotate this matrix by 90 degrees clockwise, in-place (O(1) additional space).\nExample (from Figure 5.4):\nInput Matrix A:\n[ [ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [13, 14, 15, 16] ] Output Matrix A (after rotation):\n[ [13, 9, 5, 1], [14, 10, 6, 2], [15, 11, 7, 3], [16, 12, 8, 4] ] Observations / How Elements Move:\nThe first row of the original ([1,2,3,4]) becomes the last column of the rotated, read from top to bottom ([1,2,3,4]). The last column of the original ([4,8,12,16]) becomes the first row of the rotated, but reversed (or the last row of the transposed, reversed matrix - it\u0026rsquo;s easier to think in terms of layers). More precisely: A[row][col] moves to A[col][n-1-row]. Approach 1: Using Extra Space (Not In-Place, for understanding) Create a new n x n matrix, say rotated_matrix. For each element A[row][col] in the original matrix: The new position in rotated_matrix will be rotated_matrix[col][n - 1 - row]. rotated_matrix[col][n - 1 - row] = A[row][col] Copy rotated_matrix back to A. Time: O(N2) Space: O(N2) for rotated_matrix. Fails the in-place requirement. Approach 2: In-Place Rotation by Layers This is the standard and efficient way to do it in-place.\nKey Idea: The rotation can be performed layer by layer, from the outermost layer inwards. Within each layer, elements move in groups of four.\nConsider the outermost layer of the 4x4 example:\nA[0][0] (1) moves to where A[0][3] (4) was. A[0][3] (4) moves to where A[3][3] (16) was. A[3][3] (16) moves to where A[3][0] (13) was. A[3][0] (13) moves to where A[0][0] (1) was. This is a 4-way swap for the corners: (1) -\u0026gt; (4) -\u0026gt; (16) -\u0026gt; (13) -\u0026gt; (1) Original positions: A[0][0], A[0][3], A[3][3], A[3][0] After 90-deg clockwise rotation, their new conceptual positions are:\nThe value originally at A[3][0] (13) moves to A[0][0]. The value originally at A[0][0] (1) moves to A[0][3]. The value originally at A[0][3] (4) moves to A[3][3]. The value originally at A[3][3] (16) moves to A[3][0]. So, the values move like this in a cycle (let TL=TopLeft, TR=TopRight, BR=BottomRight, BL=BottomLeft for a specific group of 4):\ntemp = value_at_TL value_at_TL = value_at_BL value_at_BL = value_at_BR value_at_BR = value_at_TR value_at_TR = temp Algorithm for In-Place Layer Rotation:\nThe number of layers is n // 2. Iterate through layers, from layer = 0 to n // 2 - 1. For each layer: Define first = layer (first row/col index of this layer). Define last = n - 1 - layer (last row/col index of this layer). Now, iterate through the elements within this layer\u0026rsquo;s top row (excluding the last one which is handled as a corner of another 4-element cycle). Iterate i from first to last - 1. An offset = i - first. This offset helps identify the corresponding elements in the other three sides of the current square within the layer. For each i (or offset), perform a 4-way swap: Save top-left element: top = matrix[first][i] (which is matrix[first][first + offset]) Left element moves to Top-Left: matrix[first][i] = matrix[last - offset][first] Bottom element moves to Left: matrix[last - offset][first] = matrix[last][last - offset] Right element moves to Bottom: matrix[last][last - offset] = matrix[i][last] (which is matrix[first + offset][last]) Saved Top element moves to Right: matrix[i][last] = top Section 5.20: Compute Rows in Pascal\u0026rsquo;s Triangle (Page 29-30 / PDF Page 65-66 in your full book)\nThe Problem: You are asked to generate the first n rows of Pascal\u0026rsquo;s Triangle. Pascal\u0026rsquo;s Triangle has a specific structure:\nThe first row (Row 0 conceptually) is just [1]. Each subsequent row has one more element than the previous row. The first and last element of every row is 1. Every other element in a row is the sum of the two elements directly above it (to its left and right) in the previous row. Figure 5.5 Example (First 5 rows):\nRow 0: 1 Row 1: 1 1 Row 2: 1 2 1 Row 3: 1 3 3 1 Row 4: 1 4 6 4 1 Input: A non-negative integer n (number of rows to generate). Output: A list of lists, where each inner list represents a row of Pascal\u0026rsquo;s Triangle.\nExample: If n = 5, output should be:\n[ [1], [1, 1], [1, 2, 1], [1, 3, 3, 1], [1, 4, 6, 4, 1] ] The Core Rule (from the problem description): \u0026ldquo;the j-th entry in the i-th row is 1 if j = 0 or j = i, otherwise it is the sum of the (j - 1)-th and j-th entries in the (i - 1)-th row.\u0026rdquo; (Assuming 0-indexed rows i and 0-indexed entries j within a row).\nLet Triangle[i][j] be the element at row i, column j.\nTriangle[i][0] = 1 Triangle[i][i] = 1 (since row i has i+1 elements, so index i is the last one) Triangle[i][j] = Triangle[i-1][j-1] + Triangle[i-1][j] for 0 \u0026lt; j \u0026lt; i. Implementation Idea (Building Row by Row):\nInitialize an empty list pascal_triangle_result = []. Iterate i from 0 to n-1 (to generate n rows, which will be Row 0 to Row n-1). For each i, create a current_row list. Row i will have i+1 elements. The first element of current_row (current_row[0]) is always 1. The last element of current_row (current_row[i]) is always 1 (if i \u0026gt; 0). For the elements in between (from j=1 to j=i-1): current_row[j] = pascal_triangle_result[i-1][j-1] + pascal_triangle_result[i-1][j]. This means we need access to the previous row (pascal_triangle_result[i-1]) that we already computed and stored. Add current_row to pascal_triangle_result. Return pascal_triangle_result. The Code Explained (generate_pascal_triangle - from the book):\ndef generate_pascal_triangle(n): if n == 0: return [] # Initialize the result list. # The book\u0026#39;s list comprehension initializes with all 1s, which is clever # because the first and last elements of each row are always 1. # Triangle[i] will have i+1 elements. result = [[1] * (i + 1) for i in range(n)] # Example for n=3: result will be [[1], [1,1], [1,1,1]] # Iterate from the third row (index 2) onwards, because Row 0 and Row 1 # are already correctly filled with all 1s by the initialization. for i in range(n): # Iterate through rows # Iterate through elements in the current row \u0026#39;i\u0026#39; # We need to calculate elements from index 1 up to index i-1 # (since index 0 and index i are already 1). for j in range(1, i): # This loop only runs if i \u0026gt;= 2 # Sum of the two elements directly above in the previous row result[i][j] = result[i - 1][j - 1] + result[i - 1][j] return result Let\u0026rsquo;s trace for n=4:\nresult = [[1] * (i + 1) for i in range(4)] result = [[1], [1,1], [1,1,1], [1,1,1,1]]\nOuter loop for i in range(4):\ni = 0: Inner loop for j in range(1, 0) does not run. result[0] is [1]. Correct. i = 1: Inner loop for j in range(1, 1) does not run. result[1] is [1,1]. Correct. i = 2: (Generating Row 2: [1,2,1]) Inner loop for j in range(1, 2): j = 1: result[2][1] = result[2-1][1-1] + result[2-1][1] result[2][1] = result[1][0] + result[1][1] result[2][1] = 1 + 1 = 2. result[2] is now [1, 2, 1]. Correct. (It was [1,1,1], element at index 1 got updated). i = 3: (Generating Row 3: [1,3,3,1]) Inner loop for j in range(1, 3): j = 1: result[3][1] = result[3-1][1-1] + result[3-1][1] result[3][1] = result[2][0] + result[2][1] result[3][1] = 1 + 2 = 3. j = 2: result[3][2] = result[3-1][2-1] + result[3-1][2] result[3][2] = result[2][1] + result[2][2] result[3][2] = 2 + 1 = 3. result[3] is now [1, 3, 3, 1]. Correct. Return result.\nComplexity:\nThe number of elements in Pascal\u0026rsquo;s triangle up to row n-1 is 1 + 2 + ... + n = n*(n+1)/2. Each element is computed in O(1) time (one addition). Time Complexity: O(n^2). Space Complexity: O(n^2) to store the triangle. Variant: Compute only the n-th row of Pascal\u0026rsquo;s triangle using O(n) space. (Here n would mean Row n, e.g., if input is 3, compute Row 3 [1,3,3,1]).\nIdea: To compute Row i, you only need Row i-1. You don\u0026rsquo;t need Row i-2, Row i-3, etc. So, you can keep track of just two rows: previous_row and current_row. Initialize previous_row = [1]. Loop k from 1 up to n (to compute up to Row n): current_row will have k+1 elements. current_row[0] = 1, current_row[k] = 1. For j from 1 to k-1: current_row[j] = previous_row[j-1] + previous_row[j]. Set previous_row = current_row for the next iteration. The final current_row (or previous_row after the loop) is the answer. This takes O(n) iterations to get to the n-th row. Each iteration does O(k) work (where k goes up to n). Total time: O(1+2+...+n) = O(n^2). Space: O(n) because we only store two rows at a time, each of length at most n+1. Further Optimization for O(n) space, O(n^2) time (generating k-th row): You can even do it with a single array of size k+1 by computing the values for the next row in reverse order within that array. If row stores row i-1, to compute row i in place: row[j] = row[j] + row[j-1] for j from i-1 down to 1. The row[0] remains 1. You\u0026rsquo;d need to append a 1 at the end for row[i]. Interview Takeaway:\nTests basic 2D array/list of lists manipulation. Understanding the recursive definition of Pascal\u0026rsquo;s triangle is key. The O(n^2) time and space for generating all n rows is standard. The variant tests if you can optimize space when only the last result is needed, a common DP optimization technique (reducing space by noticing limited dependencies). ","permalink":"https://deepskandpal.github.io/bookshelf/elements-of-programming/chapter-1/","tags":null,"title":"Chapter 5: Arrays"},{"categories":null,"content":"Excellent. Let\u0026rsquo;s move on.\nWe\u0026rsquo;ve established the \u0026ldquo;what\u0026rdquo; – we need reliable, scalable, and maintainable systems. Now we get to the first big \u0026ldquo;how\u0026rdquo;: how do we represent the data? This is Chapter 2: Data Models and Query Languages.\nThis is probably the most important decision you\u0026rsquo;ll make. It profoundly affects not only how you write the software, but how you even think about the problem you\u0026rsquo;re solving. Get this wrong, and you\u0026rsquo;ll be fighting your database for years.\nWhat are we ultimately trying to achieve here? We\u0026rsquo;re trying to choose a structure for our data that fits the access patterns of our application. We want to make common operations easy and fast, without making uncommon (but necessary) operations impossible.\nThink of it like building with LEGOs. You have different types of bricks. If you\u0026rsquo;re building a car, you\u0026rsquo;ll want wheels and axles. If you\u0026rsquo;re building a house, you\u0026rsquo;ll want rectangular bricks and roof pieces. Using the wrong bricks makes the job awkward and the result fragile. Data models are your different types of LEGO bricks.\nThe Big Three: Relational vs. Document vs. Graph For the last 30 years, one model dominated: the Relational Model (SQL). Data is in tables (relations), made of rows (tuples). It\u0026rsquo;s fantastic for structured data and was a huge leap forward because it hid the messy implementation details from developers.\nBut in the last 15 years, two other models have become major players, driven by the needs of big web companies.\nDocument Model (NoSQL): Data is stored in self-contained \u0026ldquo;documents,\u0026rdquo; usually JSON. Graph Model: Data is represented as vertices (nodes) and edges (relationships). Let\u0026rsquo;s compare them.\nRelational Model vs. Document Model This is the most common battleground. Imagine we\u0026rsquo;re building a LinkedIn-style resume.\nIn a Relational (SQL) model, you\u0026rsquo;d normalize the data. You\u0026rsquo;d have a users table, a positions table, an education table, etc. The positions table would have a user_id foreign key pointing back to the users table.\nerDiagram users { int user_id PK varchar first_name varchar last_name } positions { int position_id PK int user_id FK varchar job_title varchar organization } education { int education_id PK int user_id FK varchar school_name } users ||--o{ positions : \u0026#34;has\u0026#34; users ||--o{ education : \u0026#34;has\u0026#34; To get a full resume: You need to perform multiple queries or a complex JOIN across several tables. Relationships: Handles many-to-one and many-to-many relationships beautifully (user_id is a many-to-one link). In a Document model, you\u0026rsquo;d likely store the entire resume as a single JSON document. This is similar to the hierarchical model of IBM\u0026rsquo;s IMS database from the 1960s – history repeats itself.\n{ \u0026#34;user_id\u0026#34;: 251, \u0026#34;first_name\u0026#34;: \u0026#34;Bill\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Gates\u0026#34;, \u0026#34;positions\u0026#34;: [ {\u0026#34;job_title\u0026#34;: \u0026#34;Co-chair\u0026#34;, \u0026#34;organization\u0026#34;: \u0026#34;Gates Foundation\u0026#34;}, {\u0026#34;job_title\u0026#34;: \u0026#34;Co-founder\u0026#34;, \u0026#34;organization\u0026#34;: \u0026#34;Microsoft\u0026#34;} ], \u0026#34;education\u0026#34;: [ {\u0026#34;school_name\u0026#34;: \u0026#34;Harvard University\u0026#34;}, {\u0026#34;school_name\u0026#34;: \u0026#34;Lakeside School\u0026#34;} ] } To get a full resume: You fetch one document. That\u0026rsquo;s it. This is a huge win for locality. The \u0026ldquo;Impedance Mismatch\u0026rdquo;: This JSON structure maps very cleanly to the objects in your application code. You don\u0026rsquo;t need an awkward translation layer (like an ORM) that SQL databases often require. So, which is better? The classic \u0026ldquo;it depends.\u0026rdquo;\nIf your data has a document-like structure (a central object with nested one-to-many relationships, like a resume or a blog post with comments), the document model is a natural fit. Fetching the whole thing at once is efficient. If your data is highly interconnected, with complex many-to-many relationships, the relational model is superior. Joins are its superpower. Case Study: Interconnected Data What if we want to make \u0026ldquo;Microsoft\u0026rdquo; and \u0026ldquo;Harvard University\u0026rdquo; in our resume clickable entities with their own pages, logos, etc.? Now we have a many-to-many relationship. Many people worked at Microsoft; many people went to Harvard.\nIn the document model, this gets awkward.\nYou could store the organization\u0026rsquo;s name as a string, but then if \u0026ldquo;Microsoft\u0026rdquo; rebrands, you have to update millions of resume documents (denormalization). Or, you can store just an organization_id and then emulate a join in your application code: fetch resume -\u0026gt; for each position, fetch organization details. This is slow and shifts complexity from the database to your code. The trend (Post-2024 Era): The lines are blurring.\nTimeless: Pick the model that fits your primary access patterns. Post-2024: Relational databases like PostgreSQL have excellent JSON support. Document databases like MongoDB are adding better join-like capabilities. This is a good thing. It\u0026rsquo;s not \u0026ldquo;SQL vs NoSQL\u0026rdquo; anymore, it\u0026rsquo;s about using the right tool for the job, and increasingly, databases are becoming multi-model. Graph-Like Data Models What if everything is potentially related to everything else?\nSocial networks (who is friends with whom) Web graphs (which pages link to which) Knowledge graphs (connecting entities like people, organizations, products) This is where graph models shine. The model is simple:\nVertices (or nodes): The entities (e.g., people, companies). Edges (or relationships): The connections between them (e.g., (Lucy)-[:MARRIED_TO]-\u0026gt;(Alain), (Alain)-[:WORKS_AT]-\u0026gt;(BigTech)). You can model this in SQL, but it\u0026rsquo;s painful. A query like \u0026ldquo;Find all of my friends\u0026rsquo; friends who live in London\u0026rdquo; requires a variable number of joins, which SQL handles awkwardly with recursive common table expressions (CTEs).\nDeclarative Graph Query Languages like Cypher (Neo4j) or SPARQL (RDF triple stores) are designed for this. A query looks like you\u0026rsquo;re drawing the pattern you want to find.\nCypher example: Find people who emigrated from the US to Europe.\nMATCH (person:Person)-[:BORN_IN]-\u0026gt;(:Location)-[:WITHIN*]-\u0026gt;(us:Location {name:\u0026#39;United States\u0026#39;}), (person)-[:LIVES_IN]-\u0026gt;(:Location)-[:WITHIN*]-\u0026gt;(eu:Location {name:\u0026#39;Europe\u0026#39;}) RETURN person.name The query optimizer figures out the most efficient way to traverse the graph to find this pattern. This is a powerful, declarative way to handle highly connected data.\nKey Takeaways for System Design Interviews When designing a system, your choice of data model is a primary decision. Justify it based on the data\u0026rsquo;s structure and your application\u0026rsquo;s access patterns.\nIdentify the main entities and their relationships.\nIs it structured, like user profiles? Relational is a good default. Is it self-contained documents with nested data? Document model might be simpler and faster. Is it a web of complex relationships? Think Graph. Analyze the access patterns.\nDo you need powerful joins? Relational/SQL. Do you primarily fetch an entire object by its ID? Document. Do you need to traverse relationships (e.g., find friends of friends)? Graph. Consider evolvability.\nHow will the data change? Schemas are not a bad thing; they enforce structure. Schema-on-read (Document DBs) gives flexibility but can lead to a mess if you\u0026rsquo;re not careful. Schema-on-write (SQL) is more rigid but provides guarantees. Modern binary formats like Avro and Protobuf offer a middle ground. Don\u0026rsquo;t just say \u0026ldquo;I\u0026rsquo;ll use MongoDB.\u0026rdquo; Say, \u0026ldquo;The primary data entity is a user profile, which includes their list of recent posts. Since this is a self-contained, document-like structure and our main read pattern is fetching the full profile, a document model is a natural fit. It offers good locality and avoids complex joins for this common operation.\u0026rdquo;\nNow, we\u0026rsquo;ve talked about how to model the data. Next, we need to talk about how a database actually stores and retrieves it. That\u0026rsquo;s Chapter 3. Any questions on data models?\n","permalink":"https://deepskandpal.github.io/bookshelf/design-data-intensive/chapter-2/","tags":null,"title":"Chapter 2: Data Models and Query Languages"},{"categories":null,"content":"This chapter is the heart of the book and arguably the most important for many ML system design interviews, as recommendation systems are a classic and recurring design question. We\u0026rsquo;ll break down the three key stages: Candidate Generation, Ranking, and Re-ranking.\nCandidate Generation - Finding the Haystack Needles This is the first and arguably most critical stage of a large-scale recommender.\nBook\u0026rsquo;s Core Idea (Timeless): You cannot possibly score and rank every single item in your corpus (e.g., all 500 million videos on YouTube) for every user in real time. The goal of candidate generation (also called retrieval or sourcing) is to narrow down this massive set to a few hundred or a few thousand \u0026ldquo;pretty good\u0026rdquo; candidates, quickly and cheaply. The book presents the two classic approaches:\nContent-Based Filtering: \u0026ldquo;Show me more items like the ones I already like.\u0026rdquo; Collaborative Filtering: \u0026ldquo;Show me items that people similar to me like.\u0026rdquo; The 2024+ Perspective: The concepts remain the same, but the implementation has evolved significantly. Let\u0026rsquo;s break down the modern methods for each.\n1. Content-Based Filtering How it Works (Then): The book\u0026rsquo;s example of a binary feature matrix for apps is a good starting point. You represent items as vectors of their attributes (category, price, etc.) and find items with similar vectors to the ones a user has interacted with.\nHow it Works (Now - The 2024+ Way): This is now almost exclusively done using deep learning embeddings.\nGenerate Item Embeddings: You use a powerful pre-trained model to create a rich, semantic vector for every item. For a movie, this might involve feeding its title, synopsis, and genre into a Transformer-based text model (like BERT). For a product, you might feed its image into a vision model (like CLIP) and its description into a text model, then concatenate the resulting vectors. Generate a User Profile: You look at the items a user has recently interacted with positively (e.g., liked, purchased). You fetch the pre-computed embeddings for these items. Find Similar Items: You can now find candidates in two ways: Average User Profile: Average the embeddings of the items the user liked to create a \u0026ldquo;user profile\u0026rdquo; vector. Then, use an ANN index to find other item embeddings that are close to this profile vector. Item-to-Item Similarity: For each item the user recently liked, find the \u0026ldquo;top K\u0026rdquo; most similar items directly from your ANN index. This is the core of Amazon\u0026rsquo;s \u0026ldquo;Customers who bought this item also bought\u0026hellip;\u0026rdquo; feature. Diagram (Modern Content-Based Retrieval):\ngraph TD subgraph \u0026#34;Offline: Item Embedding Generation\u0026#34; Item_Data[\u0026#34;Item Metadata\u0026lt;br/\u0026gt;(Text, Images)\u0026#34;] --\u0026gt; Embedding_Model(\u0026#34;Pre-trained\u0026lt;br/\u0026gt;Transformer/Vision Model\u0026#34;) --\u0026gt; Item_Embeddings[(\u0026#34;Item Embeddings\u0026#34;)] Item_Embeddings --\u0026gt; ANN_Index[(\u0026#34;Vector DB / FAISS\u0026#34;)] end subgraph \u0026#34;Online: User Request\u0026#34; User_History[\u0026#34;User\u0026#39;s recent likes\u0026lt;br/\u0026gt;(Item A, Item B)\u0026#34;] --\u0026gt; Fetch_Embeds{\u0026#34;Fetch Embeddings\u0026lt;br/\u0026gt;for A and B\u0026#34;} Fetch_Embeds -- \u0026#34;Item_A_vec, Item_B_vec\u0026#34; --\u0026gt; User_Profile(\u0026#34;Create User Profile\u0026lt;br/\u0026gt;e.g., Average vectors\u0026#34;) User_Profile --\u0026gt; ANN_Index ANN_Index --\u0026gt; Candidates[\u0026#34;Candidate Items\u0026#34;] end style ANN_Index fill:#cde4ff Interview Phrasing: \u0026ldquo;For content-based candidate generation, we will pre-compute a rich semantic embedding for every item in our catalog using a pre-trained foundation model. When a user is active, we can generate a real-time user profile by averaging the embeddings of items they\u0026rsquo;ve recently interacted with. We then query our vector index with this profile vector to retrieve hundreds of content-similar candidates.\u0026rdquo; 2. Collaborative Filtering Book\u0026rsquo;s Core Idea (Timeless): This method doesn\u0026rsquo;t care about the content of the items at all. It relies solely on the user-item interaction matrix. The core idea is to find users with similar taste and recommend items that one has seen but the other has not. The book explains the classic approach: Matrix Factorization.\nMatrix Factorization: You have a giant, sparse matrix of users and items. The goal is to \u0026ldquo;factorize\u0026rdquo; this into two smaller, dense matrices: a user-embedding matrix (U) and an item-embedding matrix (V). The dot product of a user\u0026rsquo;s embedding and an item\u0026rsquo;s embedding should approximate the rating that user would give that item. How it Works (Now - The 2024+ Way): The principle is identical, but the implementation is now the Two-Tower Model we\u0026rsquo;ve discussed extensively. A two-tower model is the modern, deep learning-based way of doing collaborative filtering. Instead of using a classic algorithm like ALS (Alternating Least Squares) to find the embeddings, you train two deep neural networks with a contrastive loss. This is far more powerful because the towers can ingest rich features about the user and item, not just their IDs.\nDiagram (Matrix Factorization vs. Two-Tower):\ngraph TD subgraph \u0026#34;Classic Matrix Factorization\u0026#34; direction LR A[\u0026#34;Sparse User-Item\u0026lt;br/\u0026gt;Interaction Matrix\u0026#34;] --\u0026gt; Factorize{\u0026#34;Factorization Algorithm\u0026lt;br/\u0026gt;(e.g., ALS, SGD)\u0026#34;} Factorize --\u0026gt; U[(\u0026#34;User Embeddings\u0026#34;)] Factorize --\u0026gt; V[(\u0026#34;Item Embeddings\u0026#34;)] end subgraph \u0026#34;Modern Collaborative Filtering (Two-Tower Model)\u0026#34; direction LR B[\u0026#34;Raw User-Item Interactions\u0026lt;br/\u0026gt;+ Rich Features\u0026#34;] --\u0026gt; Train{\u0026#34;Train Two Towers\u0026lt;br/\u0026gt;with Contrastive Loss\u0026#34;} Train --\u0026gt; U2[(\u0026#34;User Embeddings\u0026#34;)] Train --\u0026gt; V2[(\u0026#34;Item Embeddings\u0026#34;)] end style B fill:#dff0d8 3. Combining Candidate Generators: The Multi-Source Approach Book\u0026rsquo;s Core Idea (Timeless): You don\u0026rsquo;t just use one method. A real-world recommender system is a blend of many candidate generators. The YouTube example in the book is perfect: they have one generator based on topic similarity (content-based), another based on co-watch patterns (collaborative filtering), and likely many others. The 2024+ Perspective: This is a crucial point to make in an interview. A senior candidate designs a system, not just a single model. Why it\u0026rsquo;s important: Diversity: Different generators find different kinds of items. A collaborative filtering source might find popular, trending items, while a content-based source can find niche items perfectly tailored to a user\u0026rsquo;s specific taste. Cold Start Problem: For a brand new user with no history, collaborative filtering is impossible. For this user, you would rely entirely on other sources, like \u0026ldquo;most popular items,\u0026rdquo; content-based recommendations based on their sign-up info (age/country), or recommendations based on the first item they click on. Resiliency: If one candidate generator goes down, the others can still provide recommendations, making the system more robust. Diagram (The Funnel): graph TD subgraph \u0026#34;Candidate Generation Stage\u0026#34; CG1[\u0026#34;Content-Based\u0026lt;br/\u0026gt;Item-to-Item\u0026#34;] CG2[\u0026#34;Collaborative Filtering\u0026lt;br/\u0026gt;Two-Tower Model\u0026#34;] CG3[\u0026#34;Trending / Most Popular\u0026#34;] CG4[\u0026#34;New Items / Exploration\u0026#34;] end CG1 --\u0026gt; Merge{\u0026#34;Merge \u0026amp; Deduplicate\u0026#34;} CG2 --\u0026gt; Merge CG3 --\u0026gt; Merge CG4 --\u0026gt; Merge Merge --\u0026gt; Candidates[\u0026#34;~500-1000 Candidates\u0026#34;] Candidates --\u0026gt; Ranking_Stage(\u0026#34;Next Stage: Ranking\u0026#34;) style Merge fill:#fff0b3 Interview Phrasing: \u0026ldquo;Our system won\u0026rsquo;t rely on a single source for candidates. I\u0026rsquo;d propose a multi-source retrieval strategy. We\u0026rsquo;d have at least three main generators: one based on collaborative filtering using a two-tower model to find items popular with similar users; a second using content-based item-to-item similarity for niche recommendations; and a third for business logic, such as boosting new or trending items. We would then merge and de-duplicate the outputs from all sources before passing the combined candidate set to the ranking model. This ensures diversity and robustness.\u0026rdquo; Excellent. We\u0026rsquo;ve successfully narrowed down a sea of billions of items to a manageable pool of ~500 candidates. Now, we enter the Ranking stage. This is where we get serious about precision.\nOrdering the Candidates with Precision Book\u0026rsquo;s Core Idea (Timeless): The goal of the ranking stage is to take the ~500 candidates from the retrieval stage and score them with a much more powerful, precise, and computationally expensive model. Because we are only dealing with a few hundred items per user, we can now afford to use rich features that were too slow for the retrieval stage. The output is a finely ordered list of the top ~10-20 items to actually display to the user.\nThe 2024+ Perspective: The core purpose remains the same. The main evolution is in the complexity and type of models used and the richness of the features they can handle. The distinction between a fast retrieval model and a slower, more accurate ranking model is a fundamental pattern in large-scale ML.\nLet\u0026rsquo;s break down the different ways to frame the ranking problem.\n1. Pointwise Ranking: The \u0026ldquo;Classification/Regression\u0026rdquo; Approach Book\u0026rsquo;s Core Idea (Timeless): This is the most common and straightforward approach. You treat each candidate item independently. The model\u0026rsquo;s job is to predict a score for a single (user, item) pair. Intuition: You are essentially building a model that answers a direct question for each candidate: Classification: \u0026ldquo;What is the probability that this user will click on this item?\u0026rdquo; (Output a score from 0 to 1). Regression: \u0026ldquo;How many minutes will this user watch this video?\u0026rdquo; (Output a numerical score). The final ranked list is created by simply sorting all the candidates by their independent scores in descending order. Diagram: graph TD subgraph \u0026#34;Input\u0026#34; U[\u0026#34;User Features\u0026lt;br/\u0026gt;(age, country, user_embedding)\u0026#34;] I[\u0026#34;Item Features\u0026lt;br/\u0026gt;(category, price, item_embedding)\u0026#34;] C[\u0026#34;Context Features\u0026lt;br/\u0026gt;(time of day, device)\u0026#34;] end U \u0026amp; I \u0026amp; C --\u0026gt; R_Model{\u0026#34;Pointwise Ranking Model\u0026lt;br/\u0026gt;(e.g., XGBoost, Deep Neural Network)\u0026#34;} R_Model --\u0026gt; Score(\u0026#34;Predicted Score\u0026lt;br/\u0026gt;e.g., p(click) = 0.083\u0026#34;) subgraph \u0026#34;Process for each candidate\u0026#34; Candidate1 --\u0026gt; R_Model --\u0026gt; Score1(\u0026#34;0.083\u0026#34;) Candidate2 --\u0026gt; R_Model --\u0026gt; Score2(\u0026#34;0.012\u0026#34;) Candidate3 --\u0026gt; R_Model --\u0026gt; Score3(\u0026#34;0.157\u0026#34;) end subgraph \u0026#34;Final List\u0026#34; Sort{\u0026#34;Sort by Score\u0026#34;} --\u0026gt; FinalList(\u0026#34;1. Candidate 3\u0026lt;br/\u0026gt;2. Candidate 1\u0026lt;br/\u0026gt;3. Candidate 2\u0026#34;) end Score1 \u0026amp; Score2 \u0026amp; Score3 --\u0026gt; Sort The 2024+ Perspective: This is still the workhorse of most industrial ranking systems. The models have become more powerful (often large Transformer-based networks that can handle raw text and cross-feature interactions), but the core idea of scoring each item individually is the same. Its strength is simplicity and the ability to directly optimize for a clear business metric like p(click). 2. Pairwise Ranking: The \u0026ldquo;Relative Order\u0026rdquo; Approach Book\u0026rsquo;s Core Idea (Timeless): This approach reframes the problem. Instead of predicting an absolute score for each item, the model learns to predict which item in a pair is better. The model is trained on pairs of documents, (Item A, Item B), and the label is 1 if A should be ranked higher than B, and 0 otherwise. The book\u0026rsquo;s example of RankNet is the classic algorithm for this. Intuition: The model learns a function f(item) that produces a score. It is trained so that if A is better than B, then f(A) \u0026gt; f(B). The actual values of the scores don\u0026rsquo;t matter, only their relative order. Equation (RankNet\u0026rsquo;s Core): Take two items, A and B, and get their scores from the model: s_a = f(A) and s_b = f(B). Calculate the difference: s_ab = s_a - s_b. Pass this through a sigmoid function to get a probability that A is better than B: P_ab = sigmoid(s_ab). The loss is simply the cross-entropy between this predicted probability and the ground-truth label (e.g., 1 if A was actually clicked and B was not). Pros \u0026amp; Cons: Pro: Can be more effective at directly optimizing for ranking metrics like NDCG. It\u0026rsquo;s focused only on getting the order right. Con (The big one): The number of possible pairs is quadratic (n²), which can be computationally explosive. You need to be clever about which pairs you train on. More importantly, the final scores are not calibrated probabilities; you can\u0026rsquo;t interpret a score of 0.8 as an 80% click probability, which is often a business requirement. 3. Listwise Ranking: The \u0026ldquo;Holistic List\u0026rdquo; Approach Book\u0026rsquo;s Core Idea (Outdated/Academic): The book mentions this as the third formulation. This approach attempts to solve the \u0026ldquo;perfect\u0026rdquo; ranking problem by taking the entire list of candidates as input to the model at once and training it to directly output the optimal ordering. Intuition: The model learns to consider the context of the whole list. For example, it might learn not to show two very similar-looking items next to each other to promote diversity. The 2024+ Perspective: While theoretically the most powerful, listwise approaches are rarely used in large-scale industrial systems. Why? The model architecture becomes incredibly complex (it needs to handle a variable-length list of items as input), and the computational cost during serving is often prohibitively high. The complexity and cost usually do not justify the marginal gains over a well-tuned pointwise model. Interview Phrasing: You should acknowledge its existence but frame it as an academic/research topic. \u0026ldquo;While pointwise ranking is the most common industrial approach, there are also pairwise and listwise formulations. Listwise methods, which optimize the entire ranked list at once, are theoretically powerful for capturing cross-item interactions like diversity, but they are often too computationally expensive for real-time, low-latency production systems. A more practical way to handle diversity is in a re-ranking stage.\u0026rdquo; Putting it all together for the interview Your go-to strategy should be to propose a pointwise ranker, as it\u0026rsquo;s the most practical and widely used.\nSenior-Level Phrasing for Ranking Design:\n\u0026ldquo;For the ranking stage, we\u0026rsquo;ll take the ~500 candidates from our retrieval models and score them using a more powerful model. I would propose a pointwise ranking approach, where we train a deep neural network to predict the probability of a click for each (user, item) pair.\nFeatures: This model can afford to use much richer and more expensive-to-compute features than the retrieval towers. We would include:\nUser features: long-term user profile embedding, real-time features like clicks_in_last_hour. Item features: detailed item content embedding, popularity statistics. Cross-features: We\u0026rsquo;d explicitly compute interactions between the user and item embeddings (e.g., dot product, element-wise product) to feed into the network, helping it learn personalized relevance. Model: A standard multi-layer perceptron (MLP) is a strong baseline. To improve, we could incorporate attention layers to better weigh the importance of different features.\nObjective: We would train this as a binary classifier using a standard cross-entropy (log loss).\nServing: At inference time, we score each of the 500 candidates independently and sort them by their predicted p(click) to generate the final list shown to the user.\u0026rdquo;\nExcellent. We\u0026rsquo;ve retrieved a few hundred candidates and ranked them precisely. We\u0026rsquo;re almost ready to show the final list to the user. But there\u0026rsquo;s one final, optional stage where we can apply crucial business logic and heuristics: Re-ranking.\nRe-ranking - The Final Polish Book\u0026rsquo;s Core Idea (Timeless): Re-ranking is a post-processing step that takes the beautifully ordered list from the ranking model and makes final adjustments. It\u0026rsquo;s not about learning complex patterns; it\u0026rsquo;s about enforcing hard constraints, promoting diversity, injecting business rules, and ensuring fairness.\nWhy have a separate stage? Why not just teach the ranking model all these rules?\nSimplicity \u0026amp; Speed: Many business rules are complex to learn and would bloat the ranking model. A simple if/then rule applied to the top 20 candidates is much faster and more reliable than trying to teach a neural network the same concept. Agility: Business rules change frequently. \u0026ldquo;This week, we want to boost all items from brand X.\u0026rdquo; It\u0026rsquo;s far easier and safer to change a rule in a simple re-ranking service than to retrain and redeploy the entire core ranking model. 1. Filtering and Hard Constraints What it is: The most basic function of re-ranking is to remove items that should never be shown. Examples: Already Seen: Remove items the user has seen in their current session to avoid repetition. User-Blocked Content: Remove content from creators the user has explicitly blocked or muted. Not Safe for Work (NSFW): Filter out inappropriate content. Inventory Check: For an e-commerce site, remove items that just went out of stock in the last few seconds. How it\u0026rsquo;s done: This is typically a series of simple lookups against blocklists or caches. 2. Promoting Diversity The Problem: Your ranking model might be too good. If a user clicks on one video about \u0026ldquo;golden retrievers,\u0026rdquo; the ranker might score 19 other golden retriever videos very highly. Showing a list of nearly identical items is a boring and unhelpful user experience. This is called over-specialization. The Solution: The re-ranker can enforce diversity. A common algorithm is Maximal Marginal Relevance (MMR), though a simpler heuristic is often used in practice. Intuition (Simple Heuristic): Start with your top-ranked item and add it to the final list. Now, iterate down your original ranked list from position 2. For each candidate: Calculate its similarity to all items already in the final list. If it\u0026rsquo;s too similar to any of them (e.g., same category, high embedding similarity), penalize its score or skip it. If it\u0026rsquo;s sufficiently different, add it to the final list. Repeat until your final list has the desired number of items (e.g., 10). Diagram (Diversity in action): graph TD subgraph \u0026#34;Ranker Output (Over-specialized)\u0026#34; R1(\u0026#34;1. Golden Retriever Video A\u0026#34;) R2(\u0026#34;2. Golden Retriever Video B\u0026#34;) R3(\u0026#34;3. Corgi Video X\u0026#34;) R4(\u0026#34;4. Golden Retriever Video C\u0026#34;) R5(\u0026#34;5. Labrador Video Y\u0026#34;) end subgraph \u0026#34;Re-ranking Logic\u0026#34; A{\u0026#34;Start with #1\u0026#34;} --\u0026gt; F1(\u0026#34;Final List: GR_A\u0026#34;) B{\u0026#34;Consider #2\u0026#34;} -- \u0026#34;Too similar to GR_A\u0026#34; --\u0026gt; Skip1(\u0026#34;Penalize/Skip\u0026#34;) C{\u0026#34;Consider #3\u0026#34;} -- \u0026#34;Sufficiently different\u0026#34; --\u0026gt; F2(\u0026#34;Final List: GR_A, Corgi_X\u0026#34;) D{\u0026#34;Consider #4\u0026#34;} -- \u0026#34;Too similar to GR_A\u0026#34; --\u0026gt; Skip2(\u0026#34;Penalize/Skip\u0026#34;) E{\u0026#34;Consider #5\u0026#34;} -- \u0026#34;Sufficiently different\u0026#34; --\u0026gt; F3(\u0026#34;Final List: GR_A, Corgi_X, Lab_Y\u0026#34;) end subgraph \u0026#34;Final Displayed List (Diverse)\u0026#34; D1(\u0026#34;1. Golden Retriever Video A\u0026#34;) D2(\u0026#34;2. Corgi Video X\u0026#34;) D3(\u0026#34;3. Labrador Video Y\u0026#34;) end 3. Injecting Business Logic \u0026amp; Boosting What it is: This is where the business and product teams get to influence the final output directly. Examples: Freshness: Boost the score of content published in the last 24 hours to promote newness (as mentioned in the book for YouTube). Merchandising: A business rule states, \u0026ldquo;For the next week, boost the score of all Samsung products by 20%.\u0026rdquo; Promote High-Margin Items: Boost the score of items that have a higher profit margin for the company. Exploration: Randomly boost a few new items that have no interaction data yet, to help them get initial exposure and escape the cold-start problem. This is a simple form of exploration. 4. Fairness and Bias Mitigation The Problem: The ranking model, trained on historical data, might have learned societal biases. For example, a job recommendation system might unintentionally rank qualified female candidates lower for a \u0026ldquo;Software Engineer\u0026rdquo; role if the training data was historically male-dominated. The Solution: The re-ranking stage is a practical place to intervene. Fairness Constraints: After scoring, you can audit the top k results. If you find that the gender or racial representation is skewed compared to the pool of qualified candidates, you can re-rank the list to achieve a more equitable representation. Example: For a \u0026ldquo;CEO\u0026rdquo; query on a people search, if the top 20 results are all male, the re-ranker might identify qualified female candidates from lower in the list (e.g., position 35) and promote them into the top 20. The 2024+ Perspective: This is an extremely active and important area. While re-ranking is a powerful tool for \u0026ldquo;post-processing\u0026rdquo; fairness, modern approaches also try to address this earlier in the pipeline, for example, by adding fairness constraints directly into the training objective of the ranking model. Putting it all together for the interview Senior-Level Phrasing for Re-ranking Design:\n\u0026ldquo;After our deep learning model produces a ranked list of ~500 candidates, I would add a lightweight re-ranking stage to apply final business logic before displaying the top 10. This stage would be responsible for several key tasks:\nFiltering: It would first perform hard filtering to remove any items the user has already seen in this session or content from blocked creators. Diversity: To avoid showing a list of nearly identical items, it would apply a heuristic to ensure category-level diversity. For example, we wouldn\u0026rsquo;t place more than two items from the same sub-category in the top 10. Business Boosting: It would apply dynamic boosts based on business rules, such as increasing the score for content that is less than 24 hours old to promote freshness, or for content that is part of a current marketing campaign. Exploration: To help new items gather data, we would give a small score boost to a fraction of candidates that have very few impressions. This modular approach is more agile and maintainable than trying to bake all this complex and frequently changing logic directly into the main ranking model.\u0026rdquo;\nThis concludes our deep dive into the three core components of a recommendation system. You now have a comprehensive, modern view of candidate generation, ranking, and re-ranking.\nOf course. These are fantastic topics to cover, as they represent the \u0026ldquo;real world\u0026rdquo; challenges of ML that go beyond textbook models. Mastering these concepts is what truly distinguishes a senior candidate. It shows you\u0026rsquo;ve dealt with the messy reality of production systems.\nLet\u0026rsquo;s dive into each one.\nPosition Bias - \u0026ldquo;The Rich Get Richer\u0026rdquo; Book\u0026rsquo;s Core Idea (Timeless): Users are inherently biased towards clicking on items at the top of a list, regardless of their quality. The item at position 1 gets more attention than the item at position 2, and so on.\nThe Vicious Cycle (The Problem):\nYour model places Item A at position 1. Because it\u0026rsquo;s at position 1, it gets a lot of clicks. You collect this click data to retrain your model. The model sees \u0026ldquo;Oh, Item A gets a ton of clicks! It must be extremely relevant.\u0026rdquo; The retrained model learns to rank Item A even higher, reinforcing the bias. Eventually, your model just learns to rank popular items at the top, and new or potentially more relevant items never get a chance to be seen. The model is learning the bias, not the true relevance. Solution 1: Use Position as a Feature (The most common approach)\nHow it works: During training, you add position as a feature to your model. The model learns that this feature is highly correlated with the click target. For instance, it learns p(click | position=1) = 0.2, but p(click | position=10) = 0.01. It effectively learns to disentangle the inherent \u0026ldquo;quality\u0026rdquo; of an item from the \u0026ldquo;bonus\u0026rdquo; it gets just by being at a certain position. The Magic Trick (At Inference): When you use the model for serving, you want to find the true relevance, stripped of the position bias. To do this, you feed a constant value for the position feature for all candidates. For example, you ask the model: \u0026ldquo;What would the click probability be for each of these items if they were all shown at position 1?\u0026rdquo; You then rank by this \u0026ldquo;unbiased\u0026rdquo; score. Solution 2: Inverse Propensity Score (IPS)\nHow it works: This is a data-weighting approach. You re-weight your training examples to correct for the bias. Clicks on items at lower positions (which are rare and thus strong signals of relevance) are given a higher weight in the loss function than clicks on items at the top. Equation: Weight = 1 / p(shown_at_position_i). p(shown_at_position_i) is the \u0026ldquo;propensity score.\u0026rdquo; An item at position 10 has a low propensity, so its weight is high. 2024+ Perspective: While statistically pure, IPS can suffer from high variance if some propensities are very small. The \u0026ldquo;Position as a Feature\u0026rdquo; method is generally more robust and more common in industry. Interview Phrasing: \u0026ldquo;Position bias is a critical issue we must address. In our training data, user clicks are heavily influenced by an item\u0026rsquo;s rank. To mitigate this, I would include position as a feature in our ranking model. The model will learn to associate the position with click propensity. Then, during inference, we will neutralize this bias by passing a fixed position value (e.g., position 1) for all candidates, allowing us to rank based on the model\u0026rsquo;s estimate of true, position-independent relevance.\u0026rdquo;\nImpression Discounting (LinkedIn PYMK Case Study) The Problem: In the \u0026ldquo;People You May Know\u0026rdquo; (PYMK) feature, LinkedIn shows you a list of potential connections. A user might see the same person recommended to them day after day. They don\u0026rsquo;t accept the connection, but they also don\u0026rsquo;t explicitly dismiss them. The model, seeing no negative signal, keeps recommending the same person because their raw relevance score is high. This creates a stale and annoying user experience. The Solution: Impression Discounting. This is a brilliant, practical heuristic applied at the re-ranking stage. Intuition: \u0026ldquo;The more times I\u0026rsquo;ve shown you this person and you haven\u0026rsquo;t connected, the less likely you are to ever connect. I should penalize this person\u0026rsquo;s score each time I show them to you.\u0026rdquo; How it works (Conceptual): Maintain a persistent impression counter for each (viewer_id, candidate_id) pair. The ranking model calculates a raw relevance score, S_raw. The re-ranker fetches the impression count, N, for this pair. It calculates a discount factor, d(N), which is a function that increases with N. For example, d(N) = 1 / (1 + α*N). The final score is S_final = S_raw * d(N). Diagram: graph TD subgraph \u0026#34;Day 1\u0026#34; Model_Score(\u0026#34;Model Score for Person X = 0.9\u0026#34;) --\u0026gt; Final_Score1(\u0026#34;Final Score = 0.9\u0026lt;br/\u0026gt;Show to User\u0026#34;) Imp_Store1[(\u0026#34;Impression Store:\u0026lt;br/\u0026gt;User to X: 0\u0026#34;)] end subgraph \u0026#34;Day 2 (User did not connect)\u0026#34; Imp_Store2[(\u0026#34;Impression Store:\u0026lt;br/\u0026gt;User to X: 1\u0026#34;)] Model_Score2(\u0026#34;Model Score for Person X = 0.9\u0026#34;) --\u0026gt; Discount1{\u0026#34;Discount Factor d(1)\u0026#34;} Imp_Store2 --\u0026gt; Discount1 Discount1 --\u0026gt; Final_Score2(\u0026#34;Final Score = 0.75\u0026lt;br/\u0026gt;Show to User\u0026#34;) end subgraph \u0026#34;Day 3 (User did not connect)\u0026#34; Imp_Store3[(\u0026#34;Impression Store:\u0026lt;br/\u0026gt;User to X: 2\u0026#34;)] Model_Score3(\u0026#34;Model Score for Person X = 0.9\u0026#34;) --\u0026gt; Discount2{\u0026#34;Discount Factor d(2)\u0026#34;} Imp_Store3 --\u0026gt; Discount2 Discount2 --\u0026gt; Final_Score3(\u0026#34;Final Score = 0.5\u0026lt;br/\u0026gt;Maybe don\u0026#39;t show...\u0026#34;) end style Imp_Store1 fill:#cde4ff style Imp_Store2 fill:#cde4ff style Imp_Store3 fill:#cde4ff Interview Phrasing: \u0026ldquo;To prevent recommendation staleness, where we repeatedly show the same candidates a user isn\u0026rsquo;t engaging with, I would implement an impression discounting system in the re-ranking stage. We would persist an impression count for each user-candidate pair. The final ranking score would be the model\u0026rsquo;s raw score multiplied by a discount factor that is inversely proportional to the number of times the item has been shown without a positive interaction. This keeps the feed fresh and improves user experience.\u0026rdquo; Calibration - \u0026ldquo;Is your 80% really an 80%?\u0026rdquo; Book\u0026rsquo;s Core Idea (Timeless): Calibration is about ensuring that a model\u0026rsquo;s predicted probability corresponds to the true probability in the real world. If your model predicts p(click) = 0.8 for a set of 100 ads, then roughly 80 of those ads should actually get clicked. Why it\u0026rsquo;s crucial: Ad Auctions: If you use your p(click) to calculate an expected value for bidding (eCPM = p(click) * advertiser_bid), an uncalibrated probability will lead to systematically over- or under-bidding, costing millions. Business Decisions: If you tell a user \u0026ldquo;There is an 80% chance of rain,\u0026rdquo; they expect it to rain 8 out of 10 times. A miscalibrated model erodes trust. Why models become miscalibrated: Downsampling: As discussed, if you downsample your negative class, your model\u0026rsquo;s outputs will be artificially high. Model Architecture: Some models, like SVMs or even modern neural networks with ReLU activations, are not inherently designed to produce calibrated probabilities. They are designed to maximize rank-ordering (AUC). Solution: Post-processing. You take the raw output of your trained model and pass it through a simple, secondary calibration model. Platt Scaling: Fits a logistic regression model on top of your primary model\u0026rsquo;s scores. It\u0026rsquo;s simple and effective. Isotonic Regression: A more powerful, non-parametric method that fits a piecewise-constant, non-decreasing function. It\u0026rsquo;s more flexible than Platt scaling but requires more data. The Downsampling Correction Formula (from the book): calibrated_p = p / (p + (1-p)/w). This is a specific form of calibration required when you downsample. Interview Phrasing: \u0026ldquo;While our ranking model is optimized for a metric like AUC, its raw probability outputs may not be well-calibrated, which is critical for our ad auction bidding logic. After training the main ranker, I would add a post-processing calibration step. We would train an Isotonic Regression model on a held-out validation set, which maps the model\u0026rsquo;s output scores to true, empirically observed probabilities. This ensures that our downstream bidding systems can trust the predictions.\u0026rdquo; Nonstationary Problem, Exploration vs. Exploitation, and Airbnb\u0026rsquo;s Lessons These final topics are about the dynamic, ever-changing nature of real-world ML.\nNonstationary Problem (Concept Drift): The world changes. User tastes change, new products are introduced, slang evolves. A model trained on 2023 data will perform poorly on 2024 data because the underlying p(y|x) has shifted.\nSolution: Constant retraining. This is the primary industrial solution. Your entire data pipeline and training infrastructure must be automated and robust enough to retrain your model frequently (daily, or even hourly for very dynamic systems). This is why the infrastructure discussions we had earlier are so important. Exploration vs. Exploitation: This is a fundamental trade-off.\nExploitation: Use the current best model and knowledge to show the user what you think they will like most. (e.g., recommend the video with the highest predicted watch time). This maximizes short-term metrics. Exploration: Show the user something new and uncertain to gather information. (e.g., recommend a video from a brand new channel). This might hurt short-term metrics but is essential for long-term discovery and preventing your system from becoming a boring echo chamber. Solution: Epsilon-Greedy is the simplest approach. 99% of the time, you \u0026ldquo;exploit.\u0026rdquo; 1% of the time (epsilon), you pick a random item to \u0026ldquo;explore.\u0026rdquo; More advanced solutions involve Multi-Armed Bandits (Thompson Sampling, UCB) which provide a more intelligent way to balance this trade-off. Airbnb\u0026rsquo;s Lessons (\u0026ldquo;Deep Learning is NOT a drop-in replacement\u0026rdquo;): This is a story of humility and respect for baselines.\nThe Story: The Airbnb team had a well-performing Gradient Boosted Decision Tree (GBDT) model. They tried to replace it with a fancy deep learning model and found that it performed no better or even worse initially. The Lesson: Deep learning models are not magic. They have a huge number of hyperparameters to tune (architecture, learning rate, initializations). A well-tuned GBDT on well-crafted features is an incredibly strong baseline. Beating it requires careful, systematic work, not just swapping model types. The book\u0026rsquo;s specifics are gold: They found that features that worked well for the GBDT didn\u0026rsquo;t work well for the NN, and vice-versa. They had to re-do their feature engineering. They had to experiment with different learning rates and weight initializations. Interview Phrasing (combining these concepts): \u0026ldquo;A key challenge in this system is that user preferences are nonstationary. To combat this drift, we must build a pipeline for frequent, automated retraining. This also ties into the exploration/exploitation trade-off. To ensure we are constantly learning about new items and tastes, I would introduce an \u0026rsquo;epsilon-greedy\u0026rsquo; exploration strategy in our re-ranking stage. This is a lesson learned from industry case studies like Airbnb\u0026rsquo;s, which showed that even powerful deep learning models can fail if they aren\u0026rsquo;t constantly fed fresh, diverse data and that beating a strong GBDT baseline requires careful, iterative engineering, not just a model swap.\u0026rdquo;\n","permalink":"https://deepskandpal.github.io/bookshelf/ml-design-interview/chapter-2/","tags":null,"title":"Chapter 2: Common Recommendation System Components."},{"categories":null,"content":"Excellent. Let\u0026rsquo;s dive into Chapter 6. This is one of the most important, practical patterns in the GenAI space right now: Retrieval-Augmented Generation (RAG).\nIf the last chapter was about the fundamentals of GenAI models, this chapter is about making them useful in the real world. A base LLM is like a brilliant student who has read the entire internet up to 2023 but has never seen your company\u0026rsquo;s private documents and doesn\u0026rsquo;t know what happened in the news this morning. RAG is the system you build to give that student an \u0026ldquo;open-book exam,\u0026rdquo; allowing them to access specific, up-to-date information to answer questions.\nLet\u0026rsquo;s break this down, piece by piece.\nChapter 6: Retrieval-Augmented Generation Introduction What are we trying to achieve? We\u0026rsquo;re solving the LLM\u0026rsquo;s biggest weaknesses:\nKnowledge Cutoff: It doesn\u0026rsquo;t know about recent events. Lack of Private Data: It hasn\u0026rsquo;t been trained on your internal company wiki, your customer support database, or a PDF you just uploaded. Hallucination: It can make things up. The book uses the example of building a ChatPDF system for internal company use. An employee should be able to ask, \u0026ldquo;What is our policy on international travel reimbursement?\u0026rdquo; and get an answer based on the latest HR documents, not on some generic policy the LLM learned from the public internet.\nStep 1: Clarifying Requirements This is where the interview starts. The book gives a fantastic example of a candidate leading the conversation. Let\u0026rsquo;s analyze it from an interviewer\u0026rsquo;s perspective.\nCandidate: \u0026ldquo;What does the external knowledge base consist of? Does it change over time?\u0026rdquo; Interviewer\u0026rsquo;s thought: Good. They\u0026rsquo;re starting with the data. They understand that the nature of the data source is the most important factor. Candidate: \u0026ldquo;Do the Wiki pages and forums contain text, images, and other modalities?\u0026rdquo; Interviewer\u0026rsquo;s thought: Excellent. They\u0026rsquo;re thinking about multimodality. This will affect our choice of embedding models. Candidate: \u0026ldquo;How many pages are there in total?\u0026rdquo; (5 million pages) \u0026ldquo;What is the expected growth?\u0026rdquo; (20% annually) Interviewer\u0026rsquo;s thought: Great, they\u0026rsquo;re quantifying the scale. This is critical for discussing scalability, cost, and choosing the right database/indexing strategy. Candidate: \u0026ldquo;Should the system respond in real time?\u0026rdquo; (Slight delay is okay) Interviewer\u0026rsquo;s thought: They\u0026rsquo;re scoping the latency requirements. This tells me I don\u0026rsquo;t need a sub-50ms system and can make trade-offs for better quality. Candidate: \u0026ldquo;Is it necessary for the system to include document references?\u0026rdquo; (Yes) Interviewer\u0026rsquo;s thought: Crucial question. This requirement immediately makes one of the potential solutions (finetuning) much less attractive. They are already thinking ahead. By the end of this, you\u0026rsquo;ve established the core problem: Build a Q\u0026amp;A system over a large (5M pages), slowly growing (+20%/year) internal knowledge base of mixed-format PDFs, which must provide verifiable answers with source references.\nStep 2: Frame the Problem as an ML Task Specifying Input and Output\nThis is straightforward but important to state clearly.\nInput: A user\u0026rsquo;s text query (e.g., \u0026ldquo;How do I submit an expense report?\u0026rdquo;). Underlying Data: A database of 5 million company documents. Output: A text-based answer, grounded in the documents, with references. graph LR subgraph User A[User Query\u0026lt;br/\u0026gt;\u0026#34;How do I submit an\u0026lt;br/\u0026gt;expense report?\u0026#34;] end subgraph System B(ChatPDF System) end subgraph Data C[Document databases] end subgraph Output D[Response\u0026lt;br/\u0026gt;\u0026#34;To submit an expense report, log into...\u0026#34;] end A --\u0026gt; B C --\u0026gt; B B --\u0026gt; D (Based on Figure 6.2)\nChoosing a Suitable ML Approach\nThis is the first major design decision, and it\u0026rsquo;s a classic interview trade-off question. For a problem like this, the book lays out three main approaches.\nFinetuning:\nWhat it is: Take a pre-trained LLM and continue training it on your internal documents. The model\u0026rsquo;s weights are updated to \u0026ldquo;absorb\u0026rdquo; the new knowledge. Pros: Can deeply learn the style and terminology of your company. Cons (Dealbreakers for our problem): Computationally Expensive: Continuously retraining an LLM is a massive cost. Stale Data: As soon as a new document is added, the model is out of date until the next expensive finetuning cycle. No References: The book correctly states: \u0026ldquo;Finetuned models usually can\u0026rsquo;t provide references for their answers, making it hard to verify or trace information back to its source.\u0026rdquo; The knowledge is baked into the weights; you can\u0026rsquo;t easily point to the source document. This violates our requirement. Prompt Engineering (In-Context Learning):\nWhat it is: Stuff the relevant documents directly into the prompt along with the user\u0026rsquo;s question. Pros: Simple, cheap, no training required. Cons (Dealbreakers for our problem): Limited Context Window: You can\u0026rsquo;t fit 5 million documents into a prompt. You can\u0026rsquo;t even fit one long document. This approach is simply not scalable. Retrieval-Augmented Generation (RAG):\nWhat it is: A two-step process. First, retrieve a few relevant document snippets from the large database. Then, generate an answer using an LLM, with the user\u0026rsquo;s query and the retrieved snippets provided as context in the prompt. Pros: Access to Current Info: The document database can be updated easily. The LLM gets the latest info at query time. Verifiable \u0026amp; Factual: Since you have the retrieved snippets, you can easily add references. It reduces hallucination by forcing the LLM to base its answer on the provided text. Scalable \u0026amp; Cost-Effective: You\u0026rsquo;re not retraining the LLM. The main work is in the retrieval step. Cons: Implementation Complexity: It\u0026rsquo;s a multi-component system (retriever + generator) that needs to work well together. Dependence on Retrieval Quality: If you retrieve irrelevant documents, the LLM will give a garbage answer. The retriever is critical. The Decision: The book concludes, \u0026ldquo;RAG offers a balanced solution in terms of ease of setup, cost, and scalability\u0026hellip; Therefore, we choose RAG to build our ChatPDF system.\u0026rdquo; This is the correct, well-justified choice.\nStep 3: Data Preparation (The \u0026ldquo;R\u0026rdquo; in RAG) This is the entire process of making your knowledge base searchable. The book outlines a three-step pipeline.\ngraph TD A[Document Databases (PDFs)] --\u0026gt; B(Document Parsing); B --\u0026gt; C(Document Chunking); C --\u0026gt; D(Indexing); D --\u0026gt; E[Indexed Embeddings\u0026lt;br/\u0026gt;(in Vector DB)]; (Simplified from Figure 6.8)\n1. Document Parsing: Getting Content out of PDFs\nPDFs are a nightmare. They can have columns, tables, images, and weird layouts.\nRule-based: You write code that assumes a certain layout. Brittle and fails on complex or varied documents. AI-based (The Winner): Use a model to \u0026ldquo;read\u0026rdquo; the PDF like a human. As the book explains, a tool like Layout-Parser does this: Layout Detection: An object detection model draws boxes around paragraphs, tables, images, etc. Text Extraction: OCR is run inside each text box. Structured Output: You get a list of content blocks with their type (text, image), coordinates, and content. This is much more useful than just a wall of raw text. 2. Document Chunking: Breaking It Down\nYou can\u0026rsquo;t create an embedding for an entire 50-page document.\nWhy? An embedding of a whole document averages out the meaning and loses specific details. A query for a specific sentence will get lost. The retrieved document chunk needs to fit into the LLM\u0026rsquo;s context window. Strategies: Length-based: Simple but dumb. Can cut sentences in half. RecursiveCharacterTextSplitter from libraries like LangChain is a smarter version that tries to split on paragraphs, then sentences, then words to keep semantic units together. Content-aware: Better. Split based on document structure, like Markdown headers (#), HTML tags, etc. 3. Indexing: Making Chunks Findable\nNow you have thousands or millions of chunks. How do you find the right ones for a given query, fast?\nKeyword/Full-text Search (e.g., Elasticsearch): Fast and good for matching exact words or phrases. But it struggles with synonyms and semantic meaning. A query for \u0026ldquo;employee compensation\u0026rdquo; might miss a document that only uses the word \u0026ldquo;staff salary.\u0026rdquo; Vector-based Search (The Winner): Why? It searches based on semantic meaning, not keywords. How? You use an embedding model (like a Transformer encoder) to convert every chunk of text/image into a high-dimensional vector (an array of numbers). Chunks with similar meanings will have vectors that are \u0026ldquo;close\u0026rdquo; to each other in this vector space. The book does a back-of-the-envelope calculation: 5M pages, chunked, results in ~40M chunks. At this scale, vector search is the only viable option for semantic retrieval. So, the data prep pipeline is: Parse PDFs into structured blocks -\u0026gt; Chunk blocks into small, meaningful pieces -\u0026gt; Embed each piece into a vector -\u0026gt; Store the vectors in a specialized Vector Database.\nStep 4: Model Development This covers the architecture of the ML models and the processes involved.\nArchitecture: The Key ML Models (Figure 6.9)\nA RAG system has three key ML model components:\nIndexing: An Encoder Model to create the vector embeddings. Retrieval: The same Encoder Model to turn the user query into a vector. (Plus an ANN search algorithm). Generation: An LLM to generate the final answer. The Indexing/Retrieval Model: Text-Image Alignment\nOur documents have text and images. Our query is text. How do you find an image relevant to a text query? The embeddings need to live in the same \u0026ldquo;space.\u0026rdquo; The book outlines two approaches (Figure 6.10):\nShared Embedding Space (Best Approach): Use a multimodal model like CLIP. CLIP is pre-trained to map related images and their text descriptions to nearby points in vector space. You can use its text encoder for text chunks and its image encoder for images. This is elegant and powerful. Image Captioning (Workaround): Use an image captioning model to generate a text description for each image. Then, use a standard text-only encoder to embed that caption. This works but is less direct and might lose information. The Retrieval Process: Finding the Needles in the Haystack\nOnce the user query is embedded into a vector Eq, we need to find the k closest chunk vectors in our database of 40 million.\nExact Nearest Neighbor (Linear Search): Compare Eq to all 40 million vectors. Guarantees a perfect result but is way too slow. O(N*D) complexity. Unacceptable. Approximate Nearest Neighbor (ANN): The only practical solution. It trades a tiny bit of accuracy for a massive speedup. The book mentions four families of ANN algorithms: Tree-based (e.g., Annoy): Recursively partition the data space. Fast, but can struggle in very high dimensions. Hashing-based (LSH): Uses clever hash functions where similar vectors are likely to get the same hash key. You only search within the query\u0026rsquo;s hash bucket. Clustering-based (The book\u0026rsquo;s choice): Pre-cluster the 40M vectors into, say, 100,000 clusters. The search becomes a two-step process: Find the few clusters whose center is closest to the query vector. Do an exact search only within those few clusters. This massively reduces the search space. Graph-based (e.g., HNSW): The state-of-the-art for many use cases. It builds a graph where nodes are data points and edges connect close neighbors. Searching is like navigating this graph to find the closest point. Modern vector databases like Pinecone, Weaviate, or libraries like FAISS (from Meta) and ScaNN (from Google) implement these advanced ANN algorithms for you.\nHere is the overall retrieval process from Figure 6.16:\ngraph TD subgraph Data Preparation direction TB A[Document databases] --\u0026gt; B(Data preparation\u0026lt;br\u0026gt;Parsing/Chunking); B --\u0026gt; C[Index (images)]; B --\u0026gt; D[Index (text)]; C \u0026amp; D --\u0026gt; E(Clustering); end subgraph Retrieval direction LR F[User query\u0026lt;br\u0026gt;\u0026#34;How many cats live\u0026lt;br\u0026gt;in the company?\u0026#34;] --\u0026gt; G(Text Encoder); G --\u0026gt; H(Inter-Cluster\u0026lt;br\u0026gt;Search); E -- Selected Clusters --\u0026gt; H; H --\u0026gt; I(Intra-Cluster\u0026lt;br\u0026gt;Search); I --\u0026gt; J[Retrieved\u0026lt;br\u0026gt;data chunks]; end The Generation Process: Crafting the Final Answer\nThis is where the LLM comes in. The process is:\nTake the original user query. Take the top k retrieved data chunks from the retrieval step. Combine them into a single, well-structured prompt. Feed this prompt to the LLM. The LLM generates the answer, using a sampling strategy like top-p sampling for a good balance of correctness and fluency. A Deeper Look at Prompt Engineering for Generation\nHow do you structure that final prompt for the best results? The book dives into several powerful techniques.\nChain-of-Thought (CoT) Prompting: Instruct the model to \u0026ldquo;think step-by-step.\u0026rdquo; This forces it to lay out its reasoning process before giving the final answer, which often improves accuracy on complex questions. Few-shot Prompting: Give the model 2-3 examples of a Q\u0026amp;A pair in the desired format before giving it the real query. This helps it understand the expected tone and structure. Role-specific Prompting: Tell the model who it is. \u0026ldquo;You are an expert contract lawyer\u0026hellip; explain this clause in simple terms.\u0026rdquo; This grounds the model and helps it adopt the correct persona and level of detail. User-context Prompting: Include metadata about the user (language, location, etc.) to get more personalized results. Here is the final prompt structure from Figure 6.22, showing how all these pieces come together.\ngraph TD subgraph \u0026#34;Final Prompt Sent to LLM\u0026#34; A[\u0026#34;User\u0026#39;s initial query: [...]\u0026#34;]; B[\u0026#34;--- RETRIEVED CONTEXT ---\u0026lt;br/\u0026gt;[Retrieved document chunk 1]\u0026lt;br/\u0026gt;[Retrieved document chunk 2]\u0026lt;br/\u0026gt;...\u0026#34;]; C[\u0026#34;--- INSTRUCTIONS ---\u0026lt;br/\u0026gt;You are a helpful assistant for Company XYZ...\u0026#34;]; D[\u0026#34;--- EXAMPLES ---\u0026lt;br/\u0026gt;Example 1: Q: ... A: ...\u0026lt;br/\u0026gt;Example 2: Q: ... A: ...\u0026#34;]; E[\u0026#34;--- REASONING ---\u0026lt;br/\u0026gt;Based on the context, think step-by-step to answer the user\u0026#39;s query.\u0026#34;]; F[\u0026#34;--- USER INFO ---\u0026lt;br/\u0026gt;User language: English\u0026#34;]; end subgraph Labels direction LR L1[Role-Specific\u0026lt;br\u0026gt;Prompting] L2[Few-Shot\u0026lt;br\u0026gt;Prompting] L3[Chain-of-Thought\u0026lt;br\u0026gt;(CoT)] L4[User-Context\u0026lt;br\u0026gt;Prompting] L5[Retrieved\u0026lt;br\u0026gt;Context] end B -- Is --\u0026gt; L5 C -- Is --\u0026gt; L1 D -- Is --\u0026gt; L2 E -- Is --\u0026gt; L3 F -- Is --\u0026gt; L4 Training (Advanced Topic: RAFT) Most of the time, you start with pre-trained models. But what if your retrieval is noisy and the LLM struggles to distinguish good context from bad? RAFT (Retrieval-Augmented Fine-Tuning) is a technique to solve this.\nThe Idea: During finetuning, you create training examples that include the question, a \u0026ldquo;golden\u0026rdquo; (correct) document, AND several \u0026ldquo;distractor\u0026rdquo; (irrelevant) documents that were also retrieved. The Goal: You train the LLM to specifically pay attention to the golden document and ignore the distractors when generating the answer. This makes the LLM more robust to imperfect retrieval. Step 5: Evaluation (CRITICAL for RAG) Evaluating a RAG system is more complex than a standard LLM. You need to evaluate both the retriever and the generator. The book introduces an excellent \u0026ldquo;Triad of RAG evaluation\u0026rdquo; (Figure 6.23).\ngraph TD Query --\u0026gt;|Context Relevance| Context Context --\u0026gt;|Faithfulness| Results Query --\u0026gt;|Answer Relevance\u0026lt;br\u0026gt;Answer Correctness| Results This diagram shows that the final Results (the generated answer) depend on the Query, the retrieved Context, and the relationships between them. This leads to four key evaluation aspects:\nContext Relevance: Is the retriever working? Did we retrieve documents that are relevant to the query?\nMetrics: Standard information retrieval metrics like Precision@k, nDCG, Hit Rate. You need a labeled dataset of (query, relevant_doc) pairs for this. Faithfulness (or Groundedness): Is the generator hallucinating? Is the generated answer factually consistent with the retrieved context? You check if every statement in the answer can be backed up by the provided snippets.\nMethods: This is hard to automate. Often requires human evaluation or using another powerful LLM as a judge. Figure 6.24 shows a great example: if the context says Marie Curie won two Nobel prizes, an answer saying she won one has low faithfulness. Answer Relevance: Did the generator answer the user\u0026rsquo;s actual question? The retrieved context might be relevant, but the LLM could get sidetracked and generate an answer that doesn\u0026rsquo;t directly address the user\u0026rsquo;s intent.\nMethods: Again, often requires a human or an LLM judge to compare the user\u0026rsquo;s query and the final answer. Answer Correctness: Is the answer factually correct according to a ground truth reference? This is the classic accuracy measure.\nIn an interview, discussing this four-part evaluation framework shows a deep, practical understanding of the challenges of building reliable RAG systems.\nStep 6: Overall ML System Design This is the final blueprint. Figure 6.27 shows the end-to-end flow. Let\u0026rsquo;s recreate and walk through it.\ngraph TD subgraph Offline Process direction TB A[Document Databases] --\u0026gt; B(Document Parsing \u0026amp; Chunking) B --\u0026gt; C{Text Encoder}; B --\u0026gt; C_img{Image Encoder}; C --\u0026gt; D[Index (text)]; C_img --\u0026gt; D_img[Index (images)]; end subgraph Online / Inference Process direction LR E[User Query] --\u0026gt; F(Safety Filtering); F --\u0026gt; G(Query Expansion); G --\u0026gt; H{Text Encoder}; H --\u0026gt; I(Nearest Neighbor Search); D \u0026amp; D_img --\u0026gt; I; I --\u0026gt; J(Prompt Engineering); E --\u0026gt; J; J --\u0026gt; K[LLM]; K --\u0026gt; L(Safety Filtering); L --\u0026gt; M[Response]; end %% Grouping for clarity subgraph Indexing Process A;B;C;C_img;D;D_img end subgraph Retrieval G;H;I; end subgraph Generation J;K;L end A user query\u0026rsquo;s journey:\nOffline: A pipeline runs periodically to Parse, Chunk, and Index all 5 million documents into a vector database. This is the Indexing Process. Online: A user sends a query. Safety Filtering: The query is checked for harmful content. Query Expansion (Optional but good): The query is expanded with synonyms or rephrased to improve retrieval. \u0026ldquo;How much do I get for trips?\u0026rdquo; -\u0026gt; \u0026ldquo;travel reimbursement policy allowance\u0026rdquo;. Retrieval: The query is encoded into a vector, and an ANN search is performed on the vector DB to get the top-k chunks. Generation: The user\u0026rsquo;s query and the retrieved chunks are assembled into a prompt using techniques like CoT and role-prompting. This is fed to the LLM. Safety Filtering: The LLM\u0026rsquo;s response is checked for safety, PII, etc. The final, safe, and grounded response is sent to the user. This diagram is your high-level design for the interview. Being able to draw this and explain each component\u0026rsquo;s purpose and the trade-offs involved is an A+ answer.\n","permalink":"https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/chapter-2/","tags":null,"title":"Chapter 2: Gmail Smart Compose"},{"categories":null,"content":"2.1 Bag of Words (BoW) What it\u0026rsquo;s ultimately trying to achieve: The most fundamental problem when dealing with text in machine learning is that algorithms understand numbers, not words. BoW is one of the simplest, oldest, yet often effective, methods to convert a piece of text (a document) into a numerical vector that an ML model can then use, typically for tasks like classification (e.g., \u0026ldquo;Is this email spam or not?\u0026rdquo; or \u0026ldquo;What is the topic of this news article?\u0026rdquo;).\nThe Intuition (The \u0026ldquo;Bag\u0026rdquo;): Imagine you have a document. You take all the words, throw them into a \u0026ldquo;bag,\u0026rdquo; shake it up, and then count how many times each unique word appears (or simply note its presence/absence). The order of words is lost, just like items in a bag.\nCorpus: This is just the collection of all documents you\u0026rsquo;re working with. Tokenization: Before you can count, you need to define what a \u0026ldquo;word\u0026rdquo; (or more generally, a \u0026ldquo;token\u0026rdquo;) is. This involves splitting text into pieces – usually words, but sometimes subwords. We often convert everything to lowercase and remove punctuation to treat \u0026ldquo;The\u0026rdquo; and \u0026ldquo;the\u0026rdquo; as the same. Vocabulary: This is the list of all unique tokens found across your entire corpus. Vectorization: For each document, you create a vector. The length of this vector is the size of your vocabulary. Each position in the vector corresponds to a unique word in the vocabulary. The value at that position can be: Binary: 1 if the word is in the document, 0 if not. Count: The number of times the word appears in the document. (Later, we\u0026rsquo;d see TF-IDF, but for basic BoW, counts or binary are common). This collection of vectors forms a Document-Term Matrix (DTM), where rows are documents and columns are terms (tokens) from the vocabulary. Realism and Limitations:\nSparsity: Most documents only contain a small subset of the entire vocabulary. So, these BoW vectors are usually very sparse (mostly zeros), which can be inefficient. Loss of Order: \u0026ldquo;Man bites dog\u0026rdquo; and \u0026ldquo;Dog bites man\u0026rdquo; would have very similar (or identical) BoW representations, even though their meanings are opposite. Context is lost. No Semantic Understanding: \u0026ldquo;Automobile\u0026rdquo; and \u0026ldquo;car\u0026rdquo; are treated as completely different words, even though they are synonyms. The model has no inherent understanding of their similarity. Out-of-Vocabulary (OOV) words: If a new word appears during prediction that wasn\u0026rsquo;t in the training corpus (and thus not in the vocabulary), BoW has no way to represent it. Connecting to ML (for classification, as in the book): Once you have these numerical vectors for your documents, and you have labels (e.g., \u0026ldquo;Cinema,\u0026rdquo; \u0026ldquo;Music,\u0026rdquo; \u0026ldquo;Science\u0026rdquo;), you can train a classifier. For more than two classes (multiclass classification), we typically use the softmax activation function in the output layer of a neural network. Softmax converts raw scores (logits) from the network into a probability distribution over the classes – each class gets a probability, and they all sum to 1. The cross-entropy loss is then used to measure how far these predicted probabilities are from the true (one-hot encoded) labels.\n2.2 Word Embeddings What it\u0026rsquo;s ultimately trying to achieve: To overcome the limitations of BoW, especially the lack of semantic understanding and sparsity. Word embeddings aim to represent words as dense, lower-dimensional vectors such that words with similar meanings have similar vector representations (i.e., they are \u0026ldquo;close\u0026rdquo; in the vector space).\nThe Intuition (Words as Dense Vectors): Instead of a massive, sparse vector where only one position is \u0026lsquo;1\u0026rsquo; (like in a one-hot encoding of a word from BoW), each word gets a shorter vector (e.g., 100-300 dimensions instead of 50,000) where most numbers are non-zero. These numbers aren\u0026rsquo;t just counts; they are learned parameters that capture semantic properties. The classic example is that the vector operation vector(\u0026quot;king\u0026quot;) - vector(\u0026quot;man\u0026quot;) + vector(\u0026quot;woman\u0026quot;) results in a vector very close to vector(\u0026quot;queen\u0026quot;).\nHow it\u0026rsquo;s typically done (conceptual, like with Word2vec\u0026rsquo;s skip-gram): The core idea, popularized by algorithms like Word2vec (Tomáš Mikolov\u0026rsquo;s work!), is that words that appear in similar contexts tend to have similar meanings.\nA neural network is trained on a large corpus of text. Its task isn\u0026rsquo;t classification, but something like: given a word (the \u0026ldquo;target\u0026rdquo; word), predict its surrounding words (the \u0026ldquo;context\u0026rdquo; words), or vice-versa. The magic happens in the \u0026ldquo;embedding layer\u0026rdquo; of this network. After training, this layer contains the dense vector representations for each word in the vocabulary. We then discard the rest of the network and just keep these learned embeddings. These embeddings are learned from unlabeled text – you don\u0026rsquo;t need explicit labels like \u0026ldquo;spam\u0026rdquo; or \u0026ldquo;sports\u0026rdquo; for every document. Benefits:\nSemantic Similarity: Cosine similarity between word vectors can quantify how similar words are in meaning. Dimensionality Reduction: Vectors are much smaller than BoW vocabularies. Improved Generalization: Models can generalize better because they understand that \u0026ldquo;car\u0026rdquo; and \u0026ldquo;automobile\u0026rdquo; are related. Realism: These embeddings are powerful but are only as good as the data they were trained on. They can still inherit biases present in the training text.\n2.3 Byte-Pair Encoding (BPE) What it\u0026rsquo;s ultimately trying to achieve: To address two main problems:\nHuge Vocabularies: If you treat every unique word as a token, vocabularies can become enormous, especially with morphologically rich languages or typos. Out-of-Vocabulary (OOV) Words: Word embedding models still struggle if a word was never seen during their training. BPE is a subword tokenization algorithm. It breaks words into smaller, frequently occurring units (subwords). The Intuition (Smart Splitting): Start by treating every individual character as a token. Then, iteratively find the most frequent pair of adjacent tokens (characters or already formed subwords) in your corpus and merge them into a new, single token. Repeat this for a fixed number of merges or until your vocabulary reaches a desired size. For example, \u0026ldquo;lowest\u0026rdquo; might initially be l o w e s t. If es is a frequent pair, it becomes l o w es t. If est then becomes frequent, it\u0026rsquo;s l o w est. The underscore _ is often added to mark the start of a word (e.g., _low est) to differentiate subwords at the beginning of words from those in the middle.\nBenefits:\nManages Vocabulary Size: You can control the final vocabulary size. Handles OOV Words: Unknown words can often be represented as a sequence of known subwords (e.g., \u0026ldquo;unfamiliarword\u0026rdquo; might become \u0026ldquo;_un\u0026rdquo;, \u0026ldquo;famil\u0026rdquo;, \u0026ldquo;iar\u0026rdquo;, \u0026ldquo;word\u0026rdquo;). Captures Morphology: \u0026ldquo;running\u0026rdquo; might be tokenized as \u0026ldquo;_runn\u0026rdquo;, \u0026ldquo;ing\u0026rdquo;, sharing the \u0026ldquo;_runn\u0026rdquo; part with \u0026ldquo;runner.\u0026rdquo; Realism: The choice of vocabulary size is a hyperparameter. There are other subword tokenization methods too (like WordPiece, SentencePiece). BPE is a common and foundational one.\n2.4 Language Model (The Core Definition) What it\u0026rsquo;s ultimately trying to achieve: To assign a probability to a sequence of tokens. More practically, and more commonly for autoregressive models (which we focus on), it\u0026rsquo;s about predicting the next token in a sequence given all the preceding tokens. P(current_token | previous_tokens)\nThe Intuition (Super Autocomplete): Think of the autocomplete on your phone, but much more powerful. Given \u0026ldquo;The cat sat on the\u0026hellip;\u0026rdquo;, a good language model would assign a high probability to \u0026ldquo;mat,\u0026rdquo; a lower one to \u0026ldquo;table,\u0026rdquo; and a very low one to \u0026ldquo;banana.\u0026rdquo;\nKey Idea: It learns the statistical patterns of language. It\u0026rsquo;s not understanding in a human sense, but it\u0026rsquo;s very good at figuring out what typically follows what. The output for the next token is a probability distribution over the entire vocabulary.\n2.5 Count-Based Language Model (e.g., n-gram models) What it\u0026rsquo;s ultimately trying to achieve: To implement the language model definition P(current_token | previous_tokens) using simple frequency counts of n-grams from a large text corpus. \u0026ldquo;n-gram\u0026rdquo; just means a sequence of \u0026rsquo;n\u0026rsquo; tokens.\nThe Intuition (Simple Frequencies): If we want to predict the word after \u0026ldquo;the cat sat on,\u0026rdquo; we look at our corpus and see how many times \u0026ldquo;the cat sat on\u0026rdquo; appears. Then, among those occurrences, we count how many times each different word followed.\nA unigram model considers only the current word\u0026rsquo;s frequency (P(word)). A bigram model considers the previous word (P(word_i | word_{i-1})). A trigram model (as in the book example) considers the two previous words (P(word_i | word_{i-2}, word_{i-1})). The probability is calculated as: P(w_i | w_{i-2}, w_{i-1}) = count(w_{i-2}, w_{i-1}, w_i) / count(w_{i-2}, w_{i-1}) This is a Maximum Likelihood Estimate (MLE). Realism and Limitations:\nSparsity \u0026amp; Zero Probabilities: What if the specific trigram (w_{i-2}, w_{i-1}, w_i) or even the bigram (w_{i-2}, w_{i-1}) never appeared in your training corpus? You\u0026rsquo;d get a zero probability, which is bad because it means the model thinks a perfectly valid sequence is impossible. Solutions: Backoff: If the trigram count is zero, \u0026ldquo;back off\u0026rdquo; and use the bigram probability. If that\u0026rsquo;s zero, use the unigram probability. Smoothing (e.g., Laplace / Add-one Smoothing): Add a small count (like 1) to all observed n-gram counts, and adjust the denominator accordingly. This ensures no probability is ever strictly zero. Limited Context: N-gram models can only look back \u0026rsquo;n-1\u0026rsquo; tokens. They can\u0026rsquo;t capture long-range dependencies in text (e.g., a pronoun referring to a noun many sentences earlier). Storage: Storing counts for all possible n-grams (especially for n \u0026gt; 3 or 4) can become massive. 2.6 Evaluating Language Models What it\u0026rsquo;s ultimately trying to achieve: To objectively measure how good a language model is. This is crucial for comparing different models or different versions of the same model during development.\nKey Metrics:\nPerplexity (PPL): Intuition: A measure of how \u0026ldquo;surprised\u0026rdquo; or \u0026ldquo;perplexed\u0026rdquo; the model is by a sequence of text it hasn\u0026rsquo;t seen before (the test set). Lower perplexity is better. It\u0026rsquo;s related to the average number of choices the model effectively has at each step of prediction. If PPL is 10, it\u0026rsquo;s as if the model is choosing uniformly from 10 words at each step. Calculation: It\u0026rsquo;s the exponential of the average negative log-likelihood of the test set words. The negative log-likelihood is high if the model assigns low probabilities to the words that actually occurred. ROUGE (Recall-Oriented Understudy for Gisting Evaluation): Intuition: Primarily used for evaluating tasks like text summarization or machine translation, where the model generates a chunk of text. It measures the overlap (e.g., of unigrams, bigrams, or longest common subsequences) between the model-generated text and one or more human-written reference texts (ground truth). Variants: ROUGE-1 (unigram overlap), ROUGE-N (n-gram overlap), ROUGE-L (longest common subsequence). Higher is generally better. Human Evaluation: Intuition: Automated metrics don\u0026rsquo;t capture everything (e.g., fluency, coherence, factual correctness, engagingness). Humans are often the gold standard. Methods: Likert Scales: Raters score model outputs on predefined scales (e.g., 1-5 for fluency). Pairwise Comparison / Elo Ratings: Raters are shown outputs from two different models and choose which one is better. Over many comparisons, this can produce a relative ranking (like Elo ratings in chess). Realism: No single metric is perfect. The best metric often depends on the specific downstream task the language model is intended for. Comparing perplexity scores is only meaningful if the models use the exact same vocabulary and tokenization.\n","permalink":"https://deepskandpal.github.io/bookshelf/100-page-lm-book/chapter-2/","tags":null,"title":"Chapter 2: Language Modeling Basics"},{"categories":null,"content":"Notes for Chapter 2 Alright class, buckle up! Last time, we got a bird\u0026rsquo;s-eye view of the Machine Learning landscape – the different continents, climates, and major landmarks. Today, with Chapter 2, \u0026ldquo;End-to-End Machine Learning Project,\u0026rdquo; we\u0026rsquo;re grabbing our hiking boots and actually trekking through one of these landscapes. This is where the rubber meets the road!\nThe book says we\u0026rsquo;re pretending to be a recently hired data scientist at a real estate company. Our mission? To predict median housing prices in California. This chapter is fantastic because it walks you through the practical steps. It’s less about the theory of one specific algorithm and more about the process of doing ML in the real world.\n(Page 35: The 8 Steps \u0026amp; Working with Real Data)\nThe chapter lays out 8 key steps, and we\u0026rsquo;ll follow them closely:\nLook at the big picture. Get the data. Discover and visualize the data to gain insights. Prepare the data for Machine Learning algorithms. Select a model and train it. Fine-tune your model. Present your solution. Launch, monitor, and maintain your system. It also rightly emphasizes using real-world data. Artificial datasets are fine for understanding a specific algorithm, but real data? That\u0026rsquo;s where you learn about the messiness, the missing values, the quirks – the stuff that makes this field challenging and fun! The book lists some great places to find datasets: UC Irvine, Kaggle, AWS datasets, etc. (Page 36).\nFor our project, we\u0026rsquo;ll use the California Housing Prices dataset (Figure 2-1, page 36). It\u0026rsquo;s from the 1990 census – a time when, as the book cheekily notes, \u0026ldquo;a nice house in the Bay Area was still affordable.\u0026rdquo; Ha! The map shows housing prices across California, with population density. It\u0026rsquo;s a good, rich dataset for learning.\n(Page 37-41: Look at the Big Picture)\nThis is step one, and it\u0026rsquo;s absolutely crucial. Before you write a single line of code, you need to understand the why.\nFrame the Problem:\nYour boss wants a model of housing prices. But why? What’s the business objective? Is it to decide where to invest? Is it to advise clients? Knowing this shapes everything: how you frame the problem, what algorithms you pick, how you measure success, and how much effort you put into tweaking. The book says our model\u0026rsquo;s output (median housing price for a district) will be fed into another ML system (Figure 2-2, page 38). This downstream system decides if investing in an area is worthwhile. So, getting our price prediction right directly impacts revenue. That makes it critical! The book also introduces the idea of Pipelines (page 38). ML systems are often sequences of data processing components. Data flows from one to the next. This modularity is great for organization and robustness but needs monitoring. What\u0026rsquo;s the current solution (if any)? The boss says experts currently estimate prices manually – costly, time-consuming, and often off by \u0026gt;20%. This gives us a baseline performance to beat and justifies building an ML model. Now, we frame it technically (page 39): Supervised, Unsupervised, or Reinforcement? We have labeled examples (districts with their median housing prices). So, it’s supervised learning. Classification or Regression? We\u0026rsquo;re predicting a value (price). So, it’s a regression task. More specifically, it\u0026rsquo;s multiple regression (using multiple features like population, median income to predict the price) and univariate regression (predicting a single value – the median housing price – per district). If we were predicting multiple values (e.g., price and rental yield), it\u0026rsquo;d be multivariate regression. Batch or Online Learning? There\u0026rsquo;s no continuous flow of new data, and the dataset is small enough. So, batch learning is fine. Select a Performance Measure (Page 39):\nHow do we know if our model is good? For regression, a common measure is Root Mean Square Error (RMSE). Equation 2-1 shows the formula. What it\u0026rsquo;s ultimately trying to achieve: RMSE gives us a sense of how much error our system typically makes in its predictions, in the same units as the thing we\u0026rsquo;re predicting (dollars, in this case). It penalizes larger errors more heavily because of the squaring. The book then introduces some common Notations (page 40), which are standard in ML:\nm: Number of instances. x⁽ⁱ⁾: Feature vector for the i-th instance (all input attributes like longitude, population, etc., for one district). y⁽ⁱ⁾: Label for the i-th instance (the actual median house price for that district). X (capital X): Matrix containing all feature vectors for all instances (one row per district). h: The prediction function (our model), also called the hypothesis. ŷ⁽ⁱ⁾ (y-hat): The predicted value for the i-th instance, i.e., h(x⁽ⁱ⁾). What if RMSE isn\u0026rsquo;t ideal? If you have many outliers, you might prefer Mean Absolute Error (MAE) (Equation 2-2, page 41). What it\u0026rsquo;s trying to achieve: MAE measures the average absolute difference. It\u0026rsquo;s less sensitive to outliers than RMSE because it doesn\u0026rsquo;t square the errors. The book connects these to different norms: RMSE is related to the Euclidean norm (l₂ norm – your standard straight-line distance), and MAE to the Manhattan norm (l₁ norm – like distance in a city grid). Generally, higher norm indices focus more on large values. RMSE (l₂) is usually preferred when outliers are rare (like in a bell-shaped distribution).(more details at the end of the chapter glossary)\nCheck the Assumptions (Page 42):\nSuper important sanity check! We assume the downstream system needs actual price values. What if it actually converts prices into categories like \u0026ldquo;cheap,\u0026rdquo; \u0026ldquo;medium,\u0026rdquo; \u0026ldquo;expensive\u0026rdquo;? Then our regression task is overkill; it should have been a classification task! Fortunately, the book says our colleagues confirm they need actual prices. Phew! (Page 42-50: Get the Data)\nTime to get our hands dirty!\nCreate the Workspace (Page 42-45):\nThis is about setting up your Python environment. The book walks through installing Python, Jupyter, NumPy, pandas, Matplotlib, and Scikit-Learn. It strongly recommends using an isolated environment (like virtualenv or conda env). This is crucial in real projects to avoid library version conflicts. Think of it as a clean, dedicated lab space for each experiment. Then, it shows how to start Jupyter and create a new notebook (Figure 2-3, 2-4). Download the Data (Page 46):\nIn real life, data might be in a database, spread across files, etc. Here, it\u0026rsquo;s simpler: a single compressed file. The book provides a nifty Python function fetch_housing_data() to download and extract the data. Why a function? Automation! If data changes regularly, you can re-run the script. Take a Quick Look at the Data Structure (Page 47-50):\nFirst, a function load_housing_data() using pandas to load the CSV into a DataFrame. .head() (Figure 2-5): Shows the top 5 rows. We see attributes like longitude, latitude, housing_median_age, total_rooms, etc. 10 attributes in total. .info() (Figure 2-6): Super useful! 20,640 instances (rows) – fairly small for ML, but good for learning. total_bedrooms has only 20,433 non-null values. This means 207 districts are missing this data. We\u0026rsquo;ll need to handle this! Most attributes are float64 (numerical), but ocean_proximity is an object. Since it\u0026rsquo;s from a CSV, it\u0026rsquo;s likely text, probably categorical. .value_counts() on ocean_proximity (page 48) confirms it\u0026rsquo;s categorical: \u0026lt;1H OCEAN, INLAND, etc., with counts for each. .describe() (Figure 2-7, page 49): Shows summary statistics for numerical attributes (count, mean, std, min, 25th/50th/75th percentiles, max). std (standard deviation) tells us how spread out the values are. Percentiles (quartiles) give us a sense of the distribution. 25% of districts have housing_median_age \u0026lt; 18. Histograms (page 49, Figure 2-8 on page 50): A histogram for each numerical attribute. This helps us see: Median Income: Doesn\u0026rsquo;t look like USD. The book clarifies it\u0026rsquo;s scaled (roughly tens of thousands of dollars) and capped at 15 (for high incomes) and 0.5 (for low incomes). Preprocessing is common. Housing Median Age \u0026amp; Median House Value: Also capped. The median_house_value cap (our target variable!) could be a problem. If the model learns prices never go above $500k, it can\u0026rsquo;t predict higher. We might need to get uncapped data or remove these districts. Different Scales: Attributes have very different ranges (e.g., total_rooms vs. median_income). Many ML algorithms don\u0026rsquo;t like this. We\u0026rsquo;ll need feature scaling. Tail-Heavy Distributions: Many histograms are skewed (e.g., extend far to the right). This can make it harder for algorithms to detect patterns. We might try transforming them (e.g., log transform) later. (Page 51-55: Create a Test Set)\n\u0026ldquo;Wait!\u0026rdquo; says the scorpion icon (page 51). \u0026ldquo;Before you look at the data any further, you need to create a test set, put it aside, and never look at it.\u0026rdquo;\nWhy so early? Our brains are amazing pattern detectors. If we look too closely at the entire dataset, including what will be our test set, we might subconsciously pick up on patterns in the test data. This is data snooping bias. When we later evaluate our model on this \u0026ldquo;seen\u0026rdquo; test set, our performance estimate will be too optimistic. The model won\u0026rsquo;t generalize as well in the real world. How to create it? Theoretically, pick ~20% randomly and set aside. The book gives a simple split_train_test function (page 52). Problem: Running it again gives a different test set. Over time, your algorithm (or you!) might see the whole dataset. Solutions: Save the test set on the first run. Set a random seed (np.random.seed(42) – 42 is the \u0026ldquo;Answer to the Ultimate Question of Life, the Universe, and Everything,\u0026rdquo; a common tongue-in-cheek seed). But these break if you fetch an updated dataset. Better solution for stable splits: Use a unique, immutable instance identifier. Compute a hash of the ID, and if the hash is below a threshold (e.g., 20% of max hash value), it goes into the test set. This ensures consistency even if the dataset is refreshed (new instances go to train/test consistently, old instances stay put). The housing dataset doesn\u0026rsquo;t have an ID column. Simplest is to use row index (if data is only ever appended). Or create an ID from stable features (e.g., latitude + longitude, though page 53 notes this can cause issues if coarse). Scikit-Learn\u0026rsquo;s train_test_split (page 53): Simpler, has random_state for reproducibility. Can also split multiple datasets (e.g., features and labels) consistently. Stratified Sampling (Page 53-55): Random sampling is fine for large datasets. For smaller ones, you risk sampling bias. Imagine surveying 1000 people: random sampling might accidentally give you 60% men, not representative of the population. Stratified sampling ensures the test set is representative of important subgroups (strata). Experts say median_income is very important. We want our test set to have a similar income distribution to the full dataset. Since median_income is continuous, we first create an income category attribute (pd.cut()) with 5 categories (Figure 2-9, page 54). Not too many strata, each large enough. Then use Scikit-Learn\u0026rsquo;s StratifiedShuffleSplit to sample, ensuring similar proportions of each income category in train and test sets. Figure 2-10 (page 55) shows stratified sampling is much better than random sampling at preserving income category proportions. Finally, drop the temporary income_cat attribute. This attention to test set creation is critical but often overlooked! (Page 56-61: Discover and Visualize the Data to Gain Insights)\nNow, working only with the training set (or a copy, strat_train_set.copy()):\nVisualizing Geographical Data (Page 56-57):\nScatter plot of longitude vs. latitude (Figure 2-11). Looks like California, but hard to see patterns. Setting alpha=0.1 (Figure 2-12) reveals high-density areas (Bay Area, LA, San Diego, Central Valley). More advanced plot (Figure 2-13, page 57): Circle radius = population, color = median house price. Clearly shows prices are higher near coasts and in dense areas. Looking for Correlations (Page 58-60):\nStandard correlation coefficient (Pearson\u0026rsquo;s r): Ranges from -1 (strong negative correlation) to 1 (strong positive correlation). 0 means no linear correlation. Use .corr() method on the DataFrame. corr_matrix[\u0026quot;median_house_value\u0026quot;].sort_values(ascending=False) shows median_income has the strongest positive correlation (0.68) with median_house_value. Figure 2-14 (page 59) shows examples: correlation only captures linear relationships. Nonlinear relationships can have r=0. scatter_matrix function (pandas.plotting): Plots every numerical attribute against every other (Figure 2-15, page 60). Diagonal shows histograms. Focus on attributes promising for predicting median_house_value. Zoom in on median_income vs. median_house_value (Figure 2-16, page 61): Strong upward trend. Price cap at $500k clearly visible. Other less obvious horizontal lines (data quirks around $450k, $350k). Might want to remove these districts to prevent model learning these quirks. Experimenting with Attribute Combinations (Page 61):\nSometimes, combining attributes makes more sense. total_rooms isn\u0026rsquo;t useful without households. So, create rooms_per_household. Similarly, bedrooms_per_room and population_per_household. Check correlations again (page 62). bedrooms_per_room is more negatively correlated with price than total_bedrooms or total_rooms. Lower bedroom/room ratio (fewer bedrooms for a given house size, or larger rooms) often means more expensive. This exploration is iterative. Get a prototype, analyze its output, come back here.\n(Page 62-71: Prepare the Data for Machine Learning Algorithms)\nTime to write functions for data transformations. Why functions?\nReproducibility. Build a library of transformations. Use in live system for new data. First, revert to a clean training set and separate predictors (housing) from labels (housing_labels).\nData Cleaning (Missing Values - page 63):\nWe saw total_bedrooms has missing values. Options: Get rid of corresponding districts: housing.dropna(subset=[\u0026quot;total_bedrooms\u0026quot;]) Get rid of the whole attribute: housing.drop(\u0026quot;total_bedrooms\u0026quot;, axis=1) Set values to something (zero, mean, median): median = housing[\u0026quot;total_bedrooms\u0026quot;].median(), then housing[\u0026quot;total_bedrooms\u0026quot;].fillna(median, inplace=True) If using option 3, compute median on training set and save it to fill missing values in test set and live data. Scikit-Learn\u0026rsquo;s SimpleImputer (page 63): Create an imputer: imputer = SimpleImputer(strategy=\u0026quot;median\u0026quot;) It only works on numerical attributes, so drop ocean_proximity first: housing_num = housing.drop(\u0026quot;ocean_proximity\u0026quot;, axis=1) Fit the imputer to numerical training data: imputer.fit(housing_num) (computes medians). The medians are stored in imputer.statistics_. Transform the training set: X = imputer.transform(housing_num) (returns NumPy array). Convert back to DataFrame if needed. Scikit-Learn Design Principles (sidebar, page 64-65): Consistency: Estimators, Transformers, Predictors. Estimators: Any object that can estimate params from data (e.g., imputer). fit(data, [labels]) method. Hyperparameters set in constructor. Transformers: Estimators that can transform data (e.g., imputer). transform(data) method. Also fit_transform() (often optimized). Predictors: Estimators that can make predictions (e.g., LinearRegression). predict(data) method. score(data, labels) method. Inspection: Hyperparameters are public instance vars (e.g., imputer.strategy). Learned params end with _ (e.g., imputer.statistics_). Nonproliferation of classes: Uses NumPy arrays, basic Python types. Composition: Easy to combine (e.g., Pipeline). Sensible defaults. Handling Text and Categorical Attributes (Page 65-67):\nocean_proximity is categorical. OrdinalEncoder (Scikit-Learn \u0026gt;= 0.20): Converts categories to numbers (0, 1, 2\u0026hellip;). (Page 66). Problem: ML algorithms assume nearby numbers are more similar (e.g., 0 and 1 more similar than 0 and 4). This isn\u0026rsquo;t true for ocean_proximity. OneHotEncoder (Scikit-Learn): Common solution (page 67). Creates one binary attribute per category (dummy variables). Only one is \u0026ldquo;hot\u0026rdquo; (1), others \u0026ldquo;cold\u0026rdquo; (0). Output is a SciPy sparse matrix (efficient for many categories, as it only stores non-zero locations). Can convert to dense with .toarray(). If many categories, one-hot encoding creates many features. Alternatives: replace with numerical related features (e.g., distance to ocean) or use embeddings (learnable low-dimensional vectors, Ch 13 \u0026amp; 17). Custom Transformers (Page 68):\nFor custom cleanup or attribute combination. Create a class, implement fit(), transform(), fit_transform(). Add TransformerMixin (gets fit_transform() for free) and BaseEstimator (gets get_params(), set_params() for hyperparameter tuning, if no *args, **kwargs in constructor). Example: CombinedAttributesAdder class to add rooms_per_household etc. Has a hyperparameter add_bedrooms_per_room. Feature Scaling (Page 69):\nCrucial! ML algos often perform badly if numerical inputs have very different scales. Two common ways: Min-max scaling (Normalization): Values shifted/rescaled to range 0-1. Subtract min, divide by (max-min). Scikit-Learn: MinMaxScaler. Standardization: Subtract mean, divide by std dev. Resulting distribution has zero mean, unit variance. Not bounded to a specific range. Less affected by outliers. Scikit-Learn: StandardScaler. Important: Fit scalers on training data only! Then use them to transform train set, test set, and new data. Transformation Pipelines (Page 70-71):\nMany data prep steps need to be in order. Scikit-Learn\u0026rsquo;s Pipeline helps. Example num_pipeline for numerical attributes (page 70): SimpleImputer -\u0026gt; CombinedAttributesAdder -\u0026gt; StandardScaler. Call pipeline.fit_transform(housing_num). ColumnTransformer (Scikit-Learn \u0026gt;= 0.20): Applies different transformers to different columns. Super convenient! (Page 71) Takes list of (name, transformer, column_list) tuples. Applies num_pipeline to numerical columns, OneHotEncoder to categorical. Concatenates outputs. Handles sparse/dense matrix mix. This gives one preprocessing pipeline for all data! (Page 72-74: Select and Train a Model)\nFinally, the exciting part!\nTraining and Evaluating on the Training Set (Page 72):\nLinear Regression: lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) Try on a few instances: predictions aren\u0026rsquo;t great. RMSE on whole training set: $68,628. Not good (most house values are $120k-$265k). This is underfitting. Features might not be good enough, or model too simple. Decision Tree Regressor (more complex model, Ch 6): tree_reg = DecisionTreeRegressor() tree_reg.fit(housing_prepared, housing_labels) Evaluate on training set: RMSE = 0.0! Perfect? (Page 73) No, much more likely it badly overfit the data. Don\u0026rsquo;t touch test set yet! Use part of training set for validation. Better Evaluation Using Cross-Validation (Page 73-74):\nOption 1: train_test_split training set into smaller train/validation. Option 2 (better): K-fold cross-validation. Scikit-Learn\u0026rsquo;s cross_val_score. Splits training set into K folds (e.g., 10). Trains K times, each time using a different fold for evaluation and other K-1 folds for training. Returns K scores. Scoring function: cross_val_score expects utility (higher is better), not cost (lower is better). So use scoring=\u0026quot;neg_mean_squared_error\u0026quot;. Then take np.sqrt(-scores). Decision Tree CV scores (page 74): Mean RMSE ~71,407. Standard deviation ~2,439. Much worse than 0! And worse than Linear Regression. Linear Regression CV scores: Mean RMSE ~69,052. Std dev ~2,732. Decision Tree is overfitting badly. Random Forest Regressor (Ensemble Learning, Ch 7): Trains many Decision Trees on random subsets of features, averages predictions. Skip code details (similar to others). Training set RMSE (page 75, forest_rmse): ~18,603. CV scores: Mean RMSE ~50,182. Std dev ~2,097. Much better! But training score is still much lower than validation scores -\u0026gt; still overfitting. Solutions: simplify model, regularize, get more data. Goal: Shortlist 2-5 promising models (SVMs, neural nets, etc.) without too much hyperparameter tweaking yet. Save your models! Use joblib (better for large NumPy arrays) or pickle. Save hyperparameters, CV scores, predictions. (Page 75-78: Fine-Tune Your Model)\nNow, take your shortlisted models and fine-tune their hyperparameters.\nGrid Search (Page 76):\nTedious to do manually. Use Scikit-Learn\u0026rsquo;s GridSearchCV. Tell it which hyperparameters to try and what values. It uses cross-validation to evaluate all combinations. Example param_grid for RandomForestRegressor. It will explore (3x4 + 2x3) = 18 combinations, each trained 5 times (for cv=5). 90 rounds of training! grid_search.best_params_ gives the best combination. grid_search.best_estimator_ gives the best model retrained on the full training set (if refit=True, default). Evaluation scores for each combo are in grid_search.cv_results_. Example result (page 77): RMSE 49,682. Better than default 50,182. Can treat data prep steps as hyperparameters too! (e.g., add_bedrooms_per_room in your custom transformer). Randomized Search (Page 78):\nRandomizedSearchCV. Good for large hyperparameter search spaces. Instead of trying all combos, evaluates a given number of random combinations. Benefits: explores more values per hyperparameter if run long enough; more control over budget. Ensemble Methods (Page 78):\nCombine best models. Often performs better than any single model, especially if they make different types of errors. (More in Ch 7). Analyze the Best Models and Their Errors (Page 78-79):\nGain insights! RandomForestRegressor can show feature importances. List importances with attribute names (page 79). median_income is most important. INLAND (categorical feature) is surprisingly important. Might decide to drop less useful features. Look at specific errors your system makes. Why? How to fix? (Page 79-80: Evaluate Your System on the Test Set)\nThe moment of truth! After all tweaking, evaluate the final model on the test set.\nGet predictors (X_test) and labels (y_test) from strat_test_set. Run full_pipeline.transform(X_test) (NOT fit_transform – don\u0026rsquo;t fit to test set!). final_model = grid_search.best_estimator_ final_predictions = final_model.predict(X_test_prepared) final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse) -\u0026gt; e.g., 47,730. Point estimate might not be enough. Compute a 95% confidence interval using scipy.stats.t.interval(). If performance is worse than CV scores (often is due to tuning on validation set), resist temptation to tweak based on test set results! Those tweaks are unlikely to generalize. (Page 80-83: Launch, Monitor, and Maintain Your System)\nYou got approval to launch!\nPrelaunch: Present solution, highlight learnings, assumptions, limitations. Document. Create clear visualizations. Launch: Polish code, write docs, tests. Deploy model. Save trained pipeline (e.g., with joblib). Load in production environment. Maybe wrap in a web service that your app queries via REST API (Figure 2-17, page 81). Easier upgrades, scaling. Or deploy on cloud (e.g., Google Cloud AI Platform). Monitor: This is not the end! Check live performance regularly. Trigger alerts if it drops. Models \u0026ldquo;rot\u0026rdquo; over time as the world changes. (Cats and dogs don\u0026rsquo;t mutate, but camera tech and breed popularity do!) Infer performance from downstream metrics (e.g., sales of recommended products). Or use human raters / crowdsourcing (Amazon Mechanical Turk) for tasks needing human judgment. Monitor input data quality (malfunctioning sensors, stale data from other teams). Maintain: Automate as much as possible: Collect fresh data regularly and label it. Script to retrain model and fine-tune hyperparameters. Script to evaluate new vs. old model on updated test set, deploy if better. Keep backups of models and datasets. Easy rollback. Compare new models to old. Evaluate on specific subsets of test data for deeper insights (e.g., recent data, specific input types like inland vs. ocean districts). ML involves a lot of infrastructure! First project is a lot of effort. Once it\u0026rsquo;s in place, future projects are faster.\n(Page 83-84: Try It Out! \u0026amp; Exercises) The chapter ends by encouraging you to try this whole process on a dataset you\u0026rsquo;re interested in (e.g., from Kaggle). And then lists some great exercises based on the housing dataset. Definitely try these!\nGlossary RMSE , MAE a Deep dive Think about it this way: both RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) are trying to tell us, on average, how \u0026ldquo;far off\u0026rdquo; our model\u0026rsquo;s predictions are from the actual values. The \u0026ldquo;how far off\u0026rdquo; part is where the idea of \u0026ldquo;distance\u0026rdquo; comes in, and norms are just formal mathematical ways of defining distance.\nLet\u0026rsquo;s look at a single prediction error first. Suppose the actual house price (y) is $300,000 and our model predicts (ŷ) $250,000. The error is $50,000.\nMAE (Mean Absolute Error) and the l₁ norm (Manhattan Distance):\nMAE takes the absolute value of this error: |$250,000 - $300,000| = $50,000. Then it averages these absolute errors over all your predictions. The l₁ norm (read as \u0026ldquo;L-one norm\u0026rdquo;) of a vector (think of a vector of errors [e₁, e₂, ..., eₘ]) is the sum of the absolute values of its components: |e₁| + |e₂| + ... + |eₘ|. The MAE is just this sum divided by m (the number of errors). Why \u0026ldquo;Manhattan Distance\u0026rdquo;? Imagine you\u0026rsquo;re in a city like Manhattan where streets form a grid. To get from point A to point B, you can\u0026rsquo;t cut diagonally through buildings. You have to travel along the streets (say, 3 blocks east and 4 blocks north). The total distance is 3 + 4 = 7 blocks. This is the l₁ distance. You\u0026rsquo;re summing the absolute differences along each axis (the \u0026ldquo;east-west\u0026rdquo; error and the \u0026ldquo;north-south\u0026rdquo; error, if you will). In our error context, we only have one \u0026ldquo;axis\u0026rdquo; of error for each prediction, and MAE sums these absolute \u0026ldquo;street-block\u0026rdquo; errors. RMSE (Root Mean Square Error) and the l₂ norm (Euclidean Distance):\nRMSE takes the error ($50,000), squares it (($50,000)² = 2,500,000,000), then averages these squared errors, and finally takes the square root of that average. The l₂ norm (read as \u0026ldquo;L-two norm\u0026rdquo;) of a vector of errors [e₁, e₂, ..., eₘ] is √(e₁² + e₂² + ... + eₘ²). Why \u0026ldquo;Euclidean Distance\u0026rdquo;? This is the \u0026ldquo;as the crow flies\u0026rdquo; distance you learned in geometry using the Pythagorean theorem: distance = √(Δx² + Δy²). If you have a vector of errors, the l₂ norm is like calculating the length of that vector in a multi-dimensional space. It\u0026rsquo;s our everyday understanding of straight-line distance. When RMSE squares the errors, it gives much more weight to larger errors. An error of 10 becomes 100, but an error of 2 becomes 4. The error of 10 contributes 25 times more to the sum of squares than the error of 2 (100 vs 4), whereas for MAE, it would only contribute 5 times more (10 vs 2). The square root at the end brings the measurement back into the original units (e.g., dollars). So, what\u0026rsquo;s the ultimate goal of connecting them to norms?\nGeneralization: Norms are a general mathematical concept for measuring the \u0026ldquo;size\u0026rdquo; or \u0026ldquo;length\u0026rdquo; of vectors. By recognizing that RMSE and MAE are related to specific norms (l₂ and l₁ respectively), it places them within a broader mathematical framework. There are other norms too (l₀, l∞, lₚ in general), and each has different properties and might be useful in different contexts for measuring error or regularizing models.\nFor instance, the l₀ norm counts the number of non-zero elements in a vector. The l∞ (L-infinity) norm gives the maximum absolute value in a vector (it focuses only on the largest error). Understanding Properties: Knowing the underlying norm helps us understand the properties of the error metric.\nBecause RMSE uses the l₂ norm (squaring), it\u0026rsquo;s more sensitive to outliers. One very large error, when squared, can dominate the RMSE. Because MAE uses the l₁ norm (absolute value), it\u0026rsquo;s more robust to outliers. A large error is just a large error; it\u0026rsquo;s not disproportionately magnified. Consistency in Terminology: In more advanced ML literature, you\u0026rsquo;ll often see error functions or regularization terms described using norm notation (e.g., \u0026ldquo;l₁ regularization\u0026rdquo; or \u0026ldquo;l₂ regularization\u0026rdquo;). Understanding this connection early on helps you navigate that.\nLet\u0026rsquo;s try a simple analogy:\nImagine you have two friends, Alex (l₁) and Beth (l₂), and you ask them to measure how \u0026ldquo;messy\u0026rdquo; a room is based on how far items are from their correct places.\nAlex (l₁ / MAE): For each misplaced item, Alex measures how far it is from its spot (e.g., book is 2 feet off, sock is 3 feet off). Alex just adds up these distances (2+3=5 feet of \u0026ldquo;mess\u0026rdquo;). Alex treats every foot of displacement equally. Beth (l₂ / RMSE): For each misplaced item, Beth measures the distance, squares it (book 2²=4, sock 3²=9), adds them up (4+9=13), and then might take a square root. Beth gets really upset by items that are very far off, because squaring makes those large distances much bigger. A sock 10 feet away (10²=100) is much, much worse to Beth than 5 socks each 2 feet away (5 * 2² = 20). So, when the book says \u0026ldquo;RMSE corresponds to the Euclidean norm\u0026rdquo; and \u0026ldquo;MAE corresponds to the l₁ norm,\u0026rdquo; it\u0026rsquo;s essentially saying:\nRMSE measures error in a way that emphasizes large errors more (like a straight-line distance calculation that involves squaring). MAE measures error in a way that treats all magnitudes of error more evenly (like adding up block-by-block distances). Did that make it a bit clearer? The key is that they are both ways to quantify \u0026ldquo;how far off\u0026rdquo; you are, but they \u0026ldquo;feel\u0026rdquo; or \u0026ldquo;react\u0026rdquo; to large errors differently because of their mathematical structure, which is captured by these different types of norms.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-2/","tags":null,"title":"Chapter 2: End-to-End Machine Learning Project"},{"categories":null,"content":"This chapter will guide us through:\nObjectives: Why are we building this system in the first place? Requirements: What qualities must our system possess? (Think reliability, scalability). Iterative Process: Spoiler: building ML systems is rarely a straight line. Problem Framing: How do we turn a vague business problem into a concrete ML task? This is crucial. And finally, a bit of a philosophical discussion: Data vs. Algorithms. You\u0026rsquo;ll see a recurring theme: \u0026ldquo;Not so soon!\u0026rdquo; Before you jump to coding a model, there\u0026rsquo;s a lot of critical thinking and planning. This upfront work is what separates a successful, production-ready ML system from a science project. This chapter is packed with concepts that will come up in ML system design interviews.\nLet\u0026rsquo;s get to it!\nPage 1 (of Chapter 2 text): Reiteration and Roadmap This page just sets the stage, reminding us of the holistic view from Chapter 1. The key components are:\nBusiness requirements Data stack Infrastructure Deployment Monitoring And, importantly, the stakeholders involved. The chapter will flow from high-level objectives down to the specifics of framing the ML task. The author teases: \u0026ldquo;The difficulty of your job can change significantly depending on how you frame your problem.\u0026rdquo; This is an understatement! A good problem framing can save you months of effort.\nPages 2-4: Business and ML Objectives – The \u0026ldquo;Why\u0026rdquo; This is the absolute starting point for any ML project, especially in a business context like FAANG.\nML Objectives vs. Business Objectives: Data Scientists often focus on ML Objectives: These are metrics we can directly measure from our model – accuracy, precision, recall, F1-score, RMSE, inference latency. We get excited about tweaking a model to push accuracy from 94% to 94.2%. Companies care about Business Objectives: The truth is, most businesses don\u0026rsquo;t care about that 0.2% accuracy bump unless it translates into something meaningful for them. This means: Increasing revenue (e.g., more sales, higher ad click-through rates) Reducing costs (e.g., less fraud, more efficient operations) Improving user engagement (e.g., more time on site, higher monthly active users) Increasing customer satisfaction. The book quotes Milton Friedman: \u0026ldquo;The social responsibility of business is to increase its profits.\u0026rdquo; While this is a specific economic viewpoint, the underlying message for us is that ML projects within a business context must ultimately contribute to the business\u0026rsquo;s success.\nThe Pitfall: A very common failure pattern, as the book highlights (citing Eugene Yan\u0026rsquo;s excellent post), is when data science teams get lost in \u0026ldquo;hacking ML metrics\u0026rdquo; without connecting them to business impact. If your manager can\u0026rsquo;t see how your fancy model is helping the business, the project (and sometimes the team) might get cut.\nFAANG Perspective: This is a constant conversation. PMs will always ask, \u0026ldquo;How does this new model move our North Star metric?\u0026rdquo; If you propose a project, you need to have a hypothesis about its business impact.\nMapping ML Metrics to Business Metrics: This is key. Easy Mappings: For ad click-through rate (CTR) prediction, an increase in model AUC (an ML metric) often directly correlates with increased ad revenue (a business metric). For fraud detection, better precision/recall directly means less money lost. Harder Mappings / Custom Metrics: Netflix\u0026rsquo;s \u0026ldquo;take-rate\u0026rdquo;: (quality plays) / (recommendations shown). They found a higher take-rate correlated with more streaming hours and lower subscription cancellations (business metrics). This is a great example of a derived metric that bridges the ML and business worlds. Recommender Systems: An e-commerce site wants to move from batch to online recommendations. Hypothesis: online recommendations are more relevant -\u0026gt; higher purchase-through rate. Experiment: Show X% improvement in predictive accuracy (ML metric) from online. Historically, Y% accuracy improvement led to Z% purchase-through rate increase (business metric). A/B Testing is Crucial: Often, the only way to truly know if your ML model is improving business metrics is through rigorous A/B testing. You might have a model with slightly worse offline ML metrics (e.g., accuracy) that performs better in an A/B test on the actual business KPI. The A/B test is usually the ultimate decider.\nThe \u0026ldquo;AI-Powered\u0026rdquo; Hype vs. ROI (Return on Investment): Many companies want to say they\u0026rsquo;re \u0026ldquo;AI-powered\u0026rdquo; because it\u0026rsquo;s trendy. But ML isn\u0026rsquo;t magic. It doesn\u0026rsquo;t transform businesses overnight. Google\u0026rsquo;s success with ML came from decades of investment. Maturity Matters (Figure 2-1): This graph from Algorithmia is insightful. Companies sophisticated in ML (models in production \u0026gt; 5 years) can often deploy new models in \u0026lt; 30 days (almost 75% of them). Those just starting often take \u0026gt; 30 days (60% of them). This shows that mature MLOps pipelines, experienced teams, and established processes significantly speed up development and deployment, leading to better ROI. Interview Tip: When asked to design a system, implicitly consider the company\u0026rsquo;s maturity. A startup will have different constraints and capabilities than Google.\nAttribution in Complex Systems: Sometimes ML is just one small cog in a huge machine. For a cybersecurity threat detection system, an ML model might detect anomalies, which then go through rule-based filters, then human review, then an automated blocking process. If a threat gets through, was it the ML model\u0026rsquo;s fault? Hard to say. This makes direct attribution of business impact tricky.\nSelf-Correction/Interview Tip: In an ML system design interview, the first thing you should clarify is the business objective. \u0026ldquo;What are we trying to achieve for the business? What is the key metric we want to move?\u0026rdquo; This shows you\u0026rsquo;re thinking about the big picture, not just the tech.\nPages 5-7: Requirements for ML Systems – The \u0026ldquo;What\u0026rdquo; Once objectives are clear, we define system requirements. These are often non-functional requirements that dictate the quality and robustness of our system. The book focuses on four:\nReliability (Page 5): The system should perform its correct function at the desired level of performance, even in the face of adversity (hardware/software faults, human error).\n\u0026ldquo;Correctness\u0026rdquo; is tricky for ML: A traditional software system might crash or throw an error. ML systems can fail silently. Google Translate might give you a grammatically correct sentence that means the opposite of the original, and if you don\u0026rsquo;t know the target language, you\u0026rsquo;d never know! How do we know a prediction is wrong if we don\u0026rsquo;t have ground truth for new, unseen data in real-time? This leads to the need for robust monitoring (Chapter 8). FAANG Perspective: Silent failures are a nightmare. Imagine a recommendation system silently starts recommending inappropriate content. Robust monitoring and alerting are paramount. Scalability (Pages 6-7): The system\u0026rsquo;s ability to handle growth. Growth can occur in several dimensions:\nModel Complexity: From logistic regression (1GB RAM) to a 100M parameter neural net (16GB RAM). Traffic Volume: From 10k daily requests to 1-10M daily requests. Model Count: One model for one use case (e.g., trending hashtag detection). Adding more models for the same use case (e.g., NSFW filter, bot filter). One model per customer in enterprise scenarios (author mentions a startup with 8,000 models for 8,000 customers). This is common in B2B SaaS where models are fine-tuned on customer-specific data. Resource Scaling: Scaling up: Bigger machine. Scaling out: More machines (footnote 8). Autoscaling: Automatically adjusting resources (e.g., GPUs) based on demand. Peak might need 100 GPUs, off-peak only 10. Keeping 100 GPUs always on is expensive. Autoscaling is tricky! Amazon\u0026rsquo;s Prime Day autoscaling failure example (costing $72M-$99M in an hour) shows even the giants can struggle. Artifact Management: Managing 100 models is vastly different from 1. Manual monitoring/retraining is not feasible. Need automated MLOps pipelines, code generation for reproducibility. The book cross-references later chapters covering distributed training, model optimization, resource management, experiment tracking, and development environments, all related to scalability. Interview Tip: Always consider scalability. \u0026ldquo;What happens if traffic 10x-es? What if the number of items to recommend 10x-es?\u0026rdquo; Be prepared to discuss strategies like sharding, caching, load balancing, and autoscaling. Maintainability (Page 7): Making it easy for various teams (ML engineers, DevOps, Subject Matter Experts - SMEs) to operate, debug, and evolve the system.\nTeams have different backgrounds, tools, programming languages. Needs: Well-structured workloads and infrastructure. Good documentation (often overlooked but critical!). Versioning of code, data, and artifacts (models). Reproducibility: Can someone else reproduce your model and results if you leave? Collaborative debugging without finger-pointing. The book mentions \u0026ldquo;Team Structure\u0026rdquo; (page 334) in this context. FAANG Perspective: Maintainability is huge for long-term cost of ownership. Complex, poorly documented systems become \u0026ldquo;tribal knowledge\u0026rdquo; nightmares. We invest heavily in tools and processes for versioning, reproducibility (e.g., model registries, feature stores), and clear ownership. Adaptability (Pages 7-8): The system\u0026rsquo;s ability to evolve with:\nShifting data distributions (data drift, concept drift): What users search for changes, spam tactics evolve. (Covered in \u0026ldquo;Data Distribution Shifts\u0026rdquo; on page 237). Changing business requirements. The system needs capacity to discover opportunities for improvement and allow updates without service interruption. (Covered in \u0026ldquo;Continual Learning\u0026rdquo; on page 264). Tightly linked to maintainability. Interview Tip: \u0026ldquo;How will your system adapt to changing user behavior or new product features?\u0026rdquo; This leads to discussions of online learning, retraining frequency, and monitoring for drift. These four requirements are often interconnected and sometimes involve trade-offs. For instance, a highly scalable system might introduce complexity that challenges maintainability.\nPages 8-10: Iterative Process – The \u0026ldquo;How Often\u0026rdquo; Forget the idea of a linear, one-shot ML project: Collect data -\u0026gt; Train model -\u0026gt; Deploy -\u0026gt; Done. It\u0026rsquo;s a myth!\nML development is iterative and often never-ending. (Footnote 10: \u0026ldquo;a property of traditional software\u0026rdquo; – true, but often more pronounced in ML due to data dependency). Once in production, it needs continuous monitoring and updating.\nExample Workflow (Ad Prediction - Steps 1-13): This is a fantastic illustration of the cyclical nature: Choose metric (e.g., impressions). Collect data/labels. Feature engineering. Train models. Error analysis -\u0026gt; realize labels are wrong -\u0026gt; relabel. Train again. Error analysis -\u0026gt; model always predicts \u0026ldquo;no ad\u0026rdquo; (due to 99.99% negative labels - class imbalance!) -\u0026gt; collect more positive data. Train again. Model good on old test data, bad on recent data (stale!) -\u0026gt; update with recent data. Train again. Deploy. Business feedback: Revenue decreasing! Ads shown, but few clicks. Original metric (impressions) was wrong. Change to optimize for click-through rate. Go to step 1. (And as footnote 11 hilariously adds: \u0026ldquo;Praying and crying not featured, but present through the entire process.\u0026rdquo; So true.) Figure 2-2 (Simplified Iterative Cycle): Project Scoping: Goals, objectives, constraints, stakeholders, resources. (Covered in Ch1 and earlier in Ch2, also Ch11 for team organization). Data Engineering: Handling sources/formats (Ch3), curating training data (sampling, labeling - Ch4). ML Model Development: Feature extraction (Ch5), model selection, training, evaluation (Ch6). (This is the \u0026ldquo;sexy\u0026rdquo; part often overemphasized in courses). Deployment: Making model accessible to users (Ch7). \u0026ldquo;Like writing, you never reach done, but you reach a point where you have to put it out there.\u0026rdquo; Monitoring and Continual Learning: Performance decay, adapting to changes (Ch8 \u0026amp; Ch9). Business Analysis: Evaluate model against business goals, generate insights, decide to kill/scope new projects. (This closes the loop back to Step 1). Notice the dashed arrows in Figure 2-2: you can jump between these stages. Error analysis in model development (3) might send you back to data engineering (2). Business analysis (6) might redefine project scope (1).\nThe perspective of an ML platform engineer or DevOps engineer might differ; they\u0026rsquo;d focus more on infrastructure setup.\nSelf-Correction/Interview Tip: When outlining your approach in an ML system design interview, always frame it as an iterative process. Mention baselines, prototyping, feedback loops, and phased rollouts. This demonstrates a mature understanding of real-world ML development.\nPages 11-19: Framing ML Problems – The \u0026ldquo;What Exactly\u0026rdquo; This is where your expertise as an ML engineer truly shines. You take a business problem and translate it into something ML can solve.\nBusiness Problem vs. ML Problem: Boss\u0026rsquo;s request: \u0026ldquo;Rival bank uses ML to speed up customer service by 2x. Do that.\u0026rdquo; This is a business problem, not an ML problem. ML problem needs: Inputs, Outputs, Objective Function. Your job: Investigate. Bottleneck is routing requests to Accounting, Inventory, HR, IT. ML Framing: Predict which department a request should go to. Input: Customer request text. Output: Predicted department (one of four). Task Type: Classification. Objective function: Minimize difference between predicted and actual department. Types of ML Tasks (Page 12-14, Figure 2-3): The model\u0026rsquo;s output dictates the task type. Classification vs. Regression:\nClassification: Output is a category (e.g., spam/not-spam, cat/dog/mouse). Regression: Output is a continuous value (e.g., house price, temperature). They can be interchanged (Figure 2-4): House price regression -\u0026gt; classification by bucketing prices (e.g., \u0026lt;$100k, $100k-$200k). Email spam classification -\u0026gt; regression by outputting a spamminess score (0-1), then thresholding. FAANG Perspective: Choosing classification vs. regression can have subtle implications for user experience and error analysis. Sometimes a score is more nuanced than a hard class label. Within Classification:\nBinary Classification: Two classes (e.g., fraud/not-fraud, toxic/not-toxic). Simplest. F1, confusion matrices are intuitive. Multiclass Classification: More than two classes, an example belongs to exactly one (e.g., classifying an image as cat OR dog OR bird). High Cardinality Classification: Many classes (e.g., thousands of diseases, tens of thousands of product categories). Challenge 1: Data Collection. Often need many examples per class (author suggests ~100). For 1000 classes, that\u0026rsquo;s 100,000 examples. Difficult for rare classes. Strategy: Hierarchical Classification. First classify into broad categories (e.g., electronics, fashion), then a second model classifies into subcategories (e.g., shoes, shirts within fashion). This is common in product categorization at Amazon or e-commerce sites. Multilabel Classification: An example can belong to multiple classes simultaneously (e.g., an article can be about \u0026rsquo;tech\u0026rsquo; AND \u0026lsquo;finance\u0026rsquo;). This is often the trickiest! Labeling: Annotator disagreement is common (see Chapter 4). One says 2 labels, another says 1. Prediction: If model outputs probabilities [0.45 (tech), 0.2 (ent), 0.02 (fin), 0.33 (pol)], how many labels do you pick? The top one? Top two? This requires careful thresholding or learning the number of labels. Two Approaches: Treat as multiclass with a multi-hot encoded target vector (e.g., [0, 1, 1, 0] for entertainment and finance). Train multiple binary classifiers (one for each topic: \u0026ldquo;is it tech?\u0026rdquo;, \u0026ldquo;is it entertainment?\u0026rdquo;). This is often simpler to manage and interpret. Interview Tip: Clearly state your problem framing, including the task type. Be ready to justify why you chose that framing and discuss alternatives.\nMultiple Ways to Frame a Problem (Pages 15-16, Figures 2-5, 2-6): Next App Prediction Problem: Predict the app a user will most likely open next. Naive Framing (Multiclass Classification - Figure 2-5): Input: User features (demographics, past apps), Environment features (time, location). Output: A probability distribution vector of size N (for N apps on the phone). [P(App0), P(App1), \u0026hellip;, P(AppN-1)]. Bad because: If a new app is installed (N changes), the output layer of your neural network changes. You have to retrain from scratch or at least a significant part of the model. Better Framing (Regression - Figure 2-6): Input: User features, Environment features, AND App features (category of app, metadata). Output: A single score (0-1) indicating likelihood of opening that specific app given the context. To recommend, you make N predictions (one for each app, feeding its features into the model) and pick the app with the highest score. Good because: If a new app is installed, you just featurize it and score it with the existing model. No retraining needed immediately. This is much more scalable and practical for dynamic environments like app stores. FAANG Perspective: This \u0026ldquo;Siamese\u0026rdquo; network-like approach (where you compare user context to item features) is very common in large-scale recommendation and search ranking systems. Objective Functions (Page 16): AKA Loss Functions. The function the model tries to minimize during training.\nFor supervised learning, it compares model outputs to ground truth labels. Examples: RMSE or MAE for regression, Logistic Loss (Log Loss) for binary classification, Cross-Entropy for multiclass classification. The book gives a Python snippet for cross-entropy: p is ground truth (e.g., [0,0,0,1]), q is model prediction (e.g., [0.45,0.2,0.02,0.33]). Most ML engineers use standard loss functions; deriving novel ones requires deeper math. Important Note (footnote 12): These mathematical objective functions are different from the business/ML objectives we discussed earlier. The hope is that optimizing the mathematical loss function will improve the ML objectives, which in turn will improve business objectives. This chain isn\u0026rsquo;t always perfect! Decoupling Objectives (Pages 17-18): What if you have multiple, potentially conflicting, goals? Example: Newsfeed Ranking. Initial goal: Maximize user engagement (clicks). Objectives: Filter spam, filter NSFW, rank by click likelihood. Problem: Prioritizing engagement alone leads to extreme content (clickbait, outrage). (Footnote 13 cites Facebook/YouTube examples). New goal: Maximize engagement AND minimize extreme views/misinformation. New objectives: Filter spam, filter NSFW, filter misinformation, rank by quality, rank by engagement. Now, quality and engagement might conflict. A high-quality post might be boring; an engaging post might be low-quality. Approach 1: Combined Loss Function: loss = alpha * quality_loss + beta * engagement_loss Train one model to minimize this combined loss. Problem: Tuning alpha and beta is tricky. Every time you adjust them (e.g., business decides quality is more important), you have to retrain the entire model. Approach 2: Decouple - Train Separate Models: quality_model: Minimizes quality_loss, predicts quality_score. engagement_model: Minimizes engagement_loss, predicts engagement_score. Combine scores at inference time: final_score = alpha * quality_score + beta * engagement_score. Advantages: You can tweak alpha and beta without retraining models. Much more agile. Easier maintenance: Spam techniques evolve faster than quality perception. The spam filter (part of quality) might need frequent updates, while the core engagement model might be more stable. Decoupling allows different maintenance schedules. FAANG Perspective: Decoupling objectives into separate models or model components is a very common and powerful pattern in complex production systems like search ranking, ad ranking, and feed ranking. It allows for modularity, independent iteration on components, and easier tuning of business trade-offs. Pages 19-22: Mind Versus Data – The Big Debate This is a fascinating, ongoing discussion in the ML community. What\u0026rsquo;s more important for progress?\nThe Premise: \u0026ldquo;More Data Usually Beats Better Algorithms\u0026rdquo; (Anand Rajaraman, footnote 16). Many successes in the last decade (AlexNet, BERT, GPT) relied heavily on massive datasets. Companies increasingly focus on managing and improving their data. The Debate: \u0026ldquo;Mind\u0026rdquo; Camp: Emphasizes inductive biases, intelligent architectural designs, causal inference. Judea Pearl (Turing Award winner): \u0026ldquo;Mind over Data,\u0026rdquo; \u0026ldquo;Data is profoundly dumb.\u0026rdquo; He controversially tweeted that data-centric ML folks might be jobless in 3-5 years. Christopher Manning: Huge data + simple algorithm = \u0026ldquo;incredibly bad learners.\u0026rdquo; Structure helps learn from less data. \u0026ldquo;Data\u0026rdquo; (and Compute) Camp: Richard Sutton (\u0026ldquo;The Bitter Lesson\u0026rdquo;): General methods leveraging computation ultimately win by a large margin. Trying to bake in human domain knowledge gives short-term gains, but scaling computation is the long-term winner. Peter Norvig (on Google Search): \u0026ldquo;We don\u0026rsquo;t have better algorithms. We just have more data.\u0026rdquo; (This is from \u0026ldquo;The Unreasonable Effectiveness of Data,\u0026rdquo; Halevy, Norvig, Pereira). Monica Rogati\u0026rsquo;s \u0026ldquo;Data Science Hierarchy of Needs\u0026rdquo; (Figure 2-7): Data is foundational. You can\u0026rsquo;t do ML/AI (top of pyramid) without the lower levels: collect, move/store, explore/transform, aggregate/label. Self-Correction: This hierarchy is a fantastic mental model. Often, companies want to jump to \u0026ldquo;AI\u0026rdquo; without solid data foundations, which is a recipe for failure. The Reality: Data is essential (for now): Quality and quantity matter. Dataset sizes are exploding (Figure 2-8): Language model datasets grew from PTB (few million tokens) to Text8 (100M) to One Billion Word (0.8B tokens, 2013) to GPT-2 (10B tokens) to GPT-3 (500B tokens). Caveat: More data isn\u0026rsquo;t always better. Low-quality data (outdated, incorrect labels) can hurt performance. This emphasizes the \u0026ldquo;quality\u0026rdquo; aspect. The debate isn\u0026rsquo;t whether finite data is necessary (it is), but whether it\u0026rsquo;s sufficient, or if \u0026ldquo;mind\u0026rdquo; offers a more efficient path. If we had infinite data, we could just look up answers. A lot of data is different from infinite data. Interview Tip: While you won\u0026rsquo;t be asked to solve this philosophical debate, understanding it shows you\u0026rsquo;re aware of the field\u0026rsquo;s trends. You can articulate the practical importance of good data pipelines and also appreciate the research into more data-efficient or structured models.\nPages 22-23: Summary Chapter 2 provides a crucial introduction to the ML system design process:\nStart with \u0026ldquo;Why\u0026rdquo;: Business objectives must drive ML project selection and definition. Translate business needs into ML objectives. Define Requirements: Systems need to be reliable, scalable, maintainable, and adaptable. Embrace Iteration: ML system development is a continuous cycle, not a one-shot deal. The Role of Data is Undeniable: While the \u0026ldquo;mind vs. data\u0026rdquo; debate continues, practical ML today heavily relies on access to large amounts of high-quality data. The book will devote significant attention to data questions. Building Blocks: Complex ML systems are made of simpler components. The following chapters will zoom into these, starting with data engineering. If any of this feels abstract, the book promises concrete examples in later chapters. This chapter provides the mental framework you need to start thinking like an ML Systems Engineer. The principles here – tying to business value, defining non-functional requirements, iterative development, and careful problem framing – are universal.\nOkay, that\u0026rsquo;s Chapter 2! A lot to digest, but incredibly important. What are your initial thoughts? Any of these points particularly resonate with your experiences or spark questions? This stuff is the bread and butter of what we do in production ML.\n","permalink":"https://deepskandpal.github.io/bookshelf/design-ml-system/chapter-2/","tags":null,"title":"Chapter 2: Introduction to Machine Learning Systems Design"},{"categories":null,"content":"As we\u0026rsquo;ve established, tokens and embeddings are the fundamental ways language models interact with and understand text. Without them, the magic simply doesn\u0026rsquo;t happen. Figure 2-1 in the book clearly shows this two-step process: raw text is first broken into tokens, and then these tokens are converted into numerical embeddings that capture their meaning.\nLet\u0026rsquo;s begin with the first part of this process.\nSection 1: LLM Tokenization The way most of us interact with LLMs, like chatbots, we see text coming out, seemingly word by word. This output, and more importantly, the input the model receives, is processed in chunks called tokens.\nHow Tokenizers Prepare the Inputs to the Language Model (Page 68-69) When you send a prompt to an LLM, it doesn\u0026rsquo;t see the raw string of characters directly. First, it passes through a tokenizer. This component breaks your input text into smaller pieces. Visual Example (Figure 2-3): The book uses the OpenAI tokenizer playground. If you input \u0026ldquo;Have the bards who preceded me left any theme unsung?\u0026rdquo;, the tokenizer might break it down into something like: Have, the, bard, s, who, preced, ed, me, left, any, theme, unsung, ?. (Note: The exact tokenization depends on the specific tokenizer used by the model.) Each of these is a token. Downloading and Running an LLM (Code Example Deep Dive) (Page 69-73) Let\u0026rsquo;s revisit the code example where we load a model (e.g., Phi-3-mini) and its tokenizer using the transformers library. # Conceptual code structure from the book # from transformers import AutoModelForCausalLM, AutoTokenizer # model = AutoModelForCausalLM.from_pretrained(...) # tokenizer = AutoTokenizer.from_pretrained(...) When we prepare an input prompt, like: prompt = \u0026#34;Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\u0026lt;|assistant|\u0026gt;\u0026#34; input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids The variable input_ids doesn\u0026rsquo;t contain the text string. It contains a sequence of integers (a tensor), like tensor([[ 1, 14350, 385, ...]]). Concept (Figure 2-4): Each integer is a unique ID mapping to a specific token in the tokenizer\u0026rsquo;s vocabulary. The LLM operates on these numerical IDs. * **Decoding Token IDs:** To see the actual tokens, we use `tokenizer.decode()` on individual IDs. ```python # for id_val in input_ids[0]: # print(tokenizer.decode(id_val)) ``` This would print each token on a new line, showing how the original prompt was segmented. * **Observations from the book's example (page 72-73):** * Special tokens like `\u0026lt;s\u0026gt;` (ID 1 for Phi-3) often mark the beginning of the text. * Some tokens are full words (e.g., \u0026quot;Write\u0026quot;, \u0026quot;an\u0026quot;, \u0026quot;email\u0026quot;). * Many tokens are *subwords* (e.g., \u0026quot;apolog\u0026quot;, \u0026quot;izing\u0026quot;, \u0026quot;trag\u0026quot;, \u0026quot;ic\u0026quot;). This allows the model to handle a vast vocabulary and new words by combining known subwords. * Punctuation usually gets its own token (e.g., \u0026quot;.\u0026quot;). * **Space Handling:** Crucially, the space character itself often isn't a distinct token. Instead, subword tokens might have a prefix (like `Ġ` in GPT-2's tokenizer, or just implied by the algorithm) indicating they are preceded by a space or are at the start of a word. Tokens without this prefix attach directly to the previous one. * **Output Side:** The LLM generates a sequence of *output token IDs*. The tokenizer is then used again to convert these IDs back to human-readable text. Figure 2-5 illustrates this decoding process. How Does the Tokenizer Break Down Text? (Page 74-75) Three key factors dictate tokenization:\nTokenization Method: Chosen by the model creators. Common methods include: Byte Pair Encoding (BPE): Starts with individual characters and iteratively merges the most frequent pair of bytes (or characters) to form new tokens. Used by GPT models. WordPiece: Similar to BPE, but merges pairs based on which new token maximizes the likelihood of the training data. Used by BERT. SentencePiece: Treats text as a sequence of Unicode characters and uses BPE or unigram language modeling to find optimal subword units. It\u0026rsquo;s language-agnostic. Unigram Language Modeling: Starts with a large vocabulary of potential subwords and iteratively removes those that contribute least to the overall likelihood of the corpus, until the desired vocabulary size is reached. Tokenizer Design Choices: Vocabulary Size: How many unique tokens will the tokenizer know? (e.g., 30k, 50k, 100k+). Special Tokens: [CLS], [SEP], [PAD], [UNK], \u0026lt;s\u0026gt;, \u0026lt;|user|\u0026gt;, etc., each serving a specific purpose. Training Dataset: The tokenizer is trained on a large corpus of text to build its vocabulary and learn the merge rules (for BPE/WordPiece) or probabilities (for Unigram). A tokenizer trained on English will differ from one trained on Python code. Word Versus Subword Versus Character Versus Byte Tokens (Page 75-78) A nice visual is Figure 2-6, showing different ways to tokenize \u0026ldquo;Have the bards who preceded\u0026hellip;\u0026rdquo;\nWord Tokens: Pros: Conceptually simple. Cons: Large vocabulary (every word form is unique), struggles with out-of-vocabulary (OOV) words, can\u0026rsquo;t represent nuances within words (e.g., \u0026ldquo;apology\u0026rdquo; vs \u0026ldquo;apologize\u0026rdquo;). Less common now for LLMs. Subword Tokens: (Most common for modern LLMs) Pros: Balances vocabulary size and expressiveness. Can represent OOV words by breaking them into known subwords. More efficient for model context length. Cons: Can sometimes break words in unintuitive ways. Character Tokens: Pros: Smallest possible vocabulary, no OOV words. Cons: Sequences become very long, making it harder for the model to learn long-range dependencies. Less information per token. Byte Tokens: Pros: Truly \u0026ldquo;tokenization-free\u0026rdquo; in a sense, handles all Unicode characters naturally. Good for highly multilingual or noisy data. GPT-2\u0026rsquo;s BPE uses byte-level operations as a fallback. Cons: Sequences can also be long, less semantically meaningful units initially. Comparing Trained LLM Tokenizers (Page 78-91) This is a fantastic section in the book that builds intuition. The key takeaway is that different models use different tokenizers, and these choices profoundly impact how the model \u0026ldquo;sees\u0026rdquo; and processes text.\nThe book takes a sample string with capitalization, emojis, non-English text, code-like syntax, and numbers. BERT (uncased): Loses capitalization, newlines. Emojis/Chinese become [UNK]. BERT (cased): Preserves case, but \u0026ldquo;CAPITALIZATION\u0026rdquo; might become 8 tokens. GPT-2: Handles newlines, capitalization. Emojis/Chinese are represented by multiple byte-level fallback tokens (often shown as � but reconstructible). Handles whitespace more granularly. Flan-T5 (SentencePiece): Loses newlines/whitespace tokens, emojis/Chinese become \u0026lt;unk\u0026gt;. GPT-4: Very efficient. Fewer tokens for many words. Has specific tokens for longer sequences of whitespace. Understands Python keywords like elif as single tokens. StarCoder2 (code-focused): Special tokens for code context (\u0026lt;filename\u0026gt;). Each digit is a separate token (e.g., \u0026ldquo;600\u0026rdquo; -\u0026gt; \u0026ldquo;6\u0026rdquo;, \u0026ldquo;0\u0026rdquo;, \u0026ldquo;0\u0026rdquo;). Galactica (science-focused): Special tokens for citations ([START_REF]), reasoning (\u0026lt;work\u0026gt;). Handles tabs as single tokens. Phi-3/Llama 2: Use special chat tokens like \u0026lt;|user|\u0026gt;, \u0026lt;|assistant|\u0026gt;. The side-by-side comparison table on pages 90-91 is incredibly illustrative. Tokenizer Properties (Recap of factors affecting tokenization) (Page 91-93)\nTokenization methods (BPE, WordPiece, etc.). Tokenizer parameters (vocab size, special tokens, handling of capitalization). The domain of the data (English text vs. code vs. multilingual). Example: A text-focused tokenizer might tokenize code indentation inefficiently (e.g., four spaces as four separate tokens), while a code-focused tokenizer might have a single token for \u0026ldquo;four spaces.\u0026rdquo; This makes the model\u0026rsquo;s job easier for code generation. Section 2: Token Embeddings Now that we have tokenized our text into a sequence of token IDs, the next step is to convert these IDs into meaningful numerical representations – embeddings.\nA Language Model Holds Embeddings for the Vocabulary of Its Tokenizer (Page 95) When an LLM is pretrained, it learns an embedding vector for each token in its tokenizer\u0026rsquo;s vocabulary. This collection of vectors forms an embedding matrix (or embedding layer) within the model (as shown in Figure 2-7). * Initially, these vectors are random, but during pretraining, their values are adjusted so that tokens with similar meanings or that appear in similar contexts have similar embedding vectors. These are *static* embeddings – each token ID always maps to the same initial vector. Creating Contextualized Word Embeddings with Language Models (Page 96-99) Static token embeddings are just the starting point. The real power comes when the language model processes these static embeddings in the context of the entire input sequence. The output of the main Transformer blocks (before the final prediction layer) for each input token is its contextualized word embedding. This means the embedding for \u0026ldquo;bank\u0026rdquo; in \u0026ldquo;river bank\u0026rdquo; will be different from \u0026ldquo;bank\u0026rdquo; in \u0026ldquo;money bank.\u0026rdquo; Visual (Figure 2-8 \u0026amp; 2-9): Raw, static token embeddings go into the language model. The model\u0026rsquo;s layers process these, considering the whole sequence (via attention, which we\u0026rsquo;ll detail in Chapter 3). The output for each token position is a new, richer, contextualized embedding vector. * *Code Example (using DeBERTa, page 97):* ```python # from transformers import AutoModel, AutoTokenizer # tokenizer_deberta = AutoTokenizer.from_pretrained(\u0026quot;microsoft/deberta-base\u0026quot;) # model_deberta = AutoModel.from_pretrained(\u0026quot;microsoft/deberta-v3-xsmall\u0026quot;) # tokens_deberta = tokenizer_deberta('Hello world', return_tensors='pt') # output_embeddings = model_deberta(**tokens_deberta)[0] # print(output_embeddings.shape) # e.g., torch.Size([1, 4, 384]) ``` This `output_embeddings` tensor contains the contextualized embeddings for each token (including special ones like `[CLS]` and `[SEP]`). These are the representations used for downstream tasks or further generation. Section 3: Text Embeddings (for Sentences and Whole Documents) While token embeddings are crucial for the internal workings of LLMs, many applications need a single vector representation for an entire sentence, paragraph, or document.\nConcept (Figure 2-10): An embedding model takes a piece of text and outputs a single vector that captures its overall meaning. Methods: A simple approach is to average the contextualized token embeddings of all tokens in the text. However, high-quality text embedding models (often called Sentence Transformers or bi-encoders, building on architectures like SBERT) are specifically trained for this task. They often outperform simple averaging. Using sentence-transformers library (page 101-102): # from sentence_transformers import SentenceTransformer # sbert_model = SentenceTransformer(\u0026#34;sentence-transformers/all-mpnet-base-v2\u0026#34;) # sentence_vector = sbert_model.encode(\u0026#34;This is the best movie ever!\u0026#34;) # print(sentence_vector.shape) # e.g., (768,) This gives a single vector for the entire sentence. These sentence/document embeddings are powerful for: Semantic search (as we\u0026rsquo;ll see in Chapter 8) Clustering (Chapter 5) Classification (Chapter 4) Text similarity tasks. Section 4: Word Embeddings Beyond LLMs This section briefly revisits older but foundational embedding techniques, primarily to set the stage for understanding contrastive learning, a key concept for training modern embedding models (which we\u0026rsquo;ll hit hard in Chapter 10).\nUsing pretrained Word Embeddings (word2vec, GloVe) (Page 103)\nLibraries like gensim allow you to download and use classic pretrained word embeddings. Example: Loading glove-wiki-gigaword-50 and finding similar words to \u0026ldquo;king\u0026rdquo;. # import gensim.downloader as api # glove_model_gensim = api.load(\u0026#34;glove-wiki-gigaword-50\u0026#34;) # print(glove_model_gensim.most_similar(\u0026#34;king\u0026#34;)) The Word2vec Algorithm and Contrastive Training (Page 103-107)\nRecap: Word2vec learns embeddings by predicting context words (Skip-gram) or a target word from its context (CBOW). Key Idea: Contrastive Approach. Positive Pairs: Words that actually appear near each other in the training text (e.g., \u0026ldquo;Thou\u0026rdquo; and \u0026ldquo;shalt\u0026rdquo; in the Dune example, Figure 2-11). Negative Pairs: Words that don\u0026rsquo;t usually appear together. These are generated by taking a context word and pairing it with a randomly sampled word from the vocabulary (Figure 2-13). Training Objective: The model (a simple neural network) is trained to output a high score for positive pairs and a low score for negative pairs. During this process, the embedding vectors for each word (which start random, Figure 2-15) are adjusted (Figure 2-16). Relevance: This \u0026ldquo;learning by contrast\u0026rdquo; is fundamental. It teaches the model not just what words are similar, but also what makes them distinct. Embeddings for Recommendation Systems (Page 108-113)\nA practical example of applying word2vec-like thinking beyond text. Idea: Treat songs as \u0026ldquo;words\u0026rdquo; and playlists as \u0026ldquo;sentences.\u0026rdquo; If two songs frequently appear in the same playlists, their learned \u0026ldquo;song embeddings\u0026rdquo; will be similar. Dataset: The book uses playlists from US radio stations (Figure 2-17). Process: Prepare data: List of playlists, where each playlist is a list of song IDs. Train a Word2Vec model from gensim on these playlists. Use model.wv.most_similar() to find songs similar to a given song ID. The example (pages 109-113) shows this working well for recommending similar artists/genres. Chapter 2 Summary To quickly summarize the key takeaways from Chapter 2:\nTokenizers are the first crucial step, breaking raw text into token IDs. The choice of tokenizer and its parameters significantly affects how an LLM processes information. Subword tokenization is the most common. Token Embeddings are the initial (often static) numerical representations of these tokens, learned during pretraining. Language models then create Contextualized Word Embeddings, which are dynamic and reflect the meaning of a token within its specific sentence. Text Embeddings provide a single vector for an entire sentence or document, crucial for many downstream tasks and often produced by specialized models like Sentence Transformers. The principles of Contrastive Learning (as seen in word2vec and later in SBERT) are fundamental for training effective embedding models by teaching them similarity and dissimilarity. These embedding concepts are versatile and can be applied beyond text, for example, in recommendation systems. Ah, an absolutely brilliant question! This gets to the very heart of building practical, effective search systems. It\u0026rsquo;s a classic \u0026ldquo;new vs. old\u0026rdquo; and \u0026ldquo;meaning vs. keywords\u0026rdquo; debate, but as with most things in science and engineering, the real answer is nuanced.\nLet\u0026rsquo;s break this down using my favorite approach: understanding what each one is ultimately trying to achieve.\nImagine you have two librarians.\nLibrarian A (The Sparse Embeddings Librarian): This librarian is a master of the old-school card catalog. They haven\u0026rsquo;t read every book, but they have meticulously indexed every single one. If you ask them for books containing the exact phrase \u0026ldquo;thermonuclear astrophysics,\u0026rdquo; they will instantly pull every card for every book that has that precise phrase. They are incredibly fast and precise for keyword matching. They are literal.\nLibrarian B (The Dense Embeddings Librarian): This librarian has read every single book in the library. They have a deep, intuitive understanding of the concepts and themes. If you ask them, \u0026ldquo;I want to read about how stars create energy,\u0026rdquo; they might not find any books with that exact phrase. Instead, they\u0026rsquo;ll hand you books on \u0026ldquo;stellar nucleosynthesis,\u0026rdquo; \u0026ldquo;nuclear fusion in stellar cores,\u0026rdquo; and yes, \u0026ldquo;thermonuclear astrophysics.\u0026rdquo; They understand the semantic meaning behind your query.\nThis analogy captures the fundamental difference. Let\u0026rsquo;s formalize it.\nWhat They Are \u0026amp; How They Work Sparse Embeddings (e.g., TF-IDF, BM25)\nWhat they are: These are typically very long vectors, with a dimension for every unique word in your entire vocabulary (could be 50,000+ dimensions). The key is that for any given document, most of the values in this vector are zero. The only non-zero values correspond to the words actually present in that document. How they work: The values are not learned in a deep neural network. They are calculated based on word frequencies. A common method is TF-IDF (Term Frequency-Inverse Document Frequency). Term Frequency (TF): How often does a word appear in this document? (More often = more important). Inverse Document Frequency (IDF): How rare is this word across all documents? (Rarer = more important and specific). The final score is a multiplication of these. Common words like \u0026ldquo;the\u0026rdquo; and \u0026ldquo;a\u0026rdquo; get a low score, while specific keywords get a high score. What they ultimately achieve: They create a representation that excels at lexical matching. They tell you which documents contain the exact words you\u0026rsquo;re looking for, and give more weight to the rare, important keywords. Dense Embeddings (e.g., from BERT, Sentence-Transformers)\nWhat they are: These are much shorter vectors (e.g., 384, 768, or 4096 dimensions). The key is that nearly all values are non-zero. Each value represents some abstract, learned feature of the text. How they work: As we saw in Chapter 2, these vectors are the output of a deep neural network (like a Transformer) that has been trained on vast amounts of text. The training process forces the model to learn the relationships between words, concepts, and contexts. What they ultimately achieve: They create a representation that excels at semantic matching. They capture the meaning and intent behind the text, allowing you to find documents that are conceptually similar, even if they don\u0026rsquo;t share any keywords. when should we use dense embeddings and when should we use sparse embeddings? When to Use Which? The Practical Guide This is the core of your question. Here’s a breakdown of the scenarios:\nUse DENSE Embeddings when: You need to search by concept or meaning. This is the killer use case. The user doesn\u0026rsquo;t know the exact keywords. They\u0026rsquo;re exploring a topic.\nQuery: \u0026ldquo;ways to make coffee at home without a machine\u0026rdquo; Dense search finds: \u0026ldquo;Guide to pour-over brewing,\u0026rdquo; \u0026ldquo;How to use an AeroPress,\u0026rdquo; \u0026ldquo;The perfect French Press.\u0026rdquo; (Notice the lack of keyword overlap). You need to handle synonyms and varied language. Your query needs to match documents regardless of the specific phrasing.\nQuery: \u0026ldquo;car problems\u0026rdquo; Dense search finds: \u0026ldquo;automobile issues,\u0026rdquo; \u0026ldquo;engine trouble,\u0026rdquo; \u0026ldquo;vehicle maintenance.\u0026rdquo; You are building recommendation or clustering systems. You want to group items (articles, products, songs) by their \u0026ldquo;vibe\u0026rdquo; or conceptual similarity, not just their keyword descriptions. This is what we did with the song recommender in Chapter 2.\nYou are dealing with multilingual content. A well-trained multilingual dense model can map queries in one language to documents in another if they are semantically equivalent.\nUse SPARSE Embeddings when: You need exact keyword matching and precision. This is where sparse models often outperform dense ones. The user knows exactly what they are looking for.\nQuery: \u0026quot;Interstellar\u0026quot; Kip Thorne Sparse search finds: Documents that explicitly contain both \u0026ldquo;Interstellar\u0026rdquo; and \u0026ldquo;Kip Thorne.\u0026rdquo; A dense model might get confused and bring up documents about other sci-fi movies or other physicists. Other examples: Searching for a specific error code E-1022, a product SKU X5A-7B2, or a legal term \u0026quot;res judicata\u0026quot;. Your domain has a lot of specific jargon or codes. Sparse models treat every word as distinct, which is perfect for domains where product_v1 and product_v2 are completely different things, even though a dense model might see them as semantically very similar.\nInterpretability is important. With sparse models, you can easily tell why a document was retrieved: it contained the specific keywords from the query. With dense models, the \u0026ldquo;similarity\u0026rdquo; is an abstract mathematical concept, making it a black box.\nYou need a computationally cheap and fast baseline. Sparse methods like BM25 are very fast and don\u0026rsquo;t require GPUs or large models. They are an excellent first step in any search system.\nThe Best of Both Worlds: Hybrid Search So, which one do we choose? In the real world, you don\u0026rsquo;t have to. The most robust and state-of-the-art search systems – the ones powering RAG in major companies – use a hybrid approach.\nThis is the two-stage process we hinted at in Chapter 8:\nStage 1: Retrieval (Recall-focused): Cast a wide net to get all potentially relevant documents. You do this by running both a sparse search (like BM25) and a dense search in parallel. You then take the results from both and combine them into a larger candidate set. This ensures you get both the documents that match keywords exactly and the ones that are conceptually similar.\nStage 2: Reranking (Precision-focused): Now that you have a smaller candidate set (say, the top 100 documents from Stage 1), you use a more powerful and computationally expensive model – like a cross-encoder reranker – to precisely score the relevance of each candidate to the original query and give you the final, beautifully ordered list.\nHere is a summary table for a quick reference:\nCharacteristic Sparse Embeddings (e.g., TF-IDF, BM25) Dense Embeddings (e.g., SBERT) What It Is A very long vector, mostly zeros. A shorter vector, all values are meaningful. How It Works Counts word frequencies \u0026amp; rarity (lexical). Learned from a deep neural network (semantic). Core Strength Keyword Matching \u0026amp; Precision. Meaning \u0026amp; Context Understanding. Key Weakness Fails if keywords don\u0026rsquo;t match (no synonym handling). Can miss exact keyword matches; can be a black box. Use When\u0026hellip; You need exact matches (product codes, legal terms, names). You need to understand user intent \u0026amp; concepts. The Winner? Neither. The best systems use a hybrid approach, combining both for retrieval, often followed by a reranker. So, when you think about the \u0026ldquo;Retrieval\u0026rdquo; in RAG, don\u0026rsquo;t think of it as just one method. It\u0026rsquo;s a sophisticated pipeline that almost always benefits from using the speed and precision of sparse embeddings alongside the deep semantic understanding of dense embeddings.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-2/","tags":null,"title":"Chapter 2: Tokens and Embeddings"},{"categories":null,"content":"Chapter 3 Notes: Model Training and Deployment Overview This chapter covers the design considerations for training ML models at scale and deploying them to production environments. It includes discussions on training infrastructure, model versioning, A/B testing, and deployment strategies.\nKey Concepts Training Infrastructure: Distributed training, resource allocation, and optimization Model Versioning: Managing different model versions and experiments Deployment Strategies: Blue-green deployments, canary releases, and shadow deployments Serving Architecture: Online vs offline serving, latency requirements Main Topics Covered Distributed training systems Model registry and versioning Deployment patterns and strategies Model serving architectures Performance optimization A/B testing for ML models Design Considerations Latency vs throughput trade-offs Model size and memory constraints Scalability requirements Rollback strategies (Your detailed notes for Chapter 3 go here\u0026hellip;)\n","permalink":"https://deepskandpal.github.io/bookshelf/ml-design-interview/chapter-3/","tags":null,"title":"Chapter 3: Model Training and Deployment"},{"categories":null,"content":"This chapter explores the system design behind Google Translate, covering neural machine translation at massive scale.\nKey Concepts Neural Machine Translation: Modern approaches using transformer models Multi-language Support: Handling 100+ languages efficiently Quality vs Speed: Balancing translation quality with response time Main Topics Covered Google Translate system architecture Model architecture (sequence-to-sequence, transformers) Training data pipeline and multilingual models Serving infrastructure and caching strategies Quality evaluation and continuous improvement System Design Considerations Supporting 100+ language pairs Real-time translation for different modalities (text, speech, images) Handling rare languages and domain-specific content Model deployment and A/B testing at scale (Your detailed notes for Chapter 3 go here\u0026hellip;)\n","permalink":"https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/chapter-3/","tags":null,"title":"Chapter 3: Google Translate"},{"categories":null,"content":"What this chapter is ultimately trying to achieve To introduce a type of neural network specifically designed to process sequences of data (like words in a sentence) one element at a time, while maintaining an internal \u0026ldquo;memory\u0026rdquo; or \u0026ldquo;state\u0026rdquo; that captures information from previous elements in the sequence. This \u0026ldquo;memory\u0026rdquo; allows RNNs to understand context that spans multiple tokens, which is something BoW or simple n-gram models struggle with significantly.\nLet\u0026rsquo;s break down the key concepts:\n3.1 Elman RNN (Simple Recurrent Neural Network) What it\u0026rsquo;s ultimately trying to achieve: To process a sequence of inputs (e.g., word embeddings) step-by-step, and at each step, produce an output and update an internal hidden state. This hidden state acts as a compressed summary of the sequence seen so far.\nThe Core Idea (The Loop): Imagine a standard neural network unit. Now, give it a loop: the output of the unit at a given time step t (specifically, its hidden state h_t) is fed back into the unit as an additional input at the next time step t+1, along with the actual next input from the sequence x_{t+1}.\nInput: At each time step t, the RNN unit takes two things: The current input from the sequence, x_t (e.g., the embedding of the current word). The hidden state from the previous time step, h_{t-1}. Calculation: Inside the unit, these inputs are typically transformed by weight matrices and an activation function (often tanh in classic RNNs) to produce: The new hidden state for the current time step, h_t. (Optionally) An output for the current time step, y_t. For language modeling, this y_t would be related to predicting the next word. Formula (Conceptual): h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h) (Hidden state update) y_t = W_hy * h_t + b_y (Output at time t, often passed through softmax for probabilities) Where W_hh, W_xh, W_hy are weight matrices and b_h, b_y are bias terms. These weights are shared across all time steps, which is key to how RNNs generalize. Visualizing It: You can \u0026ldquo;unroll\u0026rdquo; an RNN in time. It looks like a chain of identical network units, where the hidden state from one unit is passed to the next.\nRealism and Challenges:\nVanishing/Exploding Gradients: When training RNNs with backpropagation through time (BPTT), gradients can become very small (vanish) or very large (explode) as they are propagated back through many time steps. This makes it hard for simple RNNs to learn long-range dependencies (e.g., connecting a word at the beginning of a long sentence to a word at the end). ReLU helps with vanishing gradients compared to tanh/sigmoid in deep feedforward nets, but the recurrent nature still poses challenges. LSTMs and GRUs (which are more advanced RNN variants, not deeply covered in a 100-page book but important to know about) were developed to mitigate this. 3.2 Mini-Batch Gradient Descent (Revisited for Sequences) What it\u0026rsquo;s ultimately trying to achieve: To efficiently train RNNs (and other large models) by processing multiple sequences in parallel within each training step, rather than one sequence at a time or the entire dataset at once.\nThe Setup for Sequences: When we feed data to an RNN, it\u0026rsquo;s often in the shape of (batch_size, sequence_length, embedding_dimensionality).\nbatch_size: The number of sequences processed together. sequence_length: The number of tokens in each sequence (sequences are often padded to be the same length in a batch). embedding_dimensionality: The size of the vector representing each token. Why it\u0026rsquo;s important: Processing batches leverages the parallel processing capabilities of modern hardware (like GPUs), making training much faster. It also provides a more stable estimate of the gradient compared to processing single examples (stochastic gradient descent).\n3.3 Programming an RNN (in PyTorch) What it\u0026rsquo;s ultimately trying to achieve: To translate the mathematical concept of an RNN unit and a stack of RNN layers into working code.\nKey PyTorch Components:\nnn.Module: The base class for all neural network modules in PyTorch. Our RNN unit and the full RNN model will inherit from this. nn.Parameter: Wraps a tensor to tell PyTorch that it\u0026rsquo;s a learnable model parameter (like the weight matrices W_hh, W_xh). The __init__ method: Where you define the layers and parameters of your model. The forward method: Where you define how the input data flows through the layers to produce an output. For an RNN, this will involve a loop over the time steps of the input sequence. Implementing the ElmanRNNUnit:\nInitialize weight matrices (Uh for hidden-to-hidden, Wh for input-to-hidden) and a bias term (b). The forward method takes current input x and previous hidden state h_prev and computes h_new = tanh(x @ Wh + h_prev @ Uh + b). Implementing the full ElmanRNN (stacking layers):\nThe ElmanRNN class would contain a list of ElmanRNNUnit instances (one for each layer). Its forward method would: Initialize hidden states for all layers (usually to zeros). Loop through each token (time step t) in the input sequences of the batch. For each token, loop through each RNN layer: The input to the first layer is the token\u0026rsquo;s embedding. The input to subsequent layers is the hidden state from the layer below at the same time step. Each layer updates its hidden state. Collect the outputs (usually the hidden states of the last layer at each time step). 3.4 RNN as a Language Model What it\u0026rsquo;s ultimately trying to achieve: To use the RNN architecture to perform the core language modeling task: predicting the next token in a sequence.\nThe Architecture:\nEmbedding Layer: Converts input token IDs into dense embedding vectors. This is often nn.Embedding in PyTorch. RNN Layers: One or more RNN layers (like our ElmanRNN) process the sequence of embeddings and output a sequence of hidden states (usually from the final RNN layer). Output (Linear) Layer / Classification Head: A fully connected linear layer takes the RNN\u0026rsquo;s hidden state output at each time step t and transforms it into a vector of logits, where the size of this vector is the vocabulary size. Softmax (implicitly with CrossEntropyLoss): These logits are then (conceptually, often combined within the loss function) passed through a softmax function to get probabilities for each word in the vocabulary being the next word. Training:\nInput Sequence: A sequence of token IDs, e.g., [token_A, token_B, token_C]. Target Sequence: The input sequence shifted by one position, e.g., [token_B, token_C, token_D]. At each time step t, the model processes input_token_t and its goal is to output a high probability for target_token_t. The cross-entropy loss is calculated between the predicted probability distribution and the actual target token at each position, and then averaged. 3.5 Embedding Layer (Deeper Dive with nn.Embedding) What it\u0026rsquo;s ultimately trying to achieve: To provide a learnable lookup table that maps discrete token indices (integers) to dense, continuous-valued embedding vectors.\nHow it works in PyTorch (nn.Embedding):\nWhen you create nn.Embedding(vocab_size, emb_dim), PyTorch initializes a weight matrix of shape (vocab_size, emb_dim) with random values. Each row i of this matrix is the embedding vector for token ID i. When you pass a tensor of token IDs to this layer, it simply looks up and returns the corresponding rows (embedding vectors). These embedding vectors are learnable parameters. During training, gradients flow back to them, and they get updated to better represent the tokens for the given task. padding_idx: You can specify an index to be treated as a padding token. The embedding for this token will be a zero vector and (importantly) will not be updated during training. 3.6 Training an RNN Language Model (The Full Loop in PyTorch) What it\u0026rsquo;s ultimately trying to achieve: To put all the pieces together – data preparation, model instantiation, loss function, optimizer, and the training loop – to actually train an RNN LM.\nKey Steps in the Training Loop (per epoch, per batch):\nmodel.train(): Set the model to training mode. Get input_seq and target_seq from the DataLoader. Move data to the correct device (CPU/GPU). optimizer.zero_grad(): Clear old gradients. outputs = model(input_seq): Forward pass to get logits. Reshape outputs and target_seq so that the loss can be computed across all tokens in the batch efficiently. Typically, this means flattening them so that each row corresponds to a single token prediction: outputs becomes (batch_size * seq_len, vocab_size) target_seq becomes (batch_size * seq_len) loss = criterion(outputs, target_seq): Calculate the cross-entropy loss. Remember nn.CrossEntropyLoss in PyTorch expects raw logits and handles the softmax internally. It also allows an ignore_index parameter, which is crucial for not calculating loss on padding tokens in the target_seq. loss.backward(): Backward pass to compute gradients. optimizer.step(): Update model parameters. Reproducibility: Setting seeds (random.seed(), torch.manual_seed(), torch.cuda.manual_seed_all()) is important for consistent results, especially when debugging or comparing experiments.\n3.7 Dataset and DataLoader (PyTorch Utilities) What they are ultimately trying to achieve: To provide a standardized and efficient way to load, preprocess, and iterate over data in batches during training. Dataset: An abstract class representing your dataset. You need to implement: __init__(self, ...): Load/prepare your data (e.g., read from file, tokenize). __len__(self): Return the total number of samples in the dataset. __getitem__(self, idx): Return the idx-th sample (e.g., an input sequence and its corresponding target sequence, as tensors). DataLoader: Wraps a Dataset and provides an iterator to loop over the data in batches. It handles: Batching. Shuffling (optional, but good for training). Parallel data loading using multiple worker processes (optional, for speed). 3.8 Training Data and Loss Computation (for Language Modeling) What it\u0026rsquo;s ultimately trying to achieve: To clarify exactly how input and target sequences are structured for training an autoregressive language model, and how the loss is computed across all positions.\nThe \u0026ldquo;Shifted\u0026rdquo; Target: For an input sequence like [T1, T2, T3, T4], the target sequence is [T2, T3, T4, T5].\nWhen the model sees T1, it tries to predict T2. When it sees T1, T2, it tries to predict T3. And so on. The hidden state h_t carries context from T1...T_t to help predict T_{t+1}. Loss Calculation: The cross-entropy loss is calculated at each position where a prediction is made. The total loss for a sequence is typically the average of these per-position losses. When batching, it\u0026rsquo;s the average loss over all predictable tokens in the batch.\n","permalink":"https://deepskandpal.github.io/bookshelf/100-page-lm-book/chapter-3/","tags":null,"title":"Chapter 3: Recurrent Neural Network"},{"categories":null,"content":"This chapter is all about systems that predict a category or a class – Is this email spam or not? Is this image a cat or a dog? Is this handwritten digit a \u0026lsquo;5\u0026rsquo; or a \u0026lsquo;3\u0026rsquo;?\nAnd speaking of handwritten digits, we\u0026rsquo;re going to be working with a very famous dataset: MNIST.\nIt\u0026rsquo;s a set of 70,000 small images of digits (0-9) handwritten by high school students and US Census Bureau employees. Each image is labeled with the digit it represents. The book calls it the \u0026ldquo;hello world\u0026rdquo; of Machine Learning because it\u0026rsquo;s a go-to dataset for testing new classification algorithms. Everyone who learns ML eventually plays with MNIST. It’s like a rite of passage! Scikit-Learn makes it super easy to fetch popular datasets like MNIST. The code shows: from sklearn.datasets import fetch_openml mnist = fetch_openml('mnist_784', version=1)\nThis mnist object is a dictionary-like structure (a Scikit-Learn \u0026ldquo;Bunch\u0026rdquo; object, actually). The mnist.keys() output shows it contains:\n'data': The features (the pixel values of the images). 'target': The labels (the digit each image represents). 'feature_names': Names of the features. 'DESCR': A description of the dataset. And a few others. This structure is common for datasets loaded with Scikit-Learn. (Page 86: Exploring the MNIST Data)\nLet\u0026rsquo;s look at the actual data arrays: X, y = mnist[\u0026quot;data\u0026quot;], mnist[\u0026quot;target\u0026quot;] X.shape gives (70000, 784) y.shape gives (70000,)\nWhat does this mean? We have 70,000 images (X has 70,000 rows). Each image has 784 features (X has 784 columns). Why 784? Because each image is 28x28 pixels, and 28 * 28 = 784. Each feature represents the intensity of one pixel, typically from 0 (white) to 255 (black). y contains the 70,000 labels, one for each image. Let\u0026rsquo;s visualize one digit:\nGrab an instance\u0026rsquo;s feature vector: some_digit = X[0] (this is a flat array of 784 pixel values). Reshape it to a 28x28 array: some_digit_image = some_digit.reshape(28, 28). Display it using Matplotlib\u0026rsquo;s imshow(): plt.imshow(some_digit_image, cmap=\u0026quot;binary\u0026quot;) (cmap=\u0026ldquo;binary\u0026rdquo; gives a black and white image). plt.axis(\u0026quot;off\u0026quot;) (to remove the axis ticks). The image on page 86 clearly looks like a \u0026lsquo;5\u0026rsquo;. Let\u0026rsquo;s check its label: y[0] gives '5'. Notice the label is a string. Most ML algorithms expect numbers, so we convert y to integers: y = y.astype(np.uint8) (np.uint8 is an unsigned 8-bit integer, good for values 0-255, perfect for digits 0-9).\n(Page 87: MNIST Dataset Structure \u0026amp; Test Set)\nFigure 3-1 shows a sample of the digits – you can see the variety and sometimes the messiness of handwriting!\nCrucially, just like in Chapter 2, we need a test set! The MNIST dataset as loaded by fetch_openml is often already split:\nFirst 60,000 images for training. Last 10,000 images for testing. So, we can do: X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] The training set (X_train, y_train) is also typically pre-shuffled. This is good because:\nIt ensures cross-validation folds will be similar (e.g., you don\u0026rsquo;t want one fold missing all images of the digit \u0026lsquo;7\u0026rsquo;). Some algorithms are sensitive to the order of training instances and perform poorly if they see many similar instances in a row. Shuffling prevents this. (The book notes shuffling isn\u0026rsquo;t always good, e.g., for time series data where order matters). (Page 88: Training a Binary Classifier)\nLet\u0026rsquo;s start simple. Instead of classifying all 10 digits, let\u0026rsquo;s try to identify just one digit – say, the number 5. This will be a binary classifier: it distinguishes between two classes, \u0026ldquo;5\u0026rdquo; and \u0026ldquo;not-5\u0026rdquo;.\nCreate target vectors for this binary task: y_train_5 = (y_train == 5) # This will be True for all 5s, False for others. y_test_5 = (y_test == 5)\nPick a classifier and train it: A good starting point is Scikit-Learn\u0026rsquo;s SGDClassifier (Stochastic Gradient Descent classifier).\nWhy SGD? It\u0026rsquo;s efficient and can handle very large datasets because it processes training instances one at a time (making it suitable for online learning, too). from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) Why random_state=42? SGD relies on randomness during training (hence \u0026ldquo;stochastic\u0026rdquo;). Setting random_state ensures reproducible results. If you run the code again, you get the same model. sgd_clf.fit(X_train, y_train_5) Make a prediction: Let\u0026rsquo;s use that some_digit (which was a \u0026lsquo;5\u0026rsquo;) we looked at earlier: sgd_clf.predict([some_digit]) returns array([True]). The classifier correctly guessed it\u0026rsquo;s a 5!\nBut one correct guess doesn\u0026rsquo;t mean much. We need to evaluate its overall performance.\n(Page 88-93: Performance Measures for Classifiers)\nEvaluating classifiers is often \u0026ldquo;significantly trickier than evaluating a regressor.\u0026rdquo; Get ready for new concepts!\nMeasuring Accuracy Using Cross-Validation (Page 89): Just like in Chapter 2, cross-validation is a good way to evaluate.\nImplementing Cross-Validation Manually: The book shows how you could implement cross-validation yourself using StratifiedKFold. This gives more control but is more work.\nStratifiedKFold ensures each fold has a representative ratio of each class (important for skewed datasets, where one class is much more frequent). The loop creates a clone of the classifier, trains on training folds, predicts on the test fold, and calculates accuracy for that fold. Using cross_val_score(): Much easier! from sklearn.model_selection import cross_val_score cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\u0026quot;accuracy\u0026quot;) This performs K-fold cross-validation (here, 3 folds) and returns the accuracy for each fold. The output is something like array([0.96355, 0.93795, 0.95615]). Wow! Over 93% accuracy on all folds! Seems amazing, right?\nThe Pitfall of Accuracy with Skewed Datasets: Before getting too excited, let\u0026rsquo;s consider a \u0026ldquo;dumb\u0026rdquo; classifier: class Never5Classifier(BaseEstimator): def fit(self, X, y=None): return self def predict(self, X): return np.zeros((len(X), 1), dtype=bool) # Always predicts False (not-5) If we run cross_val_score on this Never5Classifier (page 90), we get over 90% accuracy!\nWhy? Only about 10% of the MNIST digits are 5s. So, if you always guess \u0026ldquo;not a 5,\u0026rdquo; you\u0026rsquo;ll be right about 90% of the time. Key takeaway: Accuracy is generally not the preferred performance measure for classifiers, especially with skewed datasets (where some classes are much more frequent than others). Confusion Matrix (Page 90-91): A much better way to evaluate! It counts how many times instances of class A are classified as class B.\nGet \u0026ldquo;clean\u0026rdquo; predictions: Use cross_val_predict(): from sklearn.model_selection import cross_val_predict y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3) This performs K-fold CV but returns the predictions made on each test fold (so each prediction is \u0026ldquo;clean\u0026rdquo; – made by a model that hadn\u0026rsquo;t seen that instance during its training). Compute the confusion matrix: from sklearn.metrics import confusion_matrix confusion_matrix(y_train_5, y_train_pred) The output (page 91) is a 2x2 matrix for our binary \u0026ldquo;5\u0026rdquo; vs \u0026ldquo;not-5\u0026rdquo; classifier: Predicted: Not-5 Predicted: 5 Actual: Not-5 [[ TN, FP ]] Actual: 5 [[ FN, TP ]] TN (True Negatives): Correctly classified as not-5 (e.g., 53,057). FP (False Positives): Wrongly classified as 5 (they were not-5) (e.g., 1,522). Also called a Type I error. FN (False Negatives): Wrongly classified as not-5 (they were 5s) (e.g., 1,325). Also called a Type II error. TP (True Positives): Correctly classified as 5 (e.g., 4,096). A perfect classifier would have only TPs and TNs (non-zero values only on the main diagonal). Figure 3-2 (page 92) provides a nice illustration of this. Precision and Recall (Page 91-92): The confusion matrix is great, but sometimes we want more concise metrics.\nPrecision (Equation 3-1): Accuracy of the positive predictions. precision = TP / (TP + FP) What it\u0026rsquo;s ultimately trying to achieve: Of all the instances the classifier claimed were positive (e.g., said were \u0026lsquo;5\u0026rsquo;s), what proportion were actually positive? A trivial way to get 100% precision: make only one positive prediction and ensure it\u0026rsquo;s correct. Not very useful! Recall (Equation 3-2): Sensitivity or True Positive Rate (TPR). Ratio of positive instances that are correctly detected. recall = TP / (TP + FN) What it\u0026rsquo;s ultimately trying to achieve: Of all the instances that were actually positive (e.g., all actual \u0026lsquo;5\u0026rsquo;s), what proportion did the classifier correctly identify? Scikit-Learn provides functions: from sklearn.metrics import precision_score, recall_score precision_score(y_train_5, y_train_pred) gives ~72.9%. recall_score(y_train_5, y_train_pred) gives ~75.6%. So, when our sgd_clf claims an image is a \u0026lsquo;5\u0026rsquo;, it\u0026rsquo;s correct only 72.9% of the time. And it only detects 75.6% of all actual \u0026lsquo;5\u0026rsquo;s. Not as shiny as the 90%+ accuracy suggested!\nF₁ Score (Page 92): It\u0026rsquo;s often convenient to combine precision and recall into a single metric, especially for comparing classifiers. The F₁ score is the harmonic mean of precision and recall (Equation 3-3). F₁ = 2 * (precision * recall) / (precision + recall)\nWhat it\u0026rsquo;s ultimately trying to achieve: The F₁ score gives more weight to low values. So, a classifier only gets a high F₁ score if both precision and recall are high. from sklearn.metrics import f1_score f1_score(y_train_5, y_train_pred) gives ~74.2%. The F₁ score isn\u0026rsquo;t always what you want. Sometimes you care more about precision (e.g., kid-safe video filter – high precision, even if recall is low). Sometimes you care more about recall (e.g., shoplifter detection – high recall, even if precision is low and there are false alarms).\nPrecision/Recall Trade-off (Page 93-96): Unfortunately, you usually can\u0026rsquo;t have it both ways: increasing precision tends to reduce recall, and vice-versa. This is the precision/recall trade-off.\nHow it works: Classifiers like SGDClassifier compute a decision score for each instance. If the score \u0026gt; threshold, it\u0026rsquo;s positive; else, negative. (Figure 3-3 illustrates this).\nRaising the threshold: Fewer instances classified as positive. This usually increases precision (fewer false positives among the ones called positive) but decreases recall (more true positives get missed and become false negatives). Lowering the threshold: More instances classified as positive. This usually increases recall (fewer true positives missed) but decreases precision (more false positives creep in). Scikit-Learn lets you access these decision scores: y_scores = sgd_clf.decision_function([some_digit]) SGDClassifier uses a threshold of 0 by default. If we set threshold = 8000, (y_scores \u0026gt; threshold) might become False, meaning a \u0026lsquo;5\u0026rsquo; is now missed (recall drops).\nChoosing a threshold:\nGet decision scores for all training instances: y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\u0026quot;decision_function\u0026quot;) Compute precision and recall for all possible thresholds: from sklearn.metrics import precision_recall_curve precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores) Plot precision and recall vs. threshold (Figure 3-4, page 95). Plot precision vs. recall (Figure 3-5, page 96). From these plots, you can choose a threshold that gives a good balance for your project. E.g., if you want 90% precision, find the threshold for that (around 8000 in the example, page 96). At this threshold, recall might be lower (e.g., 43.7%). As the book wisely notes: \u0026ldquo;If someone says, \u0026lsquo;Let\u0026rsquo;s reach 99% precision,\u0026rsquo; you should ask, \u0026lsquo;At what recall?\u0026rsquo;\u0026rdquo;\nThe ROC Curve (Page 97-99): Another common tool for binary classifiers: Receiver Operating Characteristic (ROC) curve.\nPlots True Positive Rate (TPR, which is just recall) against False Positive Rate (FPR). FPR = FP / (FP + TN): Ratio of negative instances incorrectly classified as positive. FPR is also 1 - True Negative Rate (TNR). TNR is called specificity. So, ROC plots sensitivity (recall) vs. 1 - specificity. Plotting it: Compute TPR and FPR for various thresholds: from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(y_train_5, y_scores) (using the same y_scores from decision_function). Plot FPR vs. TPR using Matplotlib (Figure 3-6, page 98). Interpretation: Dotted diagonal line: Purely random classifier. Good classifier: Stays as far away from the diagonal as possible (toward the top-left corner – high TPR, low FPR). Trade-off: Higher TPR (recall) usually means more FPRs. Area Under the Curve (AUC) or ROC AUC: A single number to compare classifiers. Perfect classifier: ROC AUC = 1. Random classifier: ROC AUC = 0.5. from sklearn.metrics import roc_auc_score roc_auc_score(y_train_5, y_scores) gives ~0.96 for SGDClassifier. ROC vs. Precision-Recall (PR) Curve (Page 98 sidebar): When to use which? Prefer PR curve when positive class is rare OR you care more about false positives than false negatives. Otherwise, ROC is fine. MNIST \u0026lsquo;5\u0026rsquo;s are ~10% of data (positive class is somewhat rare). The ROC curve might look good because there are many true negatives. The PR curve might reveal more room for improvement. Comparing with RandomForestClassifier (Page 98-99): RandomForestClassifier doesn\u0026rsquo;t have decision_function(). It has predict_proba(). predict_proba() returns an array: one row per instance, one column per class, with the probability of that instance belonging to that class. Get probabilities for the positive class: y_probas_forest = cross_val_predict(forest_clf, ..., method=\u0026quot;predict_proba\u0026quot;) y_scores_forest = y_probas_forest[:, 1] (probabilities for the positive class, i.e., \u0026lsquo;5\u0026rsquo;). Plot ROC for RandomForest (Figure 3-7, page 99). It\u0026rsquo;s much closer to top-left. roc_auc_score(y_train_5, y_scores_forest) is ~0.998. Much better! RandomForest also has much better precision (~99.0%) and recall (~86.6%). (Page 100-102: Multiclass Classification)\nSo far, binary (5 vs. not-5). Now, distinguishing all 10 digits (0-9). This is multiclass classification (or multinomial).\nSome algorithms (SGD, RandomForest, Naive Bayes) can handle multiple classes natively.\nOthers (Logistic Regression, SVMs) are strictly binary.\nStrategies for using binary classifiers for multiclass:\nOne-versus-the-Rest (OvR) or One-versus-All (OvA): Train N binary classifiers (one for each class). E.g., a 0-detector, a 1-detector, \u0026hellip;, a 9-detector. To classify a new image: get decision score from each of the 10 classifiers. Select the class whose classifier outputs the highest score. One-versus-One (OvO): Train a binary classifier for every pair of digits. 0-vs-1, 0-vs-2, \u0026hellip;, 1-vs-2, \u0026hellip;, 8-vs-9. For N classes, this is N * (N-1) / 2 classifiers. For MNIST (N=10), it\u0026rsquo;s 45 classifiers! To classify: run image through all 45 classifiers. See which class \u0026ldquo;wins\u0026rdquo; the most duels. Main advantage: Each classifier is trained only on the subset of data for the two classes it distinguishes. Good for algorithms that scale poorly with training set size (like SVMs). For most others, OvR is preferred. Scikit-Learn\u0026rsquo;s behavior (page 101):\nIt detects if you use a binary classifier for a multiclass task and automatically runs OvR or OvO. Example with SVC (Support Vector Classifier): from sklearn.svm import SVC svm_clf = SVC() svm_clf.fit(X_train, y_train) (Note: y_train, not y_train_5) svm_clf.predict([some_digit]) correctly predicts [5]. Under the hood, SVC used OvO. It trained 45 binary classifiers. svm_clf.decision_function([some_digit]) returns 10 scores (one per class). The highest score corresponds to class \u0026lsquo;5\u0026rsquo;. svm_clf.classes_ shows the list of target classes. You can force OvO or OvR: from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier ovr_clf = OneVsRestClassifier(SVC()) SGDClassifier for multiclass (page 102):\nSGD can do multiclass natively. Scikit-Learn doesn\u0026rsquo;t need OvR/OvO. sgd_clf.fit(X_train, y_train) sgd_clf.decision_function([some_digit]) returns 10 scores. Class \u0026lsquo;5\u0026rsquo; has the highest score (2412.5), class \u0026lsquo;3\u0026rsquo; has a small positive score (573.5), others negative. Evaluate with cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\u0026quot;accuracy\u0026quot;) -\u0026gt; gets ~84-87%. Scaling inputs (as in Ch 2) with StandardScaler improves accuracy to \u0026gt;89%! scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train.astype(np.float64)) cross_val_score(sgd_clf, X_train_scaled, ...) (Page 103-105: Error Analysis)\nAssume you have a promising model. Now, analyze its errors to improve it.\nMulticlass Confusion Matrix: Get predictions: y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) conf_mx = confusion_matrix(y_train, y_train_pred) It\u0026rsquo;s a 10x10 matrix. Plot it with plt.matshow(conf_mx, cmap=plt.cm.gray) (image on page 103). Most images on main diagonal (correctly classified). The \u0026lsquo;5\u0026rsquo;s look slightly darker. Either fewer 5s, or classifier performs worse on 5s. (Book says both are true). Focus on errors: Divide each value in confusion matrix by number of images in the actual class (row sums) to get error rates. row_sums = conf_mx.sum(axis=1, keepdims=True) norm_conf_mx = conf_mx / row_sums Fill diagonal with zeros to keep only errors. Plot norm_conf_mx (image on page 104). Rows = actual classes, Columns = predicted classes. Column for class \u0026lsquo;8\u0026rsquo; is bright: many images get misclassified as 8s. Row for class \u0026lsquo;8\u0026rsquo; is not too bad: actual 8s are generally classified correctly. 3s and 5s often get confused (in both directions). What to do? Improve classification of digits that look like 8s (but aren\u0026rsquo;t). Gather more such training data. Engineer new features (e.g., count closed loops: 8 has two, 6 has one, 5 has none). Preprocess images to make patterns stand out (Scikit-Image, Pillow, OpenCV). Analyzing individual errors (page 104-105): Plot examples of 3s classified as 5s, 5s classified as 3s, etc. (Figure on page 105). Some errors are understandable (even humans would struggle). Many seem like obvious errors. Why does SGDClassifier (a linear model) make them? It assigns a weight per pixel per class and sums weighted intensities. 3s and 5s differ by only a few pixels. Sensitivity to shifting/rotation. Preprocessing (centering, de-skewing) could help. (Page 106-107: Multilabel Classification)\nSometimes, an instance can belong to multiple binary classes.\nExample: Face recognition – if Alice and Charlie are in a picture, output should be [Alice:yes, Bob:no, Charlie:yes] or [1, 0, 1]. Simpler example with MNIST: Label 1: Is the digit large (7, 8, or 9)? y_train_large = (y_train \u0026gt;= 7) Label 2: Is the digit odd? y_train_odd = (y_train % 2 == 1) Combine into y_multilabel = np.c_[y_train_large, y_train_odd] (each instance has two labels). Train a classifier (e.g., KNeighborsClassifier, which supports multilabel): knn_clf = KNeighborsClassifier() knn_clf.fit(X_train, y_multilabel) Prediction for some_digit (a \u0026lsquo;5\u0026rsquo;): knn_clf.predict([some_digit]) gives array([[False, True]]). Correct! (5 is not large, 5 is odd). Evaluation: Measure F₁ score for each label, then average. f1_score(y_multilabel, y_train_knn_pred, average=\u0026quot;macro\u0026quot;) (\u0026ldquo;macro\u0026rdquo; gives equal weight to each label). Can set average=\u0026quot;weighted\u0026quot; to weight by support (number of instances per label). (Page 107-108: Multioutput Classification)\nGeneralization of multilabel: each label can be multiclass (not just binary).\nExample: Image denoising. Input: Noisy digit image (each pixel is an input feature). Output: Clean digit image (each pixel is a label, and each label can have values 0-255). This is multilabel (one label per pixel) and each label is multiclass (0-255 intensity). Create noisy data: X_train_mod = X_train + noise. Target is clean image: y_train_mod = X_train. Train KNeighborsClassifier: knn_clf.fit(X_train_mod, y_train_mod). Predict on a noisy test image: clean_digit = knn_clf.predict([X_test_mod[some_index]]). The result (image on page 108) looks close to the target! The line between classification and regression can be blurry here (predicting pixel intensity). Multioutput systems can even mix class labels and value labels.\n(Page 108-109: Exercises) The chapter concludes with some excellent exercises:\nBuild a \u0026gt;97% accuracy classifier for MNIST (hint: KNeighborsClassifier with grid search). Data augmentation: shift images, add to training set, retrain. Tackle Titanic dataset (Kaggle). Build a spam classifier (challenging!). Glossary Q . why does ROC curve exist only for binary classifiier? That\u0026rsquo;s a sharp question! You\u0026rsquo;re right to notice that the ROC curve, as we\u0026rsquo;ve discussed it so far (plotting True Positive Rate vs. False Positive Rate), is fundamentally designed for and most intuitively understood in the context of binary classification.\nLet\u0026rsquo;s break down why:\nClear Definition of \u0026ldquo;Positive\u0026rdquo; and \u0026ldquo;Negative\u0026rdquo; Classes:\nIn binary classification, you have a clear distinction: one class is designated as \u0026ldquo;positive\u0026rdquo; (e.g., \u0026ldquo;is a 5,\u0026rdquo; \u0026ldquo;is spam,\u0026rdquo; \u0026ldquo;has disease\u0026rdquo;) and the other as \u0026ldquo;negative\u0026rdquo; (e.g., \u0026ldquo;is not a 5,\u0026rdquo; \u0026ldquo;is not spam,\u0026rdquo; \u0026ldquo;does not have disease\u0026rdquo;). This allows for unambiguous definitions of: True Positives (TP): Correctly identifying a positive instance. False Positives (FP): Incorrectly identifying a negative instance as positive. True Negatives (TN): Correctly identifying a negative instance. False Negatives (FN): Incorrectly identifying a positive instance as negative. TPR (Recall) = TP / (TP + FN) FPR = FP / (FP + TN) These metrics are the axes of the ROC curve. Single Decision Threshold:\nBinary classifiers often output a score or probability that an instance belongs to the positive class. A single threshold is then used on this score to make the binary decision. The ROC curve visualizes performance across all possible values of this single threshold. Why it\u0026rsquo;s tricky for multiclass classification (more than 2 classes):\nWhen you have, say, 3 classes (A, B, C), the concepts of TP, FP, TN, FN become less straightforward if you try to apply them in the same way.\nWhat is \u0026ldquo;positive\u0026rdquo;? If you\u0026rsquo;re trying to detect class A, then A is \u0026ldquo;positive,\u0026rdquo; and B and C combined are \u0026ldquo;negative.\u0026rdquo; You could make an ROC curve for this \u0026ldquo;A vs. Not-A\u0026rdquo; scenario. What about class B? You\u0026rsquo;d then need a separate \u0026ldquo;B vs. Not-B\u0026rdquo; (where Not-B is A and C) scenario and its own ROC curve. And another for \u0026ldquo;C vs. Not-C.\u0026rdquo; So, for a multiclass problem with N classes, you can\u0026rsquo;t draw a single, simple ROC curve in the same 2D TPR vs. FPR space that directly represents the performance across all classes simultaneously.\nHow ROC is often adapted or used in multiclass settings:\nEven though a single ROC curve isn\u0026rsquo;t a direct fit, the underlying principles can be extended, typically using a One-vs-Rest (OvR) or One-vs-One (OvO) approach:\nOne-vs-Rest (OvR) ROC Curves:\nYou can create N separate binary classification problems. For each class i, you treat class i as the positive class and all other classes (N-1 of them) as the negative class. You can then compute an ROC curve and its AUC for each of these N binary problems. This gives you N ROC curves and N AUC scores. You can then, for example, average these AUC scores (e.g., macro-average or weighted-average) to get an overall performance metric for the multiclass classifier. Some libraries might even plot all N curves on the same graph or an \u0026ldquo;averaged\u0026rdquo; ROC curve, but this average curve can be harder to interpret directly than a binary ROC. One-vs-One (OvO) ROC Curves:\nYou could, in theory, create an ROC curve for each pair of classes (N*(N-1)/2 curves). This becomes unwieldy very quickly and isn\u0026rsquo;t commonly done for overall visualization. The Key Idea:\nThe ROC curve\u0026rsquo;s power lies in visualizing the trade-off for a single decision boundary between two distinct outcomes as you vary the threshold. In multiclass, you inherently have multiple potential decision boundaries between multiple classes, or you\u0026rsquo;re reducing it to multiple binary problems.\nThink of it intuitively:\nBinary: \u0026ldquo;How good am I at separating the \u0026lsquo;good guys\u0026rsquo; from the \u0026lsquo;bad guys\u0026rsquo; as I change my definition of \u0026lsquo;suspicious activity\u0026rsquo; (the threshold)?\u0026rdquo; Multiclass: If you have apples, oranges, and bananas, how would you define a single TPR and FPR that captures the misclassification rates between all pairs (apples called oranges, oranges called bananas, apples called bananas, etc.) on a single 2D plot in the same way? It\u0026rsquo;s not directly possible with the original TPR/FPR definitions. In summary:\nThe standard ROC curve (TPR vs. FPR) is a tool specifically tailored for visualizing the performance of a binary classifier by showing its ability to distinguish between a positive and a negative class across various decision thresholds. While the concept can be extended to multiclass scenarios by breaking the problem down into multiple binary subproblems (like OvR), you won\u0026rsquo;t typically see a single, direct \u0026ldquo;multiclass ROC curve\u0026rdquo; in the same simple 2D form. Instead, you\u0026rsquo;d look at metrics derived from these multiple binary evaluations or other multiclass-specific evaluation tools (like a full confusion matrix for all classes).\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-3/","tags":null,"title":"Chapter 3: Classification"},{"categories":null,"content":"As the book says, \u0026ldquo;The rise of ML in recent years is tightly coupled with the rise of big data.\u0026rdquo; At FAANG, this is our daily reality. Our ML systems, from recommendation engines to fraud detection, are built on vast, complex data landscapes. If you don\u0026rsquo;t have a solid grasp of data engineering, you\u0026rsquo;re going to struggle, no matter how fancy your model architecture is.\nThis chapter is dense with terminology and concepts that might seem overwhelming if you\u0026rsquo;re new to large-scale data systems. But don\u0026rsquo;t worry, we\u0026rsquo;ll break it down. The goal here is to give you a \u0026ldquo;steady piece of land to stand on.\u0026rdquo;\nWe\u0026rsquo;ll cover:\nData Sources: Where does our data even come from? Data Formats: How is it stored? What are the trade-offs? Data Models: How is it structured and represented? Data Storage Engines \u0026amp; Processing: Databases, transactional vs. analytical workloads, ETL. Modes of Dataflow: How does data move between different parts of our systems? Batch vs. Stream Processing: Two fundamental paradigms for handling data. This chapter is critical for ML system design interviews. Understanding data is non-negotiable. Let\u0026rsquo;s get started!\nPage 49 (Chapter Introduction): The Data Deluge The intro page sets the scene:\nML\u0026rsquo;s growth is linked to big data. Large data systems are complex, even without ML. Full of acronyms, evolving standards, diverse tools. It can feel like every company does it differently. This chapter provides the basics of data engineering. The roadmap mentioned:\nSources of data. Formats for storage. Structure of data (data models). Databases (storage engines) for transactional and analytical processing. Passing data across multiple processes and services. This chapter really emphasizes that data engineering is a distinct discipline that ML practitioners must understand.\nPages 50-52: Data Sources – Where It All Begins An ML system ingests data from various sources. Understanding these sources helps use the data more efficiently and anticipate challenges.\nUser Input Data (Page 50) Data explicitly provided by users.\nExamples: Text typed into a search bar, images/videos uploaded, form submissions. Challenges: Malformatted: \u0026ldquo;If it\u0026rsquo;s even remotely possible for users to input wrong data, they are going to do it.\u0026rdquo; This is a golden rule! Text too long/short, text in numerical fields, wrong file formats. Requires heavy-duty validation and processing. User Patience is Low: Users expect immediate results from their input. This implies a need for fast processing. FAANG Perspective: Input validation is a huge deal. For example, search queries need sanitization, image uploads need format checks and virus scans. Latency is critical – a slow search box is a bad user experience.\nSystem-Generated Data (Pages 50-51) Data generated by your system\u0026rsquo;s components.\nExamples: Logs (memory usage, services called, job results), model predictions. Purpose: Visibility into system health, debugging, improving the application. Essential \u0026ldquo;when something is on fire.\u0026rdquo; Characteristics: Less likely to be malformatted than user input. Processing doesn\u0026rsquo;t always need to be immediate (e.g., hourly/daily log processing is often fine). However, you might want faster processing for \u0026ldquo;interesting\u0026rdquo; events (footnote 1: \u0026ldquo;interesting\u0026rdquo; often means \u0026ldquo;catastrophic\u0026rdquo; – like a system crash or a runaway cloud bill!). This is where real-time alerting on logs comes in. Challenges with Logs: Volume: \u0026ldquo;Log everything you can\u0026rdquo; is common practice for debugging ML systems, leading to massive log volumes. Signal vs. Noise: Hard to find useful information. Services like Logstash, Datadog, Logz.io (often using ML themselves) help process and analyze logs. Storage Cost: Store logs only as long as useful. Low-access, cheaper storage (e.g., AWS S3 Glacier vs. S3 Standard – footnote 2 notes a 5x cost difference for much higher retrieval latency) can be used for older logs. FAANG Perspective: We generate petabytes of logs daily. Effective log management, aggregation, and analysis are critical for SREs (Site Reliability Engineers) and ML engineers alike to debug production issues.\nUser Behavior Data (Page 51) System-generated data specifically recording user actions.\nExamples: Clicks, suggestions chosen, scrolling, zooming, ignoring pop-ups, time spent on page. Crucial for ML: This is the raw material for training many models (recommenders, personalization, engagement prediction). Privacy Concerns: Even if system-generated, it\u0026rsquo;s considered user data and subject to privacy regulations (e.g., GDPR, CCPA). Footnote 3 has a great anecdote: an ML engineer says his team only uses browsing/purchase history, not \u0026ldquo;personal data\u0026rdquo; like age/location. The author rightly points out that browsing/purchase history is extremely personal! FAANG Perspective: This is gold. But handling it ethically and in compliance with regulations is paramount. Data anonymization and aggregation are key, but true anonymization is very hard.\nInternal Databases (Page 52) Generated by various services and enterprise applications within the company.\nExamples: Inventory, customer relationship management (CRM), user accounts. Usage in ML: Directly by models (e.g., a model might need current user subscription status). By components of an ML system (e.g., Amazon search: ML model detects intent for \u0026ldquo;frozen,\u0026rdquo; then system checks internal inventory database for \u0026ldquo;Frozen\u0026rdquo; movie vs. \u0026ldquo;frozen foods\u0026rdquo; availability before ranking). FAANG Perspective: These are often the \u0026ldquo;source of truth\u0026rdquo; for many entities. Integrating them into ML pipelines reliably and efficiently is a common data engineering task.\nThird-Party Data (Page 52) The \u0026ldquo;wonderfully weird world.\u0026rdquo;\nFirst-party data: What your company collects about its own users/customers. (This is the best, you own it, you know its provenance). Second-party data: Data from another company on their customers, which they make available to you (usually for a fee). Third-party data: Companies collect data about the public (not their direct customers) and sell it. Collection Mechanisms: Historically, unique advertiser IDs on phones (IDFA on iOS, AAID on Android) made it easy to aggregate activity across apps, websites, check-ins. This data is (hopefully) anonymized. Types of Data: Social media activity, purchase history, web browsing, car rentals, political leaning, demographics (e.g., \u0026ldquo;men, age 25-34, tech workers, Bay Area\u0026rdquo;). Use Cases: Inferring correlations (people liking Brand A also like Brand B), helpful for recommenders. Usually sold pre-cleaned and processed. Privacy Pushback: Apple\u0026rsquo;s IDFA opt-in (early 2021) significantly reduced third-party data on iPhones, forcing companies to focus more on first-party data (footnote 4). Advertisers seek workarounds (e.g., CAID device fingerprinting in China - footnote 5). FAANG Perspective: While first-party data is king, third-party data can be useful for cold-start problems or enriching user profiles. However, the privacy and ethical implications are significant, and reliance on it is decreasing due to regulations and platform changes.\nUnderstanding your data sources tells you about its likely quality, freshness, volume, and any constraints (privacy, cost, processing needs).\nPages 53-57: Data Formats – How Data is Represented for Storage and Transmission Once you have data, you need to store it (\u0026ldquo;persist\u0026rdquo; it). The format matters for cost, access speed, and ease of use. Key questions to consider:\nStoring multimodal data (images + text)? Cheap and fast access? Storing complex models for cross-hardware compatibility? Data Serialization: Converting a data structure/object state into a format that can be stored/transmitted and later reconstructed.\nTable 3-1: Common Data Formats\nFormat Binary/Text Human-readable Example Use Cases JSON Text Yes Everywhere CSV Text Yes Everywhere Parquet Binary No Hadoop, Amazon Redshift Avro Binary primary No Hadoop Protobuf Binary primary No Google, TensorFlow (TFRecord) Pickle Binary No Python, PyTorch serialization (models) Key characteristics to consider: human readability, access patterns (how data is read/written - footnote 6), text vs. binary (impacts file size).\nLet\u0026rsquo;s look at a few in detail:\nJSON (JavaScript Object Notation) (Page 54) Ubiquitous, language-independent, human-readable. Key-value pair paradigm, handles different levels of structuredness. Structured: {\u0026quot;firstName\u0026quot;: \u0026quot;Boatie\u0026quot;, \u0026quot;address\u0026quot;: {\u0026quot;city\u0026quot;: \u0026quot;Port Royal\u0026quot;}} Unstructured blob: {\u0026quot;text\u0026quot;: \u0026quot;Boatie McBoatFace, aged 12, is vibing...\u0026quot;} Pain points: Schema evolution is painful once committed. Changing a schema in existing JSON files is hard. Text files = take up a lot of space (more on this later). FAANG Perspective: JSON is incredibly common for API responses, configs, and semi-structured data logging. Its human readability is a big plus for debugging.\nRow-Major Versus Column-Major Format (Pages 54-55, Figure 3-1) CSV (Comma-Separated Values): Row-major. Consecutive elements in a row are stored together in memory. Example1_Feat1, Example1_Feat2, ... Example1_FeatN Example2_Feat1, Example2_Feat2, ... Example2_FeatN Parquet: Column-major (Columnar). Consecutive elements in a column are stored together. Example1_Feat1, Example2_Feat1, ... ExampleM_Feat1 Example1_Feat2, Example2_Feat2, ... ExampleM_Feat2 Performance implications (due to sequential data access being efficient): Row-major (CSV): Better for accessing entire rows/examples (e.g., \u0026ldquo;get all examples from today\u0026rdquo;). Faster writes when adding new individual examples. Column-major (Parquet): Better for accessing specific columns/features (e.g., \u0026ldquo;get timestamps for all examples\u0026rdquo;). More flexible column-based reads, especially with many features. If you have 1000 features but only need 4 (time, location, distance, price), Parquet lets you read just those 4 columns directly. With CSV, you\u0026rsquo;d often read all 1000 and then filter. Overall: Row-major for write-heavy workloads or row-based reads. Column-major for read-heavy, analytical workloads needing specific columns. FAANG Perspective: Columnar formats like Parquet (and ORC) are the standard for data warehouses and data lakes (e.g., in S3, GCS) because analytical queries usually operate on subsets of columns. This leads to huge I/O savings and faster query performance. Compression is also more effective on columnar data.\nNumPy Versus pandas (Page 56, Figure 3-2): A Common Gotcha! Many don\u0026rsquo;t realize: pandas DataFrame is built around a columnar format (inspired by R\u0026rsquo;s data frame). NumPy ndarrays are row-major by default (though configurable). People coming from NumPy often treat DataFrames like ndarrays, accessing by row, and find it slow. Figure 3-2 Performance: Iterating pandas DataFrame by column: 0.07 seconds. Iterating pandas DataFrame by row (df.iloc[i]): 2.41 seconds (MUCH SLOWER!). Converting to NumPy array (df.to_numpy()) and iterating by row: 0.019 seconds (FAST!). Iterating NumPy array by column: 0.005 seconds (FASTEST, as expected for columnar data if NumPy array was C-contiguous/columnar, but it\u0026rsquo;s likely F-contiguous/row-major here, so this illustrates pandas\u0026rsquo; overhead). Self-Correction/Teaching Point: The code snippet df_np[:, j] in Figure 3-2 iterates through a NumPy array column by column. If df_np is row-major (NumPy default), this is non-contiguous access, which should be slower than row-wise access. The 0.005s vs 0.019s suggests the test DataFrame might have few rows and many columns, or there\u0026rsquo;s something subtle about to_numpy() memory layout or caching effects. The main takeaway is pandas row iteration is slow due to its internal structure and overhead. Use vectorized operations in pandas, or convert to NumPy for row-wise loops if absolutely necessary. The author\u0026rsquo;s \u0026ldquo;Just pandas Things\u0026rdquo; GitHub repo (footnote 7) is a good resource for these quirks.\nText Versus Binary Format (Page 57, Figure 3-3) CSV, JSON are text files (plain text, human-readable). Parquet is a binary file (0s and 1s, machine-readable). Binary files are more compact: Example: Storing the number 1000000. Text file: 7 characters, 7 bytes (if 1 byte/char). Binary file (int32): 32 bits = 4 bytes. (Significant saving!) Figure 3-3 Illustration (interviews.csv): CSV (text): 17,654 rows, 10 columns. File size: 14 MB. Parquet (binary): Same data. File size: 6 MB. (Over 2x smaller). AWS recommends Parquet: \u0026ldquo;up to 2x faster to unload and consumes up to 6x less storage in Amazon S3, compared to text formats\u0026rdquo; (footnote 8). This is due to efficient encoding and compression schemes that work well with columnar data. FAANG Perspective: For large datasets, binary columnar formats (Parquet, ORC) are almost always preferred over text formats like CSV/JSON for storage in data lakes due to space savings, query performance, and schema evolution support. Text formats are fine for smaller files, human inspection, or system interchange where readability is key.\nPages 58-66: Data Models – Structuring Your Data Data models describe how data is represented and the relationships between data elements. This choice affects system build and the problems you can solve.\nRelational Model (Pages 59-61) Invented by Edgar F. Codd (1970), still dominant. Data organized into relations (tables), each a set of tuples (rows). Key Property (Figure 3-4): Relations are unordered (rows and columns can be shuffled, it\u0026rsquo;s still the same relation). Stored in formats like CSV/Parquet. Normalization: Reducing redundancy, improving integrity. Example (Tables 3-2, 3-3, 3-4): Book data. Initial Book relation (Table 3-2): Title, Author, Format, Publisher, Country, Price. Duplicates publisher info (Banana Press, UK) for different books/formats. If \u0026ldquo;Banana Press\u0026rdquo; changes to \u0026ldquo;Pineapple Press\u0026rdquo;, multiple rows need updates. Normalized: Book relation (Table 3-3): Title, Author, Format, Publisher_ID, Price. Publisher relation (Table 3-4): Publisher_ID, Publisher, Country. Now, if publisher name changes, only one row in Publisher table needs update. Standardizes spelling, easier to translate values. Downside of Normalization: Data spread across tables. Retrieving full info requires joins, which can be expensive for large tables. Relational Databases \u0026amp; SQL (Structured Query Language): SQL is the most popular query language. Declarative: You specify what data you want (pattern, conditions, transformations like join, sort, group, aggregate), not how to get it. Imperative (like Python): You specify the steps. Database system has a query optimizer to figure out the execution plan (break query, methods for each part, order of execution). This is hard! ML is even being used to improve query optimizers (footnote 14, Neo). SQL is Turing-complete (with additions), but complex queries can be \u0026ldquo;nightmarish\u0026rdquo; (footnote 12, 700-line SQL query). FAANG Perspective: Relational databases (PostgreSQL, MySQL, Spanner, Aurora) are workhorses for many transactional systems and structured data stores. SQL is a fundamental skill. Understanding query plans (EXPLAIN) is key for performance tuning.\nAside: From Declarative Data Systems to Declarative ML Systems (Page 62) Inspired by SQL\u0026rsquo;s success, \u0026ldquo;Declarative ML\u0026rdquo; aims to abstract away model construction/tuning. User declares feature schema and task; system finds the best model. Examples: Ludwig (Uber), H2O AutoML. Ludwig: User can specify model structure (layers, units) on top of schema. H2O AutoML: No need to specify structure/hyperparameters; it experiments and picks best. Example code shows simple API: aml.train(x=x, y=y, training_frame=train). Limitation: Abstracts away model development (often the easier part now with commoditized models). Hard parts remain: feature engineering, data processing, evaluation, drift detection, continual learning. Self-Correction: Declarative ML is great for baselining and for users who aren\u0026rsquo;t ML experts, but for complex, high-stakes production systems at FAANG, engineers often need finer-grained control.\nNoSQL (Not Only SQL) (Pages 63-65) Movement against relational model\u0026rsquo;s restrictions (strict schema, schema management pain - #1 reason for Couchbase adoption, footnote 16). SQL can be hard for specialized apps. Many NoSQL systems now also support relational models/SQL. Two major types discussed: Document and Graph. Document Model (Pages 63-64) Built around \u0026ldquo;documents\u0026rdquo; (often JSON, XML, or binary like BSON). Each document has a unique key. Collection of documents ~ table, document ~ row. Flexibility: Documents in a collection can have different schemas (unlike rows in a relational table). \u0026ldquo;Schemaless\u0026rdquo; is Misleading: The reading application usually assumes some structure. Responsibility shifts from write-time schema enforcement (relational) to read-time schema interpretation (document). Example (Examples 3-1, 3-2, 3-3): Book data as JSON documents. All info for one book (Harry Potter) is in one document, including \u0026ldquo;Sold as\u0026rdquo; array for formats/prices. Better Locality: All info for a book is in one place, easier retrieval than joining multiple relational tables. Worse for Joins/Cross-Document Queries: Finding all books under $25 requires reading all documents, extracting prices, comparing. Less efficient than SQL WHERE price \u0026lt; 25. Many DBs (PostgreSQL, MySQL) now support both relational and document models. FAANG Perspective: Document databases (MongoDB, DynamoDB) are great for use cases with self-contained data items, flexible schemas, and high scalability needs (e.g., user profiles, product catalogs where attributes vary widely).\nGraph Model (Page 65) Data as a graph: nodes and edges (relationships). Prioritizes relationships between data items. Example (Figure 3-5): Social network. Nodes: person, city, country. Edges: lives_in, born_in, coworker, friend, within. Efficient for Relationship-Based Queries: \u0026ldquo;Find everyone born in USA.\u0026rdquo; Start at \u0026ldquo;USA\u0026rdquo; node, traverse within and born_in edges to find \u0026ldquo;person\u0026rdquo; nodes. Hard to do this easily in SQL or document model if #hops is unknown/variable (e.g., 3 hops from Zhenzhong Xu to USA, 2 from Chloe He). FAANG Perspective: Graph databases (Neo4j, Amazon Neptune) shine for use cases like social networks, knowledge graphs, fraud detection (rings of fraudsters), recommendation (users-who-bought-this-also-bought). Query languages like Cypher or Gremlin are used.\nPicking the Right Model: Crucial for simplifying development. Many queries easy in one model are hard in another.\nStructured Versus Unstructured Data (Page 66, Table 3-5) Structured Data: Follows a predefined data model/schema (e.g., name=string(50), age=int(0-200)). Easy to analyze (e.g., average age). Disadvantage: Schema changes require retrospective updates, can cause bugs (e.g., new \u0026rsquo;email\u0026rsquo; field; or null ages becoming 0, confusing ML model - footnote 18\u0026rsquo;s anecdote, solved by using -1). Unstructured Data: No predefined schema. Usually text, but can be numbers, dates, images, audio (e.g., log files). Advantage: Appealing when business reqs change, or data from many sources can\u0026rsquo;t conform to one schema. May still have intrinsic patterns (e.g., CSV-like log lines: Lisa,43). But no guarantee all lines follow it. Storage Options: Schema-enforced storage can only store conforming data. Schema-less storage can store any data (e.g., convert all to bytestrings). Data Warehouse: Repository for structured data (processed, ready to use). Data Lake: Repository for unstructured (or raw) data, often before processing. Table 3-5 Differences:\nFeature Structured Unstructured Schema Clearly defined Doesn\u0026rsquo;t have to follow a schema Search/Analyze Easy (Implied harder until structure is imposed) Fast arrival Data Handling Specific schema only Any source Schema Changes Lots of trouble Worry shifted to downstream apps Stored In Data warehouses Data lakes FAANG Perspective: The distinction is fluid. \u0026ldquo;Schema-on-read\u0026rdquo; (data lakes) vs. \u0026ldquo;schema-on-write\u0026rdquo; (warehouses). The trend is towards data lakehouses (e.g., Databricks, Snowflake) combining flexibility of lakes with management features of warehouses. Raw data lands in lake, then curated/structured versions are created.\nPages 67-71: Data Storage Engines and Processing Data formats/models = interface. Storage engines (databases) = implementation on machines. Two main workload types:\nTransactional and Analytical Processing (Pages 67-69) Transaction: Digital world: any action (tweet, ride order, model upload, YouTube watch). Inserted as generated, occasionally updated/deleted. Online Transaction Processing (OLTP): Needs to be fast (low latency) for users. High availability. If system can\u0026rsquo;t process, transaction fails. Transactional Databases: Designed for OLTP. Often associated with ACID properties (Atomicity, Consistency, Isolation, Durability - definitions on page 68 are standard). Atomicity: All steps succeed or all fail (e.g., payment fails, driver not assigned). Consistency: Transactions follow predefined rules (e.g., valid user). Isolation: Concurrent transactions appear isolated (e.g., two users don\u0026rsquo;t book same driver simultaneously). Durability: Committed transaction persists despite system failure (e.g., phone dies, ride still coming). Not all need ACID. Some find it too restrictive. BASE (Basically Available, Soft state, Eventual consistency) is an alternative, \u0026ldquo;even more vague\u0026rdquo; (Kleppmann, footnote 20). Often row-major (transactions processed as units). Online Analytical Processing (OLAP): For analytical questions (e.g., \u0026ldquo;average ride price in SF in Sept?\u0026rdquo;). Requires aggregating columns across many rows. Analytical Databases: Designed for this. Efficient with queries from different viewpoints. Often columnar. OLTP/OLAP are Outdated Terms? (Figure 3-6, Google Trends): Separation was due to tech limits (hard to do both well). This is closing. Transactional DBs handling analytical queries (e.g., CockroachDB). Analytical DBs handling transactional queries (e.g., Apache Iceberg, DuckDB). Traditional OLTP/OLAP: Storage and processing tightly coupled. Often meant same data stored multiple times for different query types. Modern Paradigm: Decouple Storage from Processing (Compute). Data in one place, different processing layers on top. (Google BigQuery, Snowflake, IBM, Teradata - footnote 21). \u0026ldquo;Online\u0026rdquo; is Overloaded: Used to mean \u0026ldquo;internet-connected,\u0026rdquo; then \u0026ldquo;in production.\u0026rdquo; Data world: speed of processing/availability (online, nearline, offline - footnote 22). FAANG Perspective: The decoupling of storage (e.g., S3, GCS) and compute (e.g., Spark, Presto, BigQuery) is a dominant architecture. It provides flexibility, scalability, and cost-efficiency. ACID is critical for financial transactions, eventual consistency is often fine for social media feeds.\nETL: Extract, Transform, and Load (Pages 70-71, Figure 3-7) Early days: relational data, mostly structured. ETL was data warehousing process. Still relevant for ML. General purpose processing/aggregating data into desired shape/format. Extract: Get data from sources. Validate, reject corrupted/malformatted data. Notify sources of rejected data. Crucial first step. Transform: Meaty part. Join, clean, standardize values (Male/Female vs M/F vs 1/2), transpose, deduplicate, sort, aggregate, derive new features, more validation. Load: Decide how/how often to load transformed data into target (file, DB, warehouse). Figure 3-7: Shows sources (DB, App, Flat files) -\u0026gt; ETL -\u0026gt; Targets (Data warehouse, Feature store, DB). Rise of ELT (Extract, Load, Transform): Internet/hardware boom -\u0026gt; easy to collect massive, evolving data. Schemas changed. Idea: Store all raw data in a data lake first (fast arrival, little pre-processing). Applications pull and process as needed. Problem with ELT as data grows: Inefficient to search massive raw data. (Footnote 23: storage cost is rarely a problem now, but processing cost/time is). Trend: Cloud/standardized infra -\u0026gt; committing to predefined schema becomes feasible again. Hybrid: Data Lakehouse (Databricks, Snowflake). Flexibility of lakes + management of warehouses. FAANG Perspective: ETL/ELT pipelines are the backbone of our data infrastructure. Building robust, scalable, and maintainable ETLs (often using Spark, Beam, Airflow) is a core data engineering function. Feature stores are becoming common \u0026ldquo;targets\u0026rdquo; for ML features.\nPages 72-77: Modes of Dataflow – How Data Moves In production, data isn\u0026rsquo;t in one process; it flows between many. How does it pass if processes don\u0026rsquo;t share memory?\nData Passing Through Databases (Page 72) Easiest way: Process A writes to DB, Process B reads from DB. Limitations: Both processes need access to same DB (infeasible if different companies). DB read/writes can be slow, unsuitable for low-latency apps (most consumer-facing ones). FAANG Perspective: Used for asynchronous tasks or when latency isn\u0026rsquo;t paramount. E.g., a batch job updates a model quality table, a dashboard service reads it.\nData Passing Through Services (Request-Driven) (Pages 73-74) Direct network communication. Process A requests data from Process B; B returns it. Service-Oriented Architecture (SOA) / Microservices: Process B is a \u0026ldquo;service\u0026rdquo; A can call. B can also call A if A is a service. Can be different companies (e.g., investment firm service calls stock exchange service for prices). Can be components of one app (microservices). Allows independent development, testing, maintenance. ML Example: Ride-Sharing Price Optimization (Lyft): Services: Driver Management (available drivers), Ride Management (requested rides), Price Optimization. Price Optimization service needs data from other two to predict optimal price (supply/demand). It requests this data. (Footnote 24: in practice, might use cached data, refresh periodically). Popular Styles: REST (Representational State Transfer) vs. RPC (Remote Procedure Call): REST: For network requests. Often public APIs. RPC: Make remote call look like local function call. Often internal services in same org/datacenter. (Kleppmann, footnote 25). RESTful = implements REST architecture. HTTP is an implementation, not same as REST (footnote 26). FAANG Perspective: Microservices are ubiquitous. REST for external/public APIs, gRPC (an RPC framework) very common for internal service-to-service communication due to efficiency and strong typing.\nData Passing Through Real-Time Transport (Event-Driven) (Pages 74-77) Motivation (Ride-sharing example, Figure 3-8): If Price Optimization, Driver Mgmt, Ride Mgmt all need data from each other via requests, it becomes a complex web. With hundreds/thousands of services, this is a bottleneck. Request-driven is synchronous: Target service must be listening. If Driver Mgmt is down, Price Opt. keeps resending, times out. Response lost if Price Opt. goes down. Cascading failures. Broker/Event Bus (Figure 3-9): Services communicate via a central broker. Driver Mgmt makes a prediction, broadcasts it (an event) to broker. Other services wanting this data get it from broker. Technically, a DB can be a broker. But slow for low-latency. So, use in-memory storage for brokering (real-time transports). Event-driven architecture: Better for data-heavy systems. Request-driven for logic-heavy. Common Types: Pub/Sub (Publish-Subscribe): Apache Kafka, Amazon Kinesis. Services publish events to topics. Services subscribe to topics to read events. Producers don\u0026rsquo;t care about consumers. Retention Policy (Figure 3-10): Events kept in in-memory transport for a period (e.g., 7 days), then deleted or moved to permanent storage (e.g., S3). This is key for Kafka\u0026rsquo;s design – it\u0026rsquo;s a durable commit log. Message Queue: Apache RocketMQ, RabbitMQ. Event (message) often has intended consumers. Queue gets message to right consumers. Both Kafka/RabbitMQ are very popular (Figure 3-11, Stackshare). (Footnote 27: Mitch Seymour\u0026rsquo;s Kafka/otters animation is great!) FAANG Perspective: Kafka is a cornerstone of many real-time data pipelines for logging, metrics, event sourcing, stream processing. It enables decoupling of services and resilience.\nPages 78-79: Batch Processing Versus Stream Processing Two paradigms for processing data based on its nature (historical vs. in-flight).\nBatch Processing Data in storage (DBs, lakes, warehouses) = historical data. Processed in batch jobs (kicked off periodically, e.g., daily job for average surge charge). Distributed systems like MapReduce, Spark process batch data efficiently. Use in ML: Compute features that change less often (static features). E.g., driver\u0026rsquo;s overall rating (if hundreds of rides, one more doesn\u0026rsquo;t change it much day-to-day). Stream Processing Data in real-time transports (Kafka, Kinesis) = streaming data. Computation on this data. Can be periodic (shorter periods, e.g., every 5 mins) or triggered (e.g., user requests ride -\u0026gt; process stream for available drivers). Low latency: Process data as generated, without writing to DB first. Efficiency: Myth: Less efficient than batch (can\u0026rsquo;t use Spark/MapReduce). Not always true. Streaming tech (Apache Flink) is scalable, distributed (parallel computation). Strength: Stateful computation. Example: 30-day user engagement trial. Batch: recompute over last 30 days daily. Stream: compute on new day\u0026rsquo;s data, join with older computation (state). Avoids redundancy. Use in ML: Compute features that change quickly (dynamic features / streaming features). E.g., drivers available right now, rides requested last minute, median price of last 10 rides in area. Essential for optimal real-time predictions. Both Batch and Stream Features Needed: Many problems need both. Need infra to process both and join them for ML models (preview of Chapter 7). Stream Computation Engines: Kafka\u0026rsquo;s built-in stream processing is limited (various data sources). ML streaming features often need complex queries (joins, aggregations). Need efficient stream processing engines: Apache Flink, KSQL (Kafka SQL), Spark Streaming. Flink, KSQL more recognized, nice SQL abstraction for data scientists. Stream processing is harder: Unbounded data, variable rates/speeds. Argument (Flink maintainers, footnote 28): Batch is a special case of streaming. (i.e., a bounded stream). Stream engines can unify both. FAANG Perspective: This unification is a powerful trend (e.g., Apache Beam model). Lambda architectures (separate batch/stream paths) are complex; Kappa architectures (all stream) are simpler if feasible. Choosing the right features (static, dynamic, or both) is key for model performance and system complexity.\nPages 79-80: Summary of Chapter 3 This chapter built on Chapter 2\u0026rsquo;s emphasis on data\u0026rsquo;s importance. Key takeaways:\nData Formats: Choose wisely for future use. Row-major vs. column-major, text vs. binary have pros/cons. Data Models: Relational (SQL), Document, Graph. All widely used, each suited for different tasks. Structured (writer assumes schema) vs. Unstructured (reader assumes schema) is fluid. Storage \u0026amp; Processing: Traditionally coupled (OLTP DBs for transactional, OLAP for analytical). Decoupling storage/compute is the trend. Hybrid DBs emerging. Modes of Dataflow: Databases (slow, simple), Services (request-driven, microservices), Real-time Transports (event-driven, Kafka/RabbitMQ for async, low latency). Batch vs. Stream Processing: Historical data -\u0026gt; batch jobs -\u0026gt; static features. Streaming data -\u0026gt; stream engines -\u0026gt; dynamic features. Stream engines can potentially unify both. With data systems figured out, next chapter is about collecting data and creating training data!\nInterview Questions \u0026amp; Page References (Chapter 3) As promised, here\u0026rsquo;s a list of potential interview questions related to Chapter 3, with page numbers for where you can find relevant concepts in the book:\nGeneral Data Understanding: \u0026ldquo;Describe the different types of data sources you might encounter in an ML project and their characteristics.\u0026rdquo; (p. 50-52) \u0026ldquo;What are the challenges associated with user-input data? How would you handle them?\u0026rdquo; (p. 50) \u0026ldquo;Why is system-generated log data important? What are the challenges in managing it?\u0026rdquo; (p. 50-51) \u0026ldquo;What are the privacy considerations for user behavior data?\u0026rdquo; (p. 51, esp. footnote 3) \u0026ldquo;Explain the difference between first-party, second-party, and third-party data. What are the trends affecting third-party data?\u0026rdquo; (p. 52) Data Formats: \u0026ldquo;What is data serialization? Name some common data formats and their use cases.\u0026rdquo; (p. 53, Table 3-1) \u0026ldquo;Compare and contrast row-major (e.g., CSV) and column-major (e.g., Parquet) data formats. When would you choose one over the other?\u0026rdquo; (p. 54-55, Figure 3-1) \u0026ldquo;Why might iterating over pandas DataFrame rows be slow? How does its internal storage format relate to this?\u0026rdquo; (p. 56, Figure 3-2) \u0026ldquo;Discuss the trade-offs between text formats (like JSON/CSV) and binary formats (like Parquet).\u0026rdquo; (p. 57, Figure 3-3) \u0026ldquo;How would you choose a data format for storing a large dataset intended for analytical queries?\u0026rdquo; (Implied: Parquet, p. 55, 57) Data Models: \u0026ldquo;What is the relational data model? Explain the concept of normalization and its pros/cons.\u0026rdquo; (p. 59-60, Tables 3-2 to 3-4) \u0026ldquo;What does it mean for SQL to be a declarative language?\u0026rdquo; (p. 61) \u0026ldquo;What is NoSQL? Describe the document model and its advantages/disadvantages compared to the relational model.\u0026rdquo; (p. 63-64, Examples 3-1 to 3-3) \u0026ldquo;When would a graph data model be appropriate? Give an example.\u0026rdquo; (p. 65, Figure 3-5) \u0026ldquo;Explain the difference between structured and unstructured data. What are data lakes and data warehouses?\u0026rdquo; (p. 66, Table 3-5) Data Storage Engines \u0026amp; Processing: \u0026ldquo;What are OLTP and OLAP? How do their requirements differ?\u0026rdquo; (p. 67-69) \u0026ldquo;What are ACID properties? Why are they important for transactional databases?\u0026rdquo; (p. 68) \u0026ldquo;Why are the terms OLTP/OLAP becoming outdated? What is the significance of decoupling storage and compute?\u0026rdquo; (p. 69, Figure 3-6) \u0026ldquo;Describe the ETL process. What are the key steps?\u0026rdquo; (p. 70-71, Figure 3-7) \u0026ldquo;What is ELT, and how does it relate to data lakes?\u0026rdquo; (p. 71) Modes of Dataflow: \u0026ldquo;Describe different ways data can be passed between processes in a production system.\u0026rdquo; (p. 72) \u0026ldquo;When is passing data through databases suitable/unsuitable?\u0026rdquo; (p. 72) \u0026ldquo;Explain request-driven data passing and its connection to microservices. What are REST and RPC?\u0026rdquo; (p. 73-74) \u0026ldquo;What is event-driven architecture for data passing? Describe real-time transports like pub/sub (Kafka) and message queues.\u0026rdquo; (p. 74-77, Figures 3-8 to 3-11) \u0026ldquo;Why might an event-driven architecture be preferred over a request-driven one for a system with many services?\u0026rdquo; (p. 75) Batch vs. Stream Processing: \u0026ldquo;Compare batch processing and stream processing. What types of data and features are typically associated with each?\u0026rdquo; (p. 78) \u0026ldquo;What are the advantages of stream processing, especially concerning latency and stateful computation?\u0026rdquo; (p. 78) \u0026ldquo;Name some stream processing engines. Why are they necessary for complex streaming ML features?\u0026rdquo; (p. 79) \u0026ldquo;How can batch and stream processing be combined in an ML system?\u0026rdquo; (p. 79, though more in Ch7) ","permalink":"https://deepskandpal.github.io/bookshelf/design-ml-system/chapter-3/","tags":null,"title":"Chapter 3: Data Engineering Fundamentals"},{"categories":null,"content":"Now, we address the main event: what happens after tokenization? How does a model like the Transformer take those initial embeddings and generate coherent, often brilliant, text? That\u0026rsquo;s our mission for today.\nAs the book mentions, we\u0026rsquo;ll start by loading our model, just to have it ready. This is the same microsoft/Phi-3-mini-4k-instruct model we\u0026rsquo;ve seen before. It\u0026rsquo;s a powerful yet manageable model that\u0026rsquo;s perfect for our hands-on exploration.\nimport torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # Load model and tokenizer tokenizer = AutoTokenizer.from_pretrained(\u0026#34;microsoft/Phi-3-mini-4k-instruct\u0026#34;) model = AutoModelForCausalLM.from_pretrained( \u0026#34;microsoft/Phi-3-mini-4k-instruct\u0026#34;, device_map=\u0026#34;cuda\u0026#34;, torch_dtype=\u0026#34;auto\u0026#34;, trust_remote_code=True, ) # Create a pipeline generator = pipeline( \u0026#34;text-generation\u0026#34;, model=model, tokenizer=tokenizer, return_full_text=False, max_new_tokens=50, do_sample=False, ) With our tools at the ready, let\u0026rsquo;s start with a high-level view and peel back the layers one by one.\nAn Overview of Transformer Models The Inputs and Outputs of a Trained Transformer LLM At the highest level of abstraction, a generative LLM is a system that takes a text prompt as input and produces a text generation as output. Think of it as a highly sophisticated text completion machine. Figure 3-1 shows this perfectly: you provide a prompt, and the Transformer LLM generates a relevant completion.\nHowever, a crucial point often missed is that this generation is not instantaneous. The model doesn\u0026rsquo;t produce the entire email in one go. As Figure 3-2 shows below, the process is sequential.\nThe model generates one token at a time. For the prompt about the gardening mishap, it first generates \u0026ldquo;Dear\u0026rdquo;, then \u0026ldquo;Sarah\u0026rdquo;, then a comma, then a newline character (\\n), and so on.\nThis brings us to one of the most important concepts for generative models.\nAutoregressive Generation Look at Figure 3-3 below. It shows that after the model generates the first token (\u0026ldquo;Dear\u0026rdquo;), that token is appended to the original prompt.\nThis new, slightly longer sequence (\u0026ldquo;Write an email\u0026hellip; Dear\u0026rdquo;) becomes the input for the next step. The model then processes this new input to generate the next token (\u0026ldquo;Sarah\u0026rdquo;).\nThis process, where a model\u0026rsquo;s own previous outputs are fed back in as inputs for subsequent steps, is called autoregression. This is the core mechanism of generative LLMs. They are, fundamentally, autoregressive models. This is what we\u0026rsquo;re seeing when we run the generator pipeline from the book:\nprompt = \u0026#34;Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\u0026#34; output = generator(prompt) print(output[0][\u0026#39;generated_text\u0026#39;]) The output begins, token by token, until it hits the max_new_tokens limit we set.\nSubject: My Sincere Apologies for the Gardening Mishap...\nThe software wrapper around the neural network (in this case, the pipeline) handles this loop for us, but it\u0026rsquo;s essential to know it\u0026rsquo;s happening.\nThe Components of the Forward Pass So, what\u0026rsquo;s inside this \u0026ldquo;Transformer LLM\u0026rdquo; box that allows for this token-by-token generation? As we can see in Figure 3-4 below, there are three main components working in concert during a single \u0026ldquo;forward pass\u0026rdquo; (the process of taking an input and producing an output):\nTokenizer: We know this from Chapter 2. It converts the input text into a sequence of token IDs. Stack of Transformer Blocks: This is the computational core of the model. It\u0026rsquo;s a deep stack of identical layers that process the token embeddings. This is where the \u0026ldquo;magic\u0026rdquo; of understanding context happens. Language Modeling (LM) Head: This is the final layer. It takes the processed information from the Transformer blocks and converts it into a usable output: a probability score for every single token in the vocabulary. Figure 3-5 beautifully illustrates the connection we established in Chapter 2. The tokenizer has a vocabulary (a lookup table of token IDs to token strings), and the model has a corresponding token embeddings matrix. For every token ID from the tokenizer, we fetch its embedding vector to feed into the model.\nThe computation flows from top to bottom. The input token IDs are converted to embeddings, which then pass through the entire stack of Transformer blocks. Finally, as shown in Figure 3-6 below, the LM head takes the final processed vector and outputs a probability distribution over the entire vocabulary. In this example, \u0026ldquo;Dear\u0026rdquo; has a 40% probability of being the next token, \u0026ldquo;Title\u0026rdquo; has 13%, and so on.\nLet\u0026rsquo;s look at the model structure itself, as the book does on page 8. When we print the model variable, we see the architecture laid out for us.\nPhi3ForCausalLM( (model): Phi3Model( (embed_tokens): Embedding(32064, 3072, padding_idx=32000) (layers): ModuleList( (0-31): 32 x Phi3DecoderLayer(...) ) ... ) (lm_head): Linear(in_features=3072, out_features=32064, bias=False) ) Let\u0026rsquo;s break this down like a senior scientist:\n(embed_tokens): This is our embedding matrix. It has 32,064 rows, one for each token in the vocabulary. Each token is represented by a vector of 3,072 dimensions. This is the d_model or model dimension. (layers): This is our stack of Transformer blocks. We see it\u0026rsquo;s a ModuleList containing 32 identical Phi3DecoderLayer blocks. (lm_head): This is our language modeling head. It\u0026rsquo;s a simple Linear layer. Notice its dimensions: it takes an input vector of size in_features=3072 (the output from the last Transformer block) and projects it to an output vector of size out_features=32064 (one score for each token in the vocabulary). This output vector is what we call the logits. Choosing a Single Token from the Probability Distribution (Sampling/Decoding) The LM head gives us logits, which can be converted to probabilities (usually via a softmax function). Now, how do we pick just one token from this distribution? This is the decoding strategy.\nGreedy Decoding: The simplest method, as shown in Figure 3-7 below, is to always pick the token with the highest probability. This is fast and deterministic, but often leads to repetitive and boring text. This is what happens when we set do_sample=False or temperature=0. Sampling: A better approach is to sample from the distribution. A token with a 40% probability gets picked 40% of the time. This introduces randomness and creativity. We\u0026rsquo;ll explore this more in Chapter 6. Let\u0026rsquo;s trace this with the book\u0026rsquo;s code example. We can get the raw output (logits) from the LM head.\nprompt = \u0026#34;The capital of France is\u0026#34; # Tokenize the input prompt input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(\u0026#34;cuda\u0026#34;) # Get the output of the model before the lm_head # Note: model.model gets the raw Transformer block outputs model_output = model.model(input_ids) # Get the output of the lm_head (the logits) lm_head_output = model.lm_head(model_output[0]) The lm_head_output tensor has a shape of [1, 6, 32064]. This means:\n1: A batch size of 1 (one prompt). 6: Sequence length of 6 tokens (\u0026ldquo;The\u0026rdquo;, \u0026ldquo;capital\u0026rdquo;, \u0026ldquo;of\u0026rdquo;, \u0026ldquo;France\u0026rdquo;, \u0026ldquo;is\u0026rdquo;, and a special start token). 32064: The logit score for each token in the vocabulary. We only care about predicting the next token, so we look at the logits for the last input token (is). We can get this with lm_head_output[0, -1]. Then, we find the index (token ID) with the highest score using .argmax().\n# Get the token ID with the highest score from the last position\u0026#39;s logits token_id = lm_head_output[0, -1].argmax(-1) # Decode the ID back to text tokenizer.decode(token_id) And the output is, unsurprisingly, Paris.\nParallel Token Processing and Context Size A key reason Transformers superseded older architectures like RNNs is their ability to process tokens in parallel. As Figure 3-8 intuits below, when you input a prompt, the model creates a separate processing \u0026ldquo;stream\u0026rdquo; or \u0026ldquo;track\u0026rdquo; for each token simultaneously.\nThere\u0026rsquo;s a limit to how many tokens can be processed at once, known as the context length or context window. Our Phi-3-mini-4k-instruct model has a 4K (4096) context length.\nAs Figure 3-9 shows below, each of these parallel streams takes an embedding vector as input and, after passing through all the Transformer blocks, produces a final output vector. Crucially, for text generation, we only use the output vector of the very last token to feed into the LM head.\n\u0026ldquo;Wait,\u0026rdquo; you might ask, \u0026ldquo;if we only use the last output, why do we bother computing all the previous ones?\u0026rdquo; That\u0026rsquo;s an excellent question. The answer lies in the attention mechanism. The computation of the final stream depends on the intermediate calculations from all the previous streams. They talk to each other inside each Transformer block.\nSpeeding Up Generation by Caching Keys and Values Remember our autoregressive loop? Generate a token, append it, re-process everything. This is incredibly inefficient! When generating the 100th token, we would re-calculate the streams for the first 99 tokens from scratch.\nThis is where the Key-Value (KV) Cache comes in. It\u0026rsquo;s a critical optimization. Inside the attention mechanism (which we\u0026rsquo;ll dissect next), certain intermediate results called \u0026ldquo;Keys\u0026rdquo; and \u0026ldquo;Values\u0026rdquo; are calculated for each token. Instead of re-calculating them every time, we can cache them.\nAs Figure 3-10 visualizes below, when generating the second token, we don\u0026rsquo;t re-run the calculation for the first. We retrieve its cached Keys and Values and only perform the full computation for the new token.\nThe book\u0026rsquo;s %%timeit example is a fantastic demonstration of this. Generating 100 tokens:\nWith KV Cache (use_cache=True): 4.5 seconds Without KV Cache (use_cache=False): 21.8 seconds A dramatic, nearly 5x speedup! This is why KV caching is enabled by default and is essential for making LLM inference practical.\nInside the Transformer Block Now, let\u0026rsquo;s zoom in further. Figure 3-11 shows the processing flow: an embedding goes into Block 1, the output of Block 1 goes into Block 2, and so on.\nAccording to Figure 3-12 below, each Transformer block is composed of two main sub-layers:\nThe Attention Layer: Its job is to incorporate context. It allows a token to \u0026ldquo;look at\u0026rdquo; other tokens in the sequence and pull in relevant information. The Feedforward Layer (FFN): This is a standard neural network that acts as the model\u0026rsquo;s \u0026ldquo;thinking\u0026rdquo; or processing workhorse. This is where a lot of the learned knowledge is stored and applied. The \u0026ldquo;Shawshank Redemption\u0026rdquo; example in Figure 3-13 gives a good intuition for the FFN\u0026rsquo;s role. The knowledge that \u0026ldquo;Redemption\u0026rdquo; follows \u0026ldquo;The Shawshank\u0026rdquo; is likely stored within the weights of these FFNs across the model\u0026rsquo;s layers.\nThe attention layer\u0026rsquo;s role is all about context. In the sentence \u0026ldquo;The dog chased the squirrel because it was fast,\u0026rdquo; the attention mechanism helps the model figure out that \u0026ldquo;it\u0026rdquo; refers to the \u0026ldquo;squirrel,\u0026rdquo; not the \u0026ldquo;dog.\u0026rdquo; It does this by enriching the vector representation of \u0026ldquo;it\u0026rdquo; with information from the \u0026ldquo;squirrel\u0026rdquo; vector, as conceptualized in Figure 3-14.\nAttention is All You Need: The Deep Dive Let\u0026rsquo;s break down the attention mechanism inside a single attention head, following Figures 3-15 and 3-16. There are two main steps:\nRelevance Scoring: For the current token we\u0026rsquo;re processing, calculate a score for every other token in the context that tells us how relevant it is. Combining Information: Create a new vector for the current token by taking a weighted sum of all the other token vectors, using the relevance scores as weights. To do this, we use the famous Query, Key, Value (QKV) model. Imagine this:\nYou are the Query: the current token, asking for information. Every other token is a Key: a label for the information it holds. Every other token also has a Value: the actual information it holds. The process, shown in Figures 3-18 through 3-21, is:\nProjection: The input vectors are multiplied by three separate weight matrices (learned during training) to create the Q, K, and V vectors for each token (see Figure 3-19). Scoring: You (the Query) compare yourself to every other token\u0026rsquo;s Key. This is done with a dot product: Q · K^T. The result is a set of raw scores. After a softmax function, these become the attention scores (our relevance scores), summing to 1 (see Figure 3-20). Combining: These scores are then used to create a weighted sum of all the Value vectors. A token with a high score contributes more of its Value to the final output. The result is the new, context-enriched vector for the current position (see Figure 3-21). Let me draw this core QKV logic with a diagram to make it even clearer.\ngraph TD subgraph Input Vectors for each token direction LR I1(Token 1 Vector) I2(Token 2 Vector) I3(Token 3 Vector) end subgraph Projection Matrices direction LR W_Q(Query Matrix Wq) W_K(Key Matrix Wk) W_V(Value Matrix Wv) end subgraph \u0026#34;Step 1: Create Q, K, V vectors\u0026#34; direction LR Q1(Q1) K1(K1) V1(V1) Q2(Q2) K2(K2) V2(V2) Q3(Q3) K3(K3) V3(V3) end I1 -- x Wq --\u0026gt; Q1 I1 -- x Wk --\u0026gt; K1 I1 -- x Wv --\u0026gt; V1 I2 -- x Wq --\u0026gt; Q2 I2 -- x Wk --\u0026gt; K2 I2 -- x Wv --\u0026gt; V2 I3 -- x Wq --\u0026gt; Q3 I3 -- x Wk --\u0026gt; K3 I3 -- x Wv --\u0026gt; V3 subgraph \u0026#34;Step 2: Score (e.g., for Token 3)\u0026#34; S1(Score_3_1 = Q3 • K1) S2(Score_3_2 = Q3 • K2) S3(Score_3_3 = Q3 • K3) SM(Softmax) S1 \u0026amp; S2 \u0026amp; S3 --\u0026gt; SM end subgraph \u0026#34;Step 3: Combine based on Scores\u0026#34; O3(Output Vector for Token 3) end SM -- Weights --\u0026gt; O3 V1 \u0026amp; V2 \u0026amp; V3 -- Vectors to be weighted --\u0026gt; O3 linkStyle 11 stroke:#ff0000,stroke-width:2px; linkStyle 12 stroke:#0000ff,stroke-width:2px; linkStyle 13 stroke:#00ff00,stroke-width:2px; This entire QKV calculation is done in parallel inside multiple attention heads, allowing the model to focus on different kinds of relationships (e.g., one head for syntax, another for semantics) simultaneously.\nRecent Improvements to the Transformer Architecture The original Transformer was from 2017. The field has evolved. Let\u0026rsquo;s cover the modern enhancements mentioned in the book.\nMore Efficient Attention As models got bigger, the N^2 complexity of full attention became a bottleneck.\nLocal/Sparse Attention: As shown in Figure 3-23, instead of attending to all previous tokens, a token might only attend to a small, local window or a \u0026ldquo;strided\u0026rdquo; pattern. This is much faster but can lose global context. GPT-3 cleverly alternates between full and sparse attention layers. Multi-Query and Grouped-Query Attention (MQA/GQA): Full multi-head attention (MHA) has separate K and V matrices for every head. This uses a lot of memory for the KV Cache. MQA: A major optimization where all heads share a single Key and Value matrix (see Figure 3-27). This drastically reduces the KV cache size but can hurt model quality. GQA: A happy medium. Heads are divided into groups, and heads within a group share K and V matrices (see Figure 3-28). This is used by models like Llama 2 and Phi-3, offering a great balance of speed and quality. Here\u0026rsquo;s a diagram illustrating the evolution from MHA to GQA.\ngraph TD subgraph \u0026#34;Multi-Head Attention (MHA)\u0026#34; H1_MHA(\u0026#34;Head 1 \u0026lt;br/\u0026gt; Q1, K1, V1\u0026#34;) H2_MHA(\u0026#34;Head 2 \u0026lt;br/\u0026gt; Q2, K2, V2\u0026#34;) H3_MHA(\u0026#34;Head 3 \u0026lt;br/\u0026gt; Q3, K3, V3\u0026#34;) H4_MHA(\u0026#34;Head 4 \u0026lt;br/\u0026gt; Q4, K4, V4\u0026#34;) end subgraph \u0026#34;Multi-Query Attention (MQA)\u0026#34; H1_MQA(\u0026#34;Head 1 \u0026lt;br/\u0026gt; Q1\u0026#34;) --\u0026gt; KV_MQA(\u0026#34;Shared K, V\u0026#34;) H2_MQA(\u0026#34;Head 2 \u0026lt;br/\u0026gt; Q2\u0026#34;) --\u0026gt; KV_MQA H3_MQA(\u0026#34;Head 3 \u0026lt;br/\u0026gt; Q3\u0026#34;) --\u0026gt; KV_MQA H4_MQA(\u0026#34;Head 4 \u0026lt;br/\u0026gt; Q4\u0026#34;) --\u0026gt; KV_MQA end subgraph \u0026#34;Grouped-Query Attention (GQA)\u0026#34; subgraph \u0026#34;Group 1\u0026#34; H1_GQA(\u0026#34;Head 1 \u0026lt;br/\u0026gt; Q1\u0026#34;) --\u0026gt; KV1_GQA(\u0026#34;Shared K1, V1\u0026#34;) H2_GQA(\u0026#34;Head 2 \u0026lt;br/\u0026gt; Q2\u0026#34;) --\u0026gt; KV1_GQA end subgraph \u0026#34;Group 2\u0026#34; H3_GQA(\u0026#34;Head 3 \u0026lt;br/\u0026gt; Q3\u0026#34;) --\u0026gt; KV2_GQA(\u0026#34;Shared K2, V2\u0026#34;) H4_GQA(\u0026#34;Head 4 \u0026lt;br/\u0026gt; Q4\u0026#34;) --\u0026gt; KV2_GQA end end H1_MHA -.-\u0026gt; H1_MQA H1_MQA -.-\u0026gt; H1_GQA FlashAttention: Not an algorithmic change, but a brilliant re-implementation. It optimizes the way Q, K, and V matrices are moved between the GPU\u0026rsquo;s fast SRAM and slower HBM, avoiding bottlenecks and providing massive speedups for both training and inference. The Modern Transformer Block Figure 3-29 shows the original block, which used \u0026ldquo;post-normalization\u0026rdquo; (Add \u0026amp; Norm after the sub-layer). Figure 3-30 shows a modern block, like in Llama or Phi-3, with key differences:\nPre-Normalization: The normalization layer (like RMSNorm) is applied before the attention and FFN layers. This leads to more stable training. RMSNorm: A simpler, faster alternative to the original LayerNorm. SwiGLU: A more advanced activation function for the FFN, replacing the original ReLU. Rotary Positional Embeddings (RoPE). Positional Embeddings (RoPE) Transformers are inherently \u0026ldquo;bags of vectors\u0026rdquo;; they don\u0026rsquo;t know the order of tokens. We need to inject positional information. The old way was to add a positional embedding to the token embedding at the very beginning. But this is inflexible, especially when \u0026ldquo;packing\u0026rdquo; multiple documents into one context, as shown in Figure 3-31.\nRoPE is a clever solution. Instead of adding a vector, it rotates the Query and Key vectors based on their absolute position. This is done inside the attention layer, just before the Q · K multiplication (as shown in Figures 3-32 and 3-33). This method has proven to be incredibly effective, as it elegantly encodes both absolute and relative position information.\nSummary And that brings us to the end of a very dense but crucial chapter. We have peered deep inside the Transformer.\nWe\u0026rsquo;ve seen that generation is an autoregressive, token-by-token loop. We dissected the forward pass into its three main components: the tokenizer, the stack of Transformer blocks, and the LM head. We learned that this process is parallelized across tokens and that the KV cache is a vital optimization to speed it up.\nMost importantly, we opened up the Transformer block itself and demystified its two workhorses: the feedforward network for storing knowledge and the attention mechanism for incorporating context. We went deep into the QKV model of attention and looked at modern improvements like GQA and FlashAttention. Finally, we touched on architectural tweaks like pre-normalization and the powerful Rotary Positional Embeddings (RoPE).\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-3/","tags":null,"title":"Chapter 3: Looking Inside Large Language Models"},{"categories":null,"content":"Chapter 4 Notes: ML Monitoring and Maintenance Overview This chapter covers the critical aspects of monitoring ML systems in production and maintaining their performance over time. It discusses various types of monitoring, alerting strategies, and approaches to handle model drift and performance degradation.\nKey Concepts Model Performance Monitoring: Tracking accuracy, precision, recall, and other metrics Data Drift Detection: Identifying when input data distribution changes Model Drift: Detecting when model performance degrades over time Infrastructure Monitoring: System health, resource utilization, and availability Main Topics Covered Types of ML monitoring (performance, data, infrastructure) Alerting and notification systems Model retraining strategies Feedback loops and continuous learning Debugging ML systems in production Common Monitoring Metrics Business Metrics: ROI, conversion rates, user engagement Model Metrics: Accuracy, precision, recall, F1-score System Metrics: Latency, throughput, error rates Data Quality Metrics: Missing values, distribution shifts (Your detailed notes for Chapter 4 go here\u0026hellip;)\n","permalink":"https://deepskandpal.github.io/bookshelf/ml-design-interview/chapter-4/","tags":null,"title":"Chapter 4: ML Monitoring and Maintenance"},{"categories":null,"content":"This chapter dives into the system design of ChatGPT-like conversational AI systems, covering large language model deployment and optimization.\nKey Concepts Large Language Model Serving: Challenges of deploying 175B+ parameter models Conversational Memory: Maintaining context across multi-turn conversations Safety and Alignment: Ensuring responsible AI behavior Main Topics Covered ChatGPT system architecture Model serving infrastructure (distributed inference) Conversation management and memory Safety systems and content moderation Fine-tuning and reinforcement learning from human feedback (RLHF) System Design Considerations Handling millions of concurrent users Model optimization techniques (quantization, caching) Context window management and conversation history Real-time safety filtering and response generation (Your detailed notes for Chapter 4 go here\u0026hellip;)\n","permalink":"https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/chapter-4/","tags":null,"title":"Chapter 4: ChatGPT: Personal Assistant Chatbot"},{"categories":null,"content":"What this chapter is ultimately trying to achieve To introduce and explain the core mechanisms of the Transformer architecture, specifically focusing on the decoder-only variant, which is prevalent in autoregressive language models like GPT. The key is to understand how it processes input sequences, attends to relevant information, and incorporates positional context, all without the sequential recurrence of RNNs.\nLet\u0026rsquo;s break down the key concepts:\nCore Idea: Parallel Processing with Attention Unlike RNNs that process tokens one by one, Transformers can process all tokens in an input sequence simultaneously (or at least in parallel up to the context window length). The \u0026ldquo;magic\u0026rdquo; that allows them to understand relationships between tokens in this parallel setup is the self-attention mechanism.\n4.1 Decoder Block (The Building Block) What it\u0026rsquo;s ultimately trying to achieve: A Transformer model is typically a stack of identical \u0026ldquo;decoder blocks\u0026rdquo; (or encoder blocks, or both, depending on the variant). Each block takes a sequence of token representations as input and outputs a refined sequence of representations of the same length. Decoder-Only Focus: For autoregressive language modeling (predicting the next token), we typically use a stack of decoder blocks. The input is the sequence of tokens seen so far, and the output of the final block is used to predict the next token. Structure of a Decoder Block: Each block has two main sub-layers: Masked Multi-Head Self-Attention: Allows each token to \u0026ldquo;look at\u0026rdquo; other tokens in the sequence (including itself) to gather contextual information. The \u0026ldquo;masked\u0026rdquo; part is crucial for autoregressive models to prevent a token from seeing future tokens during training. Position-Wise Feedforward Network (MLP): A standard fully connected feedforward network applied independently to each token\u0026rsquo;s representation after the attention step. Additional Components within a block (crucial for making it work): Residual Connections (Skip Connections): The input to each sub-layer is added to its output. This helps with gradient flow and enables training much deeper networks. Layer Normalization (RMSNorm in the book): Applied before each sub-layer to stabilize the activations and improve training. 4.2 Self-Attention (The Heart of the Transformer) What it\u0026rsquo;s ultimately trying to achieve: For each token in the input sequence, self-attention calculates a new representation by taking a weighted sum of the representations of all tokens in the sequence (respecting the causal mask for decoders). The weights determine how much \u0026ldquo;attention\u0026rdquo; each token should pay to every other token (including itself) when computing its updated representation.\nThe QKV (Query, Key, Value) Analogy: Imagine you\u0026rsquo;re looking up information in a library (this is a common analogy):\nQuery (Q): For the current token you\u0026rsquo;re trying to update, you formulate a \u0026ldquo;query\u0026rdquo; representing what kind of information it\u0026rsquo;s looking for. Key (K): Every token in the sequence (including the current one) has a \u0026ldquo;key\u0026rdquo; that acts like an index tag, describing what kind of information it holds. Value (V): Every token also has a \u0026ldquo;value,\u0026rdquo; which is the actual content or representation of that token. The process: Project Inputs: The input embedding for each token x_t is projected into three different vectors: q_t (query), k_t (key), and v_t (value) using learnable weight matrices (WQ, WK, WV). Calculate Attention Scores: For a given query q_t, its \u0026ldquo;compatibility\u0026rdquo; or \u0026ldquo;similarity\u0026rdquo; with every key k_j in the sequence is calculated, usually via a dot product: score(q_t, k_j) = q_t ⋅ k_j. Scale Scores: The scores are scaled down (typically by the square root of the key vector\u0026rsquo;s dimension, sqrt(d_k)) to prevent very large values, which can lead to vanishing gradients in the softmax. Apply Causal Mask (for Decoders): To ensure autoregression (a token can\u0026rsquo;t see future tokens), scores corresponding to attention to future positions are set to negative infinity before the softmax. This makes their softmax probability zero. Convert Scores to Weights (Softmax): The masked, scaled scores are passed through a softmax function. This converts the scores into positive attention weights that sum to 1 across all tokens in the sequence. These weights indicate how much q_t should attend to each v_j. Compute Output: The output representation for token t (g_t in the book) is a weighted sum of all value vectors v_j, where the weights are the attention weights computed in the previous step: g_t = sum_j (attention_weight_tj * v_j). Multi-Head Attention:\nIntuition: Instead of having one set of Q, K, V matrices, self-attention is often performed multiple times in parallel, each with different learned Q, K, V projection matrices. These are called \u0026ldquo;heads.\u0026rdquo; Benefit: Each head can learn to focus on different types of relationships or different aspects of the input sequence (e.g., one head might focus on syntactic relationships, another on semantic ones over a longer distance). Process: The input is split (or projected) into multiple smaller-dimensional Q, K, V sets. Attention is computed independently for each head. The outputs of all heads are then concatenated and linearly projected back to the original embedding dimension. 4.3 Position-Wise Multilayer Perceptron (MLP) What it\u0026rsquo;s ultimately trying to achieve: After the self-attention mechanism has aggregated contextual information into each token\u0026rsquo;s representation, the MLP provides further non-linear processing. How it works: It\u0026rsquo;s a standard two-layer feedforward network (e.g., Linear -\u0026gt; ReLU -\u0026gt; Linear). Crucially, the same MLP (with the same weights) is applied independently to the representation of each token in the sequence. The book describes it as z_t = W2(ReLU(W1 * g_t + b1)) + b2. Often, the intermediate layer in the MLP is larger than the input/output dimension (e.g., 4x the embedding dimension), allowing the model to learn more complex transformations before projecting back down. 4.4 Rotary Position Embedding (RoPE) What it\u0026rsquo;s ultimately trying to achieve: To inject information about the absolute and relative positions of tokens into the model, since the self-attention mechanism itself is permutation-invariant (shuffling input tokens doesn\u0026rsquo;t change the raw attention scores if there\u0026rsquo;s no positional info). RNNs get position info for free due to sequential processing. Transformers need an explicit way.\nThe Core Idea: Relative Position from Absolute Rotation RoPE is an elegant solution that encodes the absolute position of a token in a way that allows the attention mechanism to easily deduce the relative positions between tokens. Imagine you have a spinning compass needle for each word\u0026rsquo;s query (Q) and key (K) vectors.\nAbsolute Position as Spin: For a word at position m, its Q and K compass needles are spun by an amount proportional to m. A word at position 1 spins a little, a word at position 2 spins twice as much, and so on. The final orientation of a needle tells you the word\u0026rsquo;s absolute position.\nAttention as Needle Alignment: To calculate attention between two words, the model compares the orientation of the first word\u0026rsquo;s Q-needle to the second word\u0026rsquo;s K-needle. The \u0026ldquo;comparison\u0026rdquo; measures how well the needles are aligned.\nThe Magic - Relative Distance Emerges: Due to the mathematical properties of rotation (using sine and cosine), the alignment between two needles—one spun by m units and the other by n units—depends only on the difference in their spin, m - n. The absolute positions don\u0026rsquo;t matter for the comparison, only the relative distance.\nThis is powerful because the model learns relational patterns, like \u0026ldquo;the word 2 positions after a verb is often an object,\u0026rdquo; regardless of where that verb appears in the sentence.\nKey Advantages of RoPE:\nRelative from Absolute: It encodes absolute positions, but the self-attention score naturally becomes sensitive to relative distance, which is more intuitive for language. Multi-Scale Information: It uses different rotation speeds for different parts of the vector, allowing it to capture both local (fast-rotating) and global (slow-rotating) positional context. Excellent Extrapolation: It generalizes well to sequence lengths longer than those seen during training—a significant advantage for long documents. No Extra Parameters: RoPE modifies existing Q and K vectors with fixed rotations, avoiding extra learnable parameters for positional information. 4.5 Residual Connections \u0026amp; Layer Normalization (Revisited) Residual Connections: As in Chapter 1\u0026rsquo;s general NN discussion and as seen in the block diagram, the input to a sub-layer (e.g., self-attention or MLP) is added to its output: x_output = SubLayer(x_input) + x_input. This is critical for training deep stacks of decoder blocks by preventing vanishing gradients. Layer Normalization (RMSNorm): Applied before each sub-layer (self-attention and MLP) and after the residual addition. RMSNorm(x) = (x / sqrt(mean(x^2) + epsilon)) * gamma (where gamma is a learnable scaling parameter and epsilon is for numerical stability). It normalizes the features for each token independently across its embedding dimension. This helps stabilize training, making it less sensitive to the scale of parameters and activations, and allows for faster convergence. 4.6 Key-Value Caching (For Inference) What it\u0026rsquo;s ultimately trying to achieve: To speed up text generation (inference) which is autoregressive (one token at a time). The Problem: During training, we can compute attention over the whole sequence in parallel. But during inference, when generating token t+1, we\u0026rsquo;ve already computed the Key (K) and Value (V) matrices for tokens 1...t. Without caching, we\u0026rsquo;d recompute these K and V vectors for all previous tokens every time we generate a new token. The Solution: Cache (store) the K and V vectors for all previously generated tokens in each layer. When generating the next token: Only compute the Q, K, V vectors for the newly generated token. Append the new K and V vectors to their respective cached K and V matrices. The new Q vector attends to all the K vectors in the updated cache. This significantly reduces computation because the K and V projections for past tokens don\u0026rsquo;t change. 4.9 Transformer in Python (Implementation) Putting it all together in PyTorch: AttentionHead class: Implements a single attention head, including QKV projections, RoPE, scaled dot-product attention, and masking. MultiHeadAttention class: Contains multiple AttentionHead instances, concatenates their outputs, and applies a final linear projection. MLP class: The position-wise feedforward network. RMSNorm class. DecoderBlock class: Combines MultiHeadAttention, MLP, RMSNorm, and residual connections. DecoderLanguageModel class: Stacks multiple DecoderBlocks, includes an embedding layer at the input, and a final linear layer to project outputs to vocabulary logits. The forward method also creates the causal mask. The training loop for this Transformer decoder is conceptually very similar to the RNN LM training loop: prepare input and (shifted) target sequences, pass through the model, compute cross-entropy loss, backpropagate, and update.\n","permalink":"https://deepskandpal.github.io/bookshelf/100-page-lm-book/chapter-4/","tags":null,"title":"Chapter 4: Transformer"},{"categories":null,"content":"As the book opens, it states a simple goal: classification is about assigning a label or class to some input text. This is a cornerstone of NLP. Think about it:\nIs this email spam or not? (Spam detection) Is this customer review positive, negative, or neutral? (Sentiment analysis) Is this news article about sports, politics, or technology? (Topic classification) Figure 4-1 gives us the high-level picture: text goes in, the model does its thing, and a category comes out.\nThe real meat of this chapter, and what makes it so relevant today, is that we can tackle this problem with two different families of models, as shown in Figure 4-2:\nRepresentation Models (Encoder-style, like BERT): These models are designed to understand text and output numerical representations. For classification, they typically output a final class ID (e.g., 1 for \u0026ldquo;Returns\u0026rdquo;). Generative Models (Decoder-style, like GPT, or Encoder-Decoder like T5): These models are designed to generate text. To get a class out of them, we have to cleverly prompt them to output the text of the label (e.g., the string \u0026ldquo;Returns\u0026rdquo;). We will explore both paths. But first, we need a problem to solve and data to solve it with.\nThe Sentiment of Movie Reviews The book wisely chooses a classic and intuitive dataset: rotten_tomatoes. The task is simple: determine if a movie review is positive or negative.\nLet\u0026rsquo;s load the data using the datasets library from Hugging Face. This is a standard workflow you\u0026rsquo;ll use constantly.\nfrom datasets import load_dataset # Load our data data = load_dataset(\u0026#34;rotten_tomatoes\u0026#34;) When we inspect data, we get the following structure:\nDatasetDict({ train: Dataset({ features: [\u0026#39;text\u0026#39;, \u0026#39;label\u0026#39;], num_rows: 8530 }) validation: Dataset({ features: [\u0026#39;text\u0026#39;, \u0026#39;label\u0026#39;], num_rows: 1066 }) test: Dataset({ features: [\u0026#39;text\u0026#39;, \u0026#39;label\u0026#39;], num_rows: 1066 }) }) This is great. The data is already split for us into train, validation, and test sets. As good practitioners, we\u0026rsquo;ll use the train set for any training, and the test set for our final evaluation. The validation set is there if we need to do hyperparameter tuning, to avoid \u0026ldquo;peeking\u0026rdquo; at the final test set.\nLet\u0026rsquo;s look at a couple of examples. The label 1 is positive, and 0 is negative.\n# data[\u0026#34;train\u0026#34;][0, -1] {\u0026#39;text\u0026#39;: [\u0026#34;the rock is destined to be the 21st century\u0026#39;s new conan...\u0026#34;, \u0026#39;things really get weird , though not particularly scary...\u0026#39;], \u0026#39;label\u0026#39;: [1, 0]} The task is clear. Now, let\u0026rsquo;s see how our first family of models tackles it.\nText Classification with Representation Models As we discussed in previous chapters, representation models like BERT are phenomenal at understanding language. They are typically pre-trained on a massive corpus of text (like Wikipedia) to learn the nuances of language. Then, they are fine-tuned for specific tasks.\nFigure 4-3 shows this beautifully. We start with a base foundation model (like BERT). We can then fine-tune it for a specific task like classification, creating a task-specific model. Alternatively, we can fine-tune it to produce high-quality general-purpose embeddings.\nIn this chapter, we\u0026rsquo;re not doing the fine-tuning ourselves (we\u0026rsquo;ll save that for Chapters 10 and 11). Instead, we\u0026rsquo;ll use models that others have already fine-tuned for us. Figure 4-4 shows our two strategies:\nDirectly use a task-specific model that outputs a classification. Use an embedding model to convert text to vectors, and then train a simple, classical classifier (like logistic regression) on those vectors. Model Selection The Hugging Face Hub is vast. Figure 4-5 gives a glimpse into the timeline of just the BERT-like models. How do we choose? The book gives some solid baselines like bert-base-model or roberta-base-model.\nFor our first approach, we\u0026rsquo;ll use a task-specific model: cardiffnlp/twitter-roberta-base-sentiment-latest. This is a RoBERTa model that was fine-tuned for sentiment on Tweets. It\u0026rsquo;s an interesting choice because we\u0026rsquo;re testing how well it generalizes from the short, informal language of Twitter to our movie review dataset.\nUsing a Task-Specific Model We can use the pipeline abstraction from transformers, which makes inference incredibly simple.\nfrom transformers import pipeline # Path to our HF model model_path = \u0026#34;cardiffnlp/twitter-roberta-base-sentiment-latest\u0026#34; # Load model into pipeline pipe = pipeline( \u0026#34;text-classification\u0026#34;, # We specify the task model=model_path, tokenizer=model_path, # Good practice to be explicit return_all_scores=True, device=\u0026#34;cuda:0\u0026#34; ) As Figure 4-6 illustrates, when we pass text to this pipeline, it first gets tokenized, then processed by the model, which outputs the classification. The power of subword tokenization, as shown in Figure 4-7, means the model can handle words it\u0026rsquo;s never seen before by breaking them down into known sub-parts.\nNow, let\u0026rsquo;s run this pipeline on our entire test set and evaluate it.\nimport numpy as np from tqdm import tqdm from transformers.pipelines.pt_utils import KeyDataset # Run inference y_pred = [] # The model outputs scores for \u0026#39;Negative\u0026#39;, \u0026#39;Neutral\u0026#39;, \u0026#39;Positive\u0026#39; # We are interested in Negative (index 0) vs Positive (index 2) for output in tqdm(pipe(KeyDataset(data[\u0026#34;test\u0026#34;], \u0026#34;text\u0026#34;)), total=len(data[\u0026#34;test\u0026#34;])): negative_score = output[0][\u0026#34;score\u0026#34;] positive_score = output[2][\u0026#34;score\u0026#34;] assignment = np.argmax([negative_score, positive_score]) # 0 for negative, 1 for positive y_pred.append(assignment) Now we have our predictions (y_pred) and the true labels (data[\u0026quot;test\u0026quot;][\u0026quot;label\u0026quot;]). Let\u0026rsquo;s evaluate. The book provides a handy function.\nfrom sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): \u0026#34;\u0026#34;\u0026#34;Create and print the classification report\u0026#34;\u0026#34;\u0026#34; performance = classification_report( y_true, y_pred, target_names=[\u0026#34;Negative Review\u0026#34;, \u0026#34;Positive Review\u0026#34;] ) print(performance) evaluate_performance(data[\u0026#34;test\u0026#34;][\u0026#34;label\u0026#34;], y_pred) This gives us the classification report:\nprecision recall f1-score support Negative Review 0.76 0.88 0.81 533 Positive Review 0.86 0.72 0.78 533 accuracy 0.80 1066 macro avg 0.81 0.80 0.80 1066 weighted avg 0.81 0.80 0.80 1066 To really understand this, let\u0026rsquo;s quickly break down the key metrics, which the book visualizes in Figure 4-9:\nPrecision: Of all the times we predicted \u0026ldquo;Positive\u0026rdquo;, how many were actually positive? (Here, 86%). This is about not making false positive mistakes. Recall: Of all the reviews that were actually \u0026ldquo;Positive\u0026rdquo;, how many did we find? (Here, 72%). This is about not missing any actual positives. F1-score: The harmonic mean of precision and recall. It\u0026rsquo;s a single number that balances both concerns. Weighted Avg F1-score: 0.80. This is a very respectable score, especially considering the model was trained on Twitter data, not movie reviews! This shows the power of pre-training. Classification Tasks That Leverage Embeddings What if a ready-made, task-specific model doesn\u0026rsquo;t exist for our problem? Do we have to do a full, expensive fine-tuning run? No!\nThis is where the second strategy from Figure 4-4 comes in. We can use a model that\u0026rsquo;s great at creating general-purpose embeddings. This acts as a powerful feature extractor. Then, we can train a very simple, lightweight classifier on these features. The process, shown in Figure 4-10, separates feature extraction from classification.\nFor this, we\u0026rsquo;ll use the sentence-transformers/all-mpnet-base-v2 model, a top performer on embedding benchmarks.\nStep 1: Create Embeddings (as in Figure 4-11)\nfrom sentence_transformers import SentenceTransformer # Load model model = SentenceTransformer(\u0026#34;sentence-transformers/all-mpnet-base-v2\u0026#34;) # Convert text to embeddings train_embeddings = model.encode(data[\u0026#34;train\u0026#34;][\u0026#34;text\u0026#34;], show_progress_bar=True) test_embeddings = model.encode(data[\u0026#34;test\u0026#34;][\u0026#34;text\u0026#34;], show_progress_bar=True) The shape of train_embeddings will be (8530, 768), meaning we have a 768-dimensional vector for each of our 8,530 training reviews.\nStep 2: Train a Classifier (as in Figure 4-12)\nNow we treat this like a classic ML problem. The embeddings are our features (X), and the labels are our targets (y).\nfrom sklearn.linear_model import LogisticRegression # Train a logistic regression on our train embeddings clf = LogisticRegression(random_state=42) clf.fit(train_embeddings, data[\u0026#34;train\u0026#34;][\u0026#34;label\u0026#34;]) # Predict previously unseen instances y_pred = clf.predict(test_embeddings) evaluate_performance(data[\u0026#34;test\u0026#34;][\u0026#34;label\u0026#34;], y_pred) The result:\nprecision recall f1-score support Negative Review 0.85 0.86 0.85 533 Positive Review 0.86 0.85 0.85 533 ... weighted avg 0.85 0.85 0.85 1066 An F1-score of 0.85! We actually improved our performance by using general-purpose embeddings and a simple classifier. This is a powerful and flexible technique.\nWhat If We Do Not Have Labeled Data? This is where things get really interesting. What if we don\u0026rsquo;t have any labeled data? This is called zero-shot classification. The core idea, shown in Figure 4-13, is to classify text against a set of candidate labels that the model has never been explicitly trained on.\nHow do we do this with embeddings? The book shows a brilliant trick. Instead of using abstract labels like 0 and 1, we create descriptive sentences for our labels.\n0 becomes \u0026quot;A negative movie review\u0026quot; 1 becomes \u0026quot;A positive movie review\u0026quot; Then, as Figure 4-14 illustrates, we embed both our documents and our descriptive labels into the same vector space.\n# Create embeddings for our labels label_embeddings = model.encode([\u0026#34;A negative review\u0026#34;, \u0026#34;A positive review\u0026#34;]) Now, for each document embedding, we can calculate its cosine similarity to each of the label embeddings. Cosine similarity measures the angle between two vectors, as visualized in Figure 4-15. A smaller angle (higher similarity) means they are closer in meaning. We simply assign the label with the highest similarity.\nfrom sklearn.metrics.pairwise import cosine_similarity # Find the best matching label for each document sim_matrix = cosine_similarity(test_embeddings, label_embeddings) y_pred = np.argmax(sim_matrix, axis=1) evaluate_performance(data[\u0026#34;test\u0026#34;][\u0026#34;label\u0026#34;], y_pred) The result:\nprecision recall f1-score support Negative Review 0.78 0.77 0.78 533 Positive Review 0.77 0.79 0.78 533 ... weighted avg 0.78 0.78 0.78 1066 An F1-score of 0.78! This is astonishing. Without a single labeled example, just by describing our labels, we achieved a result that\u0026rsquo;s almost as good as our fully supervised models. This is a testament to the power of high-quality embeddings.\nText Classification with Generative Models Now we switch to the other family of models. These are sequence-to-sequence models. They don\u0026rsquo;t output a class ID; they output text. To make them classify, we need to guide them with an instruction, or prompt. This iterative process of refining the instruction is called prompt engineering, shown beautifully in Figure 4-18.\nUsing the Text-to-Text Transfer Transformer (Flan-T5) First, we\u0026rsquo;ll look at an open-source generative model. T5 is an encoder-decoder model (Figure 4-19). Its big innovation was framing every NLP task as a text-to-text problem during its fine-tuning phase (Figures 4-20 and 4-21). The Flan-T5 models were further fine-tuned on many instruction-based tasks, making them great at following prompts.\nLet\u0026rsquo;s load the small version.\npipe = pipeline( \u0026#34;text2text-generation\u0026#34;, model=\u0026#34;google/flan-t5-small\u0026#34;, device=\u0026#34;cuda:0\u0026#34; ) We can\u0026rsquo;t just feed it the review. We have to create a prompt. Let\u0026rsquo;s prefix every review with a question.\n# Prepare our data prompt = \u0026#34;Is the following sentence positive or negative? \u0026#34; data = data.map(lambda example: {\u0026#34;t5\u0026#34;: prompt + example[\u0026#39;text\u0026#39;]}) Now we run inference, but we have to parse the text output.\ny_pred = [] for output in tqdm(pipe(KeyDataset(data[\u0026#34;test\u0026#34;], \u0026#34;t5\u0026#34;)), total=len(data[\u0026#34;test\u0026#34;])): text = output[0][\u0026#34;generated_text\u0026#34;] y_pred.append(0 if text.lower() == \u0026#34;negative\u0026#34; else 1) The result is an F1-score of 0.84. This is very strong and shows that prompting generative models is a highly effective classification strategy.\nChatGPT for Classification Finally, we\u0026rsquo;ll look at a closed-source, decoder-only model: ChatGPT. We access it via an API. The book explains its training process, including instruction tuning (Figure 4-22) and the crucial preference tuning (RLHF) step (Figure 4-23).\nFirst, we set up our client.\nimport openai # You need to get your own key from OpenAI client = openai.OpenAI(api_key=\u0026#34;YOUR_KEY_HERE\u0026#34;) We then write a much more detailed prompt, telling the model exactly what to do and what format to return. This is a powerful form of prompt engineering.\nprompt = \u0026#34;\u0026#34;\u0026#34;Predict whether the following document is a positive or negative movie review: [DOCUMENT] If it is positive return 1 and if it is negative return 0. Do not give any other answers.\u0026#34;\u0026#34;\u0026#34; # Example call document = \u0026#34;unpretentious , charming , quirky , original\u0026#34; # chatgpt_generation(prompt, document) # a helper function defined in the book After running this on the test set and converting the string outputs (\u0026quot;1\u0026quot; or \u0026quot;0\u0026quot;) to integers, we get the evaluation:\nprecision recall f1-score support Negative Review 0.87 0.97 0.92 533 Positive Review 0.96 0.86 0.91 533 ... weighted avg 0.92 0.91 0.91 1066 An F1-score of 0.91! A remarkable performance. However, the book makes a critical point for any data scientist: with a closed-source model, we don\u0026rsquo;t know its training data. It\u0026rsquo;s possible the rotten_tomatoes dataset was part of its training, which would make this evaluation invalid. This is a key consideration when working with proprietary models.\nSummary This chapter was our first deep dive into a practical application, and what a journey it was.\nWe saw that text classification can be tackled with two families of models.\nRepresentation models are excellent, either used directly as fine-tuned classifiers (F1=0.80) or as powerful feature extractors for classical ML models (F1=0.85). We even saw their incredible flexibility with zero-shot classification via cosine similarity (F1=0.78). Generative models are also formidable classifiers when guided by good prompt engineering. We saw strong results from an open-source encoder-decoder model like Flan-T5 (F1=0.84) and state-of-the-art performance from a closed-source model like ChatGPT (F1=0.91), with important caveats. ","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-4/","tags":null,"title":"Chapter 4: Text Classification"},{"categories":null,"content":"Okay, class, let\u0026rsquo;s settle in. We\u0026rsquo;ve journeyed through the high-level overview of ML systems, the design process, and the nitty-gritty of data engineering fundamentals. Now, we arrive at a topic that is, in many ways, the heart of supervised machine learning: Chapter 4: Training Data.\nThe author makes a critical point right at the start: ML curricula often skew heavily towards modeling – the \u0026ldquo;fun\u0026rdquo; part. But as anyone who\u0026rsquo;s worked in production ML at FAANG or elsewhere knows, \u0026ldquo;spending days wrangling with a massive amount of malformatted data that doesn’t even fit into your machine’s memory is frustrating\u0026rdquo; but essential. Bad data can sink your entire ML operation, no matter how brilliant your model.\nThis chapter shifts focus from the systems perspective of data (Chapter 3) to the data science perspective. We\u0026rsquo;re going to cover:\nSampling techniques: How to select data for training. Labeling challenges: The pains of hand labels, label multiplicity, and the beauty of natural labels. Handling lack of labels: Weak supervision, semi-supervision, transfer learning, active learning. Class imbalance: Why it\u0026rsquo;s a problem and how to tackle it. Data augmentation: Creating more data from what you have. A key terminology point: The book uses \u0026ldquo;training data\u0026rdquo; instead of \u0026ldquo;training dataset\u0026rdquo; because \u0026ldquo;dataset\u0026rdquo; implies finite and stationary. Production data is rarely either (hello, \u0026ldquo;Data Distribution Shifts\u0026rdquo; on page 237, which we\u0026rsquo;ll cover much later). Creating training data, like everything else in ML systems, is an iterative process.\nLet\u0026rsquo;s begin!\nPage 81 (Chapter Introduction): The Importance and Pain of Data\nThe intro reiterates the core message:\nData is \u0026ldquo;messy, complex, unpredictable, and potentially treacherous.\u0026rdquo; Handling it well saves time and headaches. This chapter is about techniques to obtain/create good training data. \u0026ldquo;Training data\u0026rdquo; here encompasses all data for development: train, validation, and test splits. A Crucial Word of Caution (Page 82, top): Data is full of potential biases!\nBiases can creep in during collection, sampling, or labeling. Historical data can embed human biases. ML models trained on biased data will perpetuate those biases. \u0026ldquo;Use data but don’t trust it too much!\u0026rdquo; This is a mantra every ML practitioner should live by. Always be skeptical, always question your data. Pages 82-87: Sampling – Choosing Your Data Wisely\nSampling is often overlooked in coursework but is integral to the ML lifecycle. It happens when:\nCreating training data from all possible real-world data. Creating train/validation/test splits from a given dataset. Sampling events for monitoring. Why sample?\nNecessity: You rarely have access to all possible data. Your training data is inherently a sample. Feasibility: Processing all accessible data might be too time-consuming or resource-intensive. Efficiency: Quick experiments on small subsets can validate a new model\u0026rsquo;s promise before full-scale training (footnote 1: even for large models, experimenting with dataset sizes reveals its effect). Understanding sampling helps avoid bias and improve efficiency. Two families:\nNonprobability Sampling (Pages 83-84): Selection not based on probability criteria. Often driven by convenience, leading to selection biases (footnote 2, Heckman).\nConvenience sampling: Selected based on availability. Popular because it\u0026rsquo;s easy. Example: Language models often trained on easily collected Wikipedia, Common Crawl, Reddit, not necessarily representative of all text. Example: Sentiment analysis data from IMDB/Amazon reviews. Biased towards those willing to leave reviews, not representative of all users. Example: Self-driving car data initially from sunny Phoenix/Bay Area, less from rainy Kirkland (footnote 3). Model might be great in sun, poor in rain. Snowball sampling: Future samples selected based on existing ones (e.g., scrape Twitter accounts, then accounts they follow, etc.). Judgment sampling: Experts decide what to include. Quota sampling: Select based on quotas for slices (e.g., survey: 100 responses from \u0026lt;30yo, 100 from 30-60yo, etc., regardless of actual age distribution). Usefulness: Quick way to get initial data. For reliable models, probability-based sampling is preferred. FAANG Perspective: Convenience sampling is often how initial datasets are gathered for new problem domains, but we\u0026rsquo;re acutely aware of the biases and strive to get more representative data over time. Random Sampling (Probability-Based) (Pages 84-87):\nSimple Random Sampling (Page 84): All samples in the population (footnote 4: \u0026ldquo;statistical population\u0026rdquo; = potentially infinite set of all possible samples) have equal selection probability.\nAdvantage: Easy to implement. Drawback: Rare categories might be missed. If a class is 0.01% of data, a 1% random sample will likely miss it. Model might assume rare class doesn\u0026rsquo;t exist. Stratified Sampling (Page 84):\nDivide population into groups (strata) you care about. Sample from each group separately. Example: To sample 1% from data with classes A and B, sample 1% of class A and 1% of class B. Ensures rare classes are included. Drawback: Not always possible if groups are hard to define or samples belong to multiple groups (multilabel tasks - footnote 5). FAANG Perspective: Stratified sampling is crucial for creating representative validation/test sets, especially when dealing with imbalanced classes or ensuring coverage across different user segments (e.g., regions, demographics). Weighted Sampling (Page 85): Each sample given a weight determining its selection probability.\nExample: Samples A, B, C with desired probabilities 50%, 30%, 20% get weights 0.5, 0.3, 0.2. Leverages domain expertise (e.g., more recent data is more valuable, give it higher weight). Corrects for distribution mismatch (e.g., your data is 25% red, 75% blue; real world is 50/50 red/blue. Give red samples 3x weight of blue during sampling). Python: random.choices(population, weights, k). Related concept: Sample Weights (in training, not sampling to create dataset): Assigns \u0026ldquo;importance\u0026rdquo; to training samples. Higher weight samples affect the loss function more. Can significantly change decision boundaries (Figure 4-1 from scikit-learn). FAANG Perspective: Weighted sampling is used to upweight important/rare data. Sample weights in training are used for cost-sensitive learning or to prioritize certain types of errors. Reservoir Sampling (Page 86, Figure 4-2): For streaming data when you don\u0026rsquo;t know total N, can\u0026rsquo;t fit all in memory, but want to sample k items such that each has equal selection probability.\nAlgorithm: Fill reservoir (array of size k) with first k elements. For each incoming n-th element (where n \u0026gt; k): Generate random integer i from 1 to n. If 1 \u0026lt;= i \u0026lt;= k, replace reservoir element at index i with the n-th element. Else, do nothing. Ensures: Every tweet/element has an equal probability of being selected at any point in time. If stopped, current reservoir is a fair sample. n-th element has k/n probability of being in the reservoir. Each element already in reservoir has k/n probability of staying (or more precisely, 1 - (k/n)*(1/k) = (n-1)/n probability of one specific item in reservoir not being replaced by the nth item, which then combines with prior probabilities). The math ensures uniform probability for all seen items. Figure 4-2 visualizes this. FAANG Perspective: Essential for sampling from massive, unbounded streams (e.g., sampling search queries for analysis, sampling events for monitoring from a firehose). Importance Sampling (Page 87): Sample from a distribution Q(x) (proposal/importance distribution, easy to sample from) when you actually want to sample from P(x) (target distribution, hard to sample from). Weigh sample x from Q(x) by P(x)/Q(x).\nRequires Q(x) \u0026gt; 0 whenever P(x) != 0. Equation: E_P[x] = E_Q[x * P(x)/Q(x)]. Use in ML: Policy-based reinforcement learning. Estimate value of new policy (P) using rewards from old policy (Q) and reweight. FAANG Perspective: Used in some advanced modeling scenarios, off-policy evaluation in RL, and in areas like Bayesian inference (though less common in typical production ML pipelines). Pages 88-97: Labeling – The Quest for Ground Truth\nMost production ML is supervised, needing labeled data. Quality/quantity of labels heavily impacts performance.\nAndrej Karpathy anecdote: Recruiter asked how long he\u0026rsquo;d need a labeling team. His reply: \u0026ldquo;How long do we need an engineering team?\u0026rdquo; Labeling is now a core, ongoing function. Hand Labels (Pages 88-90): The classic approach.\nChallenges: Expensive: Especially with Subject Matter Expertise (SME). Spam classification: 20 crowdworkers, 15 mins training. Chest X-rays: board-certified radiologists (limited, expensive). Privacy Threat: Someone looks at your data. Can\u0026rsquo;t ship patient records or confidential financials to third-party labelers. Might need on-premise annotators. Slow: Phonetic speech transcription can take 400x utterance duration (footnote 7). 1 hour speech = 400 hours (3 months) labeling. Author\u0026rsquo;s colleagues waited almost a year for lung cancer X-ray labels. Slow Iteration: If task/data changes, relabeling is slow, model becomes less adaptive. E.g., sentiment model (NEG/POS) needs to add ANGRY class -\u0026gt; relabel, collect more ANGRY examples. Label Multiplicity / Ambiguity (Page 89, Table 4-1): Multiple annotators, different expertise/accuracy -\u0026gt; conflicting labels for same instance. Entity recognition example: \u0026ldquo;Darth Sidious, known simply as the Emperor, was a Dark Lord of the Sith who reigned over the galaxy as Galactic Emperor of the First Galactic Empire.\u0026rdquo; Annotator 1: 3 entities. Annotator 2: 6 entities (more granular, e.g., \u0026ldquo;Dark Lord\u0026rdquo; and \u0026ldquo;Sith\u0026rdquo; separate). Annotator 3: 4 entities. Which to train on? Models will perform differently. Disagreements common, especially with high SME needed (footnote 8: if obvious, no SME needed). What is \u0026ldquo;human-level performance\u0026rdquo; if experts disagree? Mitigation: Clear problem definition (e.g., \u0026ldquo;pick longest substring for entities\u0026rdquo;) and annotator training. Data Lineage (Page 90): Track origin of samples and labels. Critical if using multiple sources/annotators. Example: Model trained on 100k good samples. Add 1M crowdsourced samples (lower accuracy). Performance decreases. If data mixed, hard to debug. Helps flag bias, debug models. If model fails on recent data, investigate its acquisition. Often, it\u0026rsquo;s bad labels, not bad model. FAANG Perspective: We have large in-house and vendor labeling operations. Managing quality, cost, throughput, and privacy is a huge operational challenge. Data lineage is critical for regulatory compliance and debugging. Adjudication (resolving labeler disagreements) is a standard part of the process. Natural Labels (Behavioral Labels) (Pages 91-93): Labels inferred from system or user behavior, no human annotation needed.\nModel predictions automatically/partially evaluated by system. Examples: Google Maps ETA: At trip end, actual travel time is known. Stock price prediction: After 2 mins, actual price is known. Recommender Systems (canonical): User clicks (POSITIVE label) or doesn\u0026rsquo;t click after time (NEGATIVE label) on recommendation. Many tasks framed as recommendation (e.g., ad CTR prediction = recommend relevant ads). Can be set up: Google Translate allows community to submit alternative translations (becomes labels for next iteration, after review). Facebook \u0026ldquo;Like\u0026rdquo; button provides feedback for newsfeed ranking. Common in Industry (Figure 4-3): Survey of 86 companies: 63% use natural labels. (Percentages don\u0026rsquo;t sum to 1 as companies use multiple sources). Likely because it\u0026rsquo;s cheaper/easier to start with. Implicit vs. Explicit Labels (Page 92): Recommendation not clicked after time = implicit negative label. User downvotes recommendation = explicit negative label. Feedback Loop Length (Pages 92-93): Time from prediction to feedback. Short loops (minutes): Many recommenders (Amazon related products, Twitter follows). Longer loops (hours/days/weeks): Recommending blog posts, YouTube videos, Stitch Fix clothes (feedback after user tries them on). Different Types of User Feedback (sidebar, page 93): Ecommerce example: Clicking product (high volume, weaker signal, fast feedback). Adding to cart. Buying product (lower volume, stronger signal, business-correlated). Rating/reviewing. Returning product. Optimizing for clicks vs. purchases is a common trade-off. Depends on use case, discuss with stakeholders. Choosing Window Length: Speed vs. accuracy. Short window = faster labels, but might prematurely label as negative. Twitter Ads study (footnote 10): most clicks in 5 mins, but some hours later. Short window underestimates true CTR. Long feedback loops (weeks/months): Fraud detection (dispute window 1-3 months). Good for quarterly reports, bad for quick issue detection. A faulty fraud model can bankrupt a small business if issues take months to fix. FAANG Perspective: Natural labels are heavily used for large-scale systems (search, ads, recommendations). Designing for good feedback collection is key. Understanding feedback delay and its impact on evaluation is critical. Handling the Lack of Labels (Pages 94-101, Table 4-2): What to do when hand labels are too hard and natural labels are absent/insufficient.\nTable 4-2 Summary:\nMethod How Ground Truths Required? Weak Supervision (Noisy) heuristics to generate labels No, but small set recommended to guide heuristic dev. Semi-supervision Structural assumptions, seed labels Yes, small initial set as seeds Transfer Learning Pretrained models from another task No (zero-shot). Yes (fine-tuning, often fewer than scratch) Active Learning Label samples most useful to model Yes Weak Supervision (Pages 95-97): Use heuristics (SME rules) to label data.\nSnorkel (Stanford AI Lab, footnote 11): Popular open-source tool. Labeling Functions (LFs): Functions encoding heuristics. Example: if \u0026quot;pneumonia\u0026quot; in nurse_note: return \u0026quot;EMERGENT\u0026quot; Types of LFs: Keyword, regex, DB lookup, outputs of other models. LFs produce noisy labels. Multiple LFs can conflict. Need to combine, denoise, reweight LFs (Figure 4-4 shows this high-level process). Theoretically no hand labels needed, but small set recommended to guide LF development/accuracy check. Benefits: Privacy (write LFs on small, cleared subset; apply to all data without looking). SME expertise versioned, reused, shared. Adaptive (data/reqs change -\u0026gt; reapply LFs). Known as programmatic labeling. Table 4-3 compares with hand labeling (cost, privacy, speed, adaptivity). Case Study (Stanford Medicine, footnote 13, Figure 4-5): Weakly supervised X-ray models (1 radiologist, 8hrs writing LFs) comparable to fully supervised (almost a year hand labeling). Models improved with more unlabeled data (LFs applied). LFs reused across tasks (CXR, EXR). (Footnote 14: study used 18-20 LFs; author has seen hundreds). Why still need ML models if heuristics work? LFs might not cover all samples. Train ML on LF-labeled data to generalize to samples not covered by any LF. Powerful, but not perfect (labels can be too noisy). Good for bootstrapping. FAANG Perspective: Weak supervision is gaining traction for problems with limited labels or high SME cost. Combining heuristics with small amounts of gold data is common. Semi-Supervision (Pages 98-99): Leverages structural assumptions to generate new labels from a small initial set of labeled data.\nUsed since \u0026rsquo;90s (footnote 16, Blum \u0026amp; Mitchell co-training). Self-training (classic): Train model on existing labeled data. Predict on unlabeled data. Add high-confidence predictions (with their predicted labels) to training set. Retrain. Repeat. Similarity-based: Assume similar samples have similar labels. Obvious similarity: Twitter hashtags. Label #AI as CS. If #ML, #BigData in same MIT CSAIL profile (Figure 4-6), label them CS too. Complex similarity: Use clustering or k-NN. Perturbation-based (popular): Small perturbations to sample shouldn\u0026rsquo;t change label. Apply small noise to images or word embeddings. (More in \u0026ldquo;Perturbation\u0026rdquo; on page 114). Can reach fully supervised performance with fewer labels (footnote 17). Consideration with limited data: How much for evaluation vs. training? Small eval set -\u0026gt; overfitting to it. Large eval set -\u0026gt; less data for training boost. Common trade-off: use reasonably large eval set, then continue training champion model on that eval data too. FAANG Perspective: Used when labeled data is scarce. Self-training and consistency regularization (perturbation-based) are common techniques. Transfer Learning (Pages 99-100): Reuse model from one task (base task) as starting point for another (downstream task).\nBase task usually has abundant/cheap data (e.g., language modeling: predict next token from vast text corpora like books, Wikipedia - footnote 18 token definition). Usage: Zero-shot: Use base model directly on downstream task. Fine-tuning: Make small changes to base model (e.g., continue training on downstream data - footnote 19 Howard \u0026amp; Ruder ULMFiT). Prompting (footnote 20): Modify inputs with a template for base model. E.g., for QA with GPT-3: Q: When was US founded? A: July 4, 1776. Q: Who wrote Dec of Ind? A: Thomas Jefferson. Q: What year Alex Hamilton born? A: [GPT-3 outputs year] Appealing for tasks with few labels. Boosts performance even with many labels. Enabled many apps (ImageNet pretrained object detectors; BERT/GPT-3 for text - footnote 21). Lowers entry barrier. Trend: Larger pretrained base model -\u0026gt; better downstream performance. Training large models (GPT-3) costs tens of millions USD. Future: few companies train huge models, rest use/fine-tune them. FAANG Perspective: Transfer learning is THE dominant paradigm in NLP and increasingly in CV. We train massive foundation models and fine-tune them for various tasks. This saves immense labeling effort and compute. Active Learning (Query Learning) (Pages 101-102): Improve label efficiency. Model (active learner) chooses which unlabeled samples to send to annotators.\nLabel samples most helpful to model. Uncertainty measurement (most straightforward): Label examples model is least certain about (hoping to clarify decision boundary). E.g., classification: samples with lowest predicted class probability. (Figure 4-7 from Burr Settles, footnote 22: toy example, 30 random labels = 70% acc; 30 active learning labels = 90% acc). Query-by-committee (ensemble method, footnote 23 Ch6): Multiple models vote on samples. Label samples committee disagrees on most. Other heuristics: samples giving highest gradient updates or reducing loss most. (Settles 2010 survey). Data regimes for samples: Synthesized (model generates uncertain input points - footnote 24 Angluin). Stationary pool of unlabeled data. Real-world stream (production data). Author most excited about active learning with real-time, changing data (Chapter 1, Chapter 8). Adapts faster. FAANG Perspective: Active learning is used to prioritize labeling efforts, especially when budgets are constrained or labeling is a bottleneck. It\u0026rsquo;s often integrated with human-in-the-loop systems. Pages 102-113: Class Imbalance – The Uneven Playing Field\nA very common real-world problem.\nDefinition (Page 102): Classification: substantial difference in number of samples per class. (E.g., 99.99% normal lung X-rays, 0.01% cancerous). Regression: can also happen with skewed continuous labels. (Eugene Yan\u0026rsquo;s healthcare bills example, footnote 25: median bill low, 95th percentile astronomical. 100% error on $250 bill ($250 vs $500) okay; 100% error on $10k bill ($10k vs $20k) not. Might need to optimize for 95th percentile prediction even if overall metrics suffer). Challenges of Class Imbalance (Page 103-104, Figure 4-8): ML (esp. Deep Learning) works well with balanced data, struggles with imbalance.\nFigure 4-8 (Andrew Ng image): ML works well when distribution is like [Cat, Dog, Chair, Bike, Person] (balanced). Not so well when like [Effusion, Atelectasis, Mass, Consolidation, Hernia] (highly imbalanced medical findings). Reason 1: Insufficient signal for minority class. Becomes few-shot learning problem. If no instances, model assumes class doesn\u0026rsquo;t exist. Reason 2: Model exploits simple heuristic. E.g., lung cancer: always predict majority class (normal) -\u0026gt; 99.99% accuracy (footnote 27: why accuracy is bad for imbalance). Hard for gradient descent to beat this trivial solution. Reason 3: Asymmetric error costs. Misclassifying cancerous X-ray (rare) far more dangerous than misclassifying normal lung (common). Standard loss functions treat all samples equally. Imbalance is the norm in real world (Page 104): Author shocked after school (balanced datasets) to find this. Rare events are often more interesting/dangerous. Examples: Fraud detection (6.8c per $100 is fraud - footnote 29), churn prediction, disease screening, resume screening (98% eliminated initially - footnote 30), object detection (most generated bounding boxes are background). Other causes: Sampling bias (spam: 85% of all email is spam, but filtered before DB, so dataset has little spam - footnote 31), labeling errors. Always examine data to understand causes of imbalance. Handling Class Imbalance (Pages 105-113): Extensively studied (footnote 32). Sensitivity varies by task complexity, imbalance level (footnote 33). Binary easier than multiclass. Deep NNs (10+ layers in 2017) better on imbalanced data than shallower ones (footnote 34).\nSome argue: don\u0026rsquo;t \u0026ldquo;fix\u0026rdquo; imbalance if it\u0026rsquo;s real-world. Good model should learn it. Challenging.\nThree approaches:\nA. Using the Right Evaluation Metrics (Page 106-108): Most important first step!\nOverall accuracy/error rate: Insufficient. Dominated by majority class. Example: CANCER (positive, 10% of data) vs. NORMAL (negative, 90%). Model A (Table 4-4): Predicts 10/100 CANCER, 890/900 NORMAL. Overall Accuracy = (10+890)/1000 = 0.9. Model B (Table 4-5): Predicts 90/100 CANCER, 810/900 NORMAL. Overall Accuracy = (90+810)/1000 = 0.9. Both 90% accurate, but Model B much better for CANCER detection. Better choice: Per-class accuracy. Model A CANCER acc: 10%. Model B CANCER acc: 90%. Precision, Recall, F1 (sidebar, Table 4-6): For binary tasks, measure performance wrt positive class (footnote 35: scikit-learn pos_label). Asymmetric (values change if you swap positive/negative class). Precision = TP / (TP + FP) (Of those predicted positive, how many were actually positive?) Recall (Sensitivity, True Positive Rate) = TP / (TP + FN) (Of all actual positives, how many did we find?) F1 = 2 * (Precision * Recall) / (Precision + Recall) (Harmonic mean) Table 4-7: Model A (CANCER positive): P=0.5, R=0.1, F1=0.17. Model B: P=0.5, R=0.9, F1=0.64. Clearly shows B is better. ROC Curve (Receiver Operating Characteristics) (Page 108, Figure 4-9): Classification often outputs probability. Threshold (e.g., 0.5) converts to class. Plot True Positive Rate (Recall) vs. False Positive Rate (1 - Specificity) for different thresholds. Perfect model: Line at top (TPR=1). Random: Diagonal. Closer to top-left = better. AUC (Area Under Curve): Measures area under ROC. Larger = better. Precision-Recall Curve (Page 108): ROC focuses on positive class, doesn\u0026rsquo;t show negative class performance. Davis \u0026amp; Goadrich (footnote 36) argue PR curve is more informative for heavy imbalance. FAANG Perspective: For imbalanced problems, we never rely on overall accuracy. We look at Precision, Recall, F1 for the classes of interest, AUC-ROC, AUC-PR. Confusion matrices are essential. B. Data-Level Methods: Resampling (Pages 109-110): Modify training data distribution.\nUndersampling: Remove instances from majority class. Simplest: random removal. Oversampling: Add instances to minority class. Simplest: random replication. Figure 4-10 (Rafael Alencar, footnote 37) visualizes this. Tomek links (undersampling, footnote 38): Find close pairs from opposite classes, remove majority class sample. Clears decision boundary, but might make model less robust (loses subtlety of true boundary). For low-dim data. SMOTE (Synthetic Minority Oversampling TEchnique, footnote 39): Synthesize new minority samples by convex combinations of existing ones (footnote 40 linear). For low-dim data. Sophisticated methods (Near-Miss, one-sided selection - footnote 41) need distance calcs, expensive for high-dim data/features (e.g., NNs). CRITICAL: Never evaluate on resampled validation/test data! Model will overfit to resampled distribution. Evaluate on original, true distribution. Risks: Undersampling loses data. Oversampling (replication) overfits. Two-phase learning (footnote 42): Train on resampled data (e.g., undersample majority to N instances per class). Fine-tune on original data. Dynamic sampling (footnote 43 Pouyanfar): Oversample low-performing classes, undersample high-performing ones during training. Show model less of what it knows, more of what it doesn\u0026rsquo;t. FAANG Perspective: Resampling is common, but needs care. SMOTE is popular. Often combined with algorithm-level methods. Evaluation on original distribution is key. C. Algorithm-Level Methods (Pages 110-112): Keep data intact, alter algorithm (usually loss function) to be robust to imbalance.\nPrioritize learning instances we care about by giving them higher weight in loss. L(X; θ) = (1/N) * Σ_x L(x; θ) (standard average loss). Treats all instances equally. Cost-sensitive learning (Elkan 2001, footnote 44, Table 4-8): Misclassification costs vary. C_ij = cost if class i classified as j. C_ii = 0. If classifying POS as NEG is 2x costly as NEG as POS, C_10 = 2 * C_01. L(x; θ) = Σ_j C_ij * P(j|x; θ) (loss for instance x of class i is weighted average of costs for possible predicted classes j). Problem: Manually define cost matrix, task/scale dependent. Class-balanced loss (Page 112): Punish model for misclassifying minority classes. Vanilla: Weight class i by W_i = N_total / N_i (rarer class = higher weight). L(x; θ) = W_i * Σ_j P(j|x; θ) * Loss(x, j) (where x is instance of class i). Sophisticated: Consider overlap among samples (effective number of samples - footnote 45 Cui et al.). Focal Loss (Lin et al. 2017, footnote 46, Figure 4-11): Incentivize model to focus on hard-to-classify samples. Adjust loss: if sample has lower probability of being right, give it higher weight. Figure 4-11 shows Focal Loss (FL) vs. Cross Entropy (CE). FL reduces loss more for well-classified examples, focusing on hard ones. γ parameter controls focusing rate. Ensembles can help (footnote 47), but not their primary purpose. (Covered in Ch6). FAANG Perspective: Modifying loss functions (class weighting, focal loss) is very common for imbalanced problems, especially in deep learning. It\u0026rsquo;s often more effective and easier to implement than complex resampling if you have large data. Pages 113-117: Data Augmentation – Creating More from Less (or More from More!)\nIncrease amount of training data. Traditionally for limited data (medical imaging). Now useful even with lots of data (robustness to noise, adversarial attacks). Standard in CV, finding way into NLP. Format-dependent.\nSimple Label-Preserving Transformations (Page 114):\nComputer Vision: Randomly modify image, preserve label. Crop, flip, rotate, invert, erase part. Rotated dog is still a dog. PyTorch, TF, Keras support this. AlexNet (footnote 48): generated on CPU while GPU trains on previous batch (computationally \u0026ldquo;free\u0026rdquo;). NLP (Table 4-9): Randomly replace word with similar one (synonym dictionary, or close in embedding space), assume meaning/sentiment preserved. I'm so happy to see you. -\u0026gt; I'm so glad to see you. / ...see y'all. / I'm very happy... Quick way to double/triple training data. FAANG Perspective: Standard practice in CV. For NLP, synonym replacement, back-translation (translate Eng-\u0026gt;Fre-\u0026gt;Eng) are common. Need to be careful not to change meaning too much. Perturbation (Pages 114-116): Also label-preserving, but sometimes used to trick models, so gets own section.\nNNs sensitive to noise. CV: Small noise can cause misclassification. One-pixel attack (Su et al., footnote 49, Figure 4-12): Changing one pixel misclassifies many CIFAR-10/ImageNet images. Adversarial attacks: Using deceptive data to trick NNs. Adding noise is common. Adversarial augmentation/training (footnote 53): Add noisy samples to training data -\u0026gt; helps model recognize weak spots, improve performance (footnote 51 Goodfellow). Noise can be random or found by search (DeepFool, footnote 52, finds min noise for misclassification). NLP: Less common (random chars -\u0026gt; gibberish). But perturbation used for robustness. BERT (footnote 54): 15% tokens chosen; of these, 10% replaced with random words (1.5% total tokens become nonsensical, e.g., \u0026ldquo;My dog is apple\u0026rdquo;). Small performance boost. Chapter 6 covers perturbation for evaluation too. FAANG Perspective: Adversarial training is important for security-sensitive models (spam, fraud, face recognition) and for improving general robustness. Data Synthesis (Pages 116-117): Sidestep expensive/slow/private data collection by synthesizing it. Still far from synthesizing all data, but can boost performance.\nNLP Templates (Table 4-10): Bootstrap chatbot training data. Template: Find me a [CUISINE] restaurant within [NUMBER] miles of [LOCATION]. Fill with lists of cuisines, numbers, locations -\u0026gt; thousands of queries. CV Mixup (Zhang et al. ICLR 2018, footnote 55): Combine existing examples with discrete labels to make continuous labels. x' = γ*x1 + (1-γ)*x2 (e.g., x1=DOG (0), x2=CAT (1)). Label for x' = γ*0 + (1-γ)*1. Improves generalization, reduces memorization of corrupt labels, robust to adversarial examples, stabilizes GAN training. NNs to synthesize data (e.g., CycleGAN, footnote 56 Sandfort): Exciting research, not yet popular in production. Adding CycleGAN images to CT segmentation improved performance. CV Augmentation Survey (Shorten \u0026amp; Khoshgoftaar 2019). FAANG Perspective: Templating is used for bootstrapping NLU models. Mixup and related techniques (CutMix, CutOut) are standard in CV training. GAN-based synthesis is still mostly research but promising for rare data or privacy-preserving data generation. Page 118: Summary of Chapter 4\nTraining data is foundational. Bad data = bad models. Invest time/effort to curate/create it.\nSampling: Nonprobability (convenience) vs. Random (simple, stratified, weighted, reservoir, importance). Labeling: Most ML is supervised. Natural labels (delivery times, recommender clicks) are great, but often delayed (feedback loop length). Hand labels: Expensive, slow, privacy issues, label multiplicity. Data lineage is key. Lack of labels: Weak supervision (heuristics, Snorkel), semi-supervision (self-training, similarity, perturbation), transfer learning (pretrained models), active learning (querying for most useful labels). Class Imbalance: Norm in real world. Hard for ML. Handle by: right metrics (Precision/Recall/F1, ROC/PR AUC), resampling (over/under, SMOTE), algorithm changes (cost-sensitive loss, focal loss). Data Augmentation: Increase data (simple transforms, perturbation/adversarial, synthesis/mixup/templates). Improves performance, generalization, robustness. Next: Feature extraction (Chapter 5).\nInterview Questions \u0026amp; Page References (Chapter 4):\nGeneral Training Data Concepts:\n\u0026ldquo;Why is handling training data well so critical in ML projects?\u0026rdquo; (p. 81) \u0026ldquo;What are some potential sources of bias in training data, and why is it important to be aware of them?\u0026rdquo; (p. 82) \u0026ldquo;Why does the author prefer the term \u0026rsquo;training data\u0026rsquo; over \u0026rsquo;training dataset\u0026rsquo; in the context of production ML?\u0026rdquo; (p. 81) Sampling:\n\u0026ldquo;Why is sampling necessary or helpful in the ML workflow?\u0026rdquo; (p. 82) \u0026ldquo;Describe different nonprobability sampling methods and their potential biases. Give examples where they might be used.\u0026rdquo; (p. 83-84) \u0026ldquo;Compare simple random sampling with stratified sampling. When would you prefer stratified sampling?\u0026rdquo; (p. 84) \u0026ldquo;What is weighted sampling, and how can it be used to leverage domain expertise or correct for distribution mismatches?\u0026rdquo; (p. 85) \u0026ldquo;Explain reservoir sampling. When is it particularly useful?\u0026rdquo; (p. 86, Figure 4-2) \u0026ldquo;What is importance sampling and where might it be applied in ML?\u0026rdquo; (p. 87) Labeling:\n\u0026ldquo;What are the main challenges associated with acquiring hand labels?\u0026rdquo; (p. 88) \u0026ldquo;What is label multiplicity? How can disagreements among annotators be minimized?\u0026rdquo; (p. 89, Table 4-1) \u0026ldquo;Explain the concept of data lineage and its importance.\u0026rdquo; (p. 90) \u0026ldquo;What are natural labels (behavioral labels)? Give some examples. How do they compare to hand labels?\u0026rdquo; (p. 91) \u0026ldquo;Discuss the concept of feedback loop length for natural labels and its implications. Provide examples of short and long feedback loops.\u0026rdquo; (p. 92-93) \u0026ldquo;Explain the difference between implicit and explicit labels.\u0026rdquo; (p. 92) Handling Lack of Labels:\n\u0026ldquo;Describe weak supervision. How do Labeling Functions (LFs) work in tools like Snorkel?\u0026rdquo; (p. 95-97, Figure 4-4, Table 4-3) \u0026ldquo;What are the advantages of programmatic labeling (weak supervision) over hand labeling?\u0026rdquo; (p. 96, Table 4-3) \u0026ldquo;What is semi-supervised learning? Describe self-training and perturbation-based methods.\u0026rdquo; (p. 98-99, Figure 4-6) \u0026ldquo;Explain transfer learning. What are base models, downstream tasks, fine-tuning, and prompting?\u0026rdquo; (p. 99-100) \u0026ldquo;What is active learning? How can uncertainty measurement or query-by-committee be used to select samples for labeling?\u0026rdquo; (p. 101-102, Figure 4-7) Class Imbalance:\n\u0026ldquo;What is class imbalance, and why does it make learning difficult for ML models?\u0026rdquo; (p. 102-104, Figure 4-8) \u0026ldquo;Give some real-world examples of tasks with class imbalance.\u0026rdquo; (p. 104) \u0026ldquo;Why is overall accuracy an insufficient metric for tasks with class imbalance? What are better alternatives?\u0026rdquo; (p. 106-108, Tables 4-4, 4-5, 4-7, Figure 4-9) \u0026ldquo;Explain Precision, Recall, and F1-score. Why are they useful for imbalanced datasets?\u0026rdquo; (p. 107, Table 4-6) \u0026ldquo;Describe data-level methods for handling class imbalance, such as oversampling and undersampling (including SMOTE and Tomek links).\u0026rdquo; (p. 109-110, Figure 4-10) \u0026ldquo;What are algorithm-level methods for class imbalance? Explain cost-sensitive learning, class-balanced loss, and focal loss.\u0026rdquo; (p. 110-112, Table 4-8, Figure 4-11) \u0026ldquo;When resampling training data, what is a critical consideration for model evaluation?\u0026rdquo; (p. 110) Data Augmentation:\n\u0026ldquo;What is data augmentation, and why is it used?\u0026rdquo; (p. 113) \u0026ldquo;Describe simple label-preserving transformations for image and text data.\u0026rdquo; (p. 114, Table 4-9) \u0026ldquo;What is perturbation in the context of data augmentation? How does it relate to adversarial attacks and adversarial training?\u0026rdquo; (p. 114-116, Figure 4-12) \u0026ldquo;Explain some data synthesis techniques like using templates for NLP or mixup for CV.\u0026rdquo; (p. 116-117, Table 4-10) This chapter is packed with practical techniques essential for any ML engineer. Getting the training data right is often more than half the battle! Any questions on these topics?\n","permalink":"https://deepskandpal.github.io/bookshelf/design-ml-system/chapter-4/","tags":null,"title":"Chapter 4: Training Data"},{"categories":null,"content":"Chapter 4: Training Models.\nIntroduction - Beyond Black Boxes Up until now, as the book says, we\u0026rsquo;ve treated ML models and their training algorithms mostly like black boxes. We fed them data, they gave us results, and we learned to evaluate those results. You\u0026rsquo;ve optimized regression, improved classifiers, even built a spam filter, often without peeking under the hood. And that\u0026rsquo;s okay for many practical purposes!\nHowever, understanding how these things work internally is incredibly powerful. It helps you:\nChoose the right model and algorithm: Knowing the mechanics helps you match the tool to the task. Select good hyper parameters: Hyper parameters often control the learning process itself. Understanding that process helps you tune them effectively. Debug issues and perform error analysis: When things go wrong, or your model makes weird mistakes, knowing the \u0026ldquo;why\u0026rdquo; is crucial. Foundation for advanced topics: Especially for neural networks (Part II of the book), the concepts here are fundamental. This chapter focuses on Linear Regression as a starting point because it\u0026rsquo;s simple, yet we can train it in very different ways. We\u0026rsquo;ll explore two main approaches:\nA direct \u0026ldquo;closed-form\u0026rdquo; equation (The Normal Equation): This is like having a magic formula that directly spits out the best model parameters in one go. An iterative optimization approach (Gradient Descent): This is more like a trial-and-error process. We start with a guess for the parameters and gradually tweak them, step by step, to minimize the error, eventually (hopefully!) arriving at the same best parameters. We\u0026rsquo;ll see different \u0026ldquo;flavors\u0026rdquo; of Gradient Descent: Batch, Mini-batch, and Stochastic. Then, the chapter will touch on:\nPolynomial Regression: How to use linear models for non-linear data. Learning Curves: Tools to diagnose over fitting or under fitting. Regularization: Techniques to prevent over fitting. Logistic Regression and Softmax Regression: Models commonly used for classification tasks. The scorpion icon on page 112 gives a fair warning: there will be some math (linear algebra, calculus). If you\u0026rsquo;re \u0026ldquo;allergic,\u0026rdquo; the book suggests you can still get the concepts by focusing on the text. My job is to make sure you get those concepts, regardless of how comfortable you are with the equations. We\u0026rsquo;ll always ask: \u0026ldquo;What is this equation ultimately trying to achieve?\u0026rdquo;\nLinear Regression - The Model Remember our life satisfaction model from Chapter 1? life_satisfaction = θ₀ + θ₁ × GDP_per_capita. That was a simple linear regression with one feature.\nMore generally, a linear model predicts a value by:\nTaking all the input features (like a house\u0026rsquo;s square footage, number of bedrooms, age, etc.). Multiplying each feature by a specific weight (a model parameter). Summing up these weighted features. Adding a constant bias term (another model parameter, also called the intercept). Equation 4-1 (page 112): Linear Regression model prediction ŷ = θ₀ + θ₁x₁ + θ₂x₂ + ⋯ + θₙxₙ\nŷ (y-hat): The predicted value. n: The number of features. xᵢ: The value of the i-th feature. θ₀: The bias term (theta-zero). What it\u0026rsquo;s ultimately trying to achieve: It\u0026rsquo;s the baseline prediction if all feature values were zero. It allows the line/plane to shift up or down. θ₁ to θₙ: The feature weights. θⱼ is the weight for the j-th feature. What they\u0026rsquo;re ultimately trying to achieve: They represent how much a one-unit change in that feature xⱼ affects the predicted value ŷ, holding other features constant. A positive weight means the feature positively influences the prediction; a negative weight means it negatively influences it. The magnitude shows the strength of the influence. Equation 4-2 (page 113): Vectorized form ŷ = h_θ(x) = θ · x\nThis is just a more compact way to write Equation 4-1 using vector notation.\nθ (theta): Is now a parameter vector [θ₀, θ₁, ..., θₙ]. x: Is the instance\u0026rsquo;s feature vector [x₀, x₁, ..., xₙ]. Here, we add a \u0026ldquo;dummy\u0026rdquo; feature x₀ which is always set to 1. This allows us to include the bias term θ₀ neatly into the dot product (because θ₀ * x₀ = θ₀ * 1 = θ₀). θ · x: This is the dot product of the two vectors. It\u0026rsquo;s exactly the sum θ₀x₀ + θ₁x₁ + ... + θₙxₙ. h_θ(x): This is our hypothesis function (our model), parameterized by θ. Given an input x, it predicts ŷ. The bird sidebar (page 113) explains that vectors are often column vectors (2D arrays with one column). So, if θ and x are column vectors, the dot product can be written as a matrix multiplication: ŷ = θᵀx (where θᵀ is the transpose of θ, making it a row vector). Don\u0026rsquo;t let this bog you down; it\u0026rsquo;s a notational convenience. The goal is the same: calculate a weighted sum of features plus a bias.\nHow do we train it? Training means finding the values for the parameters θ (the bias θ₀ and the weights θ₁ to θₙ) that make the model \u0026ldquo;best fit\u0026rdquo; the training data.\nTo do this, we need a way to measure how well (or poorly) the model fits. We learned in Chapter 2 that for regression, a common measure is RMSE (Root Mean Square Error).\nWhat it\u0026rsquo;s ultimately trying to achieve: Quantify the typical prediction error. However, for mathematical convenience during training, it\u0026rsquo;s often easier to minimize the MSE (Mean Squared Error) instead. Minimizing MSE will also minimize RMSE (since the square root function is monotonic). The footnote on page 113 is important: the function we optimize during training (the cost function, here MSE) might be different from the final performance metric we use to evaluate the model (e.g., RMSE). This is often because the cost function has nice mathematical properties (like being easily differentiable) that make optimization easier. MSE Cost Function \u0026amp; The Normal Equation Equation 4-3 (page 114): MSE cost function for a Linear Regression model MSE(X, h_θ) = (1/m) * Σᵢ (θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾)² (summing from i=1 to m, where m is the number of instances)\nWhat it\u0026rsquo;s ultimately trying to achieve: For each training instance i: θᵀx⁽ⁱ⁾ is the model\u0026rsquo;s prediction for that instance. y⁽ⁱ⁾ is the actual target value. (θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾) is the error for that instance. We square this error: (error)². (Why square? It makes all errors positive, and it penalizes larger errors more heavily). We sum these squared errors over all m training instances: Σᵢ (error)². We divide by m to get the mean of the squared errors. This function tells us, on average, how \u0026ldquo;bad\u0026rdquo; our model\u0026rsquo;s predictions are for a given set of parameters θ. Our goal in training is to find the θ that makes this MSE as small as possible. The Normal Equation: A Direct Solution\nFor Linear Regression with an MSE cost function, there\u0026rsquo;s a wonderful mathematical shortcut. Instead of iteratively searching for the best θ, there\u0026rsquo;s a direct formula that gives you the θ that minimizes the cost function in one shot! This is called the Normal Equation.\nEquation 4-4 (page 114): Normal Equation θ̂ = (XᵀX)⁻¹ Xᵀy\nθ̂ (theta-hat): This is the value of θ that minimizes the cost function. X: The matrix of input features for all training instances (each row is an instance, x₀ for each instance is 1). y: The vector of actual target values for all training instances. Xᵀ: The transpose of matrix X. (XᵀX)⁻¹: The inverse of the matrix XᵀX. What it\u0026rsquo;s ultimately trying to achieve: This equation, derived using calculus (setting the derivative of the cost function to zero and solving for θ), directly calculates the optimal parameter vector θ̂ that makes the linear model fit the training data as closely as possible (in the mean squared error sense). It\u0026rsquo;s like a direct recipe: plug in your data X and y, and out pops the best θ.\nTesting the Normal Equation The book generates some linear-looking data (Figure 4-1): X = 2 * np.random.rand(100, 1) (100 instances, 1 feature) y = 4 + 3 * X + np.random.randn(100, 1) (True model is y = 4 + 3x₁ + noise) So, the ideal θ₀ is 4, and the ideal θ₁ is 3.\nTo use the Normal Equation, we need to add x₀ = 1 to each instance in X: X_b = np.c_[np.ones((100, 1)), X] (np.c_ concatenates arrays column-wise)\nNow, apply the Normal Equation: theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) The result is something like [[4.215...], [2.770...]]. So, θ₀̂ ≈ 4.215 and θ₁̂ ≈ 2.770. It\u0026rsquo;s close to the true values (4 and 3), but not exact because of the random noise we added to y. The noise makes it impossible to recover the exact original parameters.\nWe can then use this theta_best to make predictions (Figure 4-2).\nScikit-Learn and Computational Complexity Scikit-Learn LinearRegression: from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X, y) Scikit-Learn handles adding the x₀=1 feature (or rather, it separates the bias term lin_reg.intercept_ from the feature weights lin_reg.coef_). The results are the same as the Normal Equation.\nUnderlying Method (scipy.linalg.lstsq): Scikit-Learn\u0026rsquo;s LinearRegression actually uses scipy.linalg.lstsq() (\u0026ldquo;least squares\u0026rdquo;). This function computes θ̂ = X⁺y, where X⁺ is the pseudoinverse (or Moore-Penrose inverse) of X. You can compute X⁺ using np.linalg.pinv(). The pseudoinverse is calculated using a technique called Singular Value Decomposition (SVD).\nWhy SVD/pseudoinverse instead of the direct Normal Equation (XᵀX)⁻¹ Xᵀy? More efficient: SVD is generally more computationally efficient. Handles edge cases: The Normal Equation requires XᵀX to be invertible. If it\u0026rsquo;s not (e.g., if you have more features than instances, m \u0026lt; n, or if some features are redundant/linearly dependent), the Normal Equation breaks down. The pseudoinverse is always defined, making SVD more robust. Computational Complexity:\nNormal Equation (inverting XᵀX): About O(n²·⁴) to O(n³) where n is the number of features. This gets very slow if you have many features (e.g., 100,000 features). Doubling features can increase time by 5-8x. SVD (used by Scikit-Learn): About O(n²). Doubling features increases time by ~4x. Still slow for very large n. Both are O(m) with respect to the number of instances m. So, they handle large numbers of training instances efficiently, as long as the data fits in memory. Predictions: Once trained, making predictions is very fast: O(m) and O(n) – linear with number of instances and features. The Problem with Normal Equation/SVD: They get slow with many features and require all data to be in memory. This leads us to the next method\u0026hellip;\nGradient Descent - The Iterative Approach When the Normal Equation is too slow (too many features) or the dataset is too large to fit in memory, we need a different approach. Enter Gradient Descent.\nThe Core Idea: Gradient Descent is a generic optimization algorithm. It iteratively tweaks model parameters to minimize a cost function.\nImagine you\u0026rsquo;re lost in a foggy mountain valley. You can only feel the slope of the ground under your feet. To get to the bottom, you\u0026rsquo;d take a step in the direction of the steepest downhill slope. Repeat. This is Gradient Descent: It measures the local gradient of the cost function (e.g., MSE) with respect to the parameter vector θ. The gradient tells you the direction of steepest ascent. It takes a step in the opposite direction (descending gradient) to reduce the cost. Repeat until the gradient is zero (or very close), meaning you\u0026rsquo;ve reached a minimum. The Process (Figure 4-3, page 118): Random Initialization: Start with random values for θ. Iterative Improvement: In each step: Calculate the gradient of the cost function at the current θ. Update θ by taking a small step in the negative gradient direction. Convergence: Continue until the algorithm converges to a minimum (cost stops decreasing significantly). Learning Rate (η - eta):\nThis is a crucial hyperparameter that determines the size of the steps. Too small (Figure 4-4): Many iterations needed to converge (very slow). Too large (Figure 4-5): Might jump across the valley, diverge, and fail to find a solution. Challenges (Figure 4-6, page 119):\nLocal Minima: If the cost function has multiple minima (not a smooth bowl), GD might converge to a local minimum, which isn\u0026rsquo;t as good as the global minimum. Plateaus: If the cost function has flat areas, GD can take a very long time to cross them. Irregular Terrains: Holes, ridges make convergence difficult. Good News for Linear Regression MSE:\nThe MSE cost function for Linear Regression is a convex function. What this means: It\u0026rsquo;s shaped like a bowl. It has no local minima, only one global minimum. It\u0026rsquo;s also continuous with a slope that doesn\u0026rsquo;t change abruptly. Consequence: Gradient Descent is guaranteed to approach the global minimum if you wait long enough and the learning rate isn\u0026rsquo;t too high. Feature Scaling Matters! (Figure 4-7, page 120):\nIf features have very different scales (e.g., feature 1 ranges 0-1, feature 2 ranges 0-1000), the cost function \u0026ldquo;bowl\u0026rdquo; becomes elongated. GD will take a long, zig-zag path to the minimum. Solution: Ensure all features have a similar scale (e.g., using StandardScaler). GD will then converge much faster. Parameter Space: Training a model is essentially a search in the model\u0026rsquo;s parameter space for the combination of parameters that minimizes the cost. More parameters = higher dimensional space = harder search. For Linear Regression (convex cost), it\u0026rsquo;s like finding the bottom of a D-dimensional bowl.\nBatch Gradient Descent - BGD To implement GD, we need the gradient of the cost function with respect to each model parameter θⱼ. This is the partial derivative ∂MSE(θ) / ∂θⱼ.\nEquation 4-5 (page 121): Partial derivative of MSE w.r.t. θⱼ ∂MSE(θ)/∂θⱼ = (2/m) * Σᵢ (θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾) * xⱼ⁽ⁱ⁾\nWhat it\u0026rsquo;s ultimately trying to achieve: For each parameter θⱼ, it calculates how much the MSE would change if θⱼ changed a tiny bit. (θᵀx⁽ⁱ⁾ - y⁽ⁱ⁾) is the prediction error for instance i. We multiply this error by the value of the j-th feature of instance i, xⱼ⁽ⁱ⁾. (If xⱼ⁽ⁱ⁾ is large, θⱼ has a bigger impact on the prediction and thus on the error). We average this product over all instances m. Equation 4-6 (page 122): Gradient vector ∇_θ MSE(θ) ∇_θ MSE(θ) = (2/m) Xᵀ(Xθ - y)\nThis is the compact, vectorized way to compute all partial derivatives at once. ∇_θ MSE(θ) is a vector containing ∂MSE(θ)/∂θ₀, ∂MSE(θ)/∂θ₁, \u0026hellip;, ∂MSE(θ)/∂θₙ. What it\u0026rsquo;s ultimately trying to achieve: It gives the direction of steepest increase in the cost function. Crucial point for BATCH GD: This formula uses the entire training set X at each step to calculate the gradients. This is why it\u0026rsquo;s called Batch Gradient Descent.\nConsequence: Terribly slow on very large training sets. Advantage: Scales well with the number of features (unlike Normal Equation). Equation 4-7 (page 122): Gradient Descent step θ⁽ⁿᵉˣᵗ ˢᵗᵉᵖ⁾ = θ - η ∇_θ MSE(θ)\nWhat it\u0026rsquo;s ultimately trying to achieve: Update the current parameters θ by taking a step of size η (learning rate) in the direction opposite to the gradient (downhill). Implementation (page 122): The code shows a loop: for iteration in range(n_iterations): gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradients With eta = 0.1 and n_iterations = 1000, the resulting theta is exactly what the Normal Equation found! Perfect.\nEffect of Learning Rate η (Figure 4-8, page 123):\nη = 0.02 (left): Too slow, many steps to converge. η = 0.1 (middle): Good, converges quickly. η = 0.5 (right): Too high, diverges, jumps around. Finding a good learning rate: Grid search (Chapter 2).\nSetting number of iterations: If too low, far from optimum. If too high, waste time after convergence.\nSolution: Set many iterations, but stop when the gradient vector becomes tiny (its norm \u0026lt; ϵ, a small tolerance). This means we\u0026rsquo;re (almost) at the minimum. Convergence Rate (sidebar, page 124): For convex cost functions like MSE, BGD with fixed η takes O(1/ϵ) iterations to reach within ϵ of the optimum. To get 10x more precision (divide ϵ by 10), you need ~10x more iterations.\nStochastic Gradient Descent - SGD Batch GD is slow on large datasets because it uses all training data at each step.\nStochastic Gradient Descent (SGD):\nAt each step, picks one random instance from the training set and computes gradients based only on that single instance. Advantages: Much faster per step (very little data to process). Can train on huge datasets (only one instance in memory at a time – good for out-of-core learning, Ch 1). Disadvantages (Figure 4-9, page 124): Stochastic nature: The path to the minimum is much more erratic (\u0026ldquo;bouncy\u0026rdquo;) than BGD. Cost function goes up and down, decreasing only on average. Never settles: Once near the minimum, it keeps bouncing around, never perfectly settling. Final parameters are good, but not optimal. Advantage of randomness: If cost function is irregular (non-convex, Figure 4-6), SGD\u0026rsquo;s randomness can help it jump out of local minima and find a better global minimum. Learning Schedule (page 125):\nTo help SGD settle at the minimum, we can gradually reduce the learning rate η. Start with large η (quick progress, escape local minima). Make η smaller over time (settle at global minimum). This is like simulated annealing in metallurgy. The function determining η at each iteration is the learning schedule. If η reduces too quickly -\u0026gt; stuck in local minimum or frozen half-way. If η reduces too slowly -\u0026gt; bounce around minimum for long, or stop too early with suboptimal solution. Implementation of SGD (page 125):\nOuter loop for epochs (an epoch is one pass through the entire training set, by convention). Inner loop for m iterations (number of instances). In each inner iteration: Pick a random_index. Get xi and yi for that instance. Gradients calculated using only xi and yi: gradients = 2 * xi.T.dot(xi.dot(theta) - yi). (Note: 2 not 2/m because m=1 here). Update eta using learning_schedule(epoch * m + i). Update theta. After 50 epochs (much fewer iterations than BGD\u0026rsquo;s 1000), it finds a theta very close to BGD\u0026rsquo;s. Figure 4-10 (page 126) shows the irregular first 20 steps. Important note on SGD (sidebar, page 126):\nTraining instances must be independent and identically distributed (IID) for SGD to converge to global optimum on average. Shuffle instances during training (pick randomly, or shuffle set at start of each epoch). If data is sorted (e.g., by label), SGD will optimize for one label, then the next, and not find global minimum. SGD with Scikit-Learn (SGDRegressor, page 126):\nDefaults to optimizing MSE. sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1) max_iter: max epochs. tol: stop if loss drops by less than this in one epoch. penalty=None: no regularization (more later). eta0: initial learning rate. Uses its own default learning schedule. sgd_reg.fit(X, y.ravel()) (.ravel() flattens y into 1D array, often needed by Scikit-Learn). Resulting intercept and coefficient are very close to Normal Equation\u0026rsquo;s. Mini-batch Gradient Descent A compromise between Batch GD and Stochastic GD.\nAt each step, computes gradients on small random sets of instances called mini-batches. Main advantage over SGD: Performance boost from hardware optimization of matrix operations (especially on GPUs). Behavior (Figure 4-11, page 127): Less erratic path than SGD. Ends up closer to the minimum than SGD (with fixed η). But may be harder to escape local minima (for non-convex problems) compared to SGD. All three (Batch, Stochastic, Mini-batch) end up near the minimum. Batch stops at the minimum. SGD and Mini-batch would also reach minimum with a good learning schedule. But Batch GD takes much longer per step. Comparison Table 4-1 This table nicely summarizes Normal Equation, SVD, Batch GD, Stochastic GD, Mini-batch GD for Linear Regression based on:\nLarge m (instances): Normal Eq/SVD are fast if data fits memory. GD methods are also fast (SGD/Mini-batch can do out-of-core). Out-of-core support: Only SGD/Mini-batch. Large n (features): Normal Eq/SVD are slow. GD methods are fast. Hyperparameters: Normal Eq/SVD have 0. BGD has 2 (η, iterations). SGD/Mini-batch have \u0026gt;=2 (η, iterations, schedule params). Scaling required: No for Normal Eq/SVD. Yes for GD methods. Scikit-Learn class: LinearRegression for SVD. SGDRegressor for GDs. The key is that after training, all these methods (if converged) produce very similar models and make predictions in the same way. The difference is in how they get there (the training process).\nExcellent! Glad you\u0026rsquo;re with me. Let\u0026rsquo;s push on to the next sections of Chapter 4. We\u0026rsquo;ve just laid the groundwork for how Linear Regression models are trained. Now, let\u0026rsquo;s see how we can adapt these ideas for more complex scenarios.\nPolynomial Regression So far, Linear Regression assumes a straight-line (or flat plane/hyperplane) relationship between features and the target. But what if your data is more complex, like the curved data in Figure 4-12 (page 129)? The book shows data generated by a simple quadratic equation: y = 0.5 * X2 + X + 2 + noise. Clearly, a straight line won\u0026rsquo;t fit this well.\nThis is where Polynomial Regression comes in.\nThe Core Idea: You can still use a linear model to fit nonlinear data!\nHow? By adding powers of each feature as new features, and then training a linear model on this extended set of features.\nFor example, if you have one feature X, you can create a new feature X². Then your linear model becomes ŷ = θ₀ + θ₁X + θ₂X². Even though the equation is quadratic in X, it\u0026rsquo;s linear in terms of the parameters θ₀, θ₁, θ₂ and the features X and X². Scikit-Learn\u0026rsquo;s PolynomialFeatures (page 129): This class transforms your data. from sklearn.preprocessing import PolynomialFeatures poly_features = PolynomialFeatures(degree=2, include_bias=False)\ndegree=2: We want to add 2nd-degree polynomial features (i.e., squares). include_bias=False: PolynomialFeatures can add a column of 1s (for the bias term), but LinearRegression handles that, so we set it to False to avoid redundancy. X_poly = poly_features.fit_transform(X) If X[0] was [-0.75], then X_poly[0] becomes [-0.75, 0.566] (original X, and X²). Train a Linear Model on Extended Data: Now, you just fit a standard LinearRegression model to this X_poly: lin_reg = LinearRegression() lin_reg.fit(X_poly, y) The model predictions are shown in Figure 4-13 (page 130). It\u0026rsquo;s a nice curve that fits the data much better than a straight line! The estimated equation (e.g., ŷ = 0.56x₁² + 0.93x₁ + 1.78) is close to the original y = 0.5x₁² + 1.0x₁ + 2.0 + noise. Multiple Features and Combinations (page 130): If you have multiple features (say, a and b) and use PolynomialFeatures(degree=3), it will add a², a³, b², b³, AND also the combination terms like ab, a²b, ab². This allows the model to find relationships between features.\nWarning: Combinatorial Explosion! (Scorpion icon, page 130) The number of features can explode if you have many original features and use a high polynomial degree. The formula is (n+d)! / (d!n!) where n is original features, d is degree. Be careful! This can make the model very slow and prone to overfitting.\nLearning Curves With high-degree Polynomial Regression, you can fit the training data very well, maybe too well. Figure 4-14 (page 131) shows: A 300-degree polynomial model: Wiggles wildly to hit every training point. This is severe overfitting. A plain linear model: Misses the curve. This is underfitting. A quadratic model (2nd degree): Fits best, which makes sense as the data was generated quadratically. But how do you know in general if your model is too complex (overfitting) or too simple (underfitting), especially if you don\u0026rsquo;t know the true underlying function?\nCross-Validation (from Chapter 2):\nIf model performs well on training data but poorly on cross-validation, it\u0026rsquo;s overfitting. If it performs poorly on both, it\u0026rsquo;s underfitting. Learning Curves (page 131):\nWhat they are: Plots of the model\u0026rsquo;s performance (e.g., RMSE) on the training set and the validation set, as a function of the training set size (or training iteration).\nHow to generate them: Train the model multiple times on different-sized subsets of the training set.\nThe book provides a plot_learning_curves function (page 132).\nLearning Curves for an Underfitting Model (e.g., plain Linear Regression on the quadratic data - Figure 4-15, page 132): Training error: Starts at 0 (perfect fit with 1-2 instances), then rises as more noisy/nonlinear data is added, eventually plateauing. Validation error: Starts very high (model trained on few instances generalizes poorly), then decreases as model learns from more examples, eventually plateauing close to the training error. Key characteristics of underfitting curves: Both curves plateau, they are close together, and the error is fairly high. What it\u0026rsquo;s ultimately trying to achieve: The plot tells you that the model is too simple. Adding more training examples will not help if the curves have plateaued (as the scorpion icon on page 133 says). You need a more complex model or better features. Learning Curves for an Overfitting Model (e.g., 10th-degree polynomial on the quadratic data - Figure 4-16, page 133):\nTraining error: Much lower than the Linear Regression model\u0026rsquo;s training error. It fits the training data well. Validation error: There\u0026rsquo;s a gap between the training error and the validation error. The model performs significantly better on the training data than on the validation data. This is the hallmark of overfitting. If you had a much larger training set, these two curves would eventually get closer. What it\u0026rsquo;s ultimately trying to achieve: The plot tells you the model is too complex for the current amount of data. One way to improve an overfitting model is to feed it more training data (as the scorpion icon on page 134 suggests), until the validation error meets the training error (or use regularization, which we\u0026rsquo;ll see next). The Bias/Variance Trade-off (sidebar, page 134): This is a fundamental concept in statistics and ML. A model\u0026rsquo;s generalization error (how well it performs on unseen data) can be expressed as the sum of three components:\nBias: Error due to wrong assumptions made by the model. E.g., assuming data is linear when it\u0026rsquo;s quadratic. A high-bias model is likely to underfit. What it\u0026rsquo;s ultimately trying to achieve (in a bad way): It has a strong preconceived notion of how the data should look, and it sticks to it even if the data says otherwise. Variance: Error due to the model\u0026rsquo;s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (like a high-degree polynomial) can have high variance. A high-variance model is likely to overfit. It learns the noise in the training data, not just the signal. What it\u0026rsquo;s ultimately trying to achieve (in a bad way): It tries to fit every little nook and cranny of the training data, making it unstable and perform poorly on new, slightly different data. Irreducible Error: Error due to the inherent noisiness of the data itself. This part cannot be reduced by model changes; only by cleaning the data (e.g., fixing broken sensors, removing outliers). The Trade-off:\nIncreasing model complexity typically increases variance (more likely to overfit) and reduces bias (better fit to complex patterns). Decreasing model complexity typically increases bias (more likely to underfit) and reduces variance. The goal is to find a sweet spot, balancing bias and variance. Regularized Linear Models We saw that overfitting is a problem with complex models. Regularization is a way to reduce overfitting by constraining the model.\nWhat it\u0026rsquo;s ultimately trying to achieve: For linear models, this usually means constraining the model\u0026rsquo;s weights. The idea is to prevent the weights from becoming too large, which can happen when the model tries too hard to fit the noise in the training data. Smaller weights generally lead to simpler, smoother models that generalize better. The book discusses three types of regularized linear models: Ridge, Lasso, and Elastic Net.\nRidge Regression (also Tikhonov regularization) (page 135):\nIt adds a regularization term to the MSE cost function. Equation 4-8: Ridge Regression cost function J(θ) = MSE(θ) + α * (1/2) * Σᵢ (θᵢ)² (sum from i=1 to n, so bias term θ₀ is NOT regularized) The regularization term is α * (1/2) * Σᵢ (θᵢ)². This is α/2 times the sum of the squares of the feature weights. This is related to the ℓ₂ norm of the weight vector w = [θ₁, ..., θₙ], specifically (1/2) * ||w||₂². α (alpha) is a hyperparameter that controls how much you want to regularize. If α = 0, it\u0026rsquo;s just Linear Regression. If α is very large, all weights θᵢ (for i\u0026gt;0) end up very close to zero, and the model becomes a flat line through the data\u0026rsquo;s mean. What it\u0026rsquo;s ultimately trying to achieve: The learning algorithm is now forced to not only fit the data (minimize MSE) but also keep the model weights as small as possible. Important: Scale the data (e.g., StandardScaler) before performing Ridge Regression, as it\u0026rsquo;s sensitive to the scale of input features (scorpion icon, page 136). This is true for most regularized models. Figure 4-17 (page 136) shows the effect of α: Left: Plain Ridge Regression on linear data. Higher α makes predictions flatter. Right: Polynomial features (degree=10) + scaling + Ridge. Increasing α leads to flatter (less extreme, more reasonable) predictions, reducing variance but increasing bias. Training Ridge Regression: Closed-form solution (Equation 4-9, page 137): θ̂ = (XᵀX + αA)⁻¹ Xᵀy (where A is an identity matrix with a 0 at the top-left for the bias term). Scikit-Learn\u0026rsquo;s Ridge(alpha=..., solver=\u0026quot;cholesky\u0026quot;) uses this. Gradient Descent (page 137): Add αw to the MSE gradient vector (where w is the vector of weights, excluding bias). Scikit-Learn\u0026rsquo;s SGDRegressor(penalty=\u0026quot;l2\u0026quot;) does this. The \u0026quot;l2\u0026quot; means add a regularization term equal to half the square of the ℓ₂ norm of the weights. Lasso Regression (Least Absolute Shrinkage and Selection Operator) (page 137):\nAlso adds a regularization term to the MSE cost function, but uses the ℓ₁ norm of the weight vector. Equation 4-10: Lasso Regression cost function J(θ) = MSE(θ) + α * Σᵢ |θᵢ| (sum from i=1 to n) The regularization term is α times the sum of the absolute values of the weights. Figure 4-18 (page 138) shows Lasso models, similar to Figure 4-17 for Ridge, but with smaller α values. Key characteristic of Lasso: It tends to completely eliminate the weights of the least important features (i.e., set them to zero). It automatically performs feature selection and outputs a sparse model (a model with few non-zero feature weights). Why does Lasso do this? (Figure 4-19, page 139): This is a bit more advanced, but the intuition comes from looking at the \u0026ldquo;shape\u0026rdquo; of the ℓ₁ penalty vs. the ℓ₂ penalty. The ℓ₁ penalty |θ₁| + |θ₂| has \u0026ldquo;corners\u0026rdquo; along the axes in the parameter space. When minimizing the combined MSE + ℓ₁ penalty, the optimization path often hits these corners, forcing one of the parameters to zero. The ℓ₂ penalty θ₁² + θ₂² is circular. The optimization path approaches the origin smoothly, shrinking weights but not usually making them exactly zero. What it\u0026rsquo;s ultimately trying to achieve: The ℓ₁ penalty encourages sparsity by pushing less important feature weights all the way to zero. Training Lasso Regression: The Lasso cost function is not differentiable at θᵢ = 0. However, Gradient Descent can still work if you use a subgradient vector (Equation 4-11, page 140). This is a technical detail; the main idea is that an iterative approach can still find the minimum. Scikit-Learn: Lasso(alpha=...) or SGDRegressor(penalty=\u0026quot;l1\u0026quot;). When using Lasso with GD, you often need to gradually reduce the learning rate to help it converge without bouncing around the optimum (due to the \u0026ldquo;sharp corners\u0026rdquo; of the ℓ₁ penalty). Elastic Net (page 140):\nA middle ground between Ridge and Lasso. Its regularization term is a simple mix of both Ridge (ℓ₂) and Lasso (ℓ₁) terms. Equation 4-12: Elastic Net cost function J(θ) = MSE(θ) + rα Σᵢ|θᵢ| + ((1-r)/2)α Σᵢθᵢ² r is the mix ratio. If r = 0, Elastic Net is equivalent to Ridge. If r = 1, Elastic Net is equivalent to Lasso. Scikit-Learn: ElasticNet(alpha=..., l1_ratio=...) (where l1_ratio is r). When to use which? (page 140):\nPlain Linear Regression (no regularization) should generally be avoided. Some regularization is almost always better. Ridge is a good default. If you suspect only a few features are useful, prefer Lasso or Elastic Net because they perform feature selection by shrinking useless feature weights to zero. Elastic Net is generally preferred over Lasso because Lasso can behave erratically when the number of features is greater than the number of training instances, or when several features are strongly correlated. Elastic Net is more stable in these cases. Early Stopping (page 141):\nA very different way to regularize iterative learning algorithms like Gradient Descent. The Idea: Stop training as soon as the validation error reaches a minimum. Figure 4-20 (page 141) shows a complex model (high-degree polynomial) being trained with Batch GD: As epochs go by, training error (RMSE) goes down. Validation error also goes down initially, but then starts to go back up. This indicates the model has started to overfit. With early stopping, you just stop training when the validation error is at its minimum. It\u0026rsquo;s a simple and efficient regularization technique. Geoffrey Hinton called it a \u0026ldquo;beautiful free lunch.\u0026rdquo; Implementation (page 142): Loop for epochs. In each epoch, train the model (e.g., SGDRegressor with warm_start=True so it continues training, max_iter=1 so it does one epoch). Evaluate on validation set. If validation error is less than current minimum_val_error, save the model (clone it) and update minimum_val_error and best_epoch. After the loop, the best_model is your regularized model. With SGD/Mini-batch GD, validation curves are noisy. You might stop only after validation error has been above minimum for a while, then roll back to the best model. Fantastic! It\u0026rsquo;s great that the intuition behind L1 and L2 regularization clicked. Let\u0026rsquo;s continue our journey through Chapter 4, moving on to models designed for classification.\nLogistic Regression We\u0026rsquo;ve seen Linear Regression for predicting continuous values. Now, what if we want to predict a class? For example, is an email spam or not spam? This is a binary classification problem.\nThe book points out (as we saw in Chapter 1) that some regression algorithms can be adapted for classification. Logistic Regression (also called Logit Regression) is a prime example.\nWhat it\u0026rsquo;s ultimately trying to achieve: Logistic Regression estimates the probability that an instance belongs to a particular class (typically the \u0026ldquo;positive\u0026rdquo; class, labeled \u0026lsquo;1\u0026rsquo;). If this probability is greater than a certain threshold (usually 50%), the model predicts \u0026lsquo;1\u0026rsquo;; otherwise, it predicts \u0026lsquo;0\u0026rsquo;.\nEstimating Probabilities (Page 143):\nLinear Combination: Just like Linear Regression, it first computes a weighted sum of the input features plus a bias term: t = xᵀθ. (This t is often called the logit). Logistic Function (Sigmoid): Instead of outputting t directly, it passes t through the logistic function (also called the sigmoid function), denoted as σ(t). Equation 4-13: Logistic Regression model estimated probability (vectorized form) p̂ = h_θ(x) = σ(xᵀθ) where p̂ (p-hat) is the estimated probability that the instance x belongs to the positive class. Equation 4-14: Logistic function σ(t) = 1 / (1 + exp(-t)) Figure 4-21 (page 143) shows the characteristic S-shape of the sigmoid function. What the sigmoid is ultimately trying to achieve: It squashes any input value t (which can range from -∞ to +∞) into an output value between 0 and 1. This output can then be interpreted as a probability. If t is large and positive, exp(-t) is close to 0, so σ(t) is close to 1. If t is large and negative, exp(-t) is very large, so σ(t) is close to 0. If t = 0, exp(-t) = 1, so σ(t) = 1/2 = 0.5. Making Predictions (Page 143):\nEquation 4-15: Logistic Regression model prediction ŷ = 0 if p̂ \u0026lt; 0.5 ŷ = 1 if p̂ ≥ 0.5 Since σ(t) ≥ 0.5 when t ≥ 0, the model predicts 1 if xᵀθ (the logit) is positive, and 0 if it\u0026rsquo;s negative. Training and Cost Function (Page 144):\nObjective: We want to set the parameter vector θ so that the model estimates a high probability for positive instances (actual y=1) and a low probability for negative instances (actual y=0). Cost Function for a Single Instance (Equation 4-16): c(θ) = -log(p̂) if y = 1 c(θ) = -log(1 - p̂) if y = 0 What this cost function is ultimately trying to achieve: If y=1 (actual is positive): If model predicts p̂ close to 1 (correct), -log(p̂) is close to 0 (low cost). If model predicts p̂ close to 0 (incorrect), -log(p̂) is very large (high cost). If y=0 (actual is negative): If model predicts p̂ close to 0 (so 1-p̂ is close to 1, correct), -log(1-p̂) is close to 0 (low cost). If model predicts p̂ close to 1 (so 1-p̂ is close to 0, incorrect), -log(1-p̂) is very large (high cost). This cost function penalizes the model heavily when it\u0026rsquo;s confident and wrong. Cost Function for the Whole Training Set (Log Loss - Equation 4-17): J(θ) = - (1/m) * Σᵢ [ y⁽ⁱ⁾log(p̂⁽ⁱ⁾) + (1 - y⁽ⁱ⁾)log(1 - p̂⁽ⁱ⁾) ] This is just the average cost over all training instances. It\u0026rsquo;s a single, clever expression that combines the two cases from Equation 4-16. Good news: This log loss cost function is convex. So, Gradient Descent (or other optimization algorithms) can find the global minimum. Bad news: There\u0026rsquo;s no closed-form solution (like the Normal Equation for Linear Regression) to find the θ that minimizes this cost function. We must use an iterative optimization algorithm like Gradient Descent. Partial Derivatives (Equation 4-18, page 145): ∂J(θ)/∂θⱼ = (1/m) * Σᵢ (σ(θᵀx⁽ⁱ⁾) - y⁽ⁱ⁾) * xⱼ⁽ⁱ⁾ This looks very similar to the partial derivative for Linear Regression\u0026rsquo;s MSE (Equation 4-5)! σ(θᵀx⁽ⁱ⁾) is the predicted probability p̂⁽ⁱ⁾. (p̂⁽ⁱ⁾ - y⁽ⁱ⁾) is the prediction error. This error is multiplied by the feature value xⱼ⁽ⁱ⁾ and averaged. Once you have these gradients, you can use Batch GD, Stochastic GD, or Mini-batch GD to find the optimal θ. Decision Boundaries (Page 145-147): Let\u0026rsquo;s use the Iris dataset to illustrate. We\u0026rsquo;ll try to classify Iris virginica based only on petal width.\nLoad data: X = iris[\u0026quot;data\u0026quot;][:, 3:] (petal width), y = (iris[\u0026quot;target\u0026quot;] == 2).astype(int) (1 if virginica, else 0). Train LogisticRegression(): log_reg = LogisticRegression() log_reg.fit(X, y) Figure 4-23 (page 146): Shows estimated probabilities vs. petal width. The S-shape is clear. For petal widths \u0026gt; ~2cm, probability of being Iris virginica is high. For petal widths \u0026lt; ~1cm, probability is low. The decision boundary (where p̂ = 0.5) is around 1.6 cm. If petal width \u0026gt; 1.6cm, it predicts virginica; otherwise, not virginica. Figure 4-24 (page 147): Shows decision boundary using two features (petal width and petal length). The dashed line is where the model estimates 50% probability – this is the linear decision boundary. Other parallel lines show other probability contours (e.g., 15%, 90%). Regularization: Logistic Regression models in Scikit-Learn use ℓ₂ regularization by default. The hyperparameter is C (inverse of α): higher C means less regularization. Softmax Regression Logistic Regression is for binary classification. What if we have more than two classes, and we want a model that handles them directly (not with OvR/OvO strategies)? Enter Softmax Regression (or Multinomial Logistic Regression).\nThe Idea:\nFor a given instance x, compute a score sₖ(x) for each class k. This is done just like Linear Regression: sₖ(x) = xᵀθ⁽ᵏ⁾ (Equation 4-19). Each class k has its own dedicated parameter vector θ⁽ᵏ⁾. These are often stored as rows in a parameter matrix Θ. Estimate the probability p̂ₖ that the instance belongs to class k by applying the softmax function (also called normalized exponential) to the scores. Equation 4-20: Softmax function p̂ₖ = σ(s(x))ₖ = exp(sₖ(x)) / Σⱼ exp(sⱼ(x)) (sum over all classes j=1 to K) What it\u0026rsquo;s ultimately trying to achieve: It takes a vector of arbitrary scores s(x) for all classes, computes the exponential of each score (making them all positive), and then normalizes them by dividing by their sum. The result is a set of probabilities (p̂ₖ) that are all between 0 and 1 and sum up to 1 across all classes. The class with the highest initial score sₖ(x) will get the highest probability p̂ₖ. Prediction (Equation 4-21, page 149): The classifier predicts the class k that has the highest estimated probability p̂ₖ (which is simply the class with the highest score sₖ(x)). ŷ = argmaxₖ p̂ₖ\nSoftmax Regression predicts only one class at a time (mutually exclusive classes). It\u0026rsquo;s multiclass, not multioutput. Training and Cost Function (Cross Entropy - page 149): Objective: Estimate a high probability for the target class and low probabilities for other classes. Cost Function (Cross Entropy - Equation 4-22): J(Θ) = - (1/m) * Σᵢ Σₖ yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) yₖ⁽ⁱ⁾ is the target probability that instance i belongs to class k (usually 1 if it\u0026rsquo;s the target class, 0 otherwise). p̂ₖ⁽ⁱ⁾ is the model\u0026rsquo;s estimated probability that instance i belongs to class k. What it\u0026rsquo;s ultimately trying to achieve: This cost function penalizes the model when it estimates a low probability for the actual target class. It\u0026rsquo;s a common measure for how well a set of estimated class probabilities matches the target classes. The sidebar on \u0026ldquo;Cross Entropy\u0026rdquo; (page 150) gives some information theory background – it measures the average number of bits needed to encode events based on your probability estimates vs. the true probabilities. Lower is better. When there are only two classes (K=2), this cross-entropy cost function is equivalent to the log loss for Logistic Regression. Gradient Vector (Equation 4-23, page 150): ∇_θ⁽ᵏ⁾ J(Θ) = (1/m) * Σᵢ (p̂ₖ⁽ⁱ⁾ - yₖ⁽ⁱ⁾) * x⁽ⁱ⁾ This gives the gradient for the parameter vector θ⁽ᵏ⁾ of a specific class k. Again, very similar form to previous gradient equations! You compute this for every class, then use an optimization algorithm (like GD) to find the parameter matrix Θ that minimizes the cost. Using Softmax Regression in Scikit-Learn (page 150):\nLogisticRegression can perform Softmax Regression by setting: multi_class=\u0026quot;multinomial\u0026quot; solver=\u0026quot;lbfgs\u0026quot; (or another solver that supports multinomial, like \u0026ldquo;sag\u0026rdquo; or \u0026ldquo;newton-cg\u0026rdquo;) It also applies ℓ₂ regularization by default (controlled by C). Example: Classify Iris flowers into all 3 classes using petal length and width. X = iris[\u0026quot;data\u0026quot;][:, (2, 3)] y = iris[\u0026quot;target\u0026quot;] softmax_reg = LogisticRegression(multi_class=\u0026quot;multinomial\u0026quot;, solver=\u0026quot;lbfgs\u0026quot;, C=10) softmax_reg.fit(X, y) To predict: softmax_reg.predict([[5, 2]]) might give array([2]) (Iris virginica). To get probabilities: softmax_reg.predict_proba([[5, 2]]) might give [[6.3e-07, 0.057, 0.942]], meaning 94.2% prob for class 2 (virginica), 5.8% for class 1 (versicolor), and near 0% for class 0 (setosa). Figure 4-25 (page 151) shows the decision boundaries. They are linear between any two classes. And that brings us to the end of the core content of Chapter 4! We\u0026rsquo;ve gone from simple Linear Regression to Polynomial Regression, learned about diagnosing model fit with learning curves, seen how to regularize models to prevent overfitting (Ridge, Lasso, Elastic Net, Early Stopping), and finally explored Logistic and Softmax Regression for classification tasks.\nThe recurring theme for training, especially for models without closed-form solutions, is defining a cost function that captures how \u0026ldquo;bad\u0026rdquo; our model is, and then using an iterative algorithm like Gradient Descent to find the model parameters that minimize this cost. The specific form of the cost function and how predictions are made change from model to model, but the underlying optimization principle is often very similar.\nThis chapter is dense, but the concepts are absolutely key for understanding how models actually learn. How are you feeling about Logistic and Softmax Regression? Any particular part of \u0026ldquo;what they are trying to achieve\u0026rdquo; that needs more light?\nGlossary Difference between L1 (Lasso) and L2 (Ridge) regularization! Let\u0026rsquo;s break down that intuition about the \u0026ldquo;shapes\u0026rdquo; and \u0026ldquo;corners.\u0026rdquo; Imagine we have a model with just two weights, θ₁ and θ₂. We want to find the values of θ₁ and θ₂ that minimize our main cost function (let\u0026rsquo;s say MSE), subject to some penalty on the size of these weights.\nVisualizing the Penalties (Loss Functions for Weights)\nThink of the penalty term as defining a \u0026ldquo;budget\u0026rdquo; or a \u0026ldquo;constraint region\u0026rdquo; for our weights. The optimization process is trying to find the best MSE it can, while staying within or close to this budget defined by the penalty.\nL₂ Penalty (Ridge): θ₁² + θ₂² ≤ C\nThe equation θ₁² + θ₂² = constant describes a circle (or a sphere/hypersphere in higher dimensions) centered at the origin (θ₁=0, θ₂=0). So, the L2 penalty encourages the weights (θ₁, θ₂) to stay within a circular region around the origin. Shape: Smooth and round. No sharp corners. Imagine the contours of this penalty function: they are concentric circles. L₁ Penalty (Lasso): |θ₁| + |θ₂| ≤ C\nThe equation |θ₁| + |θ₂| = constant describes a diamond (or a rotated square in 2D, and a similar shape with \u0026ldquo;pointy\u0026rdquo; corners on the axes in higher dimensions). So, the L1 penalty encourages the weights (θ₁, θ₂) to stay within a diamond-shaped region around the origin. Shape: Has sharp corners that lie on the axes. For our 2D example, the corners are at points like (C, 0), (-C, 0), (0, C), and (0, -C). Imagine the contours of this penalty function: they are concentric diamonds. Visualizing the Optimization Process (Figure 4-19)\nNow, let\u0026rsquo;s consider the main cost function, the MSE. The contours of the MSE (if we ignore the penalty for a moment) are typically elliptical. The center of these ellipses is the point where MSE is minimized without any regularization – let\u0026rsquo;s call this the \u0026ldquo;unconstrained optimum.\u0026rdquo;\nThe regularization process is trying to find a point that: a. Is on the lowest possible MSE contour (meaning good fit to the data). b. Satisfies the \u0026ldquo;budget\u0026rdquo; imposed by the L1 or L2 penalty (meaning weights are small).\nThis can be visualized as finding the point where an MSE ellipse first \u0026ldquo;touches\u0026rdquo; the boundary of the penalty region (the circle for L2, the diamond for L1).\nWith L₂ Penalty (Ridge - bottom-right plot in Figure 4-19):\nImagine an expanding MSE ellipse (as we try to get lower MSE) until it just kisses the circular L2 penalty region. Because the circle is smooth and round, the point where the ellipse touches it can be anywhere on the circle\u0026rsquo;s boundary. It\u0026rsquo;s very unlikely that this touch point will be exactly on an axis (e.g., where θ₁ = 0 or θ₂ = 0). Result: The L2 penalty shrinks both θ₁ and θ₂ towards zero, but it\u0026rsquo;s rare for either of them to become exactly zero. The optimization path (white dots) goes smoothly towards a point where both weights are small but likely non-zero. With L₁ Penalty (Lasso - top-right plot in Figure 4-19):\nImagine an expanding MSE ellipse until it just kisses the diamond-shaped L1 penalty region. Because the diamond has sharp corners on the axes, it\u0026rsquo;s much more likely that the ellipse will first touch the diamond at one of these corners. If it touches at a corner on, say, the θ₁-axis, then at that point, θ₂ will be exactly zero. Result: The L1 penalty also shrinks weights towards zero, but because of its shape, it often forces some weights to become exactly zero. This is the \u0026ldquo;feature selection\u0026rdquo; property. The optimization path (white dots) is shown hitting the θ₂=0 axis and then \u0026ldquo;rolling down the gutter\u0026rdquo; along that axis. Why the \u0026ldquo;Corners\u0026rdquo; Matter for Hitting Zero:\nThink about the gradient (the direction of steepest descent) of the penalty term itself:\nL₂ Penalty: The gradient of θ₁² + θ₂² always points directly towards the origin. It smoothly pulls all weights smaller. L₁ Penalty: The gradient of |θ₁| + |θ₂| is a bit trickier. If θ₁ \u0026gt; 0, the derivative w.r.t. θ₁ is 1. If θ₁ \u0026lt; 0, the derivative w.r.t. θ₁ is -1. If θ₁ = 0, it\u0026rsquo;s undefined (this is the \u0026ldquo;corner\u0026rdquo;). This means the L1 penalty applies a constant \u0026ldquo;force\u0026rdquo; (either +1 or -1) pulling a weight towards zero, regardless of how small the weight already is (as long as it\u0026rsquo;s not zero). Once a weight hits zero, this \u0026ldquo;force\u0026rdquo; can effectively keep it there if the pull from the MSE term isn\u0026rsquo;t strong enough to move it away. In contrast, for L2, the \u0026ldquo;force\u0026rdquo; 2θ₁ pulling θ₁ towards zero gets weaker as θ₁ gets smaller. So, it might get very close to zero but not quite hit it unless the MSE term also strongly favors zero. Intuitive Summary:\nL₂ (Ridge) is like saying: \u0026ldquo;Try to make your weights small, and distribute the \u0026lsquo;smallness\u0026rsquo; somewhat evenly. Keep them inside this round ball.\u0026rdquo; It pulls weights towards zero but doesn\u0026rsquo;t aggressively zero them out. L₁ (Lasso) is like saying: \u0026ldquo;Try to make your weights small, and if possible, make some of them exactly zero. Keep them inside this pointy diamond. It\u0026rsquo;s okay to focus your \u0026lsquo;budget\u0026rsquo; on just a few important weights and eliminate the others.\u0026rdquo; The \u0026ldquo;corners\u0026rdquo; of the L1 penalty region are key because they represent solutions where one or more weights are exactly zero. The optimization process, in trying to balance minimizing MSE with staying within the L1 budget, often finds its optimal solution precisely at one of these corners.\nDoes that help clarify why the shape of the penalty (round vs. pointy with corners on axes) leads to these different behaviors in terms of zeroing out weights?\nOkay, absolutely! Let\u0026rsquo;s revisit Softmax Regression (pages 148-150) and really focus on what it\u0026rsquo;s ultimately trying to achieve, especially in contrast to just using multiple binary Logistic Regressions.\nYou\u0026rsquo;re spot on: Logistic Regression is great for \u0026ldquo;is it A or not A?\u0026rdquo; (binary). But what if you have \u0026ldquo;is it A, B, C, or D?\u0026rdquo; (multiclass).\nThe Problem with Just Using Multiple Binary Classifiers (OvR/OvO):\nWe saw in Chapter 3 that you can use binary classifiers for multiclass problems:\nOne-vs-Rest (OvR): Train a separate binary Logistic Regression for each class.\nClassifier 1: \u0026ldquo;Is it class A (vs. B, C, D)?\u0026rdquo; Classifier 2: \u0026ldquo;Is it class B (vs. A, C, D)?\u0026rdquo; \u0026hellip;and so on. To make a prediction, you run your input through all these binary classifiers and pick the class whose classifier gives the highest \u0026ldquo;confidence\u0026rdquo; score (or probability, if the binary classifier outputs that). One-vs-One (OvO): Train a binary classifier for every pair of classes (A vs B, A vs C, A vs D, B vs C, etc.). Pick the class that wins the most \u0026ldquo;duels.\u0026rdquo;\nLimitations/Quirks of OvR/OvO for Probabilities:\nWhile these strategies work for getting a class label, there\u0026rsquo;s a slight awkwardness if you want well-calibrated probabilities for each class that naturally sum to 1.\nWith OvR, each binary Logistic Regression outputs a probability for its class versus all others. For example, P(A | not A). These probabilities from different classifiers aren\u0026rsquo;t inherently guaranteed to sum to 1 when you look across all classes for a single instance. You might get P(A)=0.7, P(B)=0.4, P(C)=0.1. These don\u0026rsquo;t sum to 1, and it\u0026rsquo;s not immediately clear how to turn them into a proper probability distribution over A, B, and C. You usually just pick the class with the highest score. Softmax Regression: The \u0026ldquo;Direct\u0026rdquo; Multiclass Probabilistic Approach Softmax Regression is designed from the ground up to handle multiple classes directly and produce a consistent set of probabilities that sum to 1 across all classes.\nHere\u0026rsquo;s the core idea and \u0026ldquo;what it\u0026rsquo;s trying to achieve\u0026rdquo;:\nGoal: For any given input instance (e.g., an image of a digit), we want to output a probability for each possible class (e.g., P(digit is 0), P(digit is 1), \u0026hellip;, P(digit is 9)). Critically, these probabilities should all add up to 100%.\nStep 1: Calculate a \u0026ldquo;Score\u0026rdquo; for Each Class (Equation 4-19)\nJust like Linear Regression or Logistic Regression, for each class k, Softmax Regression calculates a linear score: sₖ(x) = xᵀθ⁽ᵏ⁾ x is the input feature vector. θ⁽ᵏ⁾ (theta-k) is a separate vector of weights specifically for class k. So, if you have 10 classes (digits 0-9), you will have 10 different θ vectors. What these scores sₖ(x) are ultimately trying to achieve: They are like raw \u0026ldquo;evidence\u0026rdquo; or \u0026ldquo;suitability scores\u0026rdquo; for each class, given the input x. A higher score sₖ(x) suggests that class k is a more likely candidate for this input. These scores can be any real number (positive, negative, large, small). Step 2: Convert Scores into Probabilities (The Softmax Function - Equation 4-20)\nThe raw scores sₖ(x) are not probabilities yet (they don\u0026rsquo;t sum to 1, and they can be negative). We need a way to transform them into a valid probability distribution. This is where the softmax function (also called \u0026ldquo;normalized exponential\u0026rdquo;) comes in. For each class k, the probability p̂ₖ is calculated as: p̂ₖ = exp(sₖ(x)) / Σⱼ exp(sⱼ(x)) (where the sum in the denominator is over all possible classes j) What the softmax function is ultimately trying to achieve: exp(sₖ(x)) (Exponential): First, it takes the exponential of each score. This has two effects: It makes all scores positive (since e to any power is positive). It tends to exaggerate differences: if score A is slightly higher than score B, exp(A) will be significantly higher than exp(B). The \u0026ldquo;softmax\u0026rdquo; is \u0026ldquo;soft\u0026rdquo; in that it doesn\u0026rsquo;t just pick the max score and give it 100% probability, but it does give more weight to higher scores. Σⱼ exp(sⱼ(x)) (Sum of Exponentials): It then sums up these positive, exponentiated scores for all classes. This sum acts as a normalization constant. Division: Dividing each exp(sₖ(x)) by this total sum ensures two things: Each p̂ₖ will be between 0 and 1. All the p̂ₖ values will sum up to 1. So, we get a proper probability distribution across all classes. The class that had the highest initial score sₖ(x) will end up with the largest probability p̂ₖ. Analogy for Softmax:\nImagine you have several candidates for a job (the classes).\nYou give each candidate a raw \u0026ldquo;suitability score\u0026rdquo; (the sₖ(x)). Some might be high, some low, some even negative if they seem really unsuitable. To decide how to allocate a \u0026ldquo;probability of being hired\u0026rdquo; that sums to 100% across all candidates: You first want to ensure everyone\u0026rsquo;s considered \u0026ldquo;positively\u0026rdquo; and amplify the scores of strong candidates: you \u0026ldquo;exponentiate\u0026rdquo; their scores. A candidate with a score of 3 becomes e³ ≈ 20, while a candidate with a score of 1 becomes e¹ ≈ 2.7. The difference is magnified. Then, you add up all these amplified, positive scores to get a \u0026ldquo;total pool of amplified suitability.\u0026rdquo; Finally, each candidate\u0026rsquo;s share of this total pool becomes their probability of being hired. Why is this better than just running multiple OvR Logistic Regressions for probabilities?\nDirect Probabilistic Interpretation: Softmax directly outputs a set of probabilities that are inherently linked and sum to 1. It\u0026rsquo;s designed for this purpose. With OvR Logistic Regression, you\u0026rsquo;d have to do some extra (potentially ad-hoc) normalization step if you wanted the \u0026ldquo;probabilities\u0026rdquo; from different binary classifiers to sum to 1 for a given instance. Shared Information During Training (via the cost function): When Softmax Regression is trained (using the cross-entropy cost function, which we\u0026rsquo;ll get to), the updates to the weights θ⁽ᵏ⁾ for one class are influenced by the scores and target probabilities of all other classes because of that denominator in the softmax function. This allows the model to learn the relationships and distinctions between all classes simultaneously in a more coupled way. With independent OvR classifiers, each classifier only learns to distinguish its class from \u0026ldquo;everything else\u0026rdquo; without explicitly considering the fine-grained distinctions between those \u0026ldquo;everything else\u0026rdquo; classes. Making a Prediction and Training:\nPrediction (Equation 4-21): Once you have the probabilities p̂ₖ for all classes, you simply predict the class k that has the highest probability. This is equivalent to picking the class with the highest initial score sₖ(x). Training (Cross-Entropy Cost Function - Equation 4-22): The goal is to find the parameter matrix Θ (which contains all the θ⁽ᵏ⁾ vectors) such that for a given training instance, the predicted probability p̂ₖ is high for the correct class k and low for all other classes. The cross-entropy cost function is used for this. J(Θ) = - (1/m) * Σᵢ Σₖ yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) yₖ⁽ⁱ⁾ is 1 if class k is the true class for instance i, and 0 otherwise. What it\u0026rsquo;s ultimately trying to achieve: It heavily penalizes the model if log(p̂ₖ⁽ⁱ⁾) is a large negative number for the true class k (which happens if p̂ₖ⁽ⁱ⁾ is close to 0 for the true class). It wants to maximize the log-probability of the correct class. This cost function is convex, so Gradient Descent can find the optimal Θ. The gradients (Equation 4-23) are computed, and Θ is updated iteratively. In essence, Softmax Regression is a generalization of Logistic Regression to handle multiple classes by:\nCalculating a linear score for each class. Using the softmax function to convert these scores into a proper probability distribution over all classes. Being trained with a cost function (cross-entropy) that encourages high probabilities for correct classes. It provides a more principled and direct way to get multiclass probabilities compared to trying to combine multiple independent binary logistic regressions.\nFocus on the Cross-Entropy cost function Let\u0026rsquo;s break it down slowly, focusing on what it\u0026rsquo;s ultimately trying to achieve.\nRecall the equation: J(Θ) = - (1/m) * Σᵢ Σₖ yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾)\nWhere:\nJ(Θ): The total cost for our model parameters Θ. We want to minimize this. m: The number of training instances. (1/m) means we\u0026rsquo;re averaging the cost over all instances. Σᵢ: Sum over all training instances i (from 1 to m). Σₖ: Sum over all possible classes k (from 1 to K). yₖ⁽ⁱ⁾: This is the true target probability that instance i belongs to class k. For most classification problems (like digit recognition), this is a \u0026ldquo;one-hot encoded\u0026rdquo; value. If instance i is truly a \u0026lsquo;digit 3\u0026rsquo;, then: y₃⁽ⁱ⁾ = 1 (the probability of being class \u0026lsquo;3\u0026rsquo; is 100%) y₀⁽ⁱ⁾ = 0, y₁⁽ⁱ⁾ = 0, y₂⁽ⁱ⁾ = 0, y₄⁽ⁱ⁾ = 0, \u0026hellip;, y₉⁽ⁱ⁾ = 0 (the probability of being any other class is 0%). p̂ₖ⁽ⁱ⁾: This is the model\u0026rsquo;s predicted probability that instance i belongs to class k (this comes from the softmax function). log(p̂ₖ⁽ⁱ⁾): The natural logarithm of the model\u0026rsquo;s predicted probability. Understanding the Core Term: yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾)\nLet\u0026rsquo;s focus on a single instance i and a single class k.\nThe term yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) is the heart of it. Since yₖ⁽ⁱ⁾ is either 0 or 1 (for the one-hot encoded true label):\nCase 1: Class k is NOT the true class for instance i.\nThen yₖ⁽ⁱ⁾ = 0. So, yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) = 0 * log(p̂ₖ⁽ⁱ⁾) = 0. This means: For all the classes that are not the true class, this term contributes nothing to the sum Σₖ. This makes sense – we don\u0026rsquo;t directly care about the exact log-probability the model assigns to the incorrect classes, as long as it assigns a high probability to the correct class. Case 2: Class k IS the true class for instance i.\nThen yₖ⁽ⁱ⁾ = 1. So, yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) = 1 * log(p̂ₖ⁽ⁱ⁾) = log(p̂ₖ⁽ⁱ⁾). This means: For the one true class, this term contributes log(p̂ₖ⁽ⁱ⁾) to the sum Σₖ. So, for a single instance i, the inner sum Σₖ yₖ⁽ⁱ⁾ log(p̂ₖ⁽ⁱ⁾) simplifies to just log(p̂_true_class⁽ⁱ⁾). It\u0026rsquo;s the logarithm of the probability that the model assigned to the actual correct class for that instance.\nWhy log? And why the negative sign in J(Θ)?\nNow let\u0026rsquo;s consider the log and the overall negative sign in J(Θ) = - (1/m) * Σᵢ log(p̂_true_class⁽ⁱ⁾).\nProbabilities p̂ are between 0 and 1.\nThe logarithm of a number between 0 and 1 is always negative (or zero if p̂=1).\nlog(1) = 0 log(0.5) ≈ -0.69 log(0.1) ≈ -2.3 As p̂ approaches 0, log(p̂) approaches -∞. What the cost function is trying to achieve:\nWe want p̂_true_class⁽ⁱ⁾ to be as close to 1 as possible. This means our model is very confident and correct. If p̂_true_class⁽ⁱ⁾ = 1 (perfect prediction for this instance), then log(p̂_true_class⁽ⁱ⁾) = log(1) = 0. The negative sign makes it -0 = 0. This is the minimum possible cost for this instance. Good! We want to heavily penalize the model if p̂_true_class⁽ⁱ⁾ is close to 0. This means our model is very confident but wrong about the true class. If p̂_true_class⁽ⁱ⁾ is very small (e.g., 0.0001), then log(p̂_true_class⁽ⁱ⁾) is a large negative number (e.g., log(0.0001) ≈ -9.2). The negative sign in J(Θ) makes this - (large negative number) = large positive number. This is a very high cost. Good! This is what we want the cost function to do – penalize bad, confident predictions for the true class. So, minimizing J(Θ) = - Σ log(p̂_true_class) is equivalent to maximizing Σ log(p̂_true_class). And since log is a monotonically increasing function, maximizing the sum of log-probabilities is equivalent to maximizing the product of the probabilities (this is related to Maximum Likelihood Estimation, a very common principle in statistics and machine learning).\nIn simpler terms, the cross-entropy cost function is ultimately trying to:\nMake the model assign the highest possible probability to the correct class for each training example. It does this by looking at the logarithm of the predicted probability for the true class. A perfect score (probability 1 for the true class) results in a log-probability of 0, contributing zero to the (negative) cost. A very poor score (probability near 0 for the true class) results in a very large negative log-probability, which, when negated by the cost function, becomes a very large positive cost. The cost function therefore \u0026ldquo;rewards\u0026rdquo; the model for being confident and correct, and heavily \u0026ldquo;punishes\u0026rdquo; it for being confident and incorrect (or even just very uncertain about the correct class). Connection to Information Theory (as mentioned in the sidebar):\nCross-entropy is a concept from information theory.\nImagine you have a true distribution of events y (e.g., true digit classes). You have a model that produces a predicted distribution p̂. Cross-entropy H(y, p̂) measures the average number of bits you would need to encode an event drawn from the true distribution y, if you used an encoding scheme optimized for your predicted distribution p̂. If your predicted distribution p̂ perfectly matches the true distribution y, then the cross-entropy is minimized and is equal to the entropy of y itself (the inherent minimum bits needed to describe y). If p̂ is different from y, the cross-entropy will be higher. The difference H(y, p̂) - H(y) is called the Kullback-Leibler (KL) divergence, which measures how much your predicted distribution diverges from the true one. So, when we minimize the cross-entropy cost function, we are essentially trying to make our model\u0026rsquo;s predicted probability distribution p̂ as close as possible to the true underlying distribution y of the class labels.\nWhy is it better than, say, trying to minimize the squared error between p̂ and y for classification?\nRange of Output: p̂ from softmax is a probability (0 to 1). y is usually 0 or 1. Squared error (p̂ - y)² could work, but log loss (cross-entropy) has properties that often make optimization more stable and faster, especially for probabilities. Penalizing Confident Wrong Predictions: The log term in cross-entropy means that if the model predicts a very low probability (near 0) for the true class, the cost goes towards infinity. This is a very strong penalty. Squared error doesn\u0026rsquo;t penalize confident wrong predictions as harshly. For y=1, if p̂=0.01, squared error is (0.01-1)² ≈ 0.98. If p̂=0.0001, squared error is (0.0001-1)² ≈ 0.9998. The increase isn\u0026rsquo;t as dramatic as log(0.01) vs log(0.0001). Mathematical Properties for Gradient Descent: The gradients derived from the cross-entropy cost function for models like Softmax Regression (and logistic regression) have a nice, simple form (as seen in Equation 4-23), which is computationally convenient. This cost function is the workhorse for most classification problems where the model outputs probabilities. It effectively pushes the model to be accurate and appropriately confident in its predictions for the correct classes.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-4/","tags":null,"title":"Chapter 4: Training Models"},{"categories":null,"content":"The goal is simple but profound: to automatically group similar texts together based on their meaning. As Figure 5-1 illustrates, we start with a heap of unstructured documents and aim to produce clusters where documents about \u0026ldquo;pets\u0026rdquo; are in one group, \u0026ldquo;sports\u0026rdquo; in another, and \u0026ldquo;food\u0026rdquo; in a third. This is immensely powerful for quick data exploration, finding unexpected themes, or even pre-categorizing data for later supervised tasks.\nAs we move from clustering to topic modeling, our goal refines slightly. We don\u0026rsquo;t just want the groups; we want a meaningful description for each group. Figure 5-2 shows this transition: the \u0026ldquo;pets\u0026rdquo; cluster is described by keywords like pet, dog, cat, etc. This gives us a human-understandable label for the abstract group.\nThis chapter is a masterclass in combining different types of models. We\u0026rsquo;ll see how modern embedding models (encoder-only), classical NLP techniques (bag-of-words), and even powerful generative models (decoder-only) can be chained together in a pipeline to create something truly impressive.\nArXiv\u0026rsquo;s Articles: Computation and Language To make this tangible, we need a dataset. The book chooses a fantastic one: a collection of 44,949 abstracts from ArXiv\u0026rsquo;s \u0026ldquo;Computation and Language\u0026rdquo; (cs.CL) section. This is perfect because it\u0026rsquo;s real, complex data, and it\u0026rsquo;s relevant to our field.\nLet\u0026rsquo;s start by loading the data, just as the book does.\n# Load data from Hugging Face from datasets import load_dataset dataset = load_dataset(\u0026#34;maartengr/arxiv_nlp\u0026#34;)[\u0026#34;train\u0026#34;] # Extract metadata abstracts = dataset[\u0026#34;abstracts\u0026#34;] titles = dataset[\u0026#34;titles\u0026#34;] We now have our collection of abstracts ready. Our mission is to find the hidden topics within them.\nA Common Pipeline for Text Clustering The book lays out a very common and effective three-step pipeline for modern text clustering. This is our foundational recipe:\nEmbed: Convert all documents into numerical vectors (embeddings). Reduce: Lower the dimensionality of these embeddings. Cluster: Group the low-dimensional points together. Let\u0026rsquo;s walk through each step.\nEmbedding Documents Our first step, shown in Figure 5-3, is to convert the 44,949 abstracts into 44,949 embedding vectors. What are we trying to achieve here? We want vectors where the distance between them reflects semantic similarity. Abstracts about \u0026ldquo;machine translation\u0026rdquo; should be close to each other in this vector space, and far from abstracts about \u0026ldquo;phonetics.\u0026rdquo;\nChoosing the right embedding model is key. The book wisely points to the MTEB (Massive Text Embedding Benchmark) leaderboard. For this task, we want a model that performs well on clustering and is reasonably fast. The book selects thenlper/gte-small, a great choice that balances performance and size.\nfrom sentence_transformers import SentenceTransformer # Create an embedding for each abstract embedding_model = SentenceTransformer(\u0026#34;thenlper/gte-small\u0026#34;) embeddings = embedding_model.encode(abstracts, show_progress_bar=True) After running this, let\u0026rsquo;s check the shape of our result.\n# Check the dimensions of the resulting embeddings embeddings.shape Output: (44949, 384)\nThis tells us we have 44,949 vectors, each with 384 dimensions. This is our high-dimensional representation of the dataset.\nReducing the Dimensionality of Embeddings Now, why do we need this step? It comes down to a concept data scientists know well: the curse of dimensionality. In high-dimensional spaces (like 384-D), distances become less meaningful, and everything seems far apart. This makes it very difficult for clustering algorithms to find dense regions.\nOur goal is to compress this 384-D space into a much lower-dimensional one (e.g., 5-D) while preserving as much of the global structure as possible. We want the points that were close in 384-D to remain close in 5-D. Figure 5-4 and Figure 5-5 visualize this compression perfectly.\nThe book chooses UMAP (Uniform Manifold Approximation and Projection) over the classic PCA. This is a deliberate choice. UMAP is excellent at preserving the non-linear structures and local/global relationships in the data, which is exactly what we need.\nfrom umap import UMAP # We reduce the input embeddings from 384 dimensions to 5 dimensions umap_model = UMAP(n_components=5, min_dist=0.0, metric=\u0026#39;cosine\u0026#39;, random_state=42) reduced_embeddings = umap_model.fit_transform(embeddings) Let\u0026rsquo;s break down these parameters, as a senior DS would:\nn_components=5: We\u0026rsquo;re projecting down to 5 dimensions. This is a sweet spot for capturing structure without being too high-dimensional for the clustering algorithm. min_dist=0.0: This encourages UMAP to create very tight, dense clusters, which is great for our purpose. metric='cosine': We use cosine similarity because it\u0026rsquo;s generally more effective than Euclidean distance for high-dimensional text embeddings. random_state=42: This makes our UMAP results reproducible. Cluster the Reduced Embeddings This is the final step in our pipeline, as shown in Figure 5-6. We now have 44,949 points in a 5-D space. Our goal is to assign a cluster label to each point.\nThe book makes another excellent point by contrasting centroid-based clustering (like k-means) with density-based clustering (Figure 5-7).\nK-means requires you to specify the number of clusters (k) beforehand. We don\u0026rsquo;t know k! It also forces every single point into a cluster. HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) is a much better fit. It automatically determines the number of clusters based on density and, crucially, it allows for outliers. It identifies points that don\u0026rsquo;t belong to any dense cluster and labels them as such (typically with -1). Since our ArXiv dataset likely contains many niche or unique papers, this ability to handle outliers is perfect. from hdbscan import HDBSCAN # We fit the model and extract the clusters hdbscan_model = HDBSCAN(min_cluster_size=50, metric=\u0026#34;euclidean\u0026#34;, cluster_selection_method=\u0026#34;eom\u0026#34;) .fit(reduced_embeddings) clusters = hdbscan_model.labels_ min_cluster_size=50 tells the algorithm to not even consider a group of points a cluster unless it has at least 50 members. This helps avoid tiny, noisy micro-clusters.\nWhen we check len(set(clusters)), we find we\u0026rsquo;ve generated 156 clusters.\nInspecting the Clusters Now we have our clusters, but what do they mean? The book shows how to do a quick manual inspection. Let\u0026rsquo;s look at the first three documents from Cluster 0.\nimport numpy as np # Print first three documents in cluster 0 cluster = 0 for index in np.where(clusters==cluster)[0][:3]: print(abstracts[index][:300] + \u0026#34;... \\n\u0026#34;) The output shows documents about \u0026ldquo;statistical machine translation from English text to American Sign Language (ASL)\u0026rdquo;. It seems Cluster 0 is about Sign Language Translation. This is the core of exploratory analysis!\nFor a bird\u0026rsquo;s-eye view, we can create the 2D scatter plot shown in Figure 5-8. We re-run UMAP, this time with n_components=2 for visualization, and plot the points, coloring them by their cluster ID. The gray points are the outliers identified by HDBSCAN.\nFrom Text Clustering to Topic Modeling Manually inspecting clusters is insightful but not scalable. This is where we transition to Topic Modeling. The goal is to automatically generate a representation for each cluster.\nInstead of a single label like \u0026ldquo;Sign Language,\u0026rdquo; traditional topic modeling (like Latent Dirichlet Allocation, or LDA) represents a topic as a distribution of words, as shown in Figures 5-9 and 5-10. The top words in the distribution act as the topic\u0026rsquo;s description.\nBERTopic: A Modular Topic Modeling Framework This is the star of the chapter. BERTopic is a brilliant framework that formalizes and extends the pipeline we just built.\nAs Figure 5-11 shows, the first part of BERTopic is exactly what we did: SBERT (Embed) -\u0026gt; UMAP (Reduce) -\u0026gt; HDBSCAN (Cluster).\nThe second part is where it gets clever. How does it represent the topics? It uses a classical NLP technique: c-TF-IDF. Let me break this down, because it\u0026rsquo;s the secret sauce.\nBag-of-Words: First, for all documents within a single cluster, it creates a giant bag-of-words. It\u0026rsquo;s not per-document, but per-cluster (Figure 5-13). This gives us the Term Frequency (TF) for each word within its class (c). This is the \u0026ldquo;c-TF\u0026rdquo; part. Inverse Document Frequency (IDF): It then calculates how common each word is across all clusters. Words that are common everywhere (like the, and, model) are uninformative and get a low IDF score. Words that are frequent in one cluster but rare elsewhere (like asr or phonetic) are very informative and get a high IDF score (Figure 5-14). c-TF-IDF Score: The final score for a word in a topic is c-TF * IDF. This score is high for words that are frequent within a topic but rare overall. These are the perfect keywords to describe the topic! The full BERTopic pipeline is shown in Figure 5-16. It\u0026rsquo;s this beautiful marriage of modern deep learning for semantic clustering and classic, interpretable TF-IDF for topic representation.\nThe true power of BERTopic, as emphasized in Figure 5-17, is its modularity. It\u0026rsquo;s built like Lego blocks. Don\u0026rsquo;t like HDBSCAN? Swap it for k-means. Have a better embedding model? Plug it in. This makes the framework incredibly powerful and future-proof.\nLet\u0026rsquo;s run it. We can even reuse the components we already trained.\nfrom bertopic import BERTopic # Train our model with our previously defined models topic_model = BERTopic( embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=True ).fit(abstracts, embeddings) We can now inspect the topics programmatically.\n# Get a summary of the topics topic_model.get_topic_info() This gives us a table with the topic ID, the number of documents (Count), and a default Name made from the top 4 keywords. We can see Topic -1 (the outliers), Topic 0 (speech_asr_recognition_end), Topic 1 (medical_clinical_biomedical_pa), and so on.\nAdding a Special Lego Block: Representation Models The default c-TF-IDF representation is great, but we can do even better. BERTopic allows us to add new \u0026ldquo;Lego blocks\u0026rdquo; on top of the pipeline to rerank or relabel the topics. This is the concept of a Representation Model (Figure 5-20).\nKeyBERTInspired This method improves the keyword-based representation. What\u0026rsquo;s the goal? To find keywords that are not just frequent, but are also semantically very similar to the overall meaning of the topic. As shown in Figure 5-22, it calculates the average embedding of all documents in a topic. Then, it finds candidate keywords (from c-TF-IDF) whose own embeddings are most similar to this average topic embedding. This cleans up the keyword list beautifully. The comparison table on page 34 shows the \u0026ldquo;Original\u0026rdquo; vs \u0026ldquo;Updated\u0026rdquo; keywords, and the updated ones are much more coherent.\nMaximal Marginal Relevance (MMR) What\u0026rsquo;s the goal here? To improve diversity. A topic might have keywords like summaries, summary, summarization. They are redundant. MMR is an algorithm that selects a list of keywords that are both relevant to the topic and diverse from each other. The table on page 36 clearly shows this—the updated topic for \u0026ldquo;summarization\u0026rdquo; is much more diverse and informative.\nThe Text Generation Lego Block This is the ultimate upgrade. Why just have a list of keywords when we can have a full, human-readable label? Our goal: use a generative LLM to create a short, descriptive title for each topic.\nThis is incredibly efficient. Instead of running the LLM on millions of documents, we run it just once per topic (156 times in our case). As Figure 5-23 shows, we construct a prompt for the LLM that includes the top keywords and a few representative documents from the topic, and then we ask it: \u0026ldquo;Based on this, what\u0026rsquo;s a short label for this topic?\u0026rdquo;\nThe book shows examples with both Flan-T5 and GPT-3.5. The results with GPT-3.5 (page 40) are stunningly good:\nTopic 0 (speech, asr, recognition) becomes \u0026ldquo;Leveraging External Data for Improving Low-Res\u0026hellip;\u0026rdquo; Topic 1 (medical, clinical) becomes \u0026ldquo;Improved Representation Learning for Biomedica\u0026hellip;\u0026rdquo; Topic 3 (translation, nmt) becomes \u0026ldquo;Neural Machine Translation\u0026rdquo; These are not just keywords; they are meaningful, high-quality topic labels.\nWith these amazing labels, we can now create the final, beautiful visualization from Figure 5-24, using the datamapplot package. It plots the documents and adds the generated labels, giving us an interpretable map of the entire dataset.\nSummary And that\u0026rsquo;s Chapter 5. We\u0026rsquo;ve gone on a complete journey through unsupervised learning with modern tools.\nWe started with a foundational three-step clustering pipeline: Embed, Reduce, and Cluster. We made deliberate, expert choices at each step: a strong SentenceTransformer model, UMAP for dimensionality reduction, and HDBSCAN for robust, density-based clustering.\nWe then saw how BERTopic elegantly packages this pipeline and enhances it with an interpretable c-TF-IDF representation to define topics.\nFinally, and most powerfully, we explored the modular \u0026ldquo;Lego block\u0026rdquo; nature of BERTopic by adding representation models to refine our topics. We used KeyBERTInspired and MMR to improve keyword quality and diversity, and we capped it all off by using a powerful generative LLM to create high-quality, human-readable labels.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-5/","tags":null,"title":"Chapter 5: Text Clustering and Topic Modeling"},{"categories":null,"content":"Chapter 5 Notes: Case Studies - Real-world ML Systems Overview This chapter presents detailed case studies of ML systems commonly found at major tech companies. Each case study walks through the complete system design process, from requirements gathering to deployment and monitoring.\nCase Studies Covered Recommendation System (Netflix, Amazon, YouTube) Search Ranking System (Google, Bing) Feed Ranking System (Facebook, Twitter, Instagram) Fraud Detection System (PayPal, Stripe) Ad Targeting System (Google Ads, Facebook Ads) Common Interview Questions Design a recommendation system for an e-commerce platform How would you build a real-time fraud detection system? Design a search ranking system for a social media platform Build a news feed ranking algorithm Design Patterns Collaborative Filtering: User-based and item-based approaches Content-Based Filtering: Feature extraction and similarity computation Hybrid Approaches: Combining multiple recommendation strategies Real-time vs Batch Processing: When to use each approach Key Takeaways Start with simple solutions and iterate Consider both technical and business requirements Think about scalability from the beginning Plan for monitoring and maintenance (Your detailed notes for Chapter 5 go here\u0026hellip;)\n","permalink":"https://deepskandpal.github.io/bookshelf/ml-design-interview/chapter-5/","tags":null,"title":"Chapter 5: Case Studies - Real-world ML Systems"},{"categories":null,"content":"This chapter covers the design of image captioning systems that generate natural language descriptions from visual content.\nKey Concepts Vision-Language Models: Connecting visual and textual representations Multimodal Architecture: Handling both image and text inputs Caption Quality: Balancing accuracy, creativity, and informativeness Main Topics Covered Image captioning system architecture Vision encoder and language decoder design Training pipeline for multimodal models Evaluation metrics and quality assessment Real-time inference optimization System Design Considerations Handling high-resolution images efficiently Supporting different image domains (photos, artwork, medical images) Multilingual caption generation Integration with content management systems (Your detailed notes for Chapter 5 go here\u0026hellip;)\n","permalink":"https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/chapter-5/","tags":null,"title":"Chapter 5: Image Captioning"},{"categories":null,"content":"What this chapter is ultimately trying to achieve To explain why \u0026ldquo;large\u0026rdquo; matters in language models, what \u0026ldquo;large\u0026rdquo; actually entails, and how these scaled-up pretrained models are then adapted (finetuned) to become useful for a wide range of tasks beyond just predicting the next token. We also delve into practical aspects like interacting with them (sampling, prompt engineering) and addressing their inherent limitations (hallucinations, ethics).\nLet\u0026rsquo;s break down the key sections:\n5.1 Why Larger Is Better What it\u0026rsquo;s ultimately trying to achieve: To establish that the remarkable abilities of LLMs (like understanding complex instructions, generating coherent long-form text, some forms of reasoning, few-shot learning) are not just incremental improvements but often emergent properties that arise when model size, data size, and compute cross certain thresholds.\nThe Core Idea (Scale is Key): Pretraining a Transformer (like the decoder we built in Chapter 4) on a massive dataset of text (trillions of tokens) with a huge number of parameters (billions to trillions) and a large context window (thousands to hundreds of thousands of tokens) allows it to learn intricate patterns, world knowledge, and even some rudimentary reasoning skills simply from the task of predicting the next token.\nThe example of CRISPR-Cas9 in the book illustrates this: to predict the next token accurately in a scientific text, the model must implicitly learn a lot about the underlying concepts. The \u0026ldquo;Large\u0026rdquo; Factors:\nLarge Parameter Count: Our decoder model had ~8 million parameters. Modern LLMs (Llama 3.1 70B, Gemma 2 27B) have billions. More parameters mean more capacity to store information, learn complex patterns, and represent nuances of language and knowledge. Large Context Size: Our decoder used 30 tokens. LLMs can handle thousands (e.g., GPT-3\u0026rsquo;s 2K-4K) to over a hundred thousand (e.g., Llama 3.1\u0026rsquo;s 128K, some models even 1M+). A larger context allows the model to understand and generate text that maintains coherence over much longer spans, remember earlier parts of a conversation or document, and tackle tasks requiring access to more information. Achieving this involves architectural improvements like grouped-query attention and FlashAttention, and specialized training stages like long-context pretraining. Large Training Dataset: Our RNN was trained on ~25 million tokens. LLMs are trained on trillions of tokens from diverse sources (books, web pages, code, academic papers, social media). This diversity exposes the model to a vast range of language styles, topics, and knowledge. Typically, due to the sheer size, models are trained for a single epoch. Large Amount of Compute: Training these models requires enormous computational resources (thousands of GPUs running for months, costing millions of dollars). This involves sophisticated parallelization techniques (tensor, pipeline, context, and data parallelism – 4D parallelism). 5.2 Supervised Finetuning (SFT) What it\u0026rsquo;s ultimately trying to achieve: To transform a base pretrained LLM (which is good at next-token prediction but not necessarily at following instructions) into a helpful and instruction-following assistant or a model specialized for specific tasks.\nThe Core Idea (Teaching to Behave): While pretraining gives the model its raw knowledge and language understanding, SFT teaches it the format of interaction.\nThe model is further trained on a smaller, high-quality dataset of instruction-response pairs (or dialogue turns). Examples: Instruction: \u0026ldquo;Translate \u0026lsquo;Good night\u0026rsquo; into Spanish.\u0026rdquo; Response: \u0026ldquo;Buenas noches.\u0026rdquo; Instruction: \u0026ldquo;Write a poem about a cat.\u0026rdquo; Response: \u0026ldquo;[A poem about a cat]\u0026rdquo; The model is still trained to predict the next token, but the \u0026ldquo;context\u0026rdquo; is now the instruction, and the \u0026ldquo;target\u0026rdquo; is the desired response. This process \u0026ldquo;unlocks\u0026rdquo; the pretrained knowledge and makes it accessible in a conversational or task-oriented way. The book shows the difference between a base gemma-2-2b (just completes text) and gemma-2-2b-it (instruction-tuned, follows the list continuation prompt). 5.3 Finetuning a Pretrained Model (Practical Example) What it\u0026rsquo;s ultimately trying to achieve: To walk through the practical steps of finetuning an existing open-weight LLM (like GPT-2 in the book\u0026rsquo;s example) for a specific task, such as emotion classification.\nKey Steps \u0026amp; Concepts:\nBaseline: Always good to establish a baseline with a simpler model (e.g., logistic regression with BoW for text classification) to gauge if the complex LLM approach is providing significant benefits. Data Formatting: For emotion generation (LLM outputs the emotion word): Convert examples into a \u0026ldquo;task description + solution\u0026rdquo; format. E.g., Input: \u0026quot;Predict emotion: I feel very happy\\nEmotion:\u0026quot;, Target: \u0026quot;joy [EOS]\u0026quot;. The labels tensor for training masks out the input part (e.g., by using -100) so the loss is only computed on the target completion. attention_mask is used to tell the model which tokens are real and which are padding. Model Loading: Using libraries like Hugging Face Transformers (AutoModelForCausalLM, AutoTokenizer). Setting tokenizer.pad_token = tokenizer.eos_token if the model doesn\u0026rsquo;t have a dedicated pad token. Finetuning to Follow Instructions (General Case): Requires a prompting format/style (e.g., Vicuna, Alpaca, ChatML). This defines how instructions and solutions are structured. Consistency with this format is important during inference. The dataset consists of (instruction, solution) pairs. The book mentions generating an emotion (like \u0026ldquo;joy\u0026rdquo;) as text output. This is a form of instruction following. 5.4 Sampling From Language Models What it\u0026rsquo;s ultimately trying to achieve: To control the way tokens are selected from the model\u0026rsquo;s output probability distribution, balancing creativity and coherence. Greedy decoding (always picking the most probable token) can be repetitive or dull.\nTechniques:\nBasic Sampling with Temperature: Softmax output probabilities are adjusted by a temperature T. T \u0026gt; 1: Flatter distribution, more randomness (creative). T \u0026lt; 1: Sharper distribution, less randomness (focused). T = 1: Standard softmax. Top-k Sampling: Consider only the k most probable tokens and renormalize their probabilities before sampling. Top-p (Nucleus) Sampling: Consider the smallest set of tokens whose cumulative probability exceeds p (e.g., 0.9) and renormalize before sampling. This is adaptive: if the model is very confident (one token has high probability), p might be met by just a few tokens. If uncertain, many tokens might be included. Penalties: Frequency Penalty: Reduces the probability of tokens that have already appeared frequently in the generated text. Presence Penalty: Reduces the probability of tokens that have appeared at all, encouraging new topics. 5.5 Low-Rank Adaptation (LoRA) What it\u0026rsquo;s ultimately trying to achieve: To significantly reduce the computational cost and memory requirements of finetuning large LLMs, making it accessible to users with limited resources. This is a type of Parameter-Efficient Finetuning (PEFT).\nThe Core Idea (Small Changes, Big Impact): Instead of updating all the billions of parameters in an LLM, LoRA freezes the original pretrained weights and introduces a small number of new, trainable \u0026ldquo;adapter\u0026rdquo; matrices.\nFor a large weight matrix W_0 (e.g., in an attention or MLP layer), LoRA adds two much smaller matrices, A (shape d x r) and B (shape r x k), where r (the rank) is small (e.g., 8, 16). During finetuning, only A and B are trained. The effective weight matrix becomes W = W_0 + (alpha/r) * B @ A. (The book has AB which might be a slight difference in convention or a typo in my summary; typically it\u0026rsquo;s a low-rank update BA or AB depending on how A and B are defined, the key is the product of two smaller matrices). The Hugging Face PEFT library (LoraConfig, get_peft_model) simplifies applying LoRA. You specify which layers/modules to adapt (e.g., query, key, value projections in attention). 5.6 LLM as a Classifier (Alternative to Generation) What it\u0026rsquo;s ultimately trying to achieve: To use an LLM for traditional classification tasks by having it output logits for predefined classes, rather than generating class names as text.\nHow it works:\nInstead of AutoModelForCausalLM, use AutoModelForSequenceClassification. This class typically adds a classification head (a linear layer + softmax) on top of the final hidden state of the LLM (often the embedding of the last token or a special [CLS] token). This head is trained to map the LLM\u0026rsquo;s contextual representation to class probabilities. 5.7 Prompt Engineering What it\u0026rsquo;s ultimately trying to achieve: To guide a finetuned chat LLM to produce desired outputs by carefully crafting the input prompt, without further changing the model\u0026rsquo;s weights.\nFeatures of a Good Prompt:\nSituation: Context for the request. Role: Persona for the LLM to adopt. Task: Clear, specific instructions. Output Format: JSON, bullet points, etc. Constraints: Limitations, preferences. Quality Criteria: What makes a good response. Examples (Few-Shot Prompting / In-Context Learning): Provide input-output examples. Call to Action: Restate the task. Followup Actions: Iterating with the LLM, asking for corrections, using different LLMs for review.\nCode Generation: Using detailed docstrings and requirements.\nDocumentation Synchronization: Using LLMs to help keep documentation updated with code changes.\n5.8 Hallucinations What it\u0026rsquo;s ultimately trying to achieve: To understand why LLMs sometimes generate plausible-sounding but factually incorrect or nonsensical information, and how to mitigate this.\nReasons:\nModels optimize for next-token prediction (coherence) not factual accuracy. Gaps in training data. Low-quality or biased training data. Error propagation in token-by-token generation. Prevention/Mitigation:\nRetrieval-Augmented Generation (RAG): Ground responses in externally retrieved, verified information. The LLM uses this retrieved context to formulate its answer. Domain-Specific Pretraining/Finetuning: Further train on reliable, domain-specific data. Multi-step verification workflows, human review. 5.9 LLMs, Copyright, and Ethics What it\u0026rsquo;s ultimately trying to achieve: To highlight the complex legal and ethical issues surrounding LLMs. Key Issues: Training Data: Use of copyrighted material in training datasets (fair use debates). Generated Content: Copyright status of AI-generated content, potential for reproducing copyrighted material. Open-Weight Models: Legal implications of sharing weights trained on copyrighted data. Broader Ethics: Explainability: LLM explanations are post-hoc rationalizations, not true transparency into their decision-making process. Bias: LLMs can absorb and amplify societal biases from training data. ","permalink":"https://deepskandpal.github.io/bookshelf/100-page-lm-book/chapter-5/","tags":null,"title":"Chapter 5: Large Language Model"},{"categories":null,"content":"Alright class, let\u0026rsquo;s gear up for Chapter 5: Support Vector Machines (SVMs). This is a big one! SVMs are incredibly powerful and versatile models. You\u0026rsquo;ll find them used for linear or nonlinear classification, regression, and even outlier detection. They are a cornerstone of classical machine learning, and definitely a tool everyone interested in ML should have in their arsenal.\nThe book mentions they\u0026rsquo;re particularly good for complex small- or medium-sized datasets. This chapter will walk us through their core concepts, how to use them, and, importantly for our \u0026ldquo;what it\u0026rsquo;s ultimately trying to achieve\u0026rdquo; philosophy, how they actually work.\n(Page 153: Linear SVM Classification - The Big Idea) The fundamental idea behind SVMs is best explained with pictures, and Figure 5-1 (page 154) does a great job. Imagine you have two classes of data points that are linearly separable – meaning you can draw a straight line to separate them perfectly.\nThe plot on the left of Figure 5-1 shows three possible straight lines (decision boundaries) that could separate the Iris data.\nOne (dashed) is terrible; it doesn\u0026rsquo;t even separate the classes. The other two separate the classes perfectly on the training data, but their decision boundaries are very close to some of the training instances. This means they might not generalize well to new, unseen instances. A slight variation in a new point could cause it to be misclassified. The plot on the right of Figure 5-1 shows the decision boundary of an SVM classifier.\nWhat is it ultimately trying to achieve? An SVM doesn\u0026rsquo;t just find any line that separates the classes. It tries to find the line that has the largest possible margin between itself and the closest instances from each class. Think of it as fitting the widest possible street between the two classes. The decision boundary is the median line of this street, and the edges of the street are defined by the closest points. This is called large margin classification. The intuition is that a wider margin leads to better generalization because the decision boundary is less sensitive to the exact position of individual training instances and has more \u0026ldquo;room for error\u0026rdquo; with new data. Support Vectors (Page 154):\nNotice that the position of this \u0026ldquo;widest street\u0026rdquo; is entirely determined by the instances located right on the edge of the street. These instances are called the support vectors (they are circled in Figure 5-1). What they are ultimately trying to achieve: They are the critical data points that \u0026ldquo;support\u0026rdquo; or define the decision boundary and the margin. If you move a support vector, the decision boundary will likely change. If you add more training instances that are \u0026ldquo;off the street\u0026rdquo; (far away from the margin), they won\u0026rsquo;t affect the decision boundary at all! The SVM is only sensitive to the points closest to the boundary. Sensitivity to Feature Scales (Figure 5-2, page 154): The scorpion icon highlights a crucial point: SVMs are sensitive to feature scales. If one feature has a much larger range of values than another (e.g., vertical scale much larger than horizontal in the left plot of Figure 5-2), the \u0026ldquo;widest street\u0026rdquo; will tend to be oriented to accommodate the larger scale. The street might look wide in the scaled units, but it might be very narrow along the axis with smaller-scaled features. Solution: Always scale your features (e.g., using Scikit-Learn\u0026rsquo;s StandardScaler) before training an SVM. The right plot in Figure 5-2 shows a much better, more balanced decision boundary after scaling. What scaling is ultimately trying to achieve: It ensures that all features contribute more equally to the distance calculations involved in finding the largest margin, preventing features with larger numerical values from dominating. (Page 154-155: Soft Margin Classification) The \u0026ldquo;widest street\u0026rdquo; idea so far assumed hard margin classification:\nAll instances must be strictly off the street and on the correct side. This only works if the data is perfectly linearly separable. It\u0026rsquo;s very sensitive to outliers (Figure 5-3, page 155). A single outlier can make it impossible to find a hard margin, or it can drastically change the decision boundary, leading to poor generalization. To avoid these issues, we use a more flexible approach: Soft Margin Classification.\nWhat it\u0026rsquo;s ultimately trying to achieve: Find a good balance between:\nKeeping the \u0026ldquo;street\u0026rdquo; (margin) as wide as possible. Limiting margin violations – instances that end up inside the street or even on the wrong side of the decision boundary. This allows the model to handle data that isn\u0026rsquo;t perfectly linearly separable and makes it less sensitive to outliers.\nThe C Hyperparameter (Figure 5-4, page 155): When using SVMs (e.g., in Scikit-Learn), the hyperparameter C controls this trade-off. Low C value (e.g., C=1 in the left plot of Figure 5-4): Wider street (larger margin). More margin violations are tolerated. This generally leads to a model that is more regularized and might generalize better, even if it makes more mistakes on the training data margin. High C value (e.g., C=100 in the right plot of Figure 5-4): Narrower street (smaller margin). Fewer margin violations are tolerated. The model tries harder to classify all training instances correctly. This can lead to overfitting if C is too high, as the model might be too sensitive to individual data points, including noise or outliers. What C is ultimately trying to achieve: It\u0026rsquo;s a regularization parameter. A smaller C means more regularization (larger margin, more tolerance for violations). A larger C means less regularization (smaller margin, less tolerance for violations, tries to fit training data more perfectly). If your SVM model is overfitting, try reducing C. If it\u0026rsquo;s underfitting, try increasing C (but be mindful of also allowing the margin to shrink too much). Scikit-Learn Implementation (Page 155-156): The code shows using LinearSVC (Linear Support Vector Classifier) for the Iris dataset: from sklearn.svm import LinearSVC from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline svm_clf = Pipeline([ (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;linear_svc\u0026quot;, LinearSVC(C=1, loss=\u0026quot;hinge\u0026quot;)) ]) svm_clf.fit(X, y)\nNote the use of StandardScaler in a pipeline – good practice! loss=\u0026quot;hinge\u0026quot;: The hinge loss is the cost function typically associated with linear SVMs (more on this in the \u0026ldquo;Under the Hood\u0026rdquo; section). The bird icon (page 156) mentions that unlike Logistic Regression, SVM classifiers generally do not output probabilities directly. They output a decision score, and the sign of the score determines the class. Alternatives for Linear SVMs: SVC(kernel=\u0026quot;linear\u0026quot;, C=1): Uses the SVC class with a linear kernel. It\u0026rsquo;s generally slower than LinearSVC but can be useful if you later want to try other kernels. SGDClassifier(loss=\u0026quot;hinge\u0026quot;, alpha=1/(m*C)): Uses Stochastic Gradient Descent to train a linear SVM. Slower to converge than LinearSVC but good for online learning or very large datasets that don\u0026rsquo;t fit in memory (out-of-core). The scorpion icon (page 156) gives important tips for LinearSVC: It regularizes the bias term, so centering data (done by StandardScaler) is important. Ensure loss=\u0026quot;hinge\u0026quot; is set (not always the default). For better performance, set dual=False unless features \u0026gt; training instances (duality is an advanced optimization concept we\u0026rsquo;ll touch on). (Page 157-161: Nonlinear SVM Classification) Linear SVMs are great, but many datasets aren\u0026rsquo;t linearly separable. What then?\nAdding Polynomial Features (Page 157):\nAs we saw in Chapter 4, you can add polynomial features to your data. This can transform a nonlinearly separable dataset into a linearly separable one in a higher-dimensional space. Figure 5-5 illustrates this: a 1D dataset that\u0026rsquo;s not linearly separable becomes linearly separable in 2D if you add x₂ = x₁² as a new feature. Implementation (Moons dataset): from sklearn.datasets import make_moons X, y = make_moons(n_samples=100, noise=0.15) polynomial_svm_clf = Pipeline([ (\u0026quot;poly_features\u0026quot;, PolynomialFeatures(degree=3)), (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;svm_clf\u0026quot;, LinearSVC(C=10, loss=\u0026quot;hinge\u0026quot;)) ]) polynomial_svm_clf.fit(X, y) Figure 5-6 (page 158) shows the resulting decision boundary on the moons dataset. It\u0026rsquo;s curved and does a decent job! The Kernel Trick (Polynomial Kernel - Page 158):\nAdding polynomial features works, but at high degrees, it creates a huge number of features (combinatorial explosion!), making the model very slow. Enter the Kernel Trick – one of the \u0026ldquo;magic\u0026rdquo; ideas in SVMs! What it\u0026rsquo;s ultimately trying to achieve: It allows you to get the same result as if you had added many polynomial features (even very high-degree ones) without actually creating or adding those features explicitly. This avoids the computational cost and memory overhead of explicitly transforming the data into a very high-dimensional space. How it works (conceptually): Instead of transforming the data points and then taking dot products in the high-dimensional space, the kernel is a function that can compute what the dot product would have been in that high-dimensional space, using only the original low-dimensional data points. (We\u0026rsquo;ll see more math in \u0026ldquo;Under the Hood\u0026rdquo;). Using Polynomial Kernel in Scikit-Learn: from sklearn.svm import SVC poly_kernel_svm_clf = Pipeline([ (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;svm_clf\u0026quot;, SVC(kernel=\u0026quot;poly\u0026quot;, degree=3, coef0=1, C=5)) ]) poly_kernel_svm_clf.fit(X, y) kernel=\u0026quot;poly\u0026quot;: Tells SVC to use the polynomial kernel trick. degree=3: Simulates a 3rd-degree polynomial expansion. coef0=1: (Gamma_zero) Controls how much the model is influenced by high-degree vs. low-degree polynomials in the kernel. C=5: Regularization parameter. Figure 5-7 (page 159) shows the results for degree=3 (left) and degree=10 (right). Higher degree can lead to more complex boundaries (and potential overfitting). The scorpion icon suggests using grid search to find good hyperparameter values. Similarity Features (Gaussian RBF Kernel - Page 159):\nAnother way to handle nonlinear problems is to add features based on similarity to certain landmarks. Imagine you pick a few landmark points in your feature space. For each data instance, you can calculate how similar it is to each landmark. These similarity scores become new features. Gaussian Radial Basis Function (RBF) as a similarity function (Equation 5-1): φ_γ(x, ℓ) = exp(-γ ||x - ℓ||²) x: An instance. ℓ: A landmark. ||x - ℓ||²: Squared Euclidean distance between the instance and the landmark. γ (gamma): A hyperparameter. What this function is ultimately trying to achieve: It outputs a value between 0 (if x is very far from landmark ℓ) and 1 (if x is at the landmark ℓ). It\u0026rsquo;s a bell-shaped curve. A higher γ makes the bell narrower (influence of the landmark drops off more quickly). Figure 5-8 shows a 1D dataset. Two landmarks are added. Each original instance x₁ is transformed into two new features: its RBF similarity to landmark 1, and its RBF similarity to landmark 2. The transformed 2D dataset (right plot) becomes linearly separable! Choosing landmarks: A simple approach is to create a landmark at the location of every single instance in the dataset. This transforms an m x n dataset into an m x m dataset (if original features are dropped). This can make a dataset linearly separable, but it\u0026rsquo;s computationally expensive if m is large. Gaussian RBF Kernel (Page 160):\nAgain, the kernel trick comes to the rescue! The Gaussian RBF kernel allows SVMs to get the same effect as adding many RBF similarity features, without actually computing them. Using Gaussian RBF Kernel in Scikit-Learn: rbf_kernel_svm_clf = Pipeline([ (\u0026quot;scaler\u0026quot;, StandardScaler()), (\u0026quot;svm_clf\u0026quot;, SVC(kernel=\u0026quot;rbf\u0026quot;, gamma=5, C=0.001)) ]) rbf_kernel_svm_clf.fit(X, y) kernel=\u0026quot;rbf\u0026quot;: Use the Gaussian RBF kernel. gamma (γ): Controls the width of the RBF \u0026ldquo;bell.\u0026rdquo; Increasing gamma: Makes the bell narrower. Each instance\u0026rsquo;s range of influence is smaller. Decision boundary becomes more irregular, wiggling around individual instances (can lead to overfitting). (See Figure 5-9, page 161, comparing gamma=0.1 to gamma=5). Decreasing gamma: Makes the bell wider. Instances have a larger range of influence. Decision boundary becomes smoother (can lead to underfitting if too smooth). So, gamma acts like a regularization hyperparameter: if overfitting, reduce gamma; if underfitting, increase gamma. C: Regularization parameter, as before. Lower C = more regularization. Figure 5-9 shows how different gamma and C values affect the decision boundary for the RBF kernel. Which kernel to use? (Scorpion icon, page 161): Always try linear kernel first (LinearSVC is much faster than SVC(kernel=\u0026quot;linear\u0026quot;)). Especially if training set is large or has many features. If training set isn\u0026rsquo;t too large, try Gaussian RBF kernel next. It works well in most cases. If you have time/compute power, experiment with other kernels (e.g., polynomial, or specialized kernels like string kernels for text) using cross-validation and grid search. (Page 162: Computational Complexity) LinearSVC: Based on liblinear. Does not support the kernel trick. Scales almost linearly with number of instances (m) and features (n) – roughly O(m × n). Good for large datasets if a linear model is sufficient. SVC: Based on libsvm. Does support the kernel trick. Training time complexity: Between O(m² × n) and O(m³ × n). Gets dreadfully slow when m (number of instances) is large (e.g., hundreds of thousands). Perfect for complex small or medium-sized datasets. Scales well with number of features, especially sparse features. Table 5-1 summarizes this. (Page 162-163: SVM Regression) SVMs can also do regression!\nThe Goal (Reversed Objective): For classification: Fit the widest street between classes, limiting margin violations. For SVM Regression (SVR): Fit as many instances as possible on the street, while limiting margin violations (instances off the street). ϵ (epsilon) hyperparameter: Controls the width of the street. (Figure 5-10, page 163). Larger ϵ: Wider street. More instances can fit inside the margin without penalty. Smaller ϵ: Narrower street. ϵ-insensitive: Adding more training instances within the margin (inside the street) does not affect the model\u0026rsquo;s predictions. Scikit-Learn Classes: LinearSVR: For linear SVM regression. from sklearn.svm import LinearSVR svm_reg = LinearSVR(epsilon=1.5) (Data should be scaled and centered first). SVR: For nonlinear SVM regression using kernels. from sklearn.svm import SVR svm_poly_reg = SVR(kernel=\u0026quot;poly\u0026quot;, degree=2, C=100, epsilon=0.1) Figure 5-11 (page 164) shows SVR with a polynomial kernel. Large C: Little regularization. Model tries to fit data points closely. Small C: More regularization. Smoother fit. LinearSVR scales linearly with training set size. SVR gets slow as training set size grows (like SVC). SVMs can also be used for outlier detection. Under the hood Let\u0026rsquo;s start with the big picture goals for a Linear SVM:\nGoal 1: Separate the Classes. (Obvious) We want a line (or plane/hyperplane) that divides the data.\nGoal 2: Make this Separation as \u0026ldquo;Safe\u0026rdquo; or \u0026ldquo;Robust\u0026rdquo; as Possible (Large Margin). This is the core SVM idea. We don\u0026rsquo;t just want any separating line; we want the one that stays farthest away from the closest points of both classes. This \u0026ldquo;widest street\u0026rdquo; (the margin) makes the classifier less sensitive to small variations in data and hopefully better at classifying new, unseen points.\nHow do we achieve Goal 2 mathematically? (This is where w and b come in)\nThe Decision Boundary: Our line is defined by wᵀx + b = 0.\nw (weights vector): Determines the orientation or slope of the line. b (bias): Shifts the line up or down without changing its orientation. The \u0026ldquo;Street\u0026rdquo;: The edges of our street (the margins) are defined by wᵀx + b = 1 and wᵀx + b = -1.\nAll positive class points should ideally be on or \u0026ldquo;above\u0026rdquo; wᵀx + b = 1. All negative class points should ideally be on or \u0026ldquo;below\u0026rdquo; wᵀx + b = -1. Width of the Street: It turns out mathematically that the width of this street is 2 / ||w|| (where ||w|| is the magnitude or length of the vector w).\nKey Insight: To make the street (margin) WIDE, we need ||w|| to be SMALL. What are we trying to achieve by minimizing ||w||? We are trying to maximize the margin. For mathematical convenience (easier to take derivatives), instead of minimizing ||w||, we minimize (1/2)||w||² (which is (1/2)wᵀw). Minimizing one also minimizes the other. So, the Training Objective (Hard Margin - Equation 5-3) becomes:\nObjective: Minimize (1/2)wᵀw (make the margin large) Subject to (Constraints): t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 for all training instances i. t⁽ⁱ⁾ is +1 for positive class, -1 for negative. This constraint says: \u0026ldquo;Every training point must be on the correct side of its respective margin boundary, or right on it.\u0026rdquo; What this constrained optimization is ultimately trying to achieve: Find the line orientation (w) and position (b) that gives the widest possible street while ensuring all training points are correctly classified and stay out of (or on the edge of) the street.\nWhat if the data isn\u0026rsquo;t perfectly separable (Soft Margin - Equation 5-4)?\nReal data is messy. Outliers exist. We might not be able to find a street where all points are perfectly on the correct side.\nSlack Variables ζ⁽ⁱ⁾ (zeta): We introduce a \u0026ldquo;fudge factor\u0026rdquo; or \u0026ldquo;slack\u0026rdquo; ζ⁽ⁱ⁾ ≥ 0 for each point i. If ζ⁽ⁱ⁾ = 0, point i respects the margin. If ζ⁽ⁱ⁾ \u0026gt; 0, point i violates the margin (it\u0026rsquo;s inside the street or on the wrong side). The value of ζ⁽ⁱ⁾ tells us how much it violates. New Objective (Soft Margin): Minimize (1/2)wᵀw + C Σᵢ ζ⁽ⁱ⁾ Subject to: t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 - ζ⁽ⁱ⁾ and ζ⁽ⁱ⁾ ≥ 0. What this soft margin optimization is ultimately trying to achieve: Still try to make (1/2)wᵀw small (maximize margin). But also try to make Σᵢ ζ⁽ⁱ⁾ small (minimize the total amount of margin violations). The hyperparameter C is the trade-off. Large C: Penalizes violations heavily. Model will try very hard to get points right, even if it means a narrower margin. (Closer to hard margin). Small C: Tolerates more violations in favor of a wider margin. (More regularization). The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. This problem (finding w, b, and ζs) is a Quadratic Programming (QP) problem. We can give it to a standard QP solver.\nWhy Introduce the \u0026ldquo;Dual Problem\u0026rdquo; (Equation 5-6)? This is often the confusing part.\nSolving the QP problem directly (called the \u0026ldquo;primal problem\u0026rdquo;) is fine for linear SVMs. But it has limitations:\nIt can be slow if the number of features n is very large. More importantly, it doesn\u0026rsquo;t allow for the \u0026ldquo;kernel trick\u0026rdquo; which is essential for efficient non-linear SVMs. The Dual Problem is a different but related mathematical formulation of the same optimization task.\nInstead of finding w and b directly, it focuses on finding a set of new variables, αᵢ (alpha-i), one for each training instance. Key Property of these αᵢs: It turns out that αᵢ will be greater than zero only for the support vectors. For all other data points, αᵢ will be zero! What solving the dual problem is ultimately trying to achieve: It\u0026rsquo;s finding the importance (αᵢ) of each training instance in defining the optimal margin. Only the support vectors end up having non-zero importance. Why is the Dual useful?\nComputational Efficiency in some cases: If the number of training instances m is smaller than the number of features n, solving the dual can be faster. THE KERNEL TRICK (THIS IS THE BIG ONE!): Look at the dual objective function (Equation 5-6): minimize (1/2) Σᵢ Σⱼ αᵢαⱼt⁽ⁱ⁾t⁽ʲ⁾(x⁽ⁱ⁾ᵀx⁽ʲ⁾) - Σᵢ αᵢ Notice the term (x⁽ⁱ⁾ᵀx⁽ʲ⁾). This is a dot product between pairs of training instances. The solution w from the dual (Equation 5-7) also involves sums of αᵢt⁽ⁱ⁾x⁽ⁱ⁾. And crucially, when you make predictions using the dual formulation (Equation 5-11), the decision function becomes: h(x_new) = Σᵢ αᵢt⁽ⁱ⁾(x⁽ⁱ⁾ᵀx_new) + b (sum over support vectors i) Again, it only involves dot products between the support vectors and the new instance x_new. The Kernel Trick - The \u0026ldquo;Aha!\u0026rdquo; Moment for Nonlinear SVMs:\nNow, imagine we want to do nonlinear classification. One way (as discussed for Polynomial Regression) is to map our data x to a much higher-dimensional space using a transformation φ(x), where the data becomes linearly separable.\nIf we did this explicitly, we would then have to compute dot products like φ(x⁽ⁱ⁾)ᵀφ(x⁽ʲ⁾) in this very high (maybe even infinite) dimensional space. This would be computationally impossible or extremely inefficient.\nThe Kernel Trick says: What if there\u0026rsquo;s a special function K(a,b) called a kernel that can compute the dot product φ(a)ᵀφ(b) for us, using only the original vectors a and b, without us ever having to compute φ(a) or φ(b) explicitly?\nFor example, the polynomial kernel K(a,b) = (γaᵀb + r)ᵈ calculates what the dot product would be if you mapped a and b to a d-th degree polynomial feature space. The Gaussian RBF kernel K(a,b) = exp(-γ ||a - b||²) implicitly maps to an infinite-dimensional space! What the kernel trick is ultimately trying to achieve: It allows us to get the benefit of working in a very high-dimensional feature space (where complex separations might become linear) without the prohibitive computational cost of actually creating and working with those high-dimensional vectors. We just replace all dot products x⁽ⁱ⁾ᵀx⁽ʲ⁾ in the dual formulation and in the prediction equation with K(x⁽ⁱ⁾, x⁽ʲ⁾).\nSo, the \u0026ldquo;Under the Hood\u0026rdquo; Flow for Kernelized SVMs:\nStart with a nonlinear problem. Choose a kernel function K(a,b) (e.g., Polynomial, RBF). This implicitly defines a transformation φ to a higher-dimensional space where the data is hopefully linearly separable. Solve the DUAL optimization problem (Equation 5-6, but with x⁽ⁱ⁾ᵀx⁽ʲ⁾ replaced by K(x⁽ⁱ⁾, x⁽ʲ⁾)). This finds the αᵢ values (which will be non-zero only for support vectors). What this is trying to achieve: Find the \u0026ldquo;importance\u0026rdquo; of each training point in defining the optimal linear margin in that implicit high-dimensional space. Make predictions for a new instance x_new (Equation 5-11, with K): h(x_new) = Σᵢ αᵢt⁽ⁱ⁾K(x⁽ⁱ⁾, x_new) + b What this is trying to achieve: Classify the new point based on its kernel \u0026ldquo;similarity\u0026rdquo; (as defined by K) to the support vectors, effectively performing a linear separation in the implicit high-dimensional space. Think of it like this:\nPrimal problem (for linear SVM): Directly find the best \u0026ldquo;street\u0026rdquo; (w, b) in the original feature space. Dual problem (for linear SVM): Find out which data points (αᵢ \u0026gt; 0) are the crucial \u0026ldquo;support posts\u0026rdquo; for that street. This formulation happens to only use dot products. Kernelized SVM (using the dual): We want a curvy street in our original space. The kernel trick lets us pretend we\u0026rsquo;ve straightened out the data into a super high-dimensional space where a straight street works. We use the dual formulation because it only relies on dot products, and we can replace those dot products with our kernel function K. The kernel function cleverly calculates what those dot products would have been in the high-dimensional space, without us ever going there. So, the progression of \u0026ldquo;what are we trying to achieve\u0026rdquo;:\nLinear SVM (Primal): Maximize margin directly by finding w and b. Challenges: Can be slow for many features, doesn\u0026rsquo;t easily extend to non-linear. Linear SVM (Dual): Reformulate to find αs (support vector indicators). Only uses dot products. Kernel Trick Motivation: We want non-linear separation. Idea: map to a higher space where it\u0026rsquo;s linear. Problem: mapping is expensive. Kernelized SVM (Dual + Kernel): Realize the dual only needs dot products. If we can find a kernel K(a,b) that equals φ(a)ᵀφ(b) without computing φ, we can do complex non-linear classification efficiently by solving the dual problem with K and making predictions with K. The \u0026ldquo;headache\u0026rdquo; the book mentions with Equation 5-12 for computing b with kernels is because even b needs to be calculated using these kernelized dot products, as we no longer have an explicit w in the original space.\nIt\u0026rsquo;s a chain of clever mathematical steps, each designed to overcome a limitation of the previous one, ultimately allowing us to achieve large margin classification for complex, non-linear data in an efficient way. The dual formulation is the bridge that makes the kernel trick possible.\nDoes this step-by-step focus on the \u0026ldquo;why\u0026rdquo; for each part (especially the dual and kernel) help make the overall strategy a bit more intuitive? It\u0026rsquo;s okay if it still feels complex; SVM theory is one of the more mathematically rich areas in classical ML.\nGlossary (Page 164-173: Under the Hood - How SVMs Work) This is where the math gets a bit deeper, but remember our goal: \u0026ldquo;what is it ultimately trying to achieve?\u0026rdquo; The book changes notation here for SVMs, which is common:\nBias term: b (instead of θ₀)\nFeature weights vector: w (instead of θ₁ to θₙ)\nNo x₀=1 bias input feature is added.\nDecision Function and Predictions (Linear SVM - page 165):\nDecision function: wᵀx + b Equation 5-2: Linear SVM classifier prediction ŷ = 1 if wᵀx + b ≥ 0 ŷ = 0 if wᵀx + b \u0026lt; 0 (or class -1 if using -1/1 labels) Figure 5-12 shows this in 3D for 2 features: wᵀx + b is a plane. The decision boundary is where wᵀx + b = 0 (a line). The \u0026ldquo;street\u0026rdquo; is defined by wᵀx + b = 1 and wᵀx + b = -1. These are parallel lines/planes forming the margin. Training Objective (Page 166):\nGoal: Find w and b that make the margin (the \u0026ldquo;street\u0026rdquo;) as wide as possible, while controlling margin violations.\nThe slope of the decision function is ||w|| (norm of the weight vector).\nKey insight (Figure 5-13): The smaller the norm ||w||, the larger the margin. (If you divide w and b by 2, the slope ||w||/2 is halved, effectively doubling the distance to the ±1 lines, thus doubling the margin width). So, we want to minimize ||w|| (or equivalently, minimize (1/2)wᵀw which is (1/2)||w||² – easier to differentiate).\nHard Margin Objective (Equation 5-3): minimize (1/2)wᵀw subject to t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 for all instances i.\nt⁽ⁱ⁾ is 1 for positive class, -1 for negative class. The constraint t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 means: For positive instances (t⁽ⁱ⁾=1): wᵀx⁽ⁱ⁾ + b ≥ 1 (on or outside the positive margin boundary). For negative instances (t⁽ⁱ⁾=-1): wᵀx⁽ⁱ⁾ + b ≤ -1 (on or outside the negative margin boundary). What it\u0026rsquo;s ultimately trying to achieve: Find the w with the smallest norm (largest margin) such that all points are correctly classified and outside or on the margin boundaries. Soft Margin Objective (Equation 5-4, page 167): To allow for margin violations, introduce slack variables ζ⁽ⁱ⁾ ≥ 0 (zeta-i) for each instance i. ζ⁽ⁱ⁾ measures how much instance i is allowed to violate the margin. minimize (1/2)wᵀw + C Σᵢ ζ⁽ⁱ⁾ subject to t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1 - ζ⁽ⁱ⁾ and ζ⁽ⁱ⁾ ≥ 0.\nWhat it\u0026rsquo;s ultimately trying to achieve: Still minimize (1/2)wᵀw (maximize margin). But also minimize Σᵢ ζ⁽ⁱ⁾ (sum of slack/violations). C is the hyperparameter that trades off between these two conflicting objectives: large margin vs. few violations. Small C: Margin width is prioritized (more slack allowed). Large C: Few violations prioritized (margin might be smaller). Quadratic Programming (QP) (Page 167):\nBoth hard and soft margin SVM objectives are convex quadratic optimization problems with linear constraints. These are known as QP problems. Specialized QP solvers exist to find the optimal w and b. Equation 5-5 gives the general QP formulation. The book explains how to map SVM parameters to this general form. The Dual Problem (Page 168-169):\nFor constrained optimization problems (like SVMs), there\u0026rsquo;s often a \u0026ldquo;primal problem\u0026rdquo; and a related \u0026ldquo;dual problem.\u0026rdquo; Solving the dual can sometimes be easier or offer advantages. For SVMs, the dual problem: Is faster to solve than the primal when number of training instances (m) \u0026lt; number of features (n). Crucially, makes the kernel trick possible! (The primal does not). Equation 5-6: Dual form of the linear SVM objective: This involves minimizing a function with respect to new variables αᵢ (alpha-i), one for each training instance. minimize (1/2) Σᵢ Σⱼ αᵢαⱼt⁽ⁱ⁾t⁽ʲ⁾(x⁽ⁱ⁾ᵀx⁽ʲ⁾) - Σᵢ αᵢ subject to αᵢ ≥ 0. Once you find the optimal α̂ vector (using a QP solver), you can compute ŵ and b̂ for the primal problem using Equation 5-7. An important property is that αᵢ will be non-zero only for the support vectors! Most αᵢ will be zero. Kernelized SVMs (The Kernel Trick Explained - Page 169-171): This is where the magic happens for nonlinear SVMs.\nSuppose we have a mapping function φ(x) that transforms our input x into a higher-dimensional space where it might become linearly separable (Equation 5-8 for a 2nd-degree polynomial mapping). If we apply this transformation φ to all training instances, the dual problem (Equation 5-6) would contain dot products of these transformed vectors: φ(x⁽ⁱ⁾)ᵀφ(x⁽ʲ⁾). The Key Insight (Equation 5-9 for 2nd-degree polynomial): It turns out that for some transformations φ, the dot product φ(a)ᵀφ(b) in the high-dimensional space can be computed by a simpler function K(a,b) using only the original vectors a and b. For the 2nd-degree polynomial mapping, φ(a)ᵀφ(b) = (aᵀb)². The Kernel Function K(a,b): A kernel K(a,b) is a function that computes φ(a)ᵀφ(b) based only on a and b, without needing to know or compute φ itself! What it\u0026rsquo;s ultimately trying to achieve: It allows us to operate in a very high (even infinite) dimensional feature space implicitly, without ever actually creating those features. This avoids the computational nightmare of explicit transformation. So, in the dual problem (Equation 5-6), we just replace x⁽ⁱ⁾ᵀx⁽ʲ⁾ with K(x⁽ⁱ⁾, x⁽ʲ⁾). Common Kernels (Equation 5-10, page 171): Linear: K(a,b) = aᵀb (no transformation) Polynomial: K(a,b) = (γaᵀb + r)ᵈ Gaussian RBF: K(a,b) = exp(-γ ||a - b||²) Sigmoid: K(a,b) = tanh(γaᵀb + r) Mercer\u0026rsquo;s Theorem (sidebar): Provides mathematical conditions for a function K to be a valid kernel (i.e., for a corresponding φ to exist). Gaussian RBF kernel actually maps to an infinite-dimensional space! Good thing we don\u0026rsquo;t have to compute φ. Making Predictions with Kernels (Equation 5-11, page 172): We need w to make predictions, but w lives in the (potentially huge) φ space. How do we predict without computing w? Equation 5-7 for w involves φ(x⁽ⁱ⁾). If we plug this into the decision function wᵀφ(x⁽ⁿ⁾) + b, we get an equation that only involves dot products of φ terms, which can be replaced by kernels: h(x⁽ⁿ⁾) = Σᵢ αᵢt⁽ⁱ⁾K(x⁽ⁱ⁾, x⁽ⁿ⁾) + b (sum over support vectors i where αᵢ \u0026gt; 0).\nWhat it\u0026rsquo;s ultimately trying to achieve: Predictions for a new instance x⁽ⁿ⁾ are made by computing its kernel similarity to only the support vectors. The bias term b can also be computed using kernels (Equation 5-12). Online SVMs (Page 172-173): For online learning (incremental learning).\nOne method for linear SVMs: Use Gradient Descent on a cost function derived from the primal problem (Equation 5-13). Equation 5-13: Linear SVM classifier cost function (Hinge Loss) J(w,b) = (1/2)wᵀw + C Σᵢ max(0, 1 - t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b)) (1/2)wᵀw: Aims for a large margin (small w). max(0, 1 - t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b)): This is the hinge loss. What hinge loss is ultimately trying to achieve: It penalizes instances that violate the margin. If an instance i is correctly classified and outside or on the margin (t⁽ⁱ⁾(wᵀx⁽ⁱ⁾ + b) ≥ 1), then 1 - t⁽ⁱ⁾(...) ≤ 0, so max(0, ...) = 0 (zero loss for this instance). If it violates the margin, the loss is proportional to how far it is from its correct margin boundary. This cost function is what SGDClassifier(loss=\u0026quot;hinge\u0026quot;) minimizes. Online kernelized SVMs are also possible but more complex. And that\u0026rsquo;s the grand tour of Support Vector Machines! The core idea of large margin classification is simple and elegant. The kernel trick is the \u0026ldquo;magic\u0026rdquo; that allows SVMs to handle complex nonlinear data efficiently by implicitly operating in very high-dimensional feature spaces.\nThe \u0026ldquo;Under the Hood\u0026rdquo; section is definitely more mathematical, but hopefully, by focusing on \u0026ldquo;what is it ultimately trying to achieve\u0026rdquo; for each equation (like minimizing ||w|| for a large margin, or using kernels to avoid explicit high-dimensional transformations), the core concepts become clearer.\nAny part of that, especially the kernel trick or the dual problem, that still feels a bit fuzzy?\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-5/","tags":null,"title":"Chapter 5: Support Vector Machines"},{"categories":null,"content":"What are we trying to achieve? We\u0026rsquo;re solving the LLM\u0026rsquo;s biggest weaknesses:\nKnowledge Cutoff: It doesn\u0026rsquo;t know about recent events. Lack of Private Data: It hasn\u0026rsquo;t been trained on your internal company wiki, your customer support database, or a PDF you just uploaded. Hallucination: It can make things up. The book uses the example of building a ChatPDF system for internal company use. An employee should be able to ask, \u0026ldquo;What is our policy on international travel reimbursement?\u0026rdquo; and get an answer based on the latest HR documents, not on some generic policy the LLM learned from the public internet.\nStep 1: Clarifying Requirements This is where the interview starts. The book gives a fantastic example of a candidate leading the conversation. Let\u0026rsquo;s analyze it from an interviewer\u0026rsquo;s perspective.\nCandidate: \u0026ldquo;What does the external knowledge base consist of? Does it change over time?\u0026rdquo; Interviewer\u0026rsquo;s thought: Good. They\u0026rsquo;re starting with the data. They understand that the nature of the data source is the most important factor. Candidate: \u0026ldquo;Do the Wiki pages and forums contain text, images, and other modalities?\u0026rdquo; Interviewer\u0026rsquo;s thought: Excellent. They\u0026rsquo;re thinking about multimodality. This will affect our choice of embedding models. Candidate: \u0026ldquo;How many pages are there in total?\u0026rdquo; (5 million pages) \u0026ldquo;What is the expected growth?\u0026rdquo; (20% annually) Interviewer\u0026rsquo;s thought: Great, they\u0026rsquo;re quantifying the scale. This is critical for discussing scalability, cost, and choosing the right database/indexing strategy. Candidate: \u0026ldquo;Should the system respond in real time?\u0026rdquo; (Slight delay is okay) Interviewer\u0026rsquo;s thought: They\u0026rsquo;re scoping the latency requirements. This tells me I don\u0026rsquo;t need a sub-50ms system and can make trade-offs for better quality. Candidate: \u0026ldquo;Is it necessary for the system to include document references?\u0026rdquo; (Yes) Interviewer\u0026rsquo;s thought: Crucial question. This requirement immediately makes one of the potential solutions (finetuning) much less attractive. They are already thinking ahead. By the end of this, you\u0026rsquo;ve established the core problem: Build a Q\u0026amp;A system over a large (5M pages), slowly growing (+20%/year) internal knowledge base of mixed-format PDFs, which must provide verifiable answers with source references.\nStep 2: Frame the Problem as an ML Task Specifying Input and Output This is straightforward but important to state clearly.\nInput: A user\u0026rsquo;s text query (e.g., \u0026ldquo;How do I submit an expense report?\u0026rdquo;). Underlying Data: A database of 5 million company documents. Output: A text-based answer, grounded in the documents, with references. graph LR subgraph User A[User Query\u0026lt;br/\u0026gt;How do I submit an\u0026lt;br/\u0026gt;expense report?] end subgraph System B(ChatPDF System) end subgraph Data C[Document databases] end subgraph Output D[Response\u0026lt;br/\u0026gt;To submit an expense report, log into...] end A --\u0026gt; B C --\u0026gt; B B --\u0026gt; D (Based on Figure 6.2)\nChoosing a Suitable ML Approach This is the first major design decision, and it\u0026rsquo;s a classic interview trade-off question. For a problem like this, the book lays out three main approaches.\nFinetuning:\nWhat it is: Take a pre-trained LLM and continue training it on your internal documents. The model\u0026rsquo;s weights are updated to \u0026ldquo;absorb\u0026rdquo; the new knowledge. Pros: Can deeply learn the style and terminology of your company. Cons (Dealbreakers for our problem): Computationally Expensive: Continuously retraining an LLM is a massive cost. Stale Data: As soon as a new document is added, the model is out of date until the next expensive finetuning cycle. No References: The book correctly states: \u0026ldquo;Finetuned models usually can\u0026rsquo;t provide references for their answers, making it hard to verify or trace information back to its source.\u0026rdquo; The knowledge is baked into the weights; you can\u0026rsquo;t easily point to the source document. This violates our requirement. Prompt Engineering (In-Context Learning):\nWhat it is: Stuff the relevant documents directly into the prompt along with the user\u0026rsquo;s question. Pros: Simple, cheap, no training required. Cons (Dealbreakers for our problem): Limited Context Window: You can\u0026rsquo;t fit 5 million documents into a prompt. You can\u0026rsquo;t even fit one long document. This approach is simply not scalable. Retrieval-Augmented Generation (RAG):\nWhat it is: A two-step process. First, retrieve a few relevant document snippets from the large database. Then, generate an answer using an LLM, with the user\u0026rsquo;s query and the retrieved snippets provided as context in the prompt. Pros: Access to Current Info: The document database can be updated easily. The LLM gets the latest info at query time. Verifiable \u0026amp; Factual: Since you have the retrieved snippets, you can easily add references. It reduces hallucination by forcing the LLM to base its answer on the provided text. Scalable \u0026amp; Cost-Effective: You\u0026rsquo;re not retraining the LLM. The main work is in the retrieval step. Cons: Implementation Complexity: It\u0026rsquo;s a multi-component system (retriever + generator) that needs to work well together. Dependence on Retrieval Quality: If you retrieve irrelevant documents, the LLM will give a garbage answer. The retriever is critical. The Decision: The book concludes, \u0026ldquo;RAG offers a balanced solution in terms of ease of setup, cost, and scalability\u0026hellip; Therefore, we choose RAG to build our ChatPDF system.\u0026rdquo; This is the correct, well-justified choice.\nStep 3: Data Preparation (The \u0026ldquo;R\u0026rdquo; in RAG) This is the entire process of making your knowledge base searchable. The book outlines a three-step pipeline.\ngraph TD A[Document Databases - PDFs] --\u0026gt; B(UnstructuredPDFLoader\u0026lt;br/\u0026gt;Parsing + OCR) B --\u0026gt; C(Intelligent Chunking\u0026lt;br/\u0026gt;Structure-aware) C --\u0026gt; D(Vector Embedding) D --\u0026gt; E[Indexed Embeddings\u0026lt;br/\u0026gt;in Vector DB] (Enhanced pipeline using UnstructuredPDFLoader)\n1. Document Parsing: Getting Content out of PDFs PDFs are a nightmare. They can have columns, tables, images, and weird layouts.\nRule-based: You write code that assumes a certain layout. Brittle and fails on complex or varied documents. AI-based (The Winner): Use a comprehensive document processing tool like UnstructuredPDFLoader from langchain_community.document_loaders. This approach offers several key advantages: Automatic OCR: Built-in OCR capabilities that can handle scanned PDFs, images, and mixed content documents automatically. Layout Detection: Advanced algorithms that understand document structure, identifying paragraphs, tables, headers, lists, and other elements. Multi-modal Processing: Handles text, images, tables, and complex layouts in a unified way. Structured Output: Returns well-structured document chunks with metadata about element types and hierarchy. Robust Handling: Works reliably across different PDF formats, including scanned documents, forms, and complex layouts that would break simpler parsers. 2. Document Chunking: Breaking It Down You can\u0026rsquo;t create an embedding for an entire 50-page document.\nWhy? An embedding of a whole document averages out the meaning and loses specific details. A query for a specific sentence will get lost. The retrieved document chunk needs to fit into the LLM\u0026rsquo;s context window. Strategies: Length-based: Simple but dumb. Can cut sentences in half. Traditional approaches like RecursiveCharacterTextSplitter try to split on paragraphs, then sentences, but still lack true semantic understanding. Content-aware (Best with UnstructuredPDFLoader): UnstructuredPDFLoader excels here by providing intelligent, structure-aware chunking: Semantic Chunking: Automatically identifies and preserves document structure (headers, paragraphs, lists, tables) as natural chunk boundaries. Element-based Splitting: Creates chunks based on document elements rather than arbitrary character counts, preserving context and meaning. Metadata Preservation: Each chunk includes rich metadata about its position, type, and relationship to other elements. Configurable Chunking: Allows fine-tuning of chunk sizes while respecting document structure boundaries. OCR Integration: For scanned documents, combines OCR text extraction with intelligent chunking in a single step. 3. Indexing: Making Chunks Findable Now you have thousands or millions of chunks. How do you find the right ones for a given query, fast?\nKeyword/Full-text Search (e.g., Elasticsearch): Fast and good for matching exact words or phrases. But it struggles with synonyms and semantic meaning. A query for \u0026ldquo;employee compensation\u0026rdquo; might miss a document that only uses the word \u0026ldquo;staff salary.\u0026rdquo; Vector-based Search (The Winner): Why? It searches based on semantic meaning, not keywords. How? You use an embedding model (like a Transformer encoder) to convert every chunk of text/image into a high-dimensional vector (an array of numbers). Chunks with similar meanings will have vectors that are \u0026ldquo;close\u0026rdquo; to each other in this vector space. The book does a back-of-the-envelope calculation: 5M pages, chunked, results in ~40M chunks. At this scale, vector search is the only viable option for semantic retrieval. So, the data prep pipeline is: Parse \u0026amp; Chunk PDFs with UnstructuredPDFLoader (including OCR for scanned documents) -\u0026gt; Extract semantically meaningful, structure-aware chunks -\u0026gt; Embed each chunk into a vector -\u0026gt; Store the vectors with rich metadata in a specialized Vector Database.\nStep 4: Model Development This covers the architecture of the ML models and the processes involved.\nArchitecture: The Key ML Models (Figure 6.9) A RAG system has three key ML model components:\nIndexing: An Encoder Model to create the vector embeddings. Retrieval: The same Encoder Model to turn the user query into a vector. (Plus an ANN search algorithm). Generation: An LLM to generate the final answer. The Indexing/Retrieval Model: Text-Image Alignment Our documents have text and images. Our query is text. How do you find an image relevant to a text query? The embeddings need to live in the same \u0026ldquo;space.\u0026rdquo; The book outlines two approaches (Figure 6.10):\nShared Embedding Space (Best Approach): Use a multimodal model like CLIP. CLIP is pre-trained to map related images and their text descriptions to nearby points in vector space. You can use its text encoder for text chunks and its image encoder for images. This is elegant and powerful. Image Captioning (Workaround): Use an image captioning model to generate a text description for each image. Then, use a standard text-only encoder to embed that caption. This works but is less direct and might lose information. The Retrieval Process: Finding the Needles in the Haystack Once the user query is embedded into a vector Eq, we need to find the k closest chunk vectors in our database of 40 million.\nExact Nearest Neighbor (Linear Search): Compare Eq to all 40 million vectors. Guarantees a perfect result but is way too slow. O(N*D) complexity. Unacceptable. Approximate Nearest Neighbor (ANN): The only practical solution. It trades a tiny bit of accuracy for a massive speedup. The book mentions four families of ANN algorithms: Tree-based (e.g., Annoy): Recursively partition the data space. Fast, but can struggle in very high dimensions. Hashing-based (LSH): Uses clever hash functions where similar vectors are likely to get the same hash key. You only search within the query\u0026rsquo;s hash bucket. Clustering-based (The book\u0026rsquo;s choice): Pre-cluster the 40M vectors into, say, 100,000 clusters. The search becomes a two-step process: Find the few clusters whose center is closest to the query vector. Do an exact search only within those few clusters. This massively reduces the search space. Graph-based (e.g., HNSW): The state-of-the-art for many use cases. It builds a graph where nodes are data points and edges connect close neighbors. Searching is like navigating this graph to find the closest point. Modern vector databases like Pinecone, Weaviate, or libraries like FAISS (from Meta) and ScaNN (from Google) implement these advanced ANN algorithms for you.\nHere is the overall retrieval process from Figure 6.16:\ngraph TD subgraph Data Preparation direction TB A[Document databases] --\u0026gt; B(Data preparation\u0026lt;br/\u0026gt;Parsing/Chunking) B --\u0026gt; C[Index images] B --\u0026gt; D[Index text] C --\u0026gt; E(Clustering) D --\u0026gt; E end subgraph Retrieval direction LR F[User query\u0026lt;br/\u0026gt;How many cats live\u0026lt;br/\u0026gt;in the company?] --\u0026gt; G(Text Encoder) G --\u0026gt; H(Inter-Cluster\u0026lt;br/\u0026gt;Search) E -.Selected Clusters.-\u0026gt; H H --\u0026gt; I(Intra-Cluster\u0026lt;br/\u0026gt;Search) I --\u0026gt; J[Retrieved\u0026lt;br/\u0026gt;data chunks] end The Generation Process: Crafting the Final Answer This is where the LLM comes in. The process is:\nTake the original user query. Take the top k retrieved data chunks from the retrieval step. Combine them into a single, well-structured prompt. Feed this prompt to the LLM. The LLM generates the answer, using a sampling strategy like top-p sampling for a good balance of correctness and fluency. A Deeper Look at Prompt Engineering for Generation How do you structure that final prompt for the best results? The book dives into several powerful techniques.\nChain-of-Thought (CoT) Prompting: Instruct the model to \u0026ldquo;think step-by-step.\u0026rdquo; This forces it to lay out its reasoning process before giving the final answer, which often improves accuracy on complex questions. Few-shot Prompting: Give the model 2-3 examples of a Q\u0026amp;A pair in the desired format before giving it the real query. This helps it understand the expected tone and structure. Role-specific Prompting: Tell the model who it is. \u0026ldquo;You are an expert contract lawyer\u0026hellip; explain this clause in simple terms.\u0026rdquo; This grounds the model and helps it adopt the correct persona and level of detail. User-context Prompting: Include metadata about the user (language, location, etc.) to get more personalized results. Here is the final prompt structure from Figure 6.22, showing how all these pieces come together.\ngraph TD subgraph \u0026#34;Final Prompt Components\u0026#34; A[\u0026#34;User\u0026#39;s initial query\u0026#34;] B[\u0026#34;RETRIEVED CONTEXT\u0026lt;br/\u0026gt;Document chunks\u0026#34;] C[\u0026#34;INSTRUCTIONS\u0026lt;br/\u0026gt;System role\u0026#34;] D[\u0026#34;EXAMPLES\u0026lt;br/\u0026gt;Few-shot demos\u0026#34;] E[\u0026#34;REASONING\u0026lt;br/\u0026gt;CoT instruction\u0026#34;] F[\u0026#34;USER INFO\u0026lt;br/\u0026gt;Language/context\u0026#34;] end subgraph \u0026#34;Prompt Engineering Techniques\u0026#34; L5[Retrieved Context] L1[Role-Specific Prompting] L2[Few-Shot Prompting] L3[Chain-of-Thought] L4[User-Context Prompting] end B -.-\u0026gt; L5 C -.-\u0026gt; L1 D -.-\u0026gt; L2 E -.-\u0026gt; L3 F -.-\u0026gt; L4 Training (Advanced Topic: RAFT) Most of the time, you start with pre-trained models. But what if your retrieval is noisy and the LLM struggles to distinguish good context from bad? RAFT (Retrieval-Augmented Fine-Tuning) is a technique to solve this.\nThe Idea: During finetuning, you create training examples that include the question, a \u0026ldquo;golden\u0026rdquo; (correct) document, AND several \u0026ldquo;distractor\u0026rdquo; (irrelevant) documents that were also retrieved. The Goal: You train the LLM to specifically pay attention to the golden document and ignore the distractors when generating the answer. This makes the LLM more robust to imperfect retrieval. Step 5: Evaluation (CRITICAL for RAG) Evaluating a RAG system is more complex than a standard LLM. You need to evaluate both the retriever and the generator. The book introduces an excellent \u0026ldquo;Triad of RAG evaluation\u0026rdquo; (Figure 6.23).\ngraph TD Query --\u0026gt;|Context Relevance| Context Context --\u0026gt;|Faithfulness| Results Query --\u0026gt;|Answer Relevance \u0026amp; Correctness| Results This diagram shows that the final Results (the generated answer) depend on the Query, the retrieved Context, and the relationships between them. This leads to four key evaluation aspects:\nContext Relevance: Is the retriever working? Did we retrieve documents that are relevant to the query?\nMetrics: Standard information retrieval metrics like Precision@k, nDCG, Hit Rate. You need a labeled dataset of (query, relevant_doc) pairs for this. Faithfulness (or Groundedness): Is the generator hallucinating? Is the generated answer factually consistent with the retrieved context? You check if every statement in the answer can be backed up by the provided snippets.\nMethods: This is hard to automate. Often requires human evaluation or using another powerful LLM as a judge. Figure 6.24 shows a great example: if the context says Marie Curie won two Nobel prizes, an answer saying she won one has low faithfulness. Answer Relevance: Did the generator answer the user\u0026rsquo;s actual question? The retrieved context might be relevant, but the LLM could get sidetracked and generate an answer that doesn\u0026rsquo;t directly address the user\u0026rsquo;s intent.\nMethods: Again, often requires a human or an LLM judge to compare the user\u0026rsquo;s query and the final answer. Answer Correctness: Is the answer factually correct according to a ground truth reference? This is the classic accuracy measure.\nIn an interview, discussing this four-part evaluation framework shows a deep, practical understanding of the challenges of building reliable RAG systems.\nStep 6: Overall ML System Design This is the final blueprint. Figure 6.27 shows the end-to-end flow. Let\u0026rsquo;s recreate and walk through it.\ngraph TD subgraph \u0026#34;Offline Process\u0026#34; direction TB A[Document Databases] --\u0026gt; B(Document Parsing \u0026amp; Chunking) B --\u0026gt; C(Text Encoder) B --\u0026gt; C_img(Image Encoder) C --\u0026gt; D[Text Index] C_img --\u0026gt; D_img[Image Index] end subgraph \u0026#34;Online Process\u0026#34; direction TB E[User Query] --\u0026gt; F(Safety Filtering) F --\u0026gt; G(Query Expansion) G --\u0026gt; H(Text Encoder) H --\u0026gt; I(Vector DB with Nearest Neighbor Search) I --\u0026gt; J(Prompt Engineering) J --\u0026gt; K[LLM] K --\u0026gt; L(Safety Filtering) L --\u0026gt; M[Response] end D --\u0026gt; I D_img --\u0026gt; I E --\u0026gt; J A user query\u0026rsquo;s journey:\nOffline: A pipeline runs periodically to Parse, Chunk, and Index all 5 million documents into a vector database. This is the Indexing Process. Online: A user sends a query. Safety Filtering: The query is checked for harmful content. Query Expansion (Optional but good): The query is expanded with synonyms or rephrased to improve retrieval. \u0026ldquo;How much do I get for trips?\u0026rdquo; -\u0026gt; \u0026ldquo;travel reimbursement policy allowance\u0026rdquo;. Retrieval: The query is encoded into a vector, and an ANN search is performed on the vector DB to get the top-k chunks. Generation: The user\u0026rsquo;s query and the retrieved chunks are assembled into a prompt using techniques like CoT and role-prompting. This is fed to the LLM. Safety Filtering: The LLM\u0026rsquo;s response is checked for safety, PII, etc. The final, safe, and grounded response is sent to the user. This diagram is your high-level design for the interview. Being able to draw this and explain each component\u0026rsquo;s purpose and the trade-offs involved is an A+ answer.\nOf course. This is the perfect next step. A high-level design gets you hired, but a low-level design shows you\u0026rsquo;ve actually built these things. You\u0026rsquo;re demonstrating that you\u0026rsquo;ve grappled with the real-world trade-offs between specific libraries, algorithms, and cloud services.\nLet\u0026rsquo;s design the Low-Level Design (LLD) for our ChatPDF RAG system, grounding it entirely within the AWS ecosystem and making concrete choices for components. We\u0026rsquo;ll focus on the \u0026ldquo;why\u0026rdquo; for each choice.\nHere is the High-Level Design (HLD) from before, which we will now break down into a detailed LLD.\ngraph TD subgraph Offline Process direction TB A[Document Databases] --\u0026gt; B(Document Parsing \u0026amp; Chunking) B --\u0026gt; C(Embedding Model) C --\u0026gt; D[Vector Index] end subgraph Online / Inference Process direction LR E[User Query] --\u0026gt; F(Safety \u0026amp; Preprocessing) F --\u0026gt; G(Embedding Model) G --\u0026gt; H(ANN Search) D --\u0026gt; H H --\u0026gt; I(Prompt Engineering) E --\u0026gt; I I --\u0026gt; J[LLM] J --\u0026gt; K(Safety \u0026amp; Postprocessing) K --\u0026gt; M[Response] end Low-Level Design: \u0026ldquo;ChatPDF\u0026rdquo; on AWS We\u0026rsquo;ll dissect the HLD into two core pipelines: the Indexing Pipeline (Offline) and the Inference Pipeline (Online).\n1. The Indexing Pipeline (Asynchronous \u0026amp; Event-Driven) Goal: Process new or updated PDFs from a source location, chunk them, embed them, and store them in our vector database with minimal manual intervention. We\u0026rsquo;ll design this to be robust and scalable.\nAWS Services \u0026amp; Architecture: graph TD subgraph \u0026#34;Indexing Pipeline - AWS\u0026#34; A[S3 Bucket\u0026lt;br/\u0026gt;company-docs-raw] --\u0026gt;|ObjectCreated event| B(AWS Lambda\u0026lt;br/\u0026gt;s3-trigger-lambda) B --\u0026gt;|Publishes PDF key| C[SQS Queue\u0026lt;br/\u0026gt;pdf-processing-queue] D[Auto Scaling Group EC2\u0026lt;br/\u0026gt;c6i.xlarge instances\u0026lt;br/\u0026gt;Python, UnstructuredPDFLoader, LangChain] --\u0026gt;|Polls| C D --\u0026gt;|Parsed \u0026amp; Chunked Data| E[S3 Bucket\u0026lt;br/\u0026gt;company-docs-chunks] E --\u0026gt;|ObjectCreated event| F(AWS Lambda\u0026lt;br/\u0026gt;embedding-lambda) F --\u0026gt;|Invokes| G[SageMaker Endpoint\u0026lt;br/\u0026gt;g5.2xlarge with\u0026lt;br/\u0026gt;SentenceTransformer/CLIP] G --\u0026gt;|Returns embeddings| F F --\u0026gt;|Writes data| H[Amazon OpenSearch Service\u0026lt;br/\u0026gt;with k-NN index] end Low-Level Component Breakdown \u0026amp; Trade-offs: A. Document Source: S3 Bucket: 'company-docs-raw'\nWhy S3? It\u0026rsquo;s the de-facto standard for object storage on AWS. It\u0026rsquo;s infinitely scalable, durable (11 nines!), and has rich event notification features, which are perfect for triggering our pipeline. B \u0026amp; C. Triggering Mechanism: S3 Event -\u0026gt; Lambda -\u0026gt; SQS Queue\nWhy this pattern? This is a classic \u0026ldquo;decoupling\u0026rdquo; pattern for resilient systems. The s3-trigger-lambda is a tiny, fast function that simply takes the new S3 object key and puts it into an SQS queue. It\u0026rsquo;s cheap and reliable. Trade-off: We could have the Lambda do the whole processing job. Why not? PDF parsing can be slow and memory-intensive. Lambda has time limits (max 15 mins) and memory constraints. A large or complex PDF could cause it to fail. By pushing the job to a queue, we separate the trigger from the heavy lifting. Why SQS? It acts as a buffer. If we upload 10,000 documents at once, SQS holds the jobs, and our processing fleet can work through them at its own pace. It also provides automatic retries for failed jobs, making the pipeline robust. D. Parsing \u0026amp; Chunking Service: Auto Scaling Group of EC2\nWhy EC2, not Lambda? UnstructuredPDFLoader processing can be compute-intensive, especially when handling OCR for scanned documents. We need dedicated compute with more control over the environment and no execution time limits. An Auto Scaling Group allows us to scale the number of worker nodes up or down based on the queue depth in SQS, which is extremely cost-efficient. Instance Choice: c6i.xlarge are compute-optimized with sufficient CPU and memory for OCR operations and complex document processing. Tooling: Parser \u0026amp; Chunker: UnstructuredPDFLoader from langchain_community.document_loaders. This is our unified solution that handles both parsing and intelligent chunking: OCR Capabilities: Automatically handles scanned PDFs and images with built-in OCR, eliminating the need for separate OCR preprocessing. Structure Understanding: Unlike PyMuPDF\u0026rsquo;s raw text extraction, UnstructuredPDFLoader understands document hierarchy, preserving headers, paragraphs, tables, and lists as structured elements. Intelligent Chunking: Superior to RecursiveCharacterTextSplitter because it chunks based on semantic document structure rather than arbitrary character counts, leading to more meaningful embeddings. Multi-format Support: Handles various PDF types including forms, scanned documents, and complex layouts that would break simpler parsers. Metadata Enrichment: Each chunk comes with rich metadata about document structure, element types, and positioning, enabling better retrieval strategies. Configuration: Allows fine-tuning of chunk sizes while respecting natural document boundaries, with configurable overlap strategies that preserve context intelligently. E, F, G. Embedding Service: S3 -\u0026gt; Lambda -\u0026gt; SageMaker Endpoint\nWhy SageMaker? It\u0026rsquo;s AWS\u0026rsquo;s managed service for deploying ML models. It handles autoscaling, provides GPU instances for fast inference, and gives us a simple REST API endpoint. We don\u0026rsquo;t have to manage CUDA drivers or model servers ourselves.\nInstance Choice: g5.2xlarge. These have NVIDIA A10G GPUs, which are excellent for transformer inference.\nEmbedding Model Choice: Deep Dive into FastEmbed vs. Sentence Transformers\nThis is a critical decision that affects both quality and cost. Let\u0026rsquo;s break down the trade-offs:\nComprehensive Comparison: FastEmbed vs. Sentence Transformers Aspect FastEmbed Sentence Transformers Our Choice \u0026amp; Reasoning Performance Faster: ONNX runtime, quantization, optimized inference Slower: PyTorch-based, full precision by default Context-dependent: FastEmbed for high-throughput, ST for quality-first Model Quality Good: Same base models, slight quality degradation from quantization Better: Full precision models, no optimization artifacts Sentence Transformers: We prioritize retrieval quality for better user experience Infrastructure Requirements CPU-optimized: Runs efficiently on CPU instances GPU-preferred: Benefits significantly from GPU acceleration ST + GPU: Better cost/quality ratio at our scale Memory Footprint Small: Quantized models, ~100-200MB Large: Full models, ~400-500MB FastEmbed: Better for memory-constrained environments Cold Start Time Fast: Small models load quickly Slow: Larger models take time to load FastEmbed: Better for Lambda/serverless Ecosystem Integration Simple: Plug-and-play, minimal dependencies Rich: Extensive model zoo, fine-tuning capabilities ST: Better for experimentation and model iteration Total Cost Lower: Cheaper CPU instances, faster processing Higher: GPU instances, slower throughput Depends on scale: FastEmbed wins at high volume Our Decision Matrix: For Production (Current Choice): Sentence Transformers on GPU\n# SageMaker Endpoint Configuration model = SentenceTransformer(\u0026#39;all-mpnet-base-v2\u0026#39;) instance_type = \u0026#39;ml.g5.2xlarge\u0026#39; # GPU instance # Reasoning: # 1. Quality-first approach for better user experience # 2. GPU cost amortizes well at our expected volume # 3. Rich ecosystem for future model updates # 4. Consistent with research best practices When We\u0026rsquo;d Switch to FastEmbed:\n# High-volume, cost-sensitive scenario from fastembed import TextEmbedding model = TextEmbedding(model_name=\u0026#39;BAAI/bge-small-en-v1.5\u0026#39;) instance_type = \u0026#39;ml.c6i.2xlarge\u0026#39; # CPU instance # Conditions for switch: # 1. Query volume \u0026gt; 10M/month (cost becomes primary factor) # 2. Retrieval quality is \u0026#34;good enough\u0026#34; (validated through A/B testing) # 3. Infrastructure simplification is prioritized # 4. Cold start latency is critical (Lambda deployments) Evolution Strategy: Phase 1 (Launch): Sentence Transformers for quality Phase 2 (Scale): A/B test FastEmbed vs. ST on retrieval metrics Phase 3 (Optimize): Switch to FastEmbed if quality delta is acceptable\nFor Images: We\u0026rsquo;d use the pre-trained CLIP model\u0026rsquo;s image encoder, also hosted on a SageMaker endpoint, to ensure text and image embeddings are in the same space.\nH. Vector Database: Amazon OpenSearch Service\nWhy OpenSearch? It\u0026rsquo;s a managed version of Elasticsearch that AWS supports directly. Its key feature for us is the k-NN (k-Nearest Neighbor) plugin. It allows OpenSearch to function as a powerful, scalable vector database. Trade-off vs. Qdrant or Pinecone: Qdrant: An excellent, open-source vector database written in Rust, optimized for performance and memory safety. It offers advanced features like filtering during search. If we needed the absolute best retrieval performance and were willing to manage it ourselves (e.g., on EKS), Qdrant is a top contender. Pinecone: A fully managed, SaaS vector database. It\u0026rsquo;s incredibly easy to use and provides S-tier performance. However, it exists outside the core AWS ecosystem, which can complicate security, networking (VPC peering), and billing. Conclusion: We choose OpenSearch because it lives within our AWS account. This simplifies IAM permissions, networking, and keeps our data inside our VPC. It\u0026rsquo;s \u0026ldquo;good enough\u0026rdquo; for most use cases and much easier to manage than a self-hosted solution. Distance Metric Choice: Cosine Similarity Why? Cosine similarity measures the angle between two vectors, ignoring their magnitude. For text embeddings, we care about the direction (semantic meaning), not the vector\u0026rsquo;s length. It is the standard and most effective choice for normalized transformer embeddings. Trade-off vs. Euclidean Distance (L2): Measures the straight-line distance. It\u0026rsquo;s sensitive to vector magnitude. It can be a good choice for other types of data (e.g., image feature vectors) but is generally less effective for text. Dot Product is another option, very similar to Cosine for normalized vectors. We stick with the standard. 2. The Inference Pipeline (Real-Time \u0026amp; Low-Latency) Goal: Take a user\u0026rsquo;s API request, find the relevant context, and generate a safe, accurate answer as quickly as possible.\nAWS Services \u0026amp; Architecture: graph TD subgraph \u0026#34;Inference Pipeline - AWS\u0026#34; A[User via API Gateway] --\u0026gt; B(AWS Lambda\u0026lt;br/\u0026gt;inference-handler) B --\u0026gt;|Query text| C[SageMaker Endpoint\u0026lt;br/\u0026gt;SentenceTransformer] C --\u0026gt;|Returns query_embedding| B B --\u0026gt;|Search for query_embedding| D[Amazon OpenSearch Service\u0026lt;br/\u0026gt;k-NN Search] D --\u0026gt;|Top 5 chunks| B B --\u0026gt;|Builds final prompt| E[Amazon Bedrock\u0026lt;br/\u0026gt;Claude 3 Sonnet] E --\u0026gt;|Streams response| B B --\u0026gt;|Streams response| A F((Safety \u0026amp; Guardrails)) --\u0026gt;|Implemented within| B F --\u0026gt;|Configures| E end Low-Level Component Breakdown \u0026amp; Trade-offs: A. API Layer: API Gateway\nWhy? It\u0026rsquo;s the standard, managed way to create REST APIs on AWS. It handles authentication (e.g., with Cognito or IAM), throttling, caching, and routing requests to our backend logic. B. Backend Logic: AWS Lambda: 'inference-handler'\nWhy Lambda? The \u0026ldquo;glue\u0026rdquo; logic is stateless and involves a series of network calls. This is a perfect use case for Lambda. It\u0026rsquo;s fast to start, scales to zero (so we don\u0026rsquo;t pay when no one is using it), and scales out automatically under load. Function: Receive the request from API Gateway. Perform pre-processing/safety checks. Call the SentenceTransformer SageMaker endpoint to embed the user\u0026rsquo;s query. Query OpenSearch with the embedding to get the top-k chunks. Construct the final prompt from the template, user query, and retrieved chunks. Call the LLM. Perform post-processing/safety checks on the response. Stream the response back. C. Query Embedding: SageMaker Endpoint (SentenceTransformer)\nWhy? We reuse the exact same model from the indexing pipeline to ensure the query vector is in the same space as the document vectors. Model Consistency: Critical that this is identical to the indexing model. Even minor version differences can cause embedding drift, leading to poor retrieval performance. Alternative Consideration: For high-query-volume scenarios (\u0026gt;10M/month), we might consider switching both indexing and inference to FastEmbed for cost optimization, but only after validating that retrieval quality remains acceptable through A/B testing. D. ANN Search: Amazon OpenSearch Service\nWhy? We query the index we built earlier. A typical query would look like: {\u0026quot;query\u0026quot;: {\u0026quot;knn\u0026quot;: {\u0026quot;embedding_field\u0026quot;: {\u0026quot;vector\u0026quot;: [0.1, 0.2, ...], \u0026quot;k\u0026quot;: 5}}}}. OpenSearch will use its ANN algorithm (like HNSW, which it supports) to return the 5 nearest neighbors with low latency. E. LLM Generation: Amazon Bedrock (Anthropic's Claude 3 Sonnet)\nWhy Bedrock? This is AWS\u0026rsquo;s managed service for foundation models. It gives us API access to models from AI21, Anthropic, Cohere, Meta, etc., without needing to host them. This is a huge win for simplicity, security, and pay-per-use pricing. Model Choice: Claude 3 Sonnet Why? As of today, the Claude 3 family is S-tier. Sonnet is the middle model, offering a fantastic balance of intelligence, speed, and cost. It has a large context window (200k tokens), is great at following complex instructions, and has a lower hallucination rate, which is perfect for a RAG system. Trade-off vs. Claude 3 Haiku: Haiku is faster and cheaper, but less intelligent. We might use Haiku if our queries were simpler and speed was the absolute priority. Trade-off vs. Claude 3 Opus: Opus is the most powerful model, but slower and more expensive. We might use Opus for a premium \u0026ldquo;expert\u0026rdquo; version of our chatbot. Sonnet is the ideal balanced choice. F. Safety: Implemented in Lambda \u0026amp; Bedrock Guardrails\nWhy a dual approach? Lambda: We can implement simple pre-filters on the user query (e.g., regex for PII patterns, keyword blocklists). We can also do post-processing on the final output. Bedrock Guardrails: This is a powerful managed feature. We can configure policies to block harmful topics, filter out specific words, and even prevent the model from answering questions outside its scope (e.g., \u0026ldquo;Don\u0026rsquo;t answer questions about medical advice\u0026rdquo;). This is a more robust and scalable approach to safety than trying to implement it all ourselves. This LLD provides a concrete, defensible, and modern blueprint for building a production-grade RAG system on AWS. It makes specific technology choices and, most importantly, provides the reasoning and trade-offs behind each one.\nKey Architectural Decisions Summary: Document Processing: UnstructuredPDFLoader for comprehensive OCR and structure-aware parsing Embedding Strategy: Sentence Transformers (quality-first) with clear migration path to FastEmbed (cost-optimization) Vector Database: OpenSearch within AWS ecosystem vs. external specialized solutions Compute Distribution: EC2 for heavy processing, Lambda for orchestration, SageMaker for ML inference Event-Driven Architecture: S3 → Lambda → SQS → EC2 for robust, scalable document processing Each choice reflects real-world production considerations: balancing quality, cost, operational complexity, and future scalability.\nAdvanced RAG patterns These advanced patterns—Multi-Query, Multi-Hop, and Routing—all revolve around a central theme: making the retrieval step more intelligent. Instead of a single, straightforward search, we are introducing a Reasoning Engine or an Orchestration Layer that plans and executes a more complex retrieval strategy.\nLet\u0026rsquo;s break down the HLD and LLD for each.\nFoundational Concept: The Reasoning Engine Before diving into the specific patterns, let\u0026rsquo;s establish our core architectural change. We are inserting a \u0026ldquo;smart\u0026rdquo; component right after the user query comes in.\ngraph TD A[\u0026#34;User Query\u0026#34;] --\u0026gt; B{Reasoning Engine} B --\u0026gt; C[\u0026#34;Intelligent Retrieval Execution\u0026#34;] C --\u0026gt; D[\u0026#34;LLM Generator\u0026#34;] D --\u0026gt; E[\u0026#34;Response\u0026#34;] This \u0026ldquo;Reasoning Engine\u0026rdquo; is the brain of our advanced RAG. In most cases, it will be powered by an LLM itself—a fast and cheap one like Claude 3 Haiku or Llama 3 8B—whose job is not to answer the question, but to decompose the question into a plan.\n1. Multi-Query RAG Concept: The user asks a single complex question that implicitly requires looking up multiple things. The system breaks it down into several sub-queries, executes them in parallel, and synthesizes the results.\nUse Case: \u0026ldquo;Compare and contrast the battery life of the iPhone 15 and the Google Pixel 8.\u0026rdquo;\nHigh-Level Design (HLD) The key here is the \u0026ldquo;Query Decomposer\u0026rdquo; and the parallel \u0026ldquo;fan-out\u0026rdquo; retrieval.\ngraph TD subgraph \u0026#34;Multi-Query RAG HLD\u0026#34; A[\u0026#34;User Query\u0026#34;] --\u0026gt; B(\u0026#34;Query Decomposer\u0026#34;) B --\u0026gt; C1(\u0026#34;Retriever 1\u0026#34;) B --\u0026gt; C2(\u0026#34;Retriever 2\u0026#34;) C1 --\u0026gt; D{Synthesizer} C2 --\u0026gt; D D --\u0026gt; E[\u0026#34;Response\u0026#34;] end Low-Level Design (LLD) on AWS The \u0026ldquo;Query Decomposer\u0026rdquo; is an LLM call, and the parallel retrieval is an asynchronous operation within our orchestrator Lambda.\ngraph TD A[\u0026#34;User via API Gateway\u0026#34;] --\u0026gt; B(\u0026#34;AWS Lambda: orchestrator-lambda\u0026#34;) subgraph \u0026#34;Step 1: Decompose Query\u0026#34; B --\u0026gt; C[\u0026#34;Amazon Bedrock\u0026lt;br/\u0026gt;Claude 3 Haiku\u0026#34;] C --\u0026gt; B end subgraph \u0026#34;Step 2: Parallel Retrieval\u0026#34; B --\u0026gt; D1[\u0026#34;OpenSearch k-NN Search\u0026#34;] B --\u0026gt; D2[\u0026#34;OpenSearch k-NN Search\u0026#34;] end subgraph \u0026#34;Step 3: Synthesize\u0026#34; D1 --\u0026gt; B D2 --\u0026gt; B B --\u0026gt; E[\u0026#34;Amazon Bedrock\u0026lt;br/\u0026gt;Claude 3 Sonnet\u0026#34;] E --\u0026gt; B end B --\u0026gt; F[\u0026#34;Response Stream\u0026#34;] Key LLD Components \u0026amp; Trade-offs:\nOrchestrator (orchestrator-lambda): A single Lambda function coordinates the entire process. Query Decomposer (Claude 3 Haiku): Why Haiku? This is a structured task: text-to-JSON. It doesn\u0026rsquo;t require deep reasoning. Haiku is extremely fast and cheap, making it perfect for this pre-processing step. We use function calling / tool use features to ensure the LLM returns a clean, machine-readable JSON array of queries. Prompt: \u0026quot;Given the user's question, generate a JSON list of simple, self-contained search queries needed to answer it. Question: {user_question}\u0026quot; Parallel Retrieval: The Lambda function will use Python\u0026rsquo;s asyncio.gather to make multiple, concurrent calls to our OpenSearch cluster. This is a critical optimization. A naive, sequential approach would double the retrieval latency. Synthesizer (Claude 3 Sonnet): Why Sonnet? This step requires more intelligence. The model needs to understand two different sets of context and perform a comparison. Sonnet provides a good balance of reasoning power and speed for this. Prompt: \u0026quot;You have been given the following information. Context for 'iPhone 15 battery life': {context_1}. Context for 'Pixel 8 battery life': {context_2}. Now, answer the user's original question: {user_question}\u0026quot; 2. Multi-Hop RAG Concept: The system answers a question that requires a sequence of searches, where the results of the first search are needed to formulate the second search.\nUse Case: \u0026ldquo;Which actor played the main character in the movie directed by the person who directed \u0026lsquo;Inception\u0026rsquo;?\u0026rdquo;\nHop 1: \u0026ldquo;Who directed \u0026lsquo;Inception\u0026rsquo;?\u0026rdquo; -\u0026gt; Christopher Nolan Hop 2: \u0026ldquo;Which movie did Christopher Nolan direct where X was the main character?\u0026rdquo; (This is tricky, shows the limits) or more simply \u0026ldquo;Who was the main actor in \u0026lsquo;The Dark Knight\u0026rsquo;?\u0026rdquo; (assuming a known movie). A better example from the prompt: \u0026ldquo;Who are the largest car manufacturers? Do they make EVs?\u0026rdquo;. Hop 1: \u0026ldquo;largest car manufacturers 2023\u0026rdquo; -\u0026gt; Toyota, VW, Hyundai. Hop 2: \u0026ldquo;Toyota EV models\u0026rdquo;, \u0026ldquo;VW EV models\u0026rdquo;, \u0026ldquo;Hyundai EV models\u0026rdquo;. High-Level Design (HLD) The key is a loop or sequence in the reasoning engine.\ngraph TD subgraph \u0026#34;Multi-Hop RAG HLD\u0026#34; A[\u0026#34;User Query\u0026#34;] --\u0026gt; B{Reasoning Engine} B --\u0026gt; C(\u0026#34;Retriever\u0026#34;) C --\u0026gt; B B --\u0026gt; C C --\u0026gt; B B --\u0026gt; D[\u0026#34;Response\u0026#34;] end Low-Level Design (LLD) on AWS This sequential, stateful process is a perfect use case for AWS Step Functions. It\u0026rsquo;s more robust and observable than trying to code a complex loop inside a single Lambda.\ngraph TD A[\u0026#34;Start\u0026#34;] --\u0026gt; B(\u0026#34;Generate Hop 1 Query - Lambda\u0026#34;) B --\u0026gt; C(\u0026#34;Perform Search 1 - Lambda\u0026#34;) C --\u0026gt; D(\u0026#34;Generate Hop 2 Queries - Lambda\u0026#34;) D --\u0026gt; E(\u0026#34;Perform Parallel Search 2 - Map State\u0026#34;) E --\u0026gt; F(\u0026#34;Synthesize Results - Lambda\u0026#34;) F --\u0026gt; G[\u0026#34;End\u0026#34;] subgraph \u0026#34;AWS Step Functions Workflow\u0026#34; direction TB B1(\u0026#34;Lambda 1\u0026#34;) --\u0026gt; C1(\u0026#34;Lambda 2\u0026#34;) C1 --\u0026gt; D1(\u0026#34;Lambda 3\u0026#34;) D1 --\u0026gt; E1(\u0026#34;Map State\u0026#34;) E1 --\u0026gt; F1(\u0026#34;Lambda 4\u0026#34;) end Key LLD Components \u0026amp; Trade-offs:\nOrchestration (AWS Step Functions): Why? It\u0026rsquo;s designed for orchestrating multi-step workflows. It handles state management (passing the results of Hop 1 to Hop 2), error handling, and retries automatically. Debugging is much easier as you can visualize the execution flow and inspect the inputs/outputs of each step. Trade-off: There is a slight cold-start and state-transition overhead compared to a single \u0026ldquo;god Lambda\u0026rdquo;. However, for a complex workflow like multi-hop, the reliability and maintainability gains are immense. State Machine Steps: Generate Hop 1 Query (Lambda): A simple step. Takes the user query, calls Haiku to get the first search query. Perform Search 1 (Lambda): Calls OpenSearch with the query from the previous step. Generate Hop 2 Queries (Lambda): This is a key step. It takes the results from step 2 (e.g., the text \u0026ldquo;The largest manufacturers are Toyota, VW, and Hyundai\u0026rdquo;) and calls Haiku to generate the next set of queries (e.g., [\u0026quot;Toyota EV models\u0026quot;, \u0026quot;VW EV models\u0026quot;, ...]). Perform Parallel Search 2 (Map State): The Map state in Step Functions is brilliant for this. It takes the array of queries from step 3 and runs a search Lambda in parallel for each item in the array. Synthesize Results (Lambda): This final step collects the results from all previous hops and uses a powerful model like Sonnet to generate the final, coherent answer. 3. Query Routing Concept: The system has access to multiple, distinct knowledge bases (e.g., different document indexes, different databases). The router decides which data source is the most appropriate for a given query.\nUse Case: \u0026ldquo;What is our company\u0026rsquo;s PTO policy?\u0026rdquo; -\u0026gt; query HR docs. \u0026ldquo;What was the revenue from customer X last quarter?\u0026rdquo; -\u0026gt; query Salesforce.\nHigh-Level Design (HLD) The key component is the \u0026ldquo;Router\u0026rdquo; which acts as a switchboard.\ngraph TD subgraph \u0026#34;Query Routing HLD\u0026#34; A[\u0026#34;User Query\u0026#34;] --\u0026gt; B{Router} B --\u0026gt; C1(\u0026#34;HR Document Retriever\u0026#34;) B --\u0026gt; C2(\u0026#34;Sales Data Retriever\u0026#34;) B --\u0026gt; C3(\u0026#34;General Wiki Retriever\u0026#34;) C1 --\u0026gt; D{Generator} C2 --\u0026gt; D C3 --\u0026gt; D D --\u0026gt; E[\u0026#34;Response\u0026#34;] end Low-Level Design (LLD) on AWS The Router is another LLM call. The different retrievers could be different OpenSearch indexes or even completely different systems (like the Salesforce API).\ngraph TD A[\u0026#34;User via API Gateway\u0026#34;] --\u0026gt; B(\u0026#34;AWS Lambda: router-orchestrator\u0026#34;) subgraph \u0026#34;Step 1: Route Query\u0026#34; B --\u0026gt; C[\u0026#34;Amazon Bedrock\u0026lt;br/\u0026gt;Claude 3 Haiku\u0026#34;] C --\u0026gt; B end subgraph \u0026#34;Step 2: Conditional Retrieval\u0026#34; B --\u0026gt; D1[\u0026#34;OpenSearch hr-index\u0026#34;] B --\u0026gt; D2[\u0026#34;Salesforce API\u0026#34;] B --\u0026gt; D3[\u0026#34;OpenSearch wiki-index\u0026#34;] end subgraph \u0026#34;Step 3: Generate\u0026#34; D1 --\u0026gt; E{Generate Response} D2 --\u0026gt; E D3 --\u0026gt; E E --\u0026gt; F[\u0026#34;Final Answer\u0026#34;] end Key LLD Components \u0026amp; Trade-offs:\nRouter (Claude 3 Haiku): Again, a fast, cheap model is used for classification. The prompt is crucial. Prompt: \u0026quot;You are a query router. Given the user's question, determine the best data source to answer it. The available sources are: 'HR_DOCS' for questions about employment, PTO, and policies; 'SALESFORCE_DATA' for questions about specific customers, leads, or revenue; and 'GENERAL_WIKI' for all other topics. Return your answer as a single JSON object like {'source': 'CHOSEN_SOURCE'}. Question: {user_question}\u0026quot; Conditional Logic (router-orchestrator Lambda): The Lambda\u0026rsquo;s code contains a simple if/elif/else block based on the source returned by the router LLM. Heterogeneous Data Sources: This is the most realistic part. OpenSearch: We\u0026rsquo;ll have separate indexes for HR docs and the general wiki. This prevents data leakage and allows us to set different access permissions. Salesforce API: For this route, we wouldn\u0026rsquo;t use vector search. The Lambda might use the LLM to generate a Salesforce Object Query Language (SOQL) query, then execute it against the Salesforce API via a connected app. This demonstrates an ability to integrate with non-vector-search systems. Trade-off: The primary risk is the accuracy of the router. If the router misclassifies a query, the system will look in the wrong place and fail to find the answer, even if it exists. This requires careful prompt engineering and potentially finetuning the router model on examples of correctly routed queries. You might also build a fallback mechanism: if the primary source returns no results, try the GENERAL_WIKI as a backup. Beyond Traditional RAG: InfiniRetri and the Future of Long-Context Processing Excellent question. This is exactly the kind of critical thinking required at a senior level: not just knowing existing patterns like RAG, but constantly evaluating new research to see how it could evolve or even replace parts of your current system.\nYes, the paper \u0026ldquo;Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing\u0026rdquo; (arXiv:2406.19521) (which I\u0026rsquo;ll call InfiniRetri), is absolutely related to RAG. In fact, it\u0026rsquo;s a direct challenge to the \u0026ldquo;R\u0026rdquo; in RAG as we\u0026rsquo;ve designed it.\nLet\u0026rsquo;s break down what this paper proposes and how it compares to our classic RAG system.\nThe Core Problem This Paper Tackles Our current RAG design solves the long-context problem with a clear separation of concerns:\nRetriever (External Tool): An embedding model + a vector database (like OpenSearch) finds relevant information. Its job is to be a great librarian. Generator (LLM): The LLM receives the retrieved context and generates an answer. Its job is to be a great reasoner and writer, using only the documents the librarian gives it. The paper asks a fundamental question: \u0026ldquo;Why not use the retrieval capabilities of LLMs themselves to handle long contexts?\u0026rdquo;\nThey observe that the attention mechanism inside a Transformer is, in essence, a retrieval mechanism. For each token in the query (the \u0026ldquo;Query\u0026rdquo;), the model learns to \u0026ldquo;attend\u0026rdquo; to the most relevant tokens in the context (the \u0026ldquo;Keys\u0026rdquo;). They show that in the deeper layers of an LLM, this attention pattern becomes very accurate at locating the exact phrases needed to answer a question.\nSo, the core insight is: The LLM already knows how to retrieve. Let\u0026rsquo;s build a system that leverages this internal retrieval ability instead of relying on an external one.\nHow InfiniRetri Works: The \u0026ldquo;Slide and Retrieve\u0026rdquo; Method Imagine you\u0026rsquo;re reading a 1000-page book, but you can only see one page at a time (your \u0026ldquo;context window\u0026rdquo;). To answer a question about the whole book, you would:\nRead page 1. Jot down the most important sentences on a sticky note. Read page 2, keeping your sticky note in view. Update your sticky note with important sentences from page 2, maybe crossing out less important ones from page 1. Repeat until you\u0026rsquo;ve read all 1000 pages. Your final sticky note is a compressed summary of the entire book\u0026rsquo;s relevant information. This is exactly what InfiniRetri does:\ngraph TD subgraph \u0026#34;InfiniRetri: Sliding Window with Attention-Based Retrieval\u0026#34; A[\u0026#34;Long Document\u0026lt;br/\u0026gt;1M tokens\u0026#34;] --\u0026gt;|Split| B[\u0026#34;Chunk 1\u0026lt;br/\u0026gt;4K tokens\u0026#34;] A --\u0026gt;|Split| C[\u0026#34;Chunk 2\u0026lt;br/\u0026gt;4K tokens\u0026#34;] A --\u0026gt;|Split| D[\u0026#34;Chunk N\u0026lt;br/\u0026gt;4K tokens\u0026#34;] E[\u0026#34;User Query\u0026#34;] --\u0026gt; F[\u0026#34;Process Chunk 1 + Query\u0026#34;] B --\u0026gt; F F --\u0026gt;|Internal Attention| G[\u0026#34;Identify Key Sentences\u0026lt;br/\u0026gt;from Chunk 1\u0026#34;] G --\u0026gt; H[\u0026#34;Compressed Cache\u0026lt;br/\u0026gt;512 tokens\u0026#34;] H --\u0026gt; I[\u0026#34;Process Chunk 2 + Cache + Query\u0026#34;] C --\u0026gt; I I --\u0026gt;|Internal Attention| J[\u0026#34;Update Cache\u0026lt;br/\u0026gt;with Key Sentences\u0026#34;] J --\u0026gt; K[\u0026#34;Updated Cache\u0026lt;br/\u0026gt;512 tokens\u0026#34;] K --\u0026gt; L[\u0026#34;Process Chunk N + Cache + Query\u0026#34;] D --\u0026gt; L L --\u0026gt;|Internal Attention| M[\u0026#34;Final Cache\u0026lt;br/\u0026gt;Most Relevant Info\u0026#34;] M --\u0026gt; N[\u0026#34;Generate Final Answer\u0026#34;] E --\u0026gt; N end subgraph \u0026#34;Key Innovation\u0026#34; O[\u0026#34;LLM\u0026#39;s Attention Mechanism\u0026lt;br/\u0026gt;Acts as Retriever\u0026#34;] P[\u0026#34;No External Vector DB\u0026lt;br/\u0026gt;Required\u0026#34;] Q[\u0026#34;Sequential Processing\u0026lt;br/\u0026gt;Maintains Context Flow\u0026#34;] end Here\u0026rsquo;s the detailed breakdown:\nChunk: The long document (e.g., 1M tokens) is broken down into sequential chunks that fit within the model\u0026rsquo;s native context window (e.g., 4K tokens).\nSlide Window (Iterative Process):\nMerge: For the first chunk, the model processes it with the user\u0026rsquo;s question. Inference \u0026amp; Retrieval: The model uses its own internal attention scores to identify the most important sentences/phrases within that chunk. This is the \u0026ldquo;Retrieval in Attention\u0026rdquo; step. Cache: It saves these most important sentences into a small, compressed \u0026ldquo;cache\u0026rdquo; (like a sticky note). Iterate: For the next chunk, the model merges the content of the compressed cache with the new chunk, processes this combined input, and updates its cache. Final Answer: After iterating through all chunks, the final cache contains the most relevant information from the entire document.\nComprehensive Comparison: Traditional RAG vs. InfiniRetri Aspect Traditional RAG (Our Design) InfiniRetri Advantage Retrieval Mechanism External: SentenceTransformer + Vector DB (OpenSearch) Internal: LLM\u0026rsquo;s own attention mechanism InfiniRetri: Simpler architecture, potentially better semantic understanding Indexing Requirements Heavy Upfront: Must parse, chunk, and embed all 5M documents Zero Upfront: Document processed on-the-fly InfiniRetri: Real-time processing, no pre-computation Infrastructure Complexity High: S3, SQS, EC2, SageMaker, OpenSearch, Lambda Lower: Primarily LLM hosting + orchestration InfiniRetri: Reduced infrastructure footprint Contextual Cohesion Fragmented: k independent chunks to stitch together Sequential: Maintains narrative flow and order InfiniRetri: Better for document structure understanding Query Latency Fast: Vector search (ms) + single LLM call Slow: Multiple LLM calls per chunk Traditional RAG: Much better for interactive use Processing Cost Low at Query Time: Expensive indexing, cheap retrieval High at Query Time: No indexing, expensive inference Traditional RAG: Better for frequent queries Use Case Fit Multi-document Q\u0026amp;A: Large knowledge bases Single-document Analysis: Deep comprehension tasks Context-dependent Scalability Horizontal: Add more documents easily Vertical: Limited by single document processing Traditional RAG: Better for growing knowledge bases When to Choose Each Approach Traditional RAG is Best For: Interactive chatbots requiring sub-second responses Large, multi-document knowledge bases (like our 5M page corpus) Frequent, repetitive queries where indexing cost amortizes Real-time customer support scenarios InfiniRetri is Best For: Deep document analysis where users can wait minutes for comprehensive answers Novel, massive documents that haven\u0026rsquo;t been pre-processed Narrative understanding tasks requiring sequential reading One-off analysis of large documents (legal discovery, research papers) Integration Strategy: The Hybrid Approach A sophisticated system might offer both patterns:\ngraph TD subgraph \u0026#34;Hybrid RAG + InfiniRetri System\u0026#34; A[\u0026#34;User Query\u0026#34;] --\u0026gt; B{Query Type Detection} B --\u0026gt;|Quick Facts| C[\u0026#34;Traditional RAG Pipeline\u0026#34;] C --\u0026gt; D[\u0026#34;Vector Search + LLM\u0026#34;] D --\u0026gt; E[\u0026#34;Fast Response\u0026lt;br/\u0026gt;\u0026lt; 3 seconds\u0026#34;] B --\u0026gt;|Deep Analysis| F[\u0026#34;InfiniRetri Pipeline\u0026#34;] F --\u0026gt; G[\u0026#34;Sliding Window Processing\u0026#34;] G --\u0026gt; H[\u0026#34;Comprehensive Analysis\u0026lt;br/\u0026gt;2-5 minutes\u0026#34;] B --\u0026gt;|Hybrid Query| I[\u0026#34;Parallel Processing\u0026#34;] I --\u0026gt; J[\u0026#34;RAG for Quick Facts\u0026#34;] I --\u0026gt; K[\u0026#34;InfiniRetri for Deep Context\u0026#34;] J --\u0026gt; L[\u0026#34;Combined Response\u0026#34;] K --\u0026gt; L end Interview-Level Insight: Demonstrating Senior Awareness After presenting the classic RAG design, you could demonstrate senior-level awareness by saying:\n\u0026ldquo;This RAG architecture is a robust, low-latency solution for our problem. However, it\u0026rsquo;s worth noting the cutting edge of research is exploring alternatives. For instance, recent papers like \u0026lsquo;Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing\u0026rsquo; propose using the LLM\u0026rsquo;s own attention mechanism for retrieval in a streaming fashion.\nThis approach eliminates the need for an external vector database, simplifying the architecture and potentially improving retrieval quality since it uses the powerful LLM itself. The major trade-off is significantly higher inference latency, as it requires multiple passes over the document.\nTherefore, while not suitable for our interactive chat use case, this \u0026lsquo;RAG-without-an-index\u0026rsquo; pattern could be extremely powerful for a different product feature, like an offline \u0026lsquo;deep analysis\u0026rsquo; tool for digesting entire books or legal documents. This shows how the choice between these patterns is highly dependent on the specific product requirements for latency and interactivity.\u0026rdquo;\nSummary: Evolution, Not Revolution InfiniRetri is not a \u0026ldquo;RAG killer\u0026rdquo; – it\u0026rsquo;s an alternative with a different performance profile.\nFor our ChatPDF system, Traditional RAG remains the right choice due to latency requirements and multi-document nature InfiniRetri represents a \u0026ldquo;Third Way\u0026rdquo; between giant context windows (expensive) and traditional RAG (complex infrastructure) The future likely involves hybrid systems that choose the right pattern based on query type and user expectations This demonstrates that you\u0026rsquo;re not just following blueprints; you\u0026rsquo;re actively thinking about the future and making nuanced, product-aware technology choices. That\u0026rsquo;s top-tier engineering thinking.\n","permalink":"https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/chapter-6/","tags":null,"title":"Chapter 6: Retrieval-Augmented Generation"},{"categories":null,"content":"What this chapter is ultimately trying to achieve To provide a curated list of advanced topics that build upon the concepts learned in the book, encouraging continued learning and exploration. It highlights areas where innovation is happening, from architectural enhancements to security and ethical considerations.\nLet\u0026rsquo;s look at the topics mentioned:\n6.1 Mixture of Experts (MoE) What it\u0026rsquo;s ultimately trying to achieve: To significantly increase the total number of parameters in a model (making it \u0026ldquo;larger\u0026rdquo; and more knowledgeable) without proportionally increasing the computational cost during inference or training for each token. The Core Idea (Selective Specialization): Instead of every token passing through the same large MLP (Position-wise Feedforward Network) in a Transformer block, an MoE layer has multiple smaller MLP \u0026ldquo;experts.\u0026rdquo; A \u0026ldquo;router\u0026rdquo; network (or gate network) decides, for each token, which one or few experts that token should be processed by. Sparse Activation: Only a subset of experts is activated for any given token. This is why it\u0026rsquo;s computationally cheaper than if all tokens went through all parts of a giant MLP. Example: Mixtral 8x7B has 8 \u0026ldquo;experts,\u0026rdquo; each around 7 billion parameters. For each token, the router typically selects 2 experts. So, while the total parameter count is high (~47B effective, as some parameters are shared), only about 13B are active for any single token\u0026rsquo;s processing. Load Balancing: An important challenge is to ensure experts are utilized somewhat evenly. 6.2 Model Merging What it\u0026rsquo;s ultimately trying to achieve: To combine the strengths and knowledge of multiple different pretrained LLMs into a single, potentially more capable or specialized model, without necessarily retraining from scratch. The Core Idea (Frankenstein\u0026rsquo;s LLM, but hopefully better): Various techniques exist: Model Soups: Averaging the weights of several fine-tuned versions of the same base model. SLERP (Spherical Linear Interpolation): Interpolating weights in a way that maintains parameter norms. Task Vector Algorithms (TIES-Merging, DARE): Identifying and combining \u0026ldquo;task vectors\u0026rdquo; that represent what a model learned during fine-tuning for a specific task. Passthrough (Frankenmerges): More radical, involves directly concatenating or combining layers from different LLMs, sometimes creating models with unconventional parameter counts (e.g., merging two 7B models to get a 13B-like model). mergekit is mentioned as a popular open-source tool. 6.3 Model Compression What it\u0026rsquo;s ultimately trying to achieve: To make large LLMs smaller and faster for deployment, especially in resource-constrained environments (like mobile devices or edge computing), without a catastrophic loss in performance. Neural networks are often over-parameterized. Key Methods: Quantization: Reducing the precision of the model\u0026rsquo;s weights and activations (e.g., from 32-bit floating point to 8-bit integers, or even lower). Post-Training Quantization (PTQ): Quantize an already trained model. Quantization-Aware Training (QAT): Simulate quantization effects during training to make the model more robust to it. QLoRA: Combines quantization with LoRA for very efficient fine-tuning. Pruning: Removing \u0026ldquo;unimportant\u0026rdquo; parts of the model. Unstructured Pruning: Removing individual weights based on their magnitude. Structured Pruning: Removing entire neurons, attention heads, or layers. Knowledge Distillation: Training a smaller \u0026ldquo;student\u0026rdquo; model to mimic the behavior (e.g., output logits or internal representations) of a larger, more capable \u0026ldquo;teacher\u0026rdquo; model. 6.4 Preference-Based Alignment What it\u0026rsquo;s ultimately trying to achieve: To make LLMs generate outputs that are more helpful, harmless, and honest, aligning them better with human values and intentions. Pretrained LLMs might generate plausible but undesirable content. Key Methods: Reinforcement Learning from Human Feedback (RLHF): Collect human preference data: Humans rank different model responses to the same prompt. Train a Reward Model (RM): This model learns to predict which response humans would prefer (i.e., assign a higher score to better responses). Fine-tune the LLM using Reinforcement Learning (RL): The LLM\u0026rsquo;s \u0026ldquo;actions\u0026rdquo; are generating tokens. The \u0026ldquo;reward\u0026rdquo; comes from the RM. The LLM is trained to maximize this reward, effectively learning to generate responses that the RM (and by proxy, humans) would score highly. Constitutional AI (CAI): The model is given a set of guiding principles or a \u0026ldquo;constitution\u0026rdquo; (e.g., \u0026ldquo;be helpful,\u0026rdquo; \u0026ldquo;don\u0026rsquo;t generate harmful content\u0026rdquo;). The model can then self-critique its own outputs based on these principles and revise them. This reduces the need for direct human feedback for every step but still relies on a human-defined constitution. 6.5 Advanced Reasoning What it\u0026rsquo;s ultimately trying to achieve: To enable LLMs to tackle more complex tasks that require multi-step reasoning, planning, or interaction with external tools, going beyond simple prompt-response patterns. Techniques: Chain of Thought (CoT) Prompting: Encourage the LLM to generate intermediate reasoning steps before giving the final answer (e.g., \u0026ldquo;Let\u0026rsquo;s think step by step\u0026hellip;\u0026rdquo;). This often improves performance on tasks like math word problems or logical deduction. Tree of Thought (ToT): Extends CoT by allowing the model to explore multiple reasoning paths (like branches in a tree) and use heuristics or self-evaluation to choose the most promising ones. Self-Consistency: Generate multiple CoT reasoning paths and take the majority answer. ReAct (Reasoning + Action): Interleaves reasoning steps with \u0026ldquo;action\u0026rdquo; steps, where the model can decide to use external tools (like a calculator or a search engine via an API) to gather more information or perform computations. Function Calling / Tool Use: Explicitly giving the LLM the ability to call predefined functions or APIs. The LLM can decide which tool to use and what parameters to pass based on the user\u0026rsquo;s query. Program-Aided Language Models (PAL): LLMs generate code (e.g., Python) that is then executed by an interpreter to get the final answer, especially useful for precise calculations. 6.6 Language Model Security What it\u0026rsquo;s ultimately trying to achieve: To understand and mitigate vulnerabilities that can lead to LLMs being misused or generating harmful/undesirable content. Key Threats: Jailbreak Attacks: Crafting prompts that trick the model into bypassing its safety controls and generating restricted content (e.g., role-playing scenarios like \u0026ldquo;act as a pirate and tell me how to\u0026hellip;\u0026rdquo;). Prompt Injection: An attacker manipulates how an application combines user input with its own system prompts. This can lead to the LLM executing unintended instructions, potentially leaking data or performing unauthorized actions if the application has privileged access. This is generally considered more severe than jailbreaking. 6.7 Vision Language Model (VLM) What it\u0026rsquo;s ultimately trying to achieve: To create models that can understand and reason about information from both text and images (multimodal reasoning). Core Architecture: Vision Encoder: Processes the image and extracts visual features (often based on architectures like CLIP - Contrastive Language-Image Pretraining, which itself learns to align image and text representations). Language Model (LLM): The text processing and generation component. Cross-Attention Mechanism (or similar fusion method): Allows the LLM to integrate and reason about the visual features from the vision encoder alongside the textual input/output. VLMs can perform tasks like image captioning, visual question answering (VQA), and following instructions that refer to image content. 6.8 Preventing Overfitting What it\u0026rsquo;s ultimately trying to achieve: To ensure that the model learns general patterns from the training data rather than just memorizing it, so that it performs well on new, unseen data (generalization). Techniques (some are general ML, some more specific to NNs): Regularization (L1 and L2): Adding a penalty term to the loss function based on the magnitude of the model\u0026rsquo;s weights, discouraging overly complex models. Dropout: During training, randomly \u0026ldquo;dropping out\u0026rdquo; (setting to zero) a fraction of neuron activations. This forces the network to learn more robust and redundant representations. Early Stopping: Monitoring performance on a separate validation set during training. Stop training when performance on the validation set starts to degrade, even if training loss is still decreasing. Validation Set vs. Test Set: Validation Set: Used during training to tune hyperparameters (like learning rate, number of layers, dropout rate) and for decisions like early stopping. Test Set: Held out completely and used only once at the very end to get an unbiased estimate of the final model\u0026rsquo;s performance on unseen data. 6.9 Concluding Remarks A reminder of the journey from ML basics to advanced LLMs and an encouragement to stay curious, hands-on, and keep learning.\n6.10 More From the Author A nice plug for \u0026ldquo;The Hundred-Page Machine Learning Book\u0026rdquo; and \u0026ldquo;Machine Learning Engineering\u0026rdquo; as complementary resources!\n","permalink":"https://deepskandpal.github.io/bookshelf/100-page-lm-book/chapter-6/","tags":null,"title":"Chapter 6: Further Reading"},{"categories":null,"content":"Chapter 6 Overview: Strings This chapter focuses on string manipulation, a fundamental skill in programming and a common topic in interviews. Problems often involve parsing, searching, and transforming string data.\nKey String Concepts in Python: Immutability: Python strings are immutable. This means that any method that appears to modify a string (like replace() or upper()) actually returns a new string. This has performance implications; building a string piece-by-piece in a loop using + concatenation is often inefficient (O(n^2)) because it creates a new string at each step. Using a list of characters and ''.join() at the end is the preferred, efficient (O(n)) approach. Encoding: Understand that strings are sequences of Unicode characters. ord() gets the integer representation of a character, and chr() does the reverse. Slicing: Python\u0026rsquo;s string slicing s[start:stop:step] is powerful and efficient for extracting substrings. Section 6.1: Interconvert Strings and Integers The Problem: Implement functions to convert a string to an integer (string_to_int) and an integer to a string (int_to_string), without using built-in functions like int() or str(). Handle negative numbers.\nstring_to_int(s) Algorithm:\nHandle the sign. Check if the first character s[0] is '-'. If so, note it and slice the string to work with the remaining digits. Initialize result = 0. Iterate through the digits of the string. For each character c: Convert the character to its integer value. ord(c) - ord('0') is a classic trick for this. Update the result: result = result * 10 + digit. Apply the sign if the number was negative. int_to_string(x) Algorithm:\nHandle the sign. If x \u0026lt; 0, note it, and work with the absolute value of x. Handle the edge case x = 0. Use a list to store the characters of the digits. In a loop, as long as x \u0026gt; 0: Get the last digit: digit = x % 10. Convert it to a character: chr(ord('0') + digit). Append the character to your list. Update x: x //= 10. The list of characters will be in reverse order (e.g., 123 becomes ['3', '2', '1']). Reverse the list and join() it to form the string. Add the '-' prefix if the original number was negative. Section 6.2: Base Conversion The Problem: Given a string representing a number in base b1 and a target base b2, convert the number to a string in base b2. The bases are between 2 and 16. For digits greater than 9, use \u0026lsquo;A\u0026rsquo; for 10, \u0026lsquo;B\u0026rsquo; for 11, etc.\nAlgorithm (Two-Step Process): The simplest way to convert between two arbitrary bases is to use a common intermediate base, like base 10.\nStep 1: Convert from base b1 to an integer (base 10).\nThis is very similar to the string_to_int problem. Iterate through the string s in b1. For each character c: result = result * b1 + digit_value. The digit_value for \u0026lsquo;A\u0026rsquo;\u0026hellip;\u0026lsquo;F\u0026rsquo; needs to be handled (e.g., ord(c) - ord('A') + 10). Step 2: Convert the integer from base 10 to a string in base b2.\nThis is very similar to the int_to_string problem. In a loop, as long as the number x \u0026gt; 0: remainder = x % b2. Convert remainder to its character representation (handling 10-15 -\u0026gt; \u0026lsquo;A\u0026rsquo;-\u0026lsquo;F\u0026rsquo;). Prepend this character to your result. x //= b2. Section 6.5: Test for Palindromicity The Problem: Write a function that checks if a string is a palindrome, considering only alphanumeric characters and ignoring case.\nExample:\n\u0026ldquo;A man, a plan, a canal, Panama\u0026rdquo; -\u0026gt; True \u0026ldquo;race a car\u0026rdquo; -\u0026gt; False Algorithm (Two Pointers): This is a classic and efficient approach.\nInitialize two pointers: left = 0 and right = len(s) - 1. Loop while left \u0026lt; right: Move left pointer: While left \u0026lt; right and the character s[left] is not alphanumeric, increment left. Move right pointer: While left \u0026lt; right and the character s[right] is not alphanumeric, decrement right. Compare: Compare s[left].lower() and s[right].lower(). If they are not equal, the string is not a palindrome. Return False. Advance pointers: If they are equal, move the pointers inward: left += 1, right -= 1. If the loop completes, it means all valid character pairs matched. Return True. Complexity:\nTime: O(n), where n is the length of the string. Each pointer traverses the string at most once. Space: O(1), as we only use a few variables for the pointers. Section 6.8: The Look-and-Say Problem The Problem: Generate the n-th term of the \u0026ldquo;look-and-say\u0026rdquo; sequence. This sequence starts with \u0026ldquo;1\u0026rdquo;. Each subsequent term is generated by reading aloud the digits of the previous term.\nR(1) = \u0026ldquo;1\u0026rdquo; R(2) = \u0026ldquo;11\u0026rdquo; (read R(1) as \u0026ldquo;one 1\u0026rdquo;) R(3) = \u0026ldquo;21\u0026rdquo; (read R(2) as \u0026ldquo;two 1s\u0026rdquo;) R(4) = \u0026ldquo;1211\u0026rdquo; (read R(3) as \u0026ldquo;one 2, one 1\u0026rdquo;) R(5) = \u0026ldquo;111221\u0026rdquo; (read R(4) as \u0026ldquo;one 1, one 2, two 1s\u0026rdquo;) Algorithm (Iterative Generation):\nStart with s = \u0026quot;1\u0026quot;. Loop n-1 times to generate the subsequent terms. In each iteration: Create a new empty string or list next_s. Iterate through the current string s to identify groups of identical consecutive digits. Use a pointer or index i. While i \u0026lt; len(s): Let current_digit = s[i]. Count how many times it repeats consecutively. Let this be count. Advance i by count. Append the count and the digit to next_s. For example, if you see \u0026ldquo;222\u0026rdquo;, you append \u0026ldquo;3\u0026rdquo; and \u0026ldquo;2\u0026rdquo;. Replace the old s with the newly generated next_s. Return s. Section 6.9: Convert from Roman to Decimal The Problem: Given a string representing a Roman numeral, convert it to an integer.\nRoman Numeral Rules:\nI=1, V=5, X=10, L=50, C=100, D=500, M=1000 Typically, values are added (e.g., VI = 5 + 1 = 6). Subtraction Rule: A smaller value placed before a larger value is subtracted. IV = 4 (5 - 1) IX = 9 (10 - 1) XL = 40 (50 - 10) XC = 90 (100 - 10) CD = 400 (500 - 100) CM = 900 (1000 - 100) Algorithm (Iterate from Right to Left): This is a clever way to handle the subtraction rule easily.\nCreate a map T = {'I': 1, 'V': 5, ...} for Roman numeral values. Initialize the result with the value of the last character in the string: result = T[s[-1]]. Iterate through the string from the second-to-last character backwards to the front (i from len(s)-2 down to 0). If the value of the current character T[s[i]] is less than the value of the character to its right T[s[i+1]], it\u0026rsquo;s a subtraction case. So, result -= T[s[i]]. Otherwise, it\u0026rsquo;s an addition case. So, result += T[s[i]]. Return result. Example: s = \u0026quot;IX\u0026quot;\nresult starts as T['X'] = 10. Loop i at s[0] (\u0026lsquo;I\u0026rsquo;). T['I'] (1) is less than T['X'] (10). Subtract: result = 10 - 1 = 9. Return 9. Example: s = \u0026quot;LVIII\u0026quot;\nresult starts as T['I'] = 1. i at s[2] (\u0026lsquo;I\u0026rsquo;): T['I'](1) \u0026gt;= T['I'](1) is false (equal), so add. result = 1 + 1 = 2. i at s[1] (\u0026lsquo;V\u0026rsquo;): T['V'](5) \u0026gt;= T['I'](1). Add. result = 2 + 5 = 7. i at s[0] (\u0026lsquo;L\u0026rsquo;): T['L'](50) \u0026gt;= T['V'](5). Add. result = 7 + 50 = 57. Wait, this logic is slightly off. Let\u0026rsquo;s correct. result = T[s[-1]] is the initial value. s = \u0026quot;LVIII\u0026quot; result = T['I'] = 1 i at s[3] (\u0026lsquo;I\u0026rsquo;): T[s[3]] (1) is not less than T[s[4]] (1). Add T[s[3]]. result = 1 + 1 = 2. i at s[2] (\u0026lsquo;I\u0026rsquo;): T[s[2]] (1) is not less than T[s[3]] (1). Add T[s[2]]. result = 2 + 1 = 3. i at s[1] (\u0026lsquo;V\u0026rsquo;): T[s[1]] (5) is not less than T[s[2]] (1). Add T[s[1]]. result = 3 + 5 = 8. i at s[0] (\u0026lsquo;L\u0026rsquo;): T[s[0]] (50) is not less than T[s[1]] (5). Add T[s[0]]. result = 8 + 50 = 58. Correct. ","permalink":"https://deepskandpal.github.io/bookshelf/elements-of-programming/chapter-6/","tags":null,"title":"Chapter 6: Strings"},{"categories":null,"content":"Introduction to Decision Trees Alright class, settle in! After exploring the world of Support Vector Machines, we\u0026rsquo;re now turning our attention to another incredibly versatile and powerful family of algorithms: Decision Trees.\nVersatility: Like SVMs, Decision Trees can perform both classification and regression tasks. They can even handle multioutput tasks (where each instance can have multiple output labels or values). Power: They are capable of fitting complex datasets. You might recall from Chapter 2, when we looked at the California housing data, a DecisionTreeRegressor was able to fit the training data perfectly (though, as we noted, it was actually overfitting). Fundamental Building Blocks: Decision Trees are also the core components of Random Forests (which we\u0026rsquo;ll see in Chapter 7). Random Forests are among the most powerful and widely used ML algorithms today. So, understanding Decision Trees is crucial for understanding Random Forests. What this chapter will cover:\nHow to train, visualize, and make predictions with Decision Trees. The CART (Classification and Regression Tree) training algorithm, which Scikit-Learn uses. How to regularize Decision Trees (to prevent overfitting). How they\u0026rsquo;re used for regression tasks. Some of their limitations. (Page 175-176: Training and Visualizing a Decision Tree) Let\u0026rsquo;s start by building one to see how it works. We\u0026rsquo;ll use the Iris dataset (which we\u0026rsquo;ve seen before, for example, with Logistic Regression in Chapter 4).\nThe Code: from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier\niris = load_iris() X = iris.data[:, 2:] # petal length and width only y = iris.target # species (0: setosa, 1: versicolor, 2: virginica)\ntree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y)\nWe\u0026rsquo;re only using two features: petal length and petal width, to make it easy to visualize. max_depth=2: This is a crucial hyperparameter. We are telling the tree not to grow beyond a depth of 2 levels. This is a form of regularization to prevent it from becoming too complex and overfitting. If we didn\u0026rsquo;t set this, the tree might grow very deep to try and perfectly classify every single training instance. Visualizing the Tree (Page 176): One of the great things about Decision Trees is that they are very intuitive and easy to visualize. Scikit-Learn provides a function export_graphviz to help with this. from sklearn.tree import export_graphviz export_graphviz( tree_clf, out_file=\u0026quot;iris_tree.dot\u0026quot;, feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True ) This creates a .dot file. You then use the dot command-line tool (from the Graphviz package, which you\u0026rsquo;d need to install separately) to convert this into an image, like a PNG: $ dot -Tpng iris_tree.dot -o iris_tree.png\nFigure 6-1 (Iris Decision Tree): This is the visual representation of our trained tree. It\u0026rsquo;s a flowchart-like structure. Let\u0026rsquo;s understand its components: Nodes: Each box is a node. Root Node (Depth 0, at the top): This is where you start. It asks a question about a feature. In Figure 6-1, it asks: \u0026ldquo;petal length (cm) \u0026lt;= 2.45?\u0026rdquo; Child Nodes: Based on the answer (True or False), you move to a child node. Leaf Nodes: Nodes that don\u0026rsquo;t have any children. These nodes make the final prediction. Information in each node: Question/Condition: E.g., \u0026ldquo;petal length (cm) \u0026lt;= 2.45\u0026rdquo;. gini: This measures the impurity of the node. We\u0026rsquo;ll discuss this more soon. A gini score of 0 means the node is \u0026ldquo;pure\u0026rdquo; – all training instances that reach this node belong to the same class. samples: How many training instances from the dataset fall into this node. value: How many training instances of each class fall into this node. For example, value = [50, 50, 50] at the root means there were 50 Setosa, 50 Versicolor, and 50 Virginica instances in the training set that reached this node (which is all of them initially). class: The class that would be predicted if this node were a leaf node (i.e., the majority class among the samples in this node). (Page 176-177: Making Predictions) How do you use this tree (Figure 6-1) to classify a new iris flower? It\u0026rsquo;s like a game of \u0026ldquo;20 Questions.\u0026rdquo;\nStart at the root node (depth 0). Question: Is the flower\u0026rsquo;s petal length ≤ 2.45 cm? If YES (True): Move to the left child node (depth 1, left). This node in Figure 6-1 has gini = 0.0, samples = 50, value = [50, 0, 0], class = setosa. This is a leaf node (it has no further questions/children). So, the prediction is Iris setosa. If NO (False, meaning petal length \u0026gt; 2.45 cm): Move to the right child node (depth 1, right). This node is not a leaf node. It asks another question. Question: Is the flower\u0026rsquo;s petal width ≤ 1.75 cm? If YES (True, petal length \u0026gt; 2.45 cm AND petal width ≤ 1.75 cm): Move to this node\u0026rsquo;s left child (depth 2, middle of bottom row). This is a leaf node. class = versicolor. Prediction: Iris versicolor. If NO (False, petal length \u0026gt; 2.45 cm AND petal width \u0026gt; 1.75 cm): Move to this node\u0026rsquo;s right child (depth 2, rightmost bottom row). This is a leaf node. class = virginica. Prediction: Iris virginica. It\u0026rsquo;s really that simple to make a prediction once the tree is built!\nAn Important Quality (Bird Icon, page 177): Decision Trees require very little data preparation. They don\u0026rsquo;t need feature scaling or centering. This is a nice practical advantage.\nUnderstanding gini (Gini Impurity - Equation 6-1, page 177):\nA node\u0026rsquo;s gini attribute measures its impurity. A node is \u0026ldquo;pure\u0026rdquo; (gini = 0) if all training instances it applies to belong to the same class. The depth-1 left node (Setosa) is pure. Equation 6-1: Gᵢ = 1 - Σₖ (pᵢ,ₖ)² Gᵢ: Gini impurity of the i-th node. pᵢ,ₖ: Ratio of class k instances among the training instances in the i-th node. What Gini impurity is ultimately trying to achieve: It\u0026rsquo;s a measure of how \u0026ldquo;mixed up\u0026rdquo; the classes are within a node. If a node is pure (all samples belong to one class, say class C), then pᵢ,C = 1 and pᵢ,k = 0 for all other k. So Gᵢ = 1 - (1)² = 0. If a node has an equal mix of classes, the Gini impurity will be higher. For example, for the depth-2 left node (predicts versicolor): Samples = 54. Value = [0 setosa, 49 versicolor, 5 virginica]. p_setosa = 0/54 p_versicolor = 49/54 p_virginica = 5/54 G = 1 - (0/54)² - (49/54)² - (5/54)² ≈ 0.168. This is fairly low, but not zero, as there\u0026rsquo;s a mix of Versicolor and Virginica. Binary Trees (Scorpion Icon, page 177):\nScikit-Learn uses the CART algorithm, which produces binary trees: non-leaf nodes always have exactly two children (questions have yes/no answers). Other algorithms like ID3 can produce trees with nodes having more than two children. (Page 178: Decision Boundaries \u0026amp; Model Interpretation) Figure 6-2 (Decision Tree decision boundaries): This shows how the tree partitions the feature space (petal length and petal width). The root node (petal length ≤ 2.45 cm) creates the first thick vertical split. Everything to the left is classified as Setosa (pure). The area to the right is impure, so the depth-1 right node splits it further with a horizontal line (petal width ≤ 1.75 cm). Since max_depth=2, the tree stops there. The resulting regions are rectangular (or hyperrectangles in higher dimensions). If max_depth were 3, the two depth-2 nodes could be split further, adding more boundaries (dotted lines in the figure). Model Interpretation: White Box vs. Black Box (Sidebar, page 178):\nDecision Trees are very intuitive and their decisions are easy to interpret. They are often called white box models. You can see the rules. In contrast, Random Forests or Neural Networks are often considered black box models. They can make great predictions, but it\u0026rsquo;s harder to explain in simple terms why a specific prediction was made. Decision Trees provide simple classification rules that can even be applied manually. (Page 178-179: Estimating Class Probabilities) Decision Trees can also estimate the probability that an instance belongs to a particular class k.\nHow it works: Traverse the tree to find the leaf node for the instance. Return the ratio of training instances of class k in that leaf node. Example (page 179): Flower with petal length 5 cm, petal width 1.5 cm. Petal length \u0026gt; 2.45 cm (False for root) -\u0026gt; go to depth-1 right node. Petal width ≤ 1.75 cm (True for this node) -\u0026gt; go to depth-2 left node (the one that predicts Versicolor). This leaf node has value = [0, 49, 5], meaning 0 Setosa, 49 Versicolor, 5 Virginica, out of 54 samples. Probabilities: P(Setosa) = 0/54 = 0% P(Versicolor) = 49/54 ≈ 90.7% P(Virginica) = 5/54 ≈ 9.3% Scikit-Learn code: tree_clf.predict_proba([[5, 1.5]]) gives array([[0. , 0.90740741, 0.09259259]]) tree_clf.predict([[5, 1.5]]) gives array([1]) (class 1 is Versicolor). Important Note: The estimated probabilities are the same for any point that falls into the same leaf node\u0026rsquo;s region (e.g., the bottom-middle rectangle in Figure 6-2). This can sometimes be a limitation if fine-grained probability estimates are needed. Even if a flower had petal length 6cm and width 1.5cm (making it seem more likely Virginica by intuition, if it were near the boundary with Virginica region), it would still get the same probabilities if it landed in that same leaf node. (Page 179: The CART Training Algorithm) Scikit-Learn uses the Classification and Regression Tree (CART) algorithm to train (or \u0026ldquo;grow\u0026rdquo;) Decision Trees.\nHow it works (Greedy Approach):\nIt first splits the training set into two subsets using a single feature k and a threshold tₖ (e.g., \u0026ldquo;petal length ≤ 2.45 cm\u0026rdquo;). How does it choose k and tₖ? It searches for the pair (k, tₖ) that produces the purest subsets, weighted by their size. Purity is measured by Gini impurity (or entropy, which we\u0026rsquo;ll see). Equation 6-2: CART cost function for classification J(k, tₖ) = (m_left / m) * G_left + (m_right / m) * G_right m_left, m_right: Number of instances in the left/right subset after the split. m: Total number of instances. G_left, G_right: Gini impurity of the left/right subset. What this cost function is ultimately trying to achieve: Find the feature and threshold that minimize this weighted average impurity of the child nodes. It wants the \u0026ldquo;cleanest\u0026rdquo; possible split. Recursion: Once the algorithm splits the set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. Stopping Conditions (When to stop splitting/recursing):\nReaches max_depth (hyperparameter). Cannot find a split that will reduce impurity further. Other hyperparameters controlling stopping: min_samples_split: Minimum number of samples a node must have before it can be split. min_samples_leaf: Minimum number of samples a leaf node must have. min_weight_fraction_leaf: Same as min_samples_leaf but as a fraction of total weighted instances. max_leaf_nodes: Maximum number of leaf nodes. These are all regularization hyperparameters. Greedy Nature (Scorpion Icon, page 180):\nCART is a greedy algorithm. At each step, it searches for the locally optimal split at the current level. It doesn\u0026rsquo;t look ahead to see if a less optimal split now might lead to an even better overall tree (lower total impurity) a few levels down. A greedy algorithm often produces a solution that\u0026rsquo;s reasonably good, but not guaranteed to be globally optimal. Finding the truly optimal tree is an NP-Complete problem (computationally very hard, requires O(exp(m)) time), so we settle for a \u0026ldquo;reasonably good\u0026rdquo; greedy solution. (Page 180-181: Computational Complexity \u0026amp; Gini vs. Entropy) Prediction Complexity:\nTraversing a Decision Tree from root to leaf takes roughly O(log₂(m)) nodes for a balanced tree (where m is number of instances). Each node checks one feature. So, overall prediction complexity is O(log₂(m)), independent of the number of features n. Predictions are very fast! Training Complexity:\nAt each node, CART compares all features (or max_features) on all samples in that node. This results in a training complexity of roughly O(n × m log₂(m)). For small training sets (\u0026lt; few thousand), Scikit-Learn can speed up training by presorting data (presort=True parameter, though this is deprecated and will be removed; Scikit-learn now often sorts internally when beneficial). For larger sets, presorting slows it down. Gini Impurity or Entropy? (Page 180):\nBy default, CART uses Gini impurity. You can set criterion=\u0026quot;entropy\u0026quot; to use entropy as the impurity measure instead. Entropy (Equation 6-3, page 181): Hᵢ = - Σₖ (pᵢ,ₖ * log₂(pᵢ,ₖ)) (sum over classes k where pᵢ,ₖ ≠ 0) Originated in thermodynamics (molecular disorder). In information theory (Shannon), it measures average information content. Entropy is 0 if a set contains instances of only one class (pure). What it\u0026rsquo;s ultimately trying to achieve: Similar to Gini, it measures the \u0026ldquo;mixed-up-ness\u0026rdquo; or uncertainty in a node. Lower entropy means less uncertainty/more purity. Gini vs. Entropy: Does it matter? Mostly, no. They lead to similar trees. Gini is slightly faster to compute, so it\u0026rsquo;s a good default. When they differ: Gini tends to isolate the most frequent class in its own branch of the tree. Entropy tends to produce slightly more balanced trees. (Page 181-182: Regularization Hyperparameters) Decision Trees make very few assumptions about the data (they are nonparametric models – the number of parameters isn\u0026rsquo;t fixed before training, the model structure adapts to the data).\nIf left unconstrained, they will fit the training data very closely, likely overfitting. To avoid this, we need to regularize by restricting their freedom during training. Key Regularization Hyperparameters in DecisionTreeClassifier: max_depth: Maximum depth of the tree. (Default is None = unlimited). Reducing this is a common way to regularize. min_samples_split: Minimum samples a node needs to be split. min_samples_leaf: Minimum samples a leaf node must have. min_weight_fraction_leaf: As min_samples_leaf, but as a fraction. max_leaf_nodes: Limits the total number of leaf nodes. max_features: Max number of features evaluated for splitting at each node. Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model. Figure 6-3 (page 182): Shows the effect on the moons dataset. Left: Default hyperparameters (no restrictions) -\u0026gt; Overfitting (wiggly boundary). Right: min_samples_leaf=4 -\u0026gt; Simpler, smoother boundary, likely generalizes better. Pruning (Scorpion Icon, page 182): Some algorithms work by first training the tree without restrictions, then pruning (deleting) unnecessary nodes. A node is considered unnecessary if the purity improvement it provides is not statistically significant (e.g., using a chi-squared test to check if improvement is just by chance). If p-value \u0026gt; threshold, prune. (Page 183-184: Regression with Decision Trees) Decision Trees can also do regression using DecisionTreeRegressor.\nExample: Training on a noisy quadratic dataset with max_depth=2. Figure 6-4 (A Decision Tree for regression): Looks similar to a classification tree. Main difference: Instead of predicting a class in each node, it predicts a value. The value predicted at a leaf node is the average target value of the training instances that fall into that leaf. The mse attribute in each node shows the mean squared error of the training instances in that node, relative to the average value of that node. Predictions (Figure 6-5, page 184): The predictions are piecewise constant. For any region defined by a leaf node, the prediction is the average value of the training instances in that region. Increasing max_depth leads to more steps in the prediction function, fitting the training data more closely. CART Algorithm for Regression (Equation 6-4, page 184): Works mostly the same way as for classification. Difference: Instead of minimizing impurity (Gini/entropy), it tries to split the training set in a way that minimizes the MSE. J(k, tₖ) = (m_left / m) * MSE_left + (m_right / m) * MSE_right It wants to make the instances within each resulting region as close as possible to the mean value of that region. Regularization (Figure 6-6, page 184): Just like classification trees, regression trees are prone to overfitting if unregularized (left plot). Setting min_samples_leaf=10 (right plot) gives a much more reasonable, smoother model. (Page 185-186: Instability of Decision Trees) Decision Trees have many great qualities (simple, interpretable, versatile, powerful). But they have limitations.\nSensitivity to Training Set Rotation (Orthogonal Boundaries - Figure 6-7, page 185): Decision Trees prefer to create orthogonal decision boundaries (splits are perpendicular to an axis). If your dataset is rotated, a simple linear boundary in the rotated space might require a complex, step-like boundary for the Decision Tree. The left plot shows a tree easily splitting unrotated data. The right shows a convoluted boundary for data rotated 45°. The model on the right might not generalize well. One way to limit this: Use Principal Component Analysis (PCA, Chapter 8) to orient data better first. Sensitivity to Small Variations in Training Data (Figure 6-8, page 186):\nThis is a major issue. Decision Trees are very sensitive to small changes in the training data. Example: Remove one instance (the widest Iris versicolor) from the Iris dataset and retrain. The resulting tree (Figure 6-8) can look very different from the original (Figure 6-2). Since the CART algorithm used by Scikit-Learn is stochastic (randomly selects features to evaluate at nodes if max_features is less than total features, or how ties are broken), you might get different models even on the same training data unless you set random_state. How to limit this instability? Random Forests (Chapter 7!) average predictions over many trees, which greatly helps reduce this variance and instability.\nAnd that\u0026rsquo;s the essence of Decision Trees! They work by recursively partitioning the data based on simple questions about features, aiming to create purer (for classification) or more homogeneous (for regression) subsets. They are interpretable but can be unstable and prone to overfitting if not regularized.\nAny thoughts or questions on how these trees make decisions or how they are built? The idea of recursive partitioning and measuring purity/error at each step is central.\nGlossary Q. Is backtracking used while training decision trees using CART algorithm in machine learning? Yes, absolutely! The process of building a decision tree, especially using algorithms like CART (Classification and Regression Trees), shares a very strong conceptual similarity with recursive backtracking, particularly the divide-and-conquer aspect and the exploration of choices.\nHere\u0026rsquo;s how they relate:\nRecursive Partitioning (Divide and Conquer):\nDecision Trees: At each node, the algorithm searches for the \u0026ldquo;best\u0026rdquo; feature and the \u0026ldquo;best\u0026rdquo; split point for that feature to divide the current dataset into two or more subsets. This partitioning is done recursively. Backtracking/Recursion: We break a problem down into smaller sub problems. Making a Sequence of Decisions:\nDecision Trees: Building a tree is a sequence of decisions: Which feature to split on at the current node? What value of that feature to use as the split point? Backtracking: We make a sequence of choices to build a potential solution. Exploring Choices (Greedy vs. Exhaustive):\nCART (Typically Greedy): At each node, CART makes a locally optimal decision. It searches for the single feature and split point that currently gives the best improvement in purity (e.g., Gini impurity, entropy for classification; variance reduction for regression). It commits to this split and then recursively processes the resulting child nodes. It doesn\u0026rsquo;t typically \u0026ldquo;backtrack\u0026rdquo; in the sense of undoing a split if it later turns out to be globally suboptimal. This greedy nature is why decision trees can sometimes find solutions that aren\u0026rsquo;t the absolute global optimum but are good and fast to compute. Pure Backtracking (Exhaustive): Would explore all possible sequences of splits to find the globally best tree. This is computationally infeasible for most datasets. Stopping Conditions (Base Cases):\nDecision Trees: Recursion stops when: A node is \u0026ldquo;pure\u0026rdquo; (all samples belong to one class). A pre-defined maximum depth is reached. The number of samples in a node is below a threshold. No split further improves purity. Backtracking: Recursion stops when: A complete solution is found. A dead end is reached (no valid next choice). Pruning (Analogous to Constraint Checking/Pruning in Backtracking):\nDecision Trees: After a large tree is grown (often greedily), a \u0026ldquo;pruning\u0026rdquo; step is often applied. This is where the tree might be \u0026ldquo;simplified\u0026rdquo; by removing branches that don\u0026rsquo;t contribute much to overall accuracy or that lead to overfitting. This is somewhat like backtracking deciding a path is not fruitful, but it\u0026rsquo;s typically done after the initial greedy construction. Backtracking: We \u0026ldquo;prune\u0026rdquo; branches of the search space if a partial solution violates constraints. Where they differ significantly:\nGoal:\nBacktracking (as we discussed): Often aims to find all solutions or one solution that satisfies constraints (e.g., N-Queens, Sudoku). Decision Tree (CART): Aims to build a model that generalizes well to unseen data by finding optimal decision boundaries. Nature of \u0026ldquo;Undo\u0026rdquo;:\nBacktracking: The \u0026ldquo;undo\u0026rdquo; is fundamental to explore different branches of the same problem instance to find different discrete solutions. CART: Typically doesn\u0026rsquo;t \u0026ldquo;undo\u0026rdquo; a split choice in its main growing phase. The \u0026ldquo;undo\u0026rdquo; equivalent is more like the post-hoc pruning process, where parts of the already-built tree are removed. So, in summary:\nThe recursive, divide-and-conquer nature of building a decision tree by repeatedly finding the best split is very similar to the recursive calls in backtracking. Both explore a \u0026ldquo;tree\u0026rdquo; of possibilities.\nHowever, CART\u0026rsquo;s typical implementation is greedy in its choice of splits at each node, whereas the backtracking problems we\u0026rsquo;ve discussed (like N-Queens for all solutions) are more exhaustive in exploring valid choices. If CART were to try and find the globally optimal decision tree by trying every possible sequence of splits and then backtracking, it would be computationally intractable.\nSo, while the recursive structure is similar, the search strategy (greedy vs. exhaustive with backtracking) and the ultimate goal are different. The core idea of recursively breaking down the problem based on choices is a shared theme.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-6/","tags":null,"title":"Chapter 6: Decision Trees"},{"categories":null,"content":"Statistics is a core component of any data scientist\u0026rsquo;s toolkit. Since many commercial layers of a data science pipeline are built from statistical foundations (for example, A/B testing), knowing foundational topics of statistics is essential.\nInterviewers love to test a candidate\u0026rsquo;s knowledge about the statistic basics, starting with topics like the Central Limit Theorem and the Law of Large Numbers, and then progressing on to the concepts underlying hypothesis-testing, particularly p-values and confidence intervals, as well as Type 1 and Type II error, and their interpretations. All of those topics play an important role in the statistical underpinning of A/B testing. Additionally, derivations and manipulations involving random variables of various probability distributions are also common, particularly in finance interviews. Lastly, a common topic in more technical interviews will involve utilizing MLE and/or MAP.\nTopics to Review Before Your Interview Properties of Random Variables For any given random variable X, the following properties hold true (below we assume X is continuous, but it also holds true for discrete random variables). The expectation (average value, or mean) of a random variable is given by the integral of the value of X with its probability density function (PDF) fx (x): µ = E[X] = ∫ xfx(x)dx\nand the variance is given by: Var(X) = E[(X - E[X])²] = E[X²] - (E[X])²\nThe variance is always non-negative, and its square root is called the standard deviation, which is heavily used in statistics. σ = √Var(X) = √E[(X - E[X])²] = √E[X²] - (E[X])²\nThe conditional values of both the expectation and variance are as follows. For example, consider the case for the conditional expectation of X, given that Y = y: E[X|Y = y] = ∫ xf(X|Y)(x|y)dx\nFor any given random variables X and Y, the covariance, a linear measure of relationship between the two variables, is defined by the following: Cov(X,Y) = E[(X - E[X])(Y-E[Y])] = E[XY]-E[X]E[Y]\nand the normalization of covariance, represented by the Greek letter p, is the correlation between X and Y: ρ(X,Y) = Cov(X,Y) / √Var(X)Var(Y)\nAll of these properties are commonly tested in interviews, so it helps to be able to understand the mathematical details behind each and walk through an example for each. For example, assume X follows a Uniform distribution on the interval [a, b], then we have the following: fx(x) = 1 / (b-a) Therefore the expectation of X is: E[X] = ∫ xfx(x)dx = ∫ (from a to b) x * (1/(b-a)) dx = [x² / (2(b-a))] (from a to b) = (a+b)/2\nAlthough it is not necessary to memorize the derivations for all the different probability distributions, you should be comfortable deriving them as needed, as it is a common request in more technical interviews. To this end, you should make sure to understand the formulas given above and be able to apply them to some of the common probability distributions like the exponential or uniform distribution.\nLaw of Large Numbers The Law of Large Numbers (LLN) states that if you sample a random variable independently a large number of times, the measured average value should converge to the random variable\u0026rsquo;s true expectation. Stated more formally, X̄n = (X₁ + \u0026hellip; + Xn) / n → µ, as n → ∞\nThis is important in studying the longer-term behavior of random variables over time. As an example, a coin might land on heads 5 times in a row, but over a much larger n we would expect the proportion of heads to be approximately half of the total flips. Similarly, a casino might experience a loss on any individual game, but over the long run should see a predictable profit over time.\nCentral Limit Theorem The Central Limit Theorem (CLT) states that if you repeatedly sample a random variable a large number of times, the distribution of the sample mean will approach a normal distribution regardless of the initial distribution of the random variable. Recall from the probability chapter that the normal distribution takes on the form: fx(x) = (1 / (σ√(2π))) exp( -(x-µ)² / (2σ²) ) with the mean and standard deviation given by µ and σ respectively. The CLT states that: X̄n = (X₁ + \u0026hellip; + Xn) / n ~ N(µ, σ²/n) and hence (X̄n - µ) / (σ/√n) ~ N(0,1)\nThe CLT provides the basis for much of hypothesis testing, which is discussed shortly. At a very basic level, you can consider the implications of this theorem on coin flipping: the probability of getting some number of heads flipped over a large n should be approximately that of a normal distribution. Whenever you\u0026rsquo;re asked to reason about any particular distribution over a large sample size, you should remember to think of the CLT whether it is Binomial, Poisson, or any other distribution.\nHypothesis Testing General setup The process of testing whether or not a sample of data supports a particular hypotheses is called hypothesis testing. Generally, hypotheses concern particular properties of interest for a given population, such as its parameters, like μ (for example, the mean conversion rate among a set of users). The steps in testing a hypothesis are as follows:\nState a null hypothesis and an alternative hypothesis. Either the null hypothesis will be rejected (in favor of the alternative hypothesis) or it will fail to be rejected (although failing to reject the null hypothesis does not necessarily mean it is true, but rather that there is not sufficient evidence to reject it). Use a particular test statistic of the null hypothesis to calculate the corresponding p-value. Compare the p-value to a certain significance level α. Since the null hypothesis typically represents a baseline (e.g., the marketing campaign did not increase conversion rates, etc.), the goal is to reject it with statistical significance and hope that there is a significant outcome. Hypothesis tests are either one-tailed or two-tailed tests. A one-tailed test has the following types of null and alternative hypothesis: H₀: µ = µ₀ versus H₁: µ \u0026lt; µ₀ or H₁: µ \u0026gt; µ₀ whereas a two-tailed test has these types: H₀: µ = µ₀ versus H₁: µ ≠ µ₀ where H₀ is the null hypothesis and H₁ is the alternative hypothesis, and µ is the parameter of interest. Understanding hypothesis testing is the basis of A/B testing, a topic commonly covered in tech companies\u0026rsquo; interviews. In A/B testing, various versions of a feature are shown to a sample of different users, and each variant is tested to determine if there was an uplift in core engagement metrics. Say, for example, that you are working for Uber Eats, which wants to determine whether email campaigns will increase its product\u0026rsquo;s conversion rates. To conduct an appropriate hypothesis test, you would need two roughly equal groups (equal with respect to dimensions like age, gender, location, etc.) One group would receive the email campaigns and the other group would not be exposed. The null hypothesis in this case would be that the two groups exhibit equal conversion rates, and the hope is that the null hypothesis would be rejected.\nTest Statistics A test statistic is a numerical summary designed for the purpose of determining whether the null hypothesis or the alternative hypothesis should be accepted as correct. More specifically, it assumes that the parameter of interest follows a particular sampling distribution under the null hypothesis. For example, the number of heads in a series of coin flips may be distributed as a binomial distribution, but with a large enough sample size, the sampling distribution should be approximately normally distributed. Hence, the sampling distribution for the total number of heads in a large series of coin flips would be considered normally distributed. Several variations in test statistics and their distributions are the following:\nZ-test: assumes the test statistic follows a normal distribution under the null hypothesis t-test: uses a student\u0026rsquo;s t-distribution rather than a normal distribution Chi-squared: used to assess goodness of fit, and check whether two categorical variables are independent Z-Test Generally the Z-test is used when the sample size is large (to invoke the CLT) or when the population variance is known, and a t-test is used when the sample size is small and when the population variance is unknown. The Z-test for a population mean is formulated as: z = (X̄ - µ₀) / (σ/√n) ~ N(0,1) in the case where the population variance σ² is known.\nt-Test The t-test is structured similarly to the Z-test, but uses the sample variance s² in place of population variance. The t-test is parametrized by the degrees of freedom, which refers to the number of independent observations in a dataset, denoted below by n - 1: t = (X̄ - µ₀) / (s/√n) ~ t_(n-1) where s² = Σ(xᵢ - X̄)² / (n-1)\nAs stated earlier, the t distribution is similar to the normal distribution in appearance but has larger tails (i.e., extreme events happen with greater frequency than the modeled distribution would predict), a common phenomenon, particularly in economics and earth sciences.\nChi-squared Test The Chi-squared test statistic, which is used to assess goodness of fit, and is calculated as follows: χ² = Σ ((Oᵢ - Eᵢ)² / Eᵢ) where Oᵢ is the observed value of interest and Eᵢ is its expected value. A Chi-square test statistic takes on a particular number of degrees of freedom, which is based on the number of categories in the distribution. To use the squared test to check whether two categorical variables are independent, create a table of counts (called a contingency table) with the values of one variable forming the rows of the table and the values of the other variable forming its columns, and checking for intersections. It uses the same style of Chi-squared test statistic as given above.\nHypothesis Testing for Population Proportions Note that, due to the CLT, the Z-test can be applied to random variables of any distribution. For example, when estimating the sample proportion of a population having a characteristic of interest, we can view the members of the population as Bernoulli random variables with those having the characteristic represented by \u0026ldquo;1s\u0026rdquo; and those lacking it represented by \u0026ldquo;0s\u0026rdquo;. Viewing the sample proportion of interest as the sum of these Bernoulli random variables divided by the total population size, we can then compute the sample mean and variance of the overall proportion, about which we can form the following set of hypotheses: H₀: p = p₀ versus H₁: p ≠ p₀ and the corresponding test statistic to conduct a Z-test would be: z = (p̂ - p₀) / √(p₀(1-p₀)/n) In practice, these test statistics form the core of A/B testing. For instance, consider the previously discussed case, in which we seek to measure conversion rates within groups A and B, where A is the control group and B has the special treatment (in this case, a marketing campaign). Adopting the same null hypothesis as before, we can proceed to use a Z-test to assess the difference in empirical population means (in this case, conversion rates) and test its statistical significance at a pre-determined level. When asked about A/B testing or related topics, you should always cite the relevant test statistic and the cause of its validity (usually the CLT).\np-values and confidence intervals Both p-values and confidence intervals are commonly covered topics during interviews. Put simply, a p-value is the probability of observing the value of the calculated test statistic under the null hypothesis assumptions. Usually, the p-value is assessed relative to some pre-determined level of significance (0.05 is often chosen). In conducting a hypothesis test, an α, or measure of the acceptable probability of rejecting a true null hypothesis, is typically chosen prior to conducting the test. Then, a confidence interval can also be calculated to assess the test statistic. This is a range of values that, if a large sample were taken, would contain the parameter value of interest (1-α)% of the time. For instance, a 95% confidence interval would contain the true value 95% of the time. If 0 is included in the confidence intervals, then you cannot reject the null hypothesis (and vice versa). The general form for a confidence interval around the population mean looks like the following, where the term is the critical value (for the standard normal distribution): µ ± z_(α/2) * (σ/√n) In the prior example with the A/B testing on conversion rates, we see that the confidence interval for a population proportion would be p̂ ± z_(α/2) * √(p̂(1-p̂)/n) since our estimate of the true proportion will have the following parameters when estimated as approximately Gaussian: µ_p̂ = p, σ²_p̂ = np(1-p)/n² = p(1-p)/n As long as the sampling distribution of a random variable is known, the appropriate p-values and confidence intervals can be assessed. Knowing how to explain p-values and confidence intervals, in technical and nontechnical terms, is very useful during interviews, so be sure to practice these. If asked about the technical details, always remember to make sure you correctly identify the mean and variance at hand.\nType I and II errors There are two errors that are frequently assessed: type I error, which is also known as a \u0026ldquo;false positive,\u0026rdquo; and type II error, which is also known as a \u0026ldquo;false negative.\u0026rdquo; Specifically, a type I error is when one rejects the null hypothesis when it is correct, and a type II error is when the null hypothesis is not rejected when it is incorrect. Usually 1-α is the referred to as the confidence level, whereas 1-β is referred to as the power. If you plot sample size versus power, generally you should see a larger sample size corresponding to a larger power. It can be useful to look at power in order to gauge the sample size needed for detecting a significant effect. Generally, tests are set up in such a way as to have both 1-α and 1-β relatively high (say at 0.95, and 0.8 respectively). In testing multiple hypotheses, it is possible that if you ran many experiments - even if a particular outcome for one experiment is very unlikely - you would see a statistically significant outcome at least once. So, for example, if you set α = 0.05 and run 100 hypothesis tests, then by pure chance you would expect 5 of the tests to be statistically significant. However, a more desirable outcome is to have the overall α of the 100 tests be 0.05, and this can be done by setting the new α to α/n where n is the number of hypothesis tests (in this case, α/n = 0.05/100 = 0.0005). This is known as Bonferroni correction, and using it helps make sure that the overall rate of false positives is controlled within a multiple testing framework.\nGenerally most interview questions concerning Type I and II errors are qualitative in nature, for instance, requesting explanations of terms or of how you would go about assessing errors/power in an experimental setup.\nMLE and MAP Any probability distribution has parameters, and so fitting parameters is an extremely crucial part of data analysis. There are two general methods for doing so. In maximum likelihood estimation (MLE) the goal is to estimate the most likely parameters given a likelihood function: θ_MLE = arg max L(θ), where L(θ) = f(x₁,\u0026hellip;x_n|θ) Since the values of X are assumed to be i.i.d., then the likelihood function becomes the following: L(θ) = Π (from i=1 to n) f(xᵢ|θ) The natural log of L(θ) is then taken prior to calculating the maximum; since log is a monotonically increasing function, maximizing the log-likelihood log L(θ) is equivalent to maximizing the likelihood: log L(θ) = Σ (from i=1 to n) log f(xᵢ|θ) Another way of fitting parameters is through maximum a posteriori estimation (MAP), which assumes a prior distribution. θ_MAP = arg max g(θ) f(x₁,\u0026hellip;x_n|θ) where the similar log-likelihood is again employed, and g(θ) is a density function of θ. Both MLE and MAP are especially relevant in statistics and machine learning, and knowing these is recommended, especially for more technical interviews. For instance, a common question in such interviews is to derive the MLE for a particular probability distribution. Thus, understanding the above steps, along with the details of the relevant probability distributions, is crucial.\n40 Real Statistics Interview Questions Easy 6.1. Uber: Explain the Central Limit Theorem. Why it is useful?\n6.2. Facebook: How would you explain a confidence interval to a non-technical audience?\n6.3. Twitter: What are some common pitfalls encountered in A/B testing?\n6.4. Lyft: Explain both covariance and correlation formulaically, and compare and contrast them.\n6.5. Facebook: Say you flip a coin 10 times and observe only one heads. What would be your null hypothesis and p-value for testing whether the coin is fair or not?\n6.6. Uber: Describe hypothesis testing and p-values in layman\u0026rsquo;s terms?\n6.7. Groupon: Describe what Type I and Type II errors are, and the tradeoffs between them.\n6.8. Microsoft: Explain the statistical background behind power.\n6.9. Facebook: What is a Z-test and when would you use it versus a t-test?\n6.10. Amazon: Say you are testing hundreds of hypotheses, each with t-test. What considerations would you take into account when doing this?\nMedium 6.11. Google: How would you derive a confidence interval for the probability of flipping heads from a series of coin tosses? 6.12. Two Sigma: What is the expected number of coin flips needed to get two consecutive heads? 6.13. Citadel: What is the expected number of rolls needed to see all 6 sides of a fair die? 6.14. Akuna Capital: Say you\u0026rsquo;re rolling a fair six-sided dice. What is the expected number of rolls until you roll two consecutive 5s? 6.15. D.E. Shaw: A coin was flipped 1000 times, and 550 times it showed heads. Do you think the coin is biased? Why or why not? 6.16. Quora: You are drawing from a normally distributed random variable X ~ N(0, 1) once a day. What is the approximate expected number of days until you get a value greater than 2? 6.17. Akuna Capital: Say you have two random variables X and Y, each with a standard deviation of 1. What is the variance of aX + bY for constants a and b? 6.18. Google: Say we have X ~ Uniform(0, 1) and Y ~ Uniform(0, 1) and the two are independent. What is the expected value of the minimum of X and Y? 6.19. Morgan Stanley: Say you have an unfair coin which lands on heads 60% of the time. How many coin flips are needed to detect that the coin is unfair? 6.20. Uber: Say you have n numbers 1\u0026hellip;n, and you uniformly sample from this distribution with replacement n times. What is the expected number of distinct values you would draw? 6.21. Goldman Sachs: There are 100 noodles in a bowl. At each step, you randomly select two noodle ends from the bowl and tie them together. What is the expectation on the number of loops formed? 6.22. Morgan Stanley: What is the expected value of the max of two dice rolls? 6.23. Lyft: Derive the mean and variance of the uniform distribution U(a, b). 6.24. Citadel: How many cards would you expect to draw from a standard deck before seeing the first ace? 6.25. Spotify: Say you draw n samples from a uniform distribution U(a, b). What are the MLE estimates of a and b? Hard 6.26. Google: Assume you are drawing from an infinite set of i.i.d random variables that are uniformly distributed from (0, 1). You keep drawing as long as the sequence you are getting is monotonically increasing. What is the expected length of the sequence you draw? 6.27 Facebook: There are two games involving dice that you can play. In the first game, you roll two dice at once and receive a dollar amount equivalent to the product of the rolls. In the second game, you roll one die and get the dollar amount equivalent to the square of that value. Which has the higher expected value and why? 6.28. Google: What does it mean for an estimator to be unbiased? What about consistent? Give examples of an unbiased but not consistent estimator, and a biased but consistent estimator. 6.29. Netflix: What are MLE and MAP? What is the difference between the two? 6.30. Uber: Say you are given a random Bernoulli trial generator. How would you generate values from a standard normal distribution? 6.31. Facebook: Derive the expectation for a geometric random variable. 6.32. Goldman Sachs: Say we have a random variable X ~ D, where D is an arbitrary distribution. What is the distribution F(X) where F is the CDF of X? 6.33. Morgan Stanley: Describe what a moment generating function (MGF) is. Derive the MGF for a normally distributed random variable X. 6.34. Tesla: Say you have N independent and identically distributed draws of an exponential random variable. What is the best estimator for the parameter λ? 6.35. Citadel: Assume that log X ~ N(0, 1). What is the expectation of X? 6.36. Google: Say you have two distinct subsets of a dataset for which you know their means and standard deviations. How do you calculate the blended mean and standard deviation of the total dataset? Can you extend it to K subsets? 6.37. Two Sigma: Say we have two random variables X and Y. What does it mean for X and Y to be independent? What about uncorrelated? Give an example where X and Y are uncorrelated but not independent. 6.38. Citadel: Say we have X ~ Uniform(-1, 1) and Y = X². What is the covariance of X and Y? 6.39. Lyft: How do you uniformly sample points at random from a circle with radius R? 6.40. Two Sigma: Say you continually sample from some i.i.d. uniformly distributed (0, 1) random variables until the sum of the variables exceeds 1. How many samples do you expect to make? 40 Real Statistics Interview Solutions Solution #6.1 The Central Limit Theorem (CLT) states that if any random variable, regardless of distribution, is sampled a large enough number of times, the sample mean will be approximately normally distributed. This allows for studying of the properties for any statistical distribution as long as there is a large enough sample size. The mathematical definition of the CLT is as follows: For any given random variable X, as n approaches infinity, X̄n = (X₁ + \u0026hellip; + Xn) / n ~ N(µ, σ²/n) At any company with a lot of data, like Uber, this concept is core to the various experimentation platforms used in the product. For a real-world example, consider testing whether adding a new feature increases rides booked in the Uber platform, where each Xᵢ is an individual ride and is a Bernoulli random variable (i.e., the rider books or does not book a ride). Then, if the sample size is sufficiently large, we can assess the statistical properties of the total number of bookings, as well as the booking rate (rides booked / rides opened on app). These statistical properties play a key role in hypothesis testing, allowing companies like Uber to decide whether or not to add new features in a data-driven manner.\nSolution #6.2 Suppose we want to estimate some parameters of a population. For example, we might want to estimate the average height of males in the US. Given some data from a sample, we can compute a sample mean for what we think the value is, as well as a range of values around that mean. Following the previous example, we could obtain the heights of 1000 random males in the U.S. and compute the average height, or the sample mean. This sample mean is a type of point estimate, and while useful, will vary from sample to sample. Thus we can\u0026rsquo;t tell anything about the variation in the data around this estimate, which is why we need a range of values through a confidence interval. Confidence intervals are a range of values with a lower and an upper bound such that if you were to sample the parameter of interest a large number of times, the 95% confidence interval would contain the true value of this parameter 95% of the time. We can construct a confidence interval using the sample standard deviation and sample mean. The level of confidence is determined by a margin of error that is set beforehand. The narrower the confidence interval, the more precise is the estimate, since there is less uncertainty associated with the point estimate of the mean.\nSolution #6.3 A/B testing has many possible pitfalls that depend on the particular experiment and setup employed. One common drawback is that groups may not be balanced, possibly resulting in highly skewed results. Note that balance is needed for all dimensions of the groups - like user demographics or device used - because, otherwise, the potentially statistically significant results from the test may simply be due to specific factors that were not controlled for. Two types errors are frequently assessed: Type I error, which is also known as a \u0026ldquo;false positive\u0026rdquo;, and Type II error, also known as a \u0026ldquo;false negative\u0026rdquo;. Specifically, Type I error is rejecting a null hypothesis when that hypothesis is correct, whereas Type II error is failing to reject a null hypothesis when its alternative hypothesis is correct. Another common pitfall is not running an experiment for long enough. Generally speaking, experiments are run with a particular power threshold and significance threshold; however, they often do not stop immediately upon detecting an effect. For an extreme example, assume you\u0026rsquo;re at either Uber or Lyft and running a test for two days, when the metric of interest (e.g., rides booked) is subject to weekly seasonality. Lastly, dealing with multiple tests is important because there may be interactions between results of tests you are running and so properly attributing results may be impossible in simple A/B tests. In addition, as the number of variations you run increases, so does the sample size needed. In practice, while it seems technically feasible to test 1000 variations of a button when optimizing for click-through rate, variations in tests are usually based on some intuitive hypothesis concerning core behavior.\nSolution #6.4 For any given random variables X and Y, the covariance, a linear measure of relationship, is defined by the following: Cov(X, Y) = E[(X - E[X])(Y- E[Y])] = E[XY] - E[X]E[Y] Specifically, covariance indicates the direction of the linear relationship between X and Y and can take on any potential value from negative infinity to infinity. The units of covariance are based on the units of X and Y, which may differ. The correlation between X and Y is the normalized version of covariance that takes into account the variances of X and Y: ρ(X,Y) = Cov(X,Y) / √Var(X)Var(Y) Since correlation results from scaling covariance, it is dimensionless (unlike covariance) and is always between -1 and 1 (also unlike covariance).\nSolution #6.5 The null hypothesis is that the coin is fair, and the alternative hypothesis is that the coin is biased: H₀: p₀ = 0.5, H₁: p₁ ≠ 0.5 Note that, since the sample size here is 10, you cannot apply the Central Limit Theorem and so cannot approximate a binomial using a normal distribution. The p-value here is the probability of observing the results obtained given that the null hypothesis is true, i.e., under the assumption that the coin is fair. In total for 10 flips of a coin, there are 2¹⁰ = 1024 possible outcomes, and in only 10 of them are there 9 tails and one heads. Hence, the exact probability of the given result is the p-value, which is 10/1024 ≈ 0.0098. Therefore, with a significance level set, for example, at 0.05, we can reject the null hypothesis.\nSolution #6.6 The process of testing whether data supports particular hypotheses is called hypothesis testing and involves measuring parameters of a population\u0026rsquo;s probability distribution. This process typically employs at least two groups, one a control that receives no treatment and the other(s), which do receive the treatment(s) of interest. Examples could be the height of two groups of people, the conversion rates for particular user flows in a product, etc. Testing also involves two hypotheses, the null hypothesis, which assumes no significant difference between the groups, and the alternative hypothesis, which assumes a significant difference in the measured parameter(s) as a consequence of the treatment. A p-value is the probability of observing the given test results under the null hypothesis assumptions. The lower this probability, the higher the chance that the null hypothesis should be rejected. If the p-value is lower than the pre-determined significance level α, generally set at 0.05, then it indicates that the null hypothesis should be rejected in favor of the alternative hypothesis. Otherwise, the null hypothesis cannot be rejected, and it cannot be concluded that the treatment has any significant effect.\nSolution #6.7 Both errors are relevant in the context of hypothesis testing. Type I error is when one rejects the null hypothesis when it is correct, and is known as a false positive. Type II error is when the null hypothesis is not rejected when the alternative hypothesis is correct; this is known as a false negative. In layman\u0026rsquo;s terms, a type I error is when we detect a difference, when in reality there is no significant difference in an experiment. Similarly, a type II error occurs when we fail to detect a difference, when in reality there is a significant difference in an experiment. Type I error is given by the level of significance α, whereas the type II error is given by β. Usually, 1-α is referred to as the confidence level, whereas 1-β is referred to as the statistical power of the test being conducted. Note that, in any well-conducted statistical procedure, we want to have both α and β be small. However, based on the definition of the two, it is impossible to make both errors small simultaneously: the larger α is, the smaller β is. Based on the experiment and the relative importance of false positives and false negatives, a Data Scientist must decide what thresholds to adopt for any given experiment. Note that experiments are set up so as to have both 1-α and 1-β relatively high (say at .95, and .8 respectively).\nSolution #6.8 Power is the probability of rejecting the null hypothesis when, in fact, it is false. It is also the probability of avoiding a Type II error. A Type II error occurs when the null hypothesis is not rejected when the alternative hypothesis is correct. This is important because we want to detect significant effects during experiments. That is, the higher the statistical power of the test, the higher the probability of detecting a genuine effect (i.e, accepting the alternative hypothesis and rejecting the null hypothesis). A minimum sample size can be calculated for any given level of power - for example, say a power level of 0.8. An analysis of the statistical power of a test is usually performed with respect to the test\u0026rsquo;s level of significance (α) and effect size (i.e., the magnitude of the results).\nSolution #6.9 In a Z-test, your test statistic follows a normal distribution under the null hypothesis. Alternatively, in a t-test, you employ a student\u0026rsquo;s t-distribution rather than a normal distribution as your sampling distribution. Considering the population mean, we can use either Z-test or t-test only if the mean is normally distributed, which is possible in two cases: the initial population is normally distributed, or the sample size is large enough (n ≥ 30) so we can apply the Central Limit Theorem. If the condition above is satisfied, then we need to decide which type of test is more appropriate to use. In general, we use Z-tests if the population variation is known, and vice versa: we use t-test if the population variation is unknown. Additionally, if the sample size is very large (n \u0026gt; 200), we can use the Z-test in any case, since for such large degrees of freedom, t-distribution coincides with z-distribution up to thousands. Considering the population proportion, we can use Z-test (but not t-test) for it in the case and np₀ ≥ 10 and n(1-p₀) ≥ 10, i.e., when each of the number of successes and the number of failures is at least 10.\nSolution #6.10 The primary consideration is that, as the number of tests increases, the chance that a stand-alone p-value for any of the t-tests is statistically significant becomes very high due to chance alone. As an example, with 100 tests performed and a significance threshold of α = 0.05, you would expect 5 of the experiments to be statistically significant due only to chance. That is, you have a very high probability of observing at least one significant outcome. Therefore, the chance of incorrectly rejecting a null hypothesis (i.e., committing Type I error) increases. To correct for this effect, we can use a method called the Bonferroni correction, wherein we set the significance threshold to α/m, where m is the number of tests being performed. In the above scenario having 100 tests, we can set the significance threshold to instead be 0.05/100 = 0.0005. While this correction helps to protect from Type I error, it is still prone to Type II error (i.e., failing to reject the null hypothesis when it should be rejected). In general, the Bonferroni correction is mostly useful when there is a smaller number of multiple comparisons of which a few are significant. If the number becomes sufficiently high that many tests yield statistically significant results, the number of Type II errors may also increase significantly.\nSolution #6.11 The confidence interval (CI) for a population proportion is an interval that includes a true population proportion with a certain degree of confidence 1 - α. For the case of flipping heads from a series of coin tosses, the proportion follows the binomial distribution. If the series size is large enough (each of the number of successes and the number of failures is at least 10), we can utilize the Central Limit Theorem and use the normal approximation for the binomial distribution, meaning the sample proportion p̂ is approximately N(p, p(1-p)/n). We estimate this as N(p̂, p̂(1-p̂)/n). where p̂ is the proportion of heads tossed in series, and n is the series size. The CI is centered at the series proportion, and plus or minus a margin of error: p̂ ± z_(α/2) * √(p̂(1-p̂)/n) where z_(α/2) is the appropriate value from the standard normal distribution for the desired confidence level. For example, for the most commonly used level of confidence 95%, z_(α/2) = 1.96.\n","permalink":"https://deepskandpal.github.io/bookshelf/201-ds-interview/chapter-6/","tags":null,"title":"Chapter 6: Statistics"},{"categories":null,"content":"This chapter covers the design of systems for generating realistic human faces using generative adversarial networks (GANs) and diffusion models.\nKey Concepts Generative Adversarial Networks: GAN architecture for high-quality face generation Latent Space Manipulation: Controlling facial attributes and expressions Ethics and Safety: Addressing deepfake concerns and responsible AI Main Topics Covered Face generation system architecture GAN vs. diffusion model trade-offs Training pipeline and data considerations Quality control and artifact detection Safety measures and content validation System Design Considerations Generating high-resolution, photorealistic faces Real-time vs. batch generation trade-offs Preventing misuse and implementing safety guardrails Handling diverse demographic representation (Your detailed notes for Chapter 7 go here\u0026hellip;)\n","permalink":"https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/chapter-7/","tags":null,"title":"Chapter 7: Realistic Face Generation"},{"categories":null,"content":"This chapter explores the challenges and solutions for generating high-resolution images (1024x1024 and beyond) using advanced generative models.\nKey Concepts Progressive Generation: Building images from low to high resolution Memory Optimization: Handling large model inference efficiently Quality vs. Speed: Balancing generation quality with computational cost Main Topics Covered High-resolution image generation architecture Progressive growing and multi-scale approaches Memory-efficient training and inference Quality assessment and perceptual metrics Distributed generation and model parallelism System Design Considerations GPU memory constraints and optimization techniques Generating images larger than 1K x 1K resolution Batch processing for multiple image requests Caching strategies for common generation patterns (Your detailed notes for Chapter 8 go here\u0026hellip;)\n","permalink":"https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/chapter-8/","tags":null,"title":"Chapter 8: High-Resolution Image Synthesis"},{"categories":null,"content":"Prerequisites for Understanding RAG Large Language Models (LLMs) as Generative Engines (from Chapter 1):\nCore Idea: LLMs, especially decoder-only architectures like GPT, are fundamentally next-token predictors. Given some input text (a prompt), they generate a plausible continuation. The \u0026ldquo;Knowledge\u0026rdquo; Limitation: Their knowledge is \u0026ldquo;frozen\u0026rdquo; at the time of their last training. They don\u0026rsquo;t know about events or information that occurred after that. The Hallucination Problem: Because they are so good at generating fluent, confident-sounding text, they can sometimes generate incorrect or nonsensical information with high confidence. They are trying to complete a pattern, not necessarily state a verified fact from an internal database. What RAG tries to achieve here: Provide the LLM with fresh, relevant, and factual information at the time of generation to guide its output and make it more accurate and less prone to hallucination. Embeddings: The Language of Meaning (from Chapter 2):\nCore Idea: Embeddings are numerical representations (vectors) of text (words, sentences, documents) in a high-dimensional space. Semantic Similarity: The crucial property is that texts with similar meanings will have embeddings that are \u0026ldquo;close\u0026rdquo; to each other in this vector space. \u0026ldquo;The cat is furry\u0026rdquo; will be closer to \u0026ldquo;My feline is fluffy\u0026rdquo; than to \u0026ldquo;The car is fast.\u0026rdquo; How they are made: We saw word2vec, and more advanced models like BERT or Sentence-Transformers produce these. What RAG tries to achieve here: Use embeddings to find pieces of text from a knowledge source that are semantically similar (i.e., relevant) to a user\u0026rsquo;s query. Dense Retrieval / Semantic Search (Implicit in Chapter 2, core to Chapter 8):\nCore Idea: The process of finding the most relevant documents (or text chunks) from a large collection (a \u0026ldquo;text archive\u0026rdquo; or \u0026ldquo;knowledge base\u0026rdquo;) in response to a user query, based on semantic similarity of their embeddings. The Mechanism: Indexation: Convert all documents/chunks in your knowledge base into embeddings and store them (often in a specialized \u0026ldquo;vector database\u0026rdquo;). Querying: When a user asks a question, convert that question into an embedding. Search: Compare the query embedding with all the document embeddings in your index and retrieve the \u0026ldquo;nearest neighbors\u0026rdquo; – the documents whose embeddings are closest to the query embedding. What RAG tries to achieve here: This is the \u0026ldquo;Retrieval\u0026rdquo; part of RAG. It\u0026rsquo;s the engine that pulls out the relevant context. Chunking Long Texts (Practical aspect for Retrieval, hinted at in Chapter 8):\nCore Idea: LLMs have a finite context window (the maximum number of tokens they can process). You often can\u0026rsquo;t feed an entire large document to an embedding model or into the final LLM prompt. The Solution: Break down large documents into smaller, manageable \u0026ldquo;chunks\u0026rdquo; (e.g., paragraphs, sentences, or fixed-size token blocks). Each chunk then gets its own embedding. What RAG tries to achieve here: Ensure that the retrieval step can pinpoint specific relevant pieces of information from large documents, rather than just getting a vague embedding of the whole document. Prompt Engineering (Rudimentary in Chapter 1, full focus in Chapter 6):\nCore Idea: Crafting effective input prompts to guide an LLM to produce the desired output. Providing Context: A key technique in prompt engineering is giving the LLM relevant context along with the instruction or question. What RAG tries to achieve here: RAG automates the process of finding highly relevant context (from the retrieval step) and then constructs a new prompt that includes this context along with the original user query to feed into the generative LLM. So, to recap the prerequisites:\nLLMs generate text but can hallucinate. Embeddings capture meaning and allow similarity comparison. Dense retrieval uses embeddings to find relevant text for a query. Chunking makes large documents searchable. Prompt engineering is how we tell an LLM what to do, and RAG uses it to provide retrieved context. With these building blocks in mind, we are perfectly set to explore Chapter 8!\nChapter 8: Semantic Search and Retrieval-Augmented Generation.\n(Gestures to an imaginary slide with the chapter title)\nSearch engines were some of the very first large-scale applications of language models. Google announced using BERT for its search way back, calling it a \u0026ldquo;huge leap.\u0026rdquo; Microsoft Bing followed suit. Why? Because these models enabled semantic search – searching by meaning, not just keywords.\nBut then came the generative models, like ChatGPT. People started asking them questions expecting factual answers. And while they\u0026rsquo;re fluent, they aren\u0026rsquo;t always correct. This led to the problem of \u0026ldquo;hallucinations.\u0026rdquo; One of the best ways to combat this is Retrieval-Augmented Generation (RAG) – building systems that retrieve relevant information before generating an answer. This is one of the hottest applications of LLMs right now.\nOverview of Semantic Search and RAG Chapter 8 looks at three broad categories:\nDense Retrieval: This is what we just discussed as a prerequisite. It relies on embeddings. You embed your query, you embed your documents (or chunks of documents), and you find the ones whose embeddings are closest to your query\u0026rsquo;s embedding. (Figure 8-1 in the book shows this: query -\u0026gt; dense retrieval -\u0026gt; ranked documents).\nWhat it\u0026rsquo;s trying to achieve: Find semantically relevant documents from a corpus. Reranking: Often, search is a pipeline. A first-stage retriever (maybe keyword-based, or a fast dense retriever) gets a bunch of potentially relevant documents. A reranker then takes this smaller set and the original query, and scores the relevance of each document much more carefully, reordering them. (Figure 8-2 shows query + initial results -\u0026gt; reranker -\u0026gt; improved order of results).\nWhat it\u0026rsquo;s trying to achieve: Improve the quality and ordering of search results from an initial, possibly less precise, retrieval step. Retrieval-Augmented Generation (RAG): This is where we combine search with generation. The LLM doesn\u0026rsquo;t just rely on its internal knowledge; it\u0026rsquo;s augmented with retrieved information. (Figure 8-3 shows query -\u0026gt; RAG system -\u0026gt; answer + cited sources).\nWhat it\u0026rsquo;s trying to achieve: Generate factual, grounded answers by providing the LLM with relevant context from external sources, reducing hallucinations, and enabling \u0026ldquo;chat with your data\u0026rdquo; scenarios. Let\u0026rsquo;s dive deeper into these.\nSemantic Search with Language Models (Again, imagine Figure 8-4: texts as points in space, similar texts are closer) This is the core idea we\u0026rsquo;ve built up. Embeddings project text into a space where distance equals dissimilarity. When a user queries, we embed the query into this same space and find the nearest document embeddings (Figure 8-5).\nCaveats of Dense Retrieval (page 328-329):\nWhat if no good results exist? The system might still return the \u0026ldquo;least bad\u0026rdquo; ones. We might need a similarity threshold. What if the query and best result aren\u0026rsquo;t truly semantically similar, just share some keywords? This is why embedding models for retrieval are often fine-tuned on question-answer pairs (more on this in Chapter 10). Keyword matching is still good for exact phrases. Hybrid search (semantic + keyword) is often best. Domain specificity: A model trained on Wikipedia might not do well on legal texts. Chunking Long Texts (page 330-333):\nWhy? LLMs have limited context windows. One vector per document? You could embed just the title, or average all chunk embeddings. Not ideal as you lose a lot of specific information. Multiple vectors per document (Better!): Chunk the document (sentences, paragraphs, fixed-size, overlapping chunks as shown in Figures 8-7, 8-8, 8-9) and embed each chunk. Your search index then contains chunk embeddings. This allows for more precise retrieval. Strategies include: each sentence, each paragraph, or overlapping chunks to preserve context across chunk boundaries (Figure 8-10). Nearest Neighbor Search vs. Vector Databases (page 333-334):\nFor small archives, calculating all distances (e.g., with NumPy) is fine. For millions of vectors, you need optimized Approximate Nearest Neighbor (ANN) search libraries like FAISS or Annoy. They are fast, can use GPUs. Vector Databases (e.g., Weaviate, Pinecone, ChromaDB) are even more sophisticated. They allow adding/deleting vectors without rebuilding the whole index, filtering, and more complex querying beyond just vector distance (Figure 8-11). Fine-tuning Embedding Models for Dense Retrieval (page 334-336):\nJust like in classification (Chapter 4), we can fine-tune embedding models specifically for retrieval. Goal: Make embeddings of relevant query-document pairs closer and irrelevant pairs farther. Training Data: (Query, Relevant Document) pairs as positive examples, and (Query, Irrelevant Document) pairs as negative examples. (Figure 8-12 shows before fine-tuning: \u0026ldquo;Interstellar release date\u0026rdquo; and \u0026ldquo;Interstellar cast\u0026rdquo; might be equally close to a document about Interstellar\u0026rsquo;s premiere. Figure 8-13 shows after fine-tuning: \u0026ldquo;Interstellar release date\u0026rdquo; is much closer, \u0026ldquo;Interstellar cast\u0026rdquo; is pushed away.) Reranking This is often a second stage in a search pipeline.\nHow reranking models work (Figure 8-15): They are often cross-encoders. The query AND a candidate document are fed together into the LLM (like BERT). The model then outputs a relevance score (e.g., 0 to 1). This is more computationally expensive than dense retrieval (where query and documents are embedded separately), so it\u0026rsquo;s typically done on a smaller, shortlisted set of documents. Example (page 337-338): The book shows using Cohere\u0026rsquo;s Rerank endpoint. If a keyword search (BM25) brings up some results, the reranker can significantly improve their order by understanding the semantic relevance more deeply. Retrieval Evaluation Metrics How do we know if our search system is good? We need:\nA text archive. A set of queries. Relevance judgments: For each query, which documents in the archive are actually relevant? (Figure 8-16) Mean Average Precision (MAP): A popular metric. Precision@k: Out of the top k results, how many are relevant? Average Precision (AP) for a single query: (Figures 8-20, 8-21, 8-22 show this). It rewards systems that rank relevant documents higher. If a relevant document is at rank 1, AP is 1.0. If it\u0026rsquo;s at rank 3 (with 2 irrelevant ones before it), AP might be 0.33. If there are multiple relevant documents, it averages the precision at each relevant document\u0026rsquo;s position. Mean Average Precision (MAP): The average of AP scores across all queries in your test set (Figure 8-23). This gives a single number to compare systems. Another common metric is nDCG (normalized discounted cumulative gain), which handles graded relevance (some documents can be more relevant than others). Now, the main event for this session!\nRetrieval-Augmented Generation (RAG) (Imagine Figure 8-24: A diagram showing Question -\u0026gt; 1) Retrieval -\u0026gt; 2) Grounded Generation -\u0026gt; Answer)\nThis is the industry\u0026rsquo;s leading method to tackle LLM hallucinations and ground them in specific, up-to-date knowledge.\nWhat it is: A system that first retrieves relevant information from a knowledge source and then uses that information to augment the prompt given to a generative LLM, which then produces the final answer.\nWhy it\u0026rsquo;s great:\nReduces hallucinations. Improves factuality. Allows LLMs to use information beyond their training data (e.g., internal company documents, recent news). Enables \u0026ldquo;chat with your data\u0026rdquo; applications. The Basic RAG Pipeline (Figure 8-24):\nRetrieval Step: User asks a question. This question is used to query a knowledge base (using dense retrieval, keyword search, or hybrid). The top N relevant document chunks are retrieved. Grounded Generation Step: The original question AND the retrieved document chunks are combined into a new, augmented prompt. This prompt is then fed to a generative LLM to produce the final answer. The LLM is instructed to use the provided context. (Figure 8-25 shows this with sources cited, Figure 8-26 shows the context being added to the prompt). Example: Grounded Generation with an LLM API (page 351):\nThe book shows using Cohere\u0026rsquo;s co.chat endpoint which has built-in RAG capabilities. You provide the message (query) and documents (retrieved chunks). The LLM generates an answer and can even provide citations pointing to which parts of the retrieved documents support its answer. Example: RAG with Local Models (page 352-355):\nThis demonstrates the flow if you\u0026rsquo;re building it yourself. Load Generation Model: e.g., a quantized Phi-3 using llama-cpp-python and LangChain. Load Embedding Model: e.g., BAAI/bge-small-en-v1.5. Create Vector Database: Use FAISS (or ChromaDB, etc.) to index your document chunks with their embeddings. The RAG Prompt: This is crucial. It typically looks something like: \u0026lt;|user|\u0026gt; Relevant information: {context} \u0026lt;-- This is where retrieved chunks go Provide a concise answer the following question using the relevant information provided above: {question} \u0026lt;--- This is the original user question \u0026lt;|end|\u0026gt; \u0026lt;|assistant|\u0026gt; LangChain\u0026rsquo;s RetrievalQA chain can orchestrate this: it takes the LLM, the retriever (from the vector DB), and the prompt template. Advanced RAG Techniques (page 355-357):\nQuery Rewriting: If the user\u0026rsquo;s question is verbose or conversational (e.g., \u0026ldquo;I need an essay on dolphins, where do they live?\u0026rdquo;), an LLM can rewrite it into a more effective search query (\u0026ldquo;Where do dolphins live\u0026rdquo;). Multi-Query RAG: For questions like \u0026ldquo;Compare Nvidia\u0026rsquo;s financial results in 2020 vs. 2023,\u0026rdquo; the system might generate multiple search queries (\u0026ldquo;Nvidia 2020 financial results\u0026rdquo;, \u0026ldquo;Nvidia 2023 financial results\u0026rdquo;) and then synthesize the information. Multi-Hop RAG: For questions requiring sequential reasoning (e.g., \u0026ldquo;Who are the largest car manufacturers in 2023? Do they each make EVs?\u0026rdquo;). Search: \u0026ldquo;largest car manufacturers 2023\u0026rdquo; -\u0026gt; Gets Toyota, VW, Hyundai. Search: \u0026ldquo;Toyota electric vehicles\u0026rdquo;, \u0026ldquo;VW electric vehicles\u0026rdquo;, \u0026ldquo;Hyundai electric vehicles\u0026rdquo;. Query Routing: If you have multiple knowledge bases (e.g., HR documents in Notion, customer data in Salesforce), an LLM can decide which source to query based on the question. Agentic RAG: This is where RAG starts to look like the agents we\u0026rsquo;ll discuss more (or that the book covers in Chapter 7, which you\u0026rsquo;ve skipped for now!). The LLM becomes more autonomous, deciding which tools (search, specific databases, etc.) to use and in what order. Cohere\u0026rsquo;s Command R+ is good at this. RAG Evaluation (page 357-358):\nHow do you know your RAG system is good? It\u0026rsquo;s not just about search relevance. The paper \u0026ldquo;Evaluating verifiability in generative search engines\u0026rdquo; suggests axes like: Fluency: Is the generated text smooth and cohesive? Perceived Utility: Is the answer helpful and informative? Citation Recall: Are statements supported by the cited sources? Citation Precision: Do the citations actually support the statements they\u0026rsquo;re linked to? LLM-as-a-Judge: Using another capable LLM to evaluate the RAG output. Ragas: A software library for this. It looks at: Faithfulness: Is the answer consistent with the provided context? Answer Relevance: Is the answer relevant to the question? Summary of RAG RAG is a powerful technique that combines the strengths of information retrieval with the generative capabilities of LLMs.\nIt retrieves relevant information. It augments the LLM\u0026rsquo;s prompt with this information. It allows the LLM to generate more factual, grounded, and up-to-date responses. It\u0026rsquo;s key to reducing hallucinations and making LLMs more trustworthy and useful in real-world applications. Phew! That was a deep dive into RAG and its foundations. It\u0026rsquo;s a cornerstone of modern LLM applications. It\u0026rsquo;s about giving your LLM a library card and teaching it how to read relevant books before answering your question!\nWhat are your thoughts? Does this give you a clearer picture of what RAG is trying to achieve and how it goes about it?\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-large-language-models/chapter-8/","tags":null,"title":"Chapter 8: Retrieval Augmented Generation"},{"categories":null,"content":"This chapter covers text-to-image generation systems like DALL-E and Stable Diffusion that create images from natural language descriptions.\nKey Concepts Text-Image Alignment: Connecting textual descriptions with visual concepts Diffusion Models: Modern approach to controllable image generation Prompt Engineering: Optimizing text inputs for better image outputs Main Topics Covered Text-to-image system architecture CLIP embeddings and cross-modal understanding Diffusion model training and inference Prompt processing and optimization Safety filtering and content moderation System Design Considerations Handling complex and creative text prompts Ensuring generated content aligns with user intent Implementing content safety and filtering systems Supporting multiple art styles and domains (Your detailed notes for Chapter 9 go here\u0026hellip;)\n","permalink":"https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/chapter-9/","tags":null,"title":"Chapter 9: Text-to-Image Generation"},{"categories":null,"content":"Chapter 9 Overview: Binary Trees Binary trees are fundamental hierarchical data structures. Each node has at most two children, referred to as the left child and the right child. They are essential for representing data with a branching structure, such as file systems, expression evaluation, and search trees.\nKey Binary Tree Concepts: Node: The basic unit of a tree, containing data and pointers to its children. Root: The topmost node in a tree. Leaf: A node with no children. Depth: The length of the path from the root to a node. The root\u0026rsquo;s depth is 0. Height: The length of the longest path from a node to a leaf. The height of a leaf is 0. The height of a tree is the height of its root. Full Binary Tree: A tree where every node has either 0 or 2 children. Complete Binary Tree: A tree where all levels are completely filled except possibly the last, and the last level has all keys as left as possible. Perfect Binary Tree: A tree where all interior nodes have two children and all leaves are at the same level. Tree Traversal: The three primary ways to visit all nodes in a tree are:\nIn-order Traversal: Left subtree -\u0026gt; Visit Root -\u0026gt; Right subtree. For a Binary Search Tree (BST), this yields the nodes in sorted order. Pre-order Traversal: Visit Root -\u0026gt; Left subtree -\u0026gt; Right subtree. Often used to create a copy of the tree. Post-order Traversal: Left subtree -\u0026gt; Right subtree -\u0026gt; Visit Root. Often used to delete nodes from the tree. Section 9.1: Test if a Binary Tree is Height-Balanced The Problem: A binary tree is height-balanced if, for every node, the difference in height between its left and right subtrees is at most 1. Write a function to determine if a given binary tree is height-balanced.\nAlgorithm (Recursive Post-order Traversal): A brute-force approach would be to compute the height of the left and right subtrees for every node, which is inefficient (O(n^2)). A better approach integrates the height calculation with the balance check.\nCreate a recursive helper function, say check_balance(node). This function will return two pieces of information: whether the subtree at node is balanced, and what its height is. A common way to do this is to return a special value (like -1) to signal imbalance.\ncheck_balance(node) logic:\nBase Case: If node is None, it\u0026rsquo;s a balanced subtree of height -1 (or 0, depending on convention). Return (True, -1). Recursive Step: Recursively call check_balance on the left child. Get is_left_balanced and left_height. If is_left_balanced is False, this subtree is not balanced. Propagate the failure up: return (False, 0). Recursively call check_balance on the right child. Get is_right_balanced and right_height. If is_right_balanced is False, propagate the failure up: return (False, 0). Check Current Node: Check if the current node is balanced: abs(left_height - right_height) \u0026lt;= 1. If it is, the height of the current node\u0026rsquo;s subtree is 1 + max(left_height, right_height). Return (True, height). If it\u0026rsquo;s not, return (False, 0). Complexity:\nTime: O(n), as each node is visited once. Space: O(h), where h is the height of the tree, for the recursion stack. O(log n) for a balanced tree, O(n) for a skewed tree. Section 9.2: Test if a Binary Tree is a Binary Search Tree (BST) The Problem: Write a function to check if a given binary tree is a valid Binary Search Tree (BST).\nBST Property: For any given node N:\nAll values in N\u0026rsquo;s left subtree must be less than N\u0026rsquo;s value. All values in N\u0026rsquo;s right subtree must be greater than N\u0026rsquo;s value. Both the left and right subtrees must also be binary search trees. Pitfall: A common mistake is to only check if node.left.data \u0026lt; node.data and node.right.data \u0026gt; node.data. This is not sufficient. The property must hold for all nodes in the subtree, not just the immediate children.\nAlgorithm (Recursive with Min/Max Range): The most robust method is to perform a recursive traversal, passing down the valid range of values (min_val, max_val) that a node\u0026rsquo;s value is allowed to have.\nCreate a recursive helper function is_bst_helper(node, min_val, max_val).\nis_bst_helper logic:\nBase Case: If node is None, it\u0026rsquo;s a valid BST. Return True. Check Current Node: If node.data \u0026lt;= min_val or node.data \u0026gt;= max_val, the BST property is violated. Return False. Recursive Step: Recursively check the left subtree. The new max_val for the left subtree is the current node\u0026rsquo;s value: is_bst_helper(node.left, min_val, node.data). Recursively check the right subtree. The new min_val for the right subtree is the current node\u0026rsquo;s value: is_bst_helper(node.right, node.data, max_val). The tree is a BST if both recursive calls return True. Initial call: is_bst_helper(root, -infinity, +infinity).\nSection 9.4: Find the Lowest Common Ancestor (LCA) in a BST The Problem: Given a BST and two nodes s and b (smaller and bigger value), find their Lowest Common Ancestor (LCA). The LCA is the deepest node in the tree that has both s and b as descendants.\nAlgorithm (Exploiting BST Properties): The search for the LCA in a BST is much simpler than in a generic binary tree. We can use the BST property to guide our search from the root.\nStart with current = root. Loop as long as current is not None: If current.data is greater than both s.data and b.data, it means both nodes are in the left subtree. The LCA must also be in the left subtree. So, move left: current = current.left. If current.data is less than both s.data and b.data, it means both nodes are in the right subtree. The LCA must also be in the right subtree. So, move right: current = current.right. Otherwise, current.data is between s.data and b.data (or equal to one of them). This means the path to s and b diverges at current. Therefore, current is the LCA. Return current. Complexity:\nTime: O(h), where h is the height of the tree. In the worst case (a skewed tree), this is O(n). For a balanced tree, it\u0026rsquo;s O(log n). Space: O(1) for the iterative approach. Section 9.7: Implement an In-order Traversal without Recursion The Problem: Perform an in-order traversal of a binary tree without using recursion, which implicitly uses the call stack. You must use an explicit stack data structure.\nAlgorithm (Iterative with a Stack): This approach mimics what the recursion call stack does.\nInitialize an empty stack and set current = root. Loop as long as current is not None or the stack is not empty. Go Left: If current is not None: Push current onto the stack. Move to the left child: current = current.left. Visit and Go Right: If current is None (meaning we\u0026rsquo;ve gone as far left as possible): Pop a node from the stack. This is the node we need to \u0026ldquo;visit\u0026rdquo;. Process its data (e.g., add to a result list). Now, move to the right subtree of the popped node: current = popped_node.right. The loop terminates when both the stack is empty and the current node is None. ","permalink":"https://deepskandpal.github.io/bookshelf/elements-of-programming/chapter-9/","tags":null,"title":"Chapter 9: Binary Trees"},{"categories":null,"content":"This chapter explores the design of systems that generate personalized professional headshots based on user photos and specifications.\nKey Concepts Face Identity Preservation: Maintaining user\u0026rsquo;s facial features across different styles Style Transfer: Applying professional photography styles to generated images Personalization Pipeline: End-to-end system for custom headshot creation Main Topics Covered Personalized generation system architecture Identity encoding and preservation techniques Style control and background generation Quality assurance and user feedback loops Privacy and data handling considerations System Design Considerations Processing and storing user-uploaded photos securely Generating multiple style variations efficiently Maintaining consistency across generated images Handling diverse facial features and ethnicities (Your detailed notes for Chapter 10 go here\u0026hellip;)\n","permalink":"https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/chapter-10/","tags":null,"title":"Chapter 10: Personalized Headshot Generation"},{"categories":null,"content":"Introduction - Inspiration from Nature The chapter beautifully starts by reminding us how nature has often inspired human inventions: birds inspired planes, burdock plants inspired Velcro. So, it\u0026rsquo;s logical to look at the brain\u0026rsquo;s architecture for inspiration on building intelligent machines. This is the core idea that sparked ANNs.\nANNs vs. Biological Neurons: While ANNs were inspired by the networks of biological neurons in our brains, they have evolved to be quite different. Just like planes don\u0026rsquo;t flap their wings, ANNs don\u0026rsquo;t need to be biologically perfectly realistic to be effective. The footnote mentions a good philosophy: be open to biological inspiration but don\u0026rsquo;t be afraid to create biologically unrealistic models if they work well. Some researchers even prefer calling the components \u0026ldquo;units\u0026rdquo; rather than \u0026ldquo;neurons\u0026rdquo; to avoid this restrictive analogy.\nThe Power of ANNs:\nThey are at the heart of Deep Learning. Versatile, powerful, and scalable. Ideal for large, complex tasks like: Image classification (Google Images) Speech recognition (Apple\u0026rsquo;s Siri) Recommendation systems (YouTube) Game playing (DeepMind\u0026rsquo;s AlphaGo) Chapter Structure:\nPart 1: Introduces ANNs, starting from early architectures and leading up to Multilayer Perceptrons (MLPs), which are heavily used today. Part 2: Focuses on implementing neural networks using the Keras API. Keras is described as a \u0026ldquo;beautifully designed and simple high-level API\u0026rdquo; for building, training, evaluating, and running neural networks. (Page 280: From Biological to Artificial Neurons - A Brief History) Early Beginnings (1943): ANNs are surprisingly old! They were first introduced by neurophysiologist Warren McCulloch and mathematician Walter Pitts. Their landmark paper proposed a simplified computational model of how biological neurons might perform complex computations using propositional logic. This was the first ANN architecture.\nThe First \u0026ldquo;AI Winter\u0026rdquo; (1960s-1970s): Early successes led to widespread belief in imminent truly intelligent machines. When this didn\u0026rsquo;t materialize quickly, funding dried up, and ANNs entered a long \u0026ldquo;winter.\u0026rdquo;\nRevival (1980s - Connectionism): New architectures and better training techniques sparked renewed interest. However, progress was slow.\nThe Second \u0026ldquo;AI Winter\u0026rdquo; (1990s): Other ML techniques like Support Vector Machines (Chapter 5) emerged, seeming to offer better results and stronger theoretical foundations, pushing ANNs to the background again.\nThe Current Wave (Now!): We\u0026rsquo;re in another, much stronger wave of interest in ANNs. Why is this time different?\nHuge Quantity of Data: We now have vast amounts of data to train large neural networks (e.g., ImageNet). ANNs often outperform other ML techniques on very large and complex problems. Tremendous Increase in Computing Power: Moore\u0026rsquo;s Law (components in circuits doubling roughly every 2 years). Powerful GPUs (Graphics Processing Units), initially driven by the gaming industry, are exceptionally good at the kind of parallel computations needed for ANNs. Cloud platforms make this power accessible to everyone. Improved Training Algorithms: While often only slight tweaks from 1990s algorithms, these have had a huge positive impact (e.g., better optimization algorithms, initialization techniques, regularization). Theoretical Limitations Turning Benign: Fears that ANNs would always get stuck in poor local optima have largely proven less of an issue in practice. When they do get stuck, the local optima are often fairly close to the global optimum. Virtuous Circle of Funding and Progress: Amazing products based on ANNs make headlines (AlphaGo, GPT-3/4, etc.), attracting more attention, funding, and talent, leading to further progress. (Page 281-283: Biological Neurons and a Simple Artificial Neuron Model) Biological Neurons (Figure 10-1, page 282):\nA quick look at the structure: cell body (soma), dendrites (receive signals), axon (transmits signals), synaptic terminals (connect to other neurons). Neurons produce electrical impulses (action potentials). When a neuron receives enough neurotransmitter signals at its synapses within a short period, it \u0026ldquo;fires\u0026rdquo; its own impulse. Some neurotransmitters are excitatory (encourage firing), some are inhibitory. Individual neurons are relatively simple, but billions of them, each connected to thousands of others, form a vast network capable of highly complex computations. The brain\u0026rsquo;s architecture, especially the layered structure of the cerebral cortex (Figure 10-2), provides inspiration. Logical Computations with Artificial Neurons (McCulloch \u0026amp; Pitts Model - Page 283):\nTheir early model was very simple: Binary (on/off) inputs. One binary (on/off) output. The neuron activates its output if a certain number of its inputs are active. Figure 10-3 shows how such simple neurons can perform basic logical computations (assuming activation if at least two inputs are active): Identity (C=A): Neuron A sends two signals to C. If A is on, C gets two active inputs and turns on. Logical AND (C = A ∧ B): C activates only if both A and B are active (one active input isn\u0026rsquo;t enough). Logical OR (C = A ∨ B): C activates if A is active, or B is active, or both (any one provides two inputs to a common intermediate neuron, which then activates C, or if A and B both directly input to C and one active input is enough, though the diagram is a bit more complex). The diagram shows intermediate neurons. The idea is that if A is active, it can trigger enough input for C to fire, same for B. Complex Logic (e.g., A AND NOT B): If we assume an input can inhibit activity, this is also possible. If A is active and B is off, C activates. If B is on, it inhibits C. What this was ultimately trying to achieve: To show that even a very simplified model of a neuron, when networked, could perform fundamental logical computations, suggesting a path towards building computational intelligence. (Page 284-288: The Perceptron) Invented by Frank Rosenblatt in 1957. One of the simplest ANN architectures.\nBased on a Threshold Logic Unit (TLU) or Linear Threshold Unit (LTU) (Figure 10-4, page 284):\nInputs \u0026amp; Output: Numbers (not just binary on/off). Weights: Each input connection i has an associated weight wᵢ. Weighted Sum: The TLU computes a weighted sum of its inputs: z = w₁x₁ + w₂x₂ + ... + wₙxₙ = wᵀx. Step Function: It then applies a step function to this sum z to produce the output: h_w(x) = step(z). What the TLU is ultimately trying to achieve: It makes a decision based on whether a weighted combination of evidence (z) exceeds some threshold. Common Step Functions (Equation 10-1, page 285):\nHeaviside step function: Outputs 0 if z \u0026lt; 0, outputs 1 if z ≥ 0 (assuming threshold is 0). Sign function: Outputs -1 if z \u0026lt; 0, 0 if z = 0, +1 if z \u0026gt; 0. Single TLU for Classification:\nA single TLU can perform simple linear binary classification. It\u0026rsquo;s very similar to a Logistic Regression or linear SVM classifier, but with a hard threshold output instead of a probability or a margin. Example: Classify Iris flowers based on petal length and width. You\u0026rsquo;d add a bias feature x₀=1. Training means finding weights w₀, w₁, w₂. Perceptron Architecture (Figure 10-5, page 286): A Perceptron is typically a single layer of TLUs. Each TLU in this layer is connected to all inputs. This is a fully connected layer (or dense layer). Inputs are fed through special \u0026ldquo;passthrough\u0026rdquo; input neurons. A bias neuron (always outputting 1) is usually added and connected to each TLU, providing the bias term w₀ (or b in the new notation used later). The Perceptron in Figure 10-5 has 2 inputs, 1 bias neuron, and 3 output TLUs. This can classify instances into three different binary classes simultaneously (making it a multioutput classifier). For example, output 1 could be \u0026ldquo;is it a cat?\u0026rdquo;, output 2 \u0026ldquo;is it a dog?\u0026rdquo;, output 3 \u0026ldquo;is it a bird?\u0026rdquo;. An input could be classified as a cat AND a bird if both TLUs fire (though that specific example isn\u0026rsquo;t ideal for mutually exclusive animal classes). Computing Outputs for a Layer (Equation 10-2, page 286): For a whole layer of artificial neurons, for several instances at once: h_W,b(X) = φ(XW + b)\nX: Matrix of input features (instances x features). W: Weight matrix (input neurons x artificial neurons in the layer). Contains connection weights excluding bias. b: Bias vector (one bias term per artificial neuron in the layer). XW + b: Computes the weighted sum z for every neuron and every instance. φ (phi): The activation function. For TLUs, this is a step function. What this equation is ultimately trying to achieve: Efficiently calculate the output of every neuron in a layer for every instance in a batch of data, using matrix multiplication. Perceptron Training (Hebbian Learning \u0026amp; Perceptron Learning Rule - Page 286):\nInspired by Hebb\u0026rsquo;s Rule (\u0026ldquo;Cells that fire together, wire together\u0026rdquo;): When neuron A often triggers neuron B, the connection between them strengthens. Perceptrons use a variant: The Perceptron learning rule reinforces connections that help reduce the error. Process: Feed one training instance at a time. For each instance, make predictions. For every output neuron that produced a wrong prediction, reinforce the connection weights from the inputs that would have contributed to the correct prediction. Equation 10-3 (Weight Update Rule): wᵢⱼ⁽ⁿᵉˣᵗ ˢᵗᵉᵖ⁾ = wᵢⱼ + η(yⱼ - ŷⱼ)xᵢ wᵢⱼ: Weight between i-th input and j-th output neuron. η (eta): Learning rate. yⱼ: Target output for j-th neuron. ŷⱼ: Predicted output for j-th neuron. xᵢ: Value of i-th input for the current instance. What this rule is ultimately trying to achieve: If ŷⱼ is correct (yⱼ - ŷⱼ = 0), weights don\u0026rsquo;t change. If ŷⱼ is wrong: If yⱼ=1 and ŷⱼ=0 (neuron should have fired but didn\u0026rsquo;t): yⱼ - ŷⱼ = 1. Weights wᵢⱼ are increased if xᵢ was positive (strengthening connections that should have contributed to firing). If yⱼ=0 and ŷⱼ=1 (neuron fired but shouldn\u0026rsquo;t have): yⱼ - ŷⱼ = -1. Weights wᵢⱼ are decreased if xᵢ was positive (weakening connections that wrongly contributed to firing). Perceptron Convergence Theorem (Page 287): If training instances are linearly separable, Rosenblatt showed this algorithm would converge to a solution (a set of weights that separates the classes).\nScikit-Learn Perceptron class: Implements a single-TLU network. from sklearn.linear_model import Perceptron per_clf = Perceptron() per_clf.fit(X, y)\nThe book notes this is equivalent to SGDClassifier(loss=\u0026quot;perceptron\u0026quot;, learning_rate=\u0026quot;constant\u0026quot;, eta0=1, penalty=None). Unlike Logistic Regression, Perceptrons output hard predictions (0 or 1), not probabilities. This is one reason to prefer Logistic Regression. Limitations of Perceptrons (Minsky \u0026amp; Papert, 1969 - Page 288):\nHighlighted serious weaknesses, famously that Perceptrons (being linear classifiers) cannot solve some trivial problems like the Exclusive OR (XOR) problem (Figure 10-6, left). XOR is not linearly separable. This disappointment led to another decline in ANN research (part of the first AI winter). (Page 288-293: The Multilayer Perceptron (MLP) and Backpropagation) Overcoming Perceptron Limitations: Stacking Perceptrons (Page 288):\nLimitations can be overcome by stacking multiple layers of Perceptrons. The resulting ANN is a Multilayer Perceptron (MLP). Figure 10-6 (right) shows an MLP that can solve the XOR problem. It uses an intermediate \u0026ldquo;hidden\u0026rdquo; layer of neurons. What the MLP is ultimately trying to achieve: By having hidden layers, MLPs can learn more complex, non-linear decision boundaries. The hidden layers can transform the input features into a new representation where the problem becomes linearly separable for the output layer. MLP Architecture (Figure 10-7, page 289): One (passthrough) input layer. One or more layers of TLUs, called hidden layers. One final layer of TLUs called the output layer. Layers near input are \u0026ldquo;lower layers\u0026rdquo;; layers near output are \u0026ldquo;upper layers.\u0026rdquo; Every layer (except output) usually includes a bias neuron and is fully connected to the next layer. Signal flows one way (input -\u0026gt; output): Feedforward Neural Network (FNN). Deep Neural Network (DNN): An ANN with a \u0026ldquo;deep\u0026rdquo; stack of hidden layers (definition of \u0026ldquo;deep\u0026rdquo; is fuzzy but generally means more than one or two these days). Deep Learning studies DNNs. Training MLPs: The Backpropagation Algorithm (Page 289-290): For many years, training MLPs was a major challenge. In 1986, Rumelhart, Hinton, and Williams published the backpropagation training algorithm, still fundamental today.\nWhat it is: Essentially, it\u0026rsquo;s Gradient Descent (Chapter 4) applied to an MLP, using an efficient technique to compute all the necessary gradients. How it computes gradients (Autodiff - sidebar, page 290): It uses reverse-mode automatic differentiation (autodiff). In just two passes through the network (one forward, one backward), it can compute the gradient of the network\u0026rsquo;s error with respect to every single model parameter (all weights and biases in all layers). This tells us how each weight/bias should be tweaked to reduce the error. Once gradients are computed, it performs a regular Gradient Descent step. Repeat until convergence. Backpropagation Algorithm in More Detail (Page 290):\nMini-batch Processing: Handles one mini-batch of training instances at a time. Goes through the full training set multiple times; each full pass is an epoch. Forward Pass: Pass the mini-batch to the input layer. Compute outputs of neurons in the first hidden layer. Pass these outputs to the next layer, compute its outputs, and so on, until the output layer. This is like making predictions, but all intermediate results (activations of all neurons) are preserved because they are needed for the backward pass. Measure Error: Use a loss function (e.g., MSE for regression, Cross-Entropy for classification) to compare the network\u0026rsquo;s output with the desired output (true labels). Backward Pass (Propagating Error Gradients): Compute how much each output connection contributed to the error (using the chain rule of calculus). Propagate these error contributions backward: Measure how much connections in the layer below contributed to the output layer\u0026rsquo;s error contributions (again, using chain rule). Continue backward until the input layer is reached. This reverse pass efficiently measures the error gradient across all connection weights. Gradient Descent Step: Tweak all connection weights in the network using the computed error gradients to reduce the overall error. Key Change for Backpropagation: Activation Functions (Page 291):\nThe original Perceptron used a step function. Step functions have flat segments (zero gradient), so Gradient Descent gets stuck. Crucial innovation for MLPs: Replace the step function with a differentiable activation function, like the logistic (sigmoid) function σ(z) = 1 / (1 + exp(-z)). It has a well-defined, non-zero derivative everywhere, allowing GD to make progress. Other Popular Activation Functions (Figure 10-8, page 292): Hyperbolic Tangent (tanh): tanh(z) = 2σ(2z) - 1. S-shaped, continuous, differentiable. Output range: -1 to 1 (vs. 0 to 1 for sigmoid). Centering output around 0 often helps speed up convergence at start of training. Rectified Linear Unit (ReLU): ReLU(z) = max(0, z). Outputs 0 if z \u0026lt; 0, outputs z if z ≥ 0. Continuous, but not differentiable at z=0 (slope changes abruptly). In practice, GD still works (can use a subgradient or just assume gradient is 0 or 1 at z=0). Derivative is 0 for z \u0026lt; 0. Advantages: Fast to compute. Has become the default in many cases. No maximum output value (helps with some GD issues like vanishing gradients, Ch 11). The footnote on page 292 notes that ReLU, despite being less biologically plausible than sigmoids, often works better in ANNs – a case where the biological analogy can be misleading. Why Activation Functions? (Page 292):\nIf you chain several linear transformations, all you get is another linear transformation. f(x) = 2x + 3, g(x) = 5x - 1 =\u0026gt; f(g(x)) = 2(5x - 1) + 3 = 10x + 1 (still linear). If you have no nonlinearity between layers, even a deep stack of layers is equivalent to a single linear layer. It can\u0026rsquo;t solve complex non-linear problems. Nonlinear activation functions are essential for giving ANNs the power to approximate complex, non-linear functions. A large enough DNN with nonlinear activations can theoretically approximate any continuous function. (Page 292-294: Regression and Classification MLPs) Now that we know the architecture and training algorithm (backpropagation), what can we do?\nRegression MLPs (Page 292-293):\nSingle value prediction (e.g., house price): Need a single output neuron. Multivariate regression (e.g., 2D coordinates for object center): One output neuron per output dimension. (4 output neurons if predicting bounding box: x, y, width, height). Output Layer Activation: Usually no activation function for output neurons in regression (so they can output any range of values). If output must be positive: Use ReLU or softplus (log(1 + exp(z)), a smooth ReLU variant). If output must be in a specific range: Use logistic (for 0-1) or tanh (for -1 to 1) and scale labels accordingly. Loss Function: Typically MSE. If many outliers: Prefer MAE or Huber loss (quadratic for small errors, linear for large errors – less sensitive to outliers than MSE but converges faster than MAE). Table 10-1 (Typical Regression MLP Architecture): Summarizes typical choices for number of neurons, layers, activations, and loss. Classification MLPs (Page 294):\nBinary classification: Single output neuron, logistic (sigmoid) activation function. Output is probability of positive class. Multilabel binary classification (e.g., email is spam/ham AND urgent/non-urgent): One output neuron per positive class label (e.g., one for \u0026ldquo;is spam,\u0026rdquo; one for \u0026ldquo;is urgent\u0026rdquo;). Each uses logistic activation. Output probabilities don\u0026rsquo;t necessarily sum to 1 (an email can be \u0026ldquo;not spam\u0026rdquo; and \u0026ldquo;urgent\u0026rdquo;). Multiclass classification (mutually exclusive classes, e.g., digits 0-9): One output neuron per class. Use softmax activation function for the whole output layer (as in Chapter 4). This ensures probabilities are between 0-1 and sum to 1. Figure 10-9 shows a modern MLP for classification (ReLU in hidden layers, softmax in output). Loss Function: Cross-entropy loss (log loss, as in Chapter 4) is generally a good choice when predicting probability distributions. Table 10-2 (Typical Classification MLP Architecture): Summarizes typical choices. Phew! That\u0026rsquo;s a dense introduction to the historical context, the biological inspiration (and divergence from it), the basic Perceptron, the jump to Multilayer Perceptrons, the crucial backpropagation algorithm, and how MLPs are structured for regression and classification.\nThe key takeaway is that MLPs are layered networks of simple processing units (neurons), where hidden layers learn increasingly complex representations of the input, enabled by non-linear activation functions and trained by backpropagation (Gradient Descent with efficient gradient calculation).\nExcellent! That detour through \u0026ldquo;The Matrix Calculus You Need For Deep Learning\u0026rdquo; was intense but hopefully gave you a much deeper appreciation for what\u0026rsquo;s happening when we say a neural network \u0026ldquo;learns\u0026rdquo; by minimizing a loss function using gradients. You now have a good intuitive (and even some mathematical) backing for how those weight and bias updates are calculated for individual neurons via the chain rule.\n(Page 295-306: Implementing MLPs with Keras) This is where the practical fun begins! We\u0026rsquo;ve talked a lot about the \u0026ldquo;what\u0026rdquo; and \u0026ldquo;why\u0026rdquo; of neural networks; now we get to the \u0026ldquo;how\u0026rdquo; of actually building and training them using a popular library.\nKeras: A High-Level Deep Learning API (Page 295):\nKeras allows you to easily build, train, evaluate, and run all sorts of neural networks. It was developed by François Chollet and is known for its ease of use, flexibility, and beautiful design. Backend Reliance: Keras itself doesn\u0026rsquo;t do the heavy numerical computations. It relies on a computation backend. Popular choices include: TensorFlow Microsoft Cognitive Toolkit (CNTK) Theano (though its development has largely ceased) The book refers to the original, multi-backend implementation as multibackend Keras. tf.keras: Since late 2016/2017, TensorFlow has bundled its own Keras implementation called tf.keras. This is what the book (and most of the community now) uses. It only supports TensorFlow as a backend but offers extra TensorFlow-specific features (like the Data API for efficient data loading, which we\u0026rsquo;ll see later). Figure 10-10 (page 296) illustrates these two Keras API implementations. PyTorch (Page 296):\nAnother very popular Deep Learning library from Facebook. Its API is quite similar to Keras (both inspired by Scikit-Learn and Chainer). Gained immense popularity due to its simplicity and excellent documentation, especially compared to TensorFlow 1.x. TensorFlow 2.x (which uses tf.keras as its official high-level API) has significantly improved, making it just as simple as PyTorch in many respects. Healthy competition is good! Installing TensorFlow 2 (Page 296):\nThe book assumes you\u0026rsquo;ve followed Chapter 2\u0026rsquo;s setup for Jupyter and Scikit-Learn. You\u0026rsquo;d typically use pip install -U tensorflow. The bird icon notes that for GPU support, you might need tensorflow-gpu and extra libraries (though this is evolving, and TensorFlow aims for a single library). Chapter 19 will cover GPUs. Test installation by importing tensorflow as tf and from tensorflow import keras, then printing tf.__version__ and keras.__version__. Now, let\u0026rsquo;s build an image classifier!\nBuilding an Image Classifier Using the Sequential API (Page 297-301)\nWe\u0026rsquo;ll use the Fashion MNIST dataset.\nA drop-in replacement for MNIST (introduced in Chapter 3).\nSame format: 70,000 grayscale images of 28x28 pixels, 10 classes.\nImages are fashion items (T-shirt, trouser, coat, etc.) instead of handwritten digits.\nMore challenging than MNIST (e.g., a simple linear model gets ~92% on MNIST but only ~83% on Fashion MNIST).\nUsing Keras to Load the Dataset (Page 297): fashion_mnist = keras.datasets.fashion_mnist (X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\nDifference from Scikit-Learn\u0026rsquo;s fetch_openml for MNIST: Images are 28x28 arrays (not flattened 784-element vectors). Pixel intensities are integers (0-255), not floats. X_train_full.shape is (60000, 28, 28). Data Preparation (Page 298):\nCreate a validation set: The loaded data is split into train and test only. We need a validation set for monitoring training and hyperparameter tuning. X_valid, X_train = X_train_full[:5000], X_train_full[5000:] y_valid, y_train = y_train_full[:5000], y_train_full[5000:] (First 5000 instances for validation, rest for training). Scale input features: Neural networks with Gradient Descent require feature scaling. We\u0026rsquo;ll scale pixel intensities from 0-255 down to the 0-1 range by dividing by 255.0 (this also converts them to floats). X_valid, X_train = X_valid / 255.0, X_train / 255.0 (Note: It\u0026rsquo;s generally better to scale the test set using parameters derived from the training set, e.g., (X_test - X_train_mean) / X_train_std. But for pixel values 0-255, dividing by 255.0 is a common and simple approach.) Class Names: For Fashion MNIST, labels are numbers (0-9). We need a list of class names to interpret them: class_names = [\u0026quot;T-shirt/top\u0026quot;, \u0026quot;Trouser\u0026quot;, ..., \u0026quot;Ankle boot\u0026quot;] class_names[y_train[0]] might give 'Coat'. Figure 10-11 shows sample images from Fashion MNIST. Creating the Model Using the Sequential API (Page 299): This is the simplest way to build a Keras model: a linear stack of layers. We\u0026rsquo;ll build a classification MLP with two hidden layers.\nmodel = keras.models.Sequential() model.add(keras.layers.Flatten(input_shape=[28, 28])) model.add(keras.layers.Dense(300, activation=\u0026quot;relu\u0026quot;)) model.add(keras.layers.Dense(100, activation=\u0026quot;relu\u0026quot;)) model.add(keras.layers.Dense(10, activation=\u0026quot;softmax\u0026quot;))\nLet\u0026rsquo;s break this down:\nmodel = keras.models.Sequential(): Creates a Sequential model, which is just a stack of layers. model.add(keras.layers.Flatten(input_shape=[28, 28])): This is the first layer. Its role is to take each input image (28x28 array) and flatten it into a 1D array (of 784 pixels). X.reshape(-1, 1) was mentioned, but a common operation in NNs is X.reshape(batch_size, -1). Keras Flatten layer handles this conversion. It has no parameters to learn; it\u0026rsquo;s just a preprocessing step. input_shape=[28, 28]: Since it\u0026rsquo;s the first layer, you must specify the shape of the input instances (excluding the batch size). model.add(keras.layers.Dense(300, activation=\u0026quot;relu\u0026quot;)): Adds a Dense (fully connected) hidden layer with 300 neurons. activation=\u0026quot;relu\u0026quot;: Specifies the ReLU activation function for these neurons. Each Dense layer manages its own weight matrix (W) and bias vector (b). When it receives input, it computes XW + b (Equation 10-2 from the book). model.add(keras.layers.Dense(100, activation=\u0026quot;relu\u0026quot;)): Adds a second Dense hidden layer with 100 neurons, also using ReLU. model.add(keras.layers.Dense(10, activation=\u0026quot;softmax\u0026quot;)): Adds a Dense output layer with 10 neurons (one for each class, 0-9). activation=\u0026quot;softmax\u0026quot;: Uses the softmax activation function because the classes are exclusive (an item belongs to only one class). Softmax will ensure the outputs are probabilities that sum to 1. Alternative Sequential Model Creation (Page 300): You can also pass a list of layers directly to the Sequential constructor: model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=\u0026quot;relu\u0026quot;), keras.layers.Dense(100, activation=\u0026quot;relu\u0026quot;), keras.layers.Dense(10, activation=\u0026quot;softmax\u0026quot;) ]) Model Summary (Page 300-301): model.summary() displays all the model\u0026rsquo;s layers:\nLayer name (auto-generated or custom). Output shape (None for batch size means it can be anything). Number of parameters. Flatten: Output shape (None, 784), 0 params. dense (first hidden layer): Output (None, 300). Params: (784 inputs * 300 neurons) + 300 biases = 235,200 + 300 = 235,500. dense_1 (second hidden): Output (None, 100). Params: (300 inputs * 100 neurons) + 100 biases = 30,000 + 100 = 30,100. dense_2 (output): Output (None, 10). Params: (100 inputs * 10 neurons) + 10 biases = 1,000 + 10 = 1,010. Total params: 266,610. All are trainable. This gives the model a lot of flexibility but also risks overfitting if data is scarce. Accessing Layers and Weights (Page 301):\nmodel.layers gives a list of layers. hidden1 = model.layers[1] hidden1.name model.get_layer('dense') (if name is \u0026lsquo;dense\u0026rsquo;) weights, biases = hidden1.get_weights() gets the layer\u0026rsquo;s parameters. Weights are initialized randomly (to break symmetry for backpropagation). Biases are initialized to zeros (which is fine). You can set custom initializers for weights (kernel_initializer) or biases (bias_initializer) when creating the layer. (More in Ch 11). When input_shape is Determined (Bird Icon, page 302): It\u0026rsquo;s best to specify input_shape for the first layer. If you don\u0026rsquo;t, Keras waits until it sees actual data (e.g., during fit()) or until you call model.build() to build the layers (i.e., create their weights). Before that, layers won\u0026rsquo;t have weights, and model.summary() or saving the model might not work.\n(Page 302-306: Compiling, Training, Evaluating, and Predicting) Compiling the Model (Page 302): After creating the model, you must call compile() to specify:\nLoss function Optimizer Optionally, extra metrics to compute during training/evaluation. model.compile(loss=\u0026quot;sparse_categorical_crossentropy\u0026quot;, optimizer=\u0026quot;sgd\u0026quot;, metrics=[\u0026quot;accuracy\u0026quot;])\nloss=\u0026quot;sparse_categorical_crossentropy\u0026quot;: We use this because our labels (y_train) are \u0026ldquo;sparse\u0026rdquo; – just target class indices (0 to 9). And the classes are exclusive. If labels were one-hot encoded (e.g., class 3 is [0,0,0,1,0,0,0,0,0,0]), we\u0026rsquo;d use loss=\u0026quot;categorical_crossentropy\u0026quot;. If binary classification (output layer with sigmoid), we\u0026rsquo;d use loss=\u0026quot;binary_crossentropy\u0026quot;. The bird icon (page 302) notes you can use full Keras objects too: loss=keras.losses.sparse_categorical_crossentropy. optimizer=\u0026quot;sgd\u0026quot;: This means use simple Stochastic Gradient Descent. Keras will perform backpropagation (reverse-mode autodiff + Gradient Descent). More advanced optimizers in Chapter 11. Important (bird icon, page 303): For SGD, tuning the learning rate is crucial. You\u0026rsquo;d typically use optimizer=keras.optimizers.SGD(learning_rate=...) instead of the string \u0026quot;sgd\u0026quot; (which defaults to lr=0.01). metrics=[\u0026quot;accuracy\u0026quot;]: Since it\u0026rsquo;s a classifier, we want to track accuracy during training and evaluation. Training and Evaluating the Model (Page 303-304): Call fit(): history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\nPass input features (X_train) and target classes (y_train). epochs=30: Number of times to iterate over the entire training dataset. (Defaults to 1, which is usually not enough). validation_data=(X_valid, y_valid): Optional. Keras will measure loss and metrics on this validation set at the end of each epoch. Very useful to see how well the model is generalizing and to detect overfitting. Output during training: For each epoch, Keras displays: Progress bar. Mean training time per sample. Loss and accuracy on the training set (average over the epoch). Loss and accuracy on the validation set (at the end of the epoch). The example output shows training loss decreasing and validation accuracy reaching ~89% after 30 epochs. Training and validation accuracy are close, so not much overfitting. The bird icon (page 304) mentions validation_split=0.1 as an alternative to validation_data, to use the last 10% of training data for validation (before shuffling). Also mentions class_weight (to give more importance to underrepresented classes) and sample_weight (for per-instance weighting) arguments in fit(). Learning Curves (Page 304-305):\nfit() returns a History object. history.history is a dictionary containing the loss and metrics measured at the end of each epoch (e.g., loss, accuracy, val_loss, val_accuracy). You can use this to plot learning curves with pandas and Matplotlib (Figure 10-12, page 305). The plot shows training/validation accuracy increasing and loss decreasing. Validation curves are close to training curves, confirming little overfitting. The book notes that validation metrics are computed at the end of an epoch, while training metrics are a running mean during the epoch. So, for a fair comparison, the training curve should be shifted left by half an epoch. Further Training and Hyperparameter Tuning (Page 306):\nIf validation loss is still decreasing (as in the example), the model hasn\u0026rsquo;t fully converged. You can call fit() again; Keras continues training from where it left off. If not satisfied, tune hyperparameters: Learning rate (most important first check). Try another optimizer (and retune learning rate). Number of layers, neurons per layer, activation functions. Batch size (in fit(), defaults to 32). Evaluating on the Test Set (Page 306):\nOnce satisfied with validation accuracy, evaluate on the test set to estimate generalization error. model.evaluate(X_test, y_test) Returns [loss, accuracy]. Example gives ~88.5% accuracy. Common to get slightly lower performance on test set than validation (since HPs tuned on validation). Resist tweaking HPs based on test set results! Using the Model to Make Predictions (Page 206-207): predict() method for new instances. X_new = X_test[:3] y_proba = model.predict(X_new)\ny_proba contains one probability per class for each instance (from the softmax output layer). Example: [[0. , ..., 0.03, ..., 0.96], ...] for the first image (96% prob for class 9 \u0026lsquo;Ankle boot\u0026rsquo;, 3% for class 5 \u0026lsquo;Sandal\u0026rsquo;). To get the class with the highest probability: y_pred = model.predict_classes(X_new) (Note: predict_classes is deprecated in newer TensorFlow/Keras; use np.argmax(model.predict(X_new), axis=-1) instead). This might give array([9, 2, 1]). Figure 10-13 (page 307) shows these three test images, which were correctly classified. That\u0026rsquo;s a complete walkthrough of building, training, and using a classification MLP with Keras\u0026rsquo;s Sequential API! The process is quite streamlined.\nGreat! It\u0026rsquo;s satisfying to see how those theoretical concepts translate into a working Keras model.\nLet\u0026rsquo;s continue with Chapter 10, moving on to how we\u0026rsquo;d build a regression MLP with Keras and then explore more advanced ways to define model architectures.\n(Page 307-308: Building a Regression MLP Using the Sequential API) We\u0026rsquo;ve just built an image classifier. Now, let\u0026rsquo;s switch to a regression task: predicting California housing prices, similar to what we did in Chapter 2, but this time with a neural network.\nLoading the Data:\nFor simplicity, the book uses Scikit-Learn\u0026rsquo;s fetch_california_housing() to load the data.\nThis version is simpler than the one in Chapter 2: only numerical features, no missing values.\nStandard train-validation-test split and feature scaling (using StandardScaler) are performed as usual. from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler\nhousing = fetch_california_housing() X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target) X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\nscaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_valid = scaler.transform(X_valid) X_test = scaler.transform(X_test)\nBuilding the Regression MLP (Page 308): The process is very similar to the classification MLP, with a few key differences:\nmodel = keras.models.Sequential([ keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;, input_shape=X_train.shape[1:]), keras.layers.Dense(1) # Output layer ])\nOutput Layer: Has a single neuron (because we\u0026rsquo;re predicting a single value – the housing price). Uses no activation function (or you could say a \u0026ldquo;linear\u0026rdquo; activation activation=None). This allows the output neuron to produce any range of values, which is what we want for regression. (Recall for classification we used softmax or sigmoid). Hidden Layer: The example uses a single hidden layer with 30 neurons and ReLU activation. The book mentions using fewer neurons and a shallower network because the dataset is quite noisy, to avoid overfitting. input_shape=X_train.shape[1:] correctly sets the input dimension based on the number of features in X_train. What this architecture is ultimately trying to achieve: The hidden layer learns complex combinations of the input features, and the final output neuron combines these learned features linearly to produce the price prediction. Compiling the Model: model.compile(loss=\u0026quot;mean_squared_error\u0026quot;, optimizer=\u0026quot;sgd\u0026quot;)\nLoss Function: loss=\u0026quot;mean_squared_error\u0026quot; (or keras.losses.mean_squared_error). This is standard for regression. Optimizer: \u0026quot;sgd\u0026quot; (again, you\u0026rsquo;d likely want to specify keras.optimizers.SGD(learning_rate=...) for tuning). Metrics: For regression, common metrics might be MAE (Mean Absolute Error) if MSE is the loss, or just watching the loss itself. Accuracy isn\u0026rsquo;t used for regression. Training, Evaluating, Predicting: These steps are identical to the classification MLP: history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid)) mse_test = model.evaluate(X_test, y_test) y_pred = model.predict(X_new)\nThe core workflow with the Sequential API is consistent. The main changes for regression are the structure of the output layer (number of neurons, no activation) and the choice of loss function (MSE).\n(Page 308-312: Building Complex Models Using the Functional API) The Sequential API is easy to use for simple stacks of layers. However, sometimes you need to build neural networks with more complex topologies:\nMultiple inputs. Multiple outputs. Layers that branch off and then merge back. For these, Keras offers the Functional API.\nWide \u0026amp; Deep Neural Network (Example Architecture - Page 308, Figure 10-14):\nIntroduced in a 2016 paper by Google for recommender systems. The Idea: Combine the strengths of deep learning (learning complex patterns through a \u0026ldquo;deep path\u0026rdquo; of stacked layers) with the ability to learn simple rules (through a \u0026ldquo;wide path\u0026rdquo; where inputs connect directly, or via a shallow path, to the output). What it\u0026rsquo;s ultimately trying to achieve: Prevent simple, easily learnable patterns in the data from being distorted or lost by forcing them through many layers of transformations. It allows the network to memorize simple rules while also discovering intricate patterns. Building a Wide \u0026amp; Deep Network with the Functional API (Page 309): Let\u0026rsquo;s tackle the California housing problem with this architecture. input_ = keras.layers.Input(shape=X_train.shape[1:]) hidden1 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(input_) hidden2 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(hidden1) concat = keras.layers.Concatenate()([input_, hidden2]) output = keras.layers.Dense(1)(concat) model = keras.Model(inputs=[input_], outputs=[output])\nLet\u0026rsquo;s break this down step-by-step:\ninput_ = keras.layers.Input(shape=X_train.shape[1:]): This creates an Input object (a symbolic tensor). It defines the shape and dtype of the input the model will receive. This is like declaring the entry point for your data. hidden1 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(input_): We create a Dense layer. Then, we call it like a function, passing it the input_ object. This connects input_ to hidden1. hidden1 now represents the symbolic output of this layer. This \u0026ldquo;calling a layer on a tensor\u0026rdquo; is the essence of the Functional API. You are defining how layers connect. No actual data is processed yet. hidden2 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(hidden1): Create another Dense layer and connect it to the output of hidden1. concat = keras.layers.Concatenate()([input_, hidden2]): Create a Concatenate layer. Call it with a list of tensors you want to concatenate: the original input_ (this is the \u0026ldquo;wide\u0026rdquo; path) and the output of hidden2 (the \u0026ldquo;deep\u0026rdquo; path). concat is now the symbolic concatenated tensor. output = keras.layers.Dense(1)(concat): Create the output Dense layer (single neuron, no activation for regression) and connect it to the concat layer. model = keras.Model(inputs=[input_], outputs=[output]): Finally, create the Model object by specifying its inputs and outputs. Keras then figures out the graph of layers. Once this model is built, compile(), fit(), evaluate(), and predict() work exactly the same as with the Sequential API.\nHandling Multiple Inputs (Figure 10-15, page 310): What if you want to send different subsets of features through the wide and deep paths?\nDefine multiple Input objects: input_A = keras.layers.Input(shape=[5], name=\u0026quot;wide_input\u0026quot;) (e.g., features 0-4) input_B = keras.layers.Input(shape=[6], name=\u0026quot;deep_input\u0026quot;) (e.g., features 2-7, notice overlap is possible)\nBuild the paths: hidden1 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(input_B) hidden2 = keras.layers.Dense(30, activation=\u0026quot;relu\u0026quot;)(hidden1)\nConcatenate: concat = keras.layers.concatenate([input_A, hidden2]) (using the functional form concatenate() which creates and calls the layer in one step).\nOutput: output = keras.layers.Dense(1, name=\u0026quot;output\u0026quot;)(concat)\nCreate the model, specifying multiple inputs: model = keras.Model(inputs=[input_A, input_B], outputs=[output])\nTraining with Multiple Inputs (Page 311): When calling fit(), evaluate(), or predict(), you must pass data for each input.\nIf inputs are ordered in the inputs list of keras.Model, you pass a tuple/list of NumPy arrays: model.fit((X_train_A, X_train_B), y_train, ...) Where X_train_A would be X_train[:, :5] and X_train_B would be X_train[:, 2:]. Alternatively (and often better if many inputs), you can pass a dictionary mapping input names (defined in keras.layers.Input(name=...)) to the data arrays: model.fit({\u0026quot;wide_input\u0026quot;: X_train_A, \u0026quot;deep_input\u0026quot;: X_train_B}, y_train, ...) Handling Multiple Outputs (Figure 10-16, page 312): Sometimes a task demands multiple outputs, or it\u0026rsquo;s useful for regularization.\nExample: Adding an auxiliary output deeper in the network (e.g., from hidden2). This can encourage the main network to learn useful features earlier on, as this auxiliary output also contributes to the loss. [...] # Up to hidden2 output = keras.layers.Dense(1, name=\u0026quot;main_output\u0026quot;)(concat) aux_output = keras.layers.Dense(1, name=\u0026quot;aux_output\u0026quot;)(hidden2) model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output]) Compiling with Multiple Outputs/Losses (Page 312): Each output needs its own loss function. You can pass a list of losses: model.compile(loss=[\u0026quot;mse\u0026quot;, \u0026quot;mse\u0026quot;], optimizer=\u0026quot;sgd\u0026quot;, ...) You can also specify loss_weights to give different importance to each loss: model.compile(loss=[\u0026quot;mse\u0026quot;, \u0026quot;mse\u0026quot;], loss_weights=[0.9, 0.1], optimizer=\u0026quot;sgd\u0026quot;) Here, the main output\u0026rsquo;s loss contributes 90% to the total loss, and the auxiliary output\u0026rsquo;s loss contributes 10%. (You can also pass dictionaries for loss and loss_weights mapping output names to values). Training with Multiple Outputs/Labels: You need to provide labels for each output during fit(): model.fit([X_train_A, X_train_B], [y_train, y_train], ...) (If the main and aux outputs predict the same thing, you can pass y_train twice). Evaluating with Multiple Outputs: model.evaluate(...) will return the total loss, as well as the individual losses for each output. Predicting with Multiple Outputs: model.predict(...) will return predictions for each output. The Functional API is very powerful for creating custom, non-sequential network architectures!\nExcellent! Let\u0026rsquo;s proceed with that plan.\nWe\u0026rsquo;ve covered the foundational concepts of Artificial Neural Networks, including their structure (Perceptrons, MLPs), how they learn (the idea of backpropagation and gradient descent), and how they are designed for regression and classification.\n(Page 320-327: Fine-Tuning Neural Network Hyperparameters) This section is critical because, as the book states, \u0026ldquo;The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters to tweak.\u0026rdquo;\nWhat we are ultimately trying to achieve with hyperparameter tuning: We want to find the specific configuration of the network\u0026rsquo;s architecture and training process that allows it to learn the underlying patterns in our data effectively and generalize well to new, unseen data, without overfitting or underfitting. Even in a simple MLP, you can change:\nNumber of hidden layers. Number of neurons per hidden layer. Type of activation function for each layer. Weight initialization logic. Optimizer and its specific parameters (like learning rate). Batch size. Regularization techniques and their strengths. And many more! How do you find the best combination? General Strategies for Hyperparameter Tuning (Page 320):\nTrial and Error (Guided by Best Practices): Try many combinations and see which performs best on a validation set (or using K-fold cross-validation). Automated Hyperparameter Optimization: Tools like Scikit-Learn\u0026rsquo;s GridSearchCV or RandomizedSearchCV can be used. To do this with Keras models, you need to wrap your Keras model in an object that mimics a Scikit-Learn regressor/classifier. The book shows how to create a build_model function that Keras-wrapping classes (like KerasRegressor or KerasClassifier from tf.keras.wrappers.scikit_learn or a similar older Keras utility) can use. The build_model function would take hyperparameters as arguments (e.g., n_hidden, n_neurons, learning_rate) and return a compiled Keras model. RandomizedSearchCV is often preferred over GridSearchCV when there are many hyperparameters, as it explores the space more efficiently. The book provides an example of setting up param_distribs for n_hidden, n_neurons, and learning_rate to use with RandomizedSearchCV. Challenges with Automated Search for NNs (Page 321-322): Training NNs can be slow, especially with large datasets or complex models. Exploring a large hyperparameter space can take many hours or days. Manual Assistance: You can guide the search: start with a wide random search, then do a finer search around the best values found. This is time-consuming. More Efficient Search Techniques: The core idea is that when a region of the hyperparameter space looks promising, it should be explored more. Libraries that help with this (beyond simple random search or grid search): Hyperopt: Optimizes over complex search spaces (real, discrete values). Hyperas, kopt, Talos: Based on Hyperopt, specifically for Keras. Keras Tuner: Easy-to-use library from Google for Keras models, with visualization. Scikit-Optimize (skopt): General-purpose, BayesSearchCV class uses Bayesian optimization. Spearmint: Bayesian optimization library. Hyperband: Fast tuning based on a novel bandit-based approach. Sklearn-Deap: Uses evolutionary algorithms. Many cloud providers (like Google Cloud AI Platform) also offer hyperparameter tuning services. Evolutionary Algorithms \u0026amp; AutoML (Page 323): Research is active in using evolutionary approaches not just for hyperparameters but also for finding the best network architecture itself (AutoML). Even training individual NNs with evolutionary algorithms instead of Gradient Descent is being explored (e.g., Uber\u0026rsquo;s Deep Neuroevolution). Guidelines for Choosing Key Hyperparameters (Page 323-327):\nEven with advanced tuning tools, having some intuition about reasonable starting values and search ranges is very helpful.\nNumber of Hidden Layers (Page 323-324):\nStart Simple: For many problems, you can begin with just one or two hidden layers and get reasonable results. An MLP with one hidden layer can theoretically model even very complex functions, if it has enough neurons. Parameter Efficiency of Deep Networks: For complex problems, deep networks (more layers) have much higher parameter efficiency than shallow ones. They can model complex functions using exponentially fewer neurons than a shallow net would need to achieve similar performance. This means they can often reach better performance with the same amount of training data. Hierarchical Structure of Real-World Data: Deep networks naturally take advantage of hierarchical structures in data. Lower hidden layers tend to learn low-level structures (e.g., edges, simple shapes in images). Intermediate hidden layers combine these to model intermediate-level structures (e.g., eyes, noses, squares, circles). Highest hidden layers and the output layer combine these to model high-level structures (e.g., faces, specific objects). This hierarchical learning helps DNNs converge faster and generalize better. Transfer Learning: This hierarchical nature enables transfer learning. If you\u0026rsquo;ve trained a network to recognize faces, you can reuse its lower layers (which learned general visual features) to kickstart training for a new, related task like recognizing hairstyles. The new network doesn\u0026rsquo;t have to learn low-level features from scratch. (More in Chapter 11). General Guideline: Start with 1-2 hidden layers. For more complex problems, gradually ramp up the number of hidden layers until you start overfitting the training set, then use regularization techniques (like early stopping, dropout, etc., which we\u0026rsquo;ll see more of). Very complex tasks (large image classification, speech recognition) might need dozens of layers (but often specialized architectures like CNNs, not fully connected MLPs). You\u0026rsquo;ll rarely train huge networks from scratch; usually, you\u0026rsquo;ll reuse parts of a pretrained state-of-the-art network (transfer learning). Number of Neurons per Hidden Layer (Page 324-325):\nInput/Output Layers: Determined by your task (number of input features, number of output classes/values). Hidden Layers: Old Practice (Pyramid): Fewer neurons in higher layers (e.g., 300 -\u0026gt; 200 -\u0026gt; 100). Rationale: many low-level features coalesce into fewer high-level features. Largely abandoned. Current Practice: Using the same number of neurons in all hidden layers often performs just as well or better, and it\u0026rsquo;s simpler (only one hyperparameter for neuron count per layer, instead of one per layer). Sometimes, making the first hidden layer larger than subsequent ones can be beneficial, depending on the dataset. \u0026ldquo;Stretch Pants\u0026rdquo; Approach (Vincent Vanhoucke): It\u0026rsquo;s often simpler and more efficient to pick a model with more layers and neurons than you actually need, and then use early stopping and other regularization techniques to prevent it from overfitting. This is like buying large stretch pants that shrink to the right size. This avoids creating \u0026ldquo;bottleneck\u0026rdquo; layers (layers with too few neurons) that might lose important information from the inputs. Once information is lost by a bottleneck, subsequent larger layers cannot recover it. More Bang for Your Buck (Scorpion Icon, page 325): In general, you\u0026rsquo;ll get better performance improvements by increasing the number of layers rather than just the number of neurons in a single layer. Learning Rate (Page 325):\nArguably the most important hyperparameter. Optimal Learning Rate: Often about half of the maximum learning rate (the rate above which training diverges). Finding a Good Learning Rate: Train the model for a few hundred iterations. Start with a very low learning rate (e.g., 10⁻⁵). Gradually increase it exponentially up to a very large value (e.g., 10 or 100). Plot the loss as a function of the learning rate (use a log scale for the learning rate axis). The loss will typically drop, then flatten, then shoot back up. The optimal learning rate is usually a bit lower (e.g., 10 times lower) than the point where the loss starts to climb. (More techniques for learning rate schedules in Chapter 11). Optimizer (Page 326):\nChoosing a better optimizer than plain Mini-batch Gradient Descent (and tuning its specific hyperparameters) is also very important. (Advanced optimizers like Adam, RMSprop, etc., will be covered in Chapter 11). Batch Size (Page 326):\nCan significantly impact model performance and training time. Large Batch Sizes: Benefit: Hardware accelerators (GPUs) can process them efficiently, leading to more instances seen per second. Catch: Often lead to training instabilities (especially at the start), and the resulting model may not generalize as well as one trained with a small batch size. Small Batch Sizes (e.g., 2 to 32): A 2018 paper (Masters and Luschi) concluded these were often preferable, leading to better models in less training time. Yann LeCun famously tweeted \u0026ldquo;Friends don\u0026rsquo;t let friends use mini-batches larger than 32.\u0026rdquo; The Counter-Argument (Very Large Batches): Other research (Hoffer et al. 2017, Goyal et al. 2017) showed it\u0026rsquo;s possible to use very large batch sizes (up to 8,192) effectively with techniques like learning rate warmup (start with a small learning rate, then ramp it up). This can lead to very short training times without a generalization gap. Practical Strategy: Try using a large batch size with learning rate warmup. If training is unstable or final performance is disappointing, then try using a small batch size. Activation Function (Page 326):\nHidden Layers: In general, ReLU is a good default. (SELU and other variants will be discussed in Chapter 11). Output Layer: Depends entirely on your task: Regression (unbounded): None (linear). Regression (positive output): ReLU or softplus. Regression (output in a specific range, e.g., 0-1): Logistic (sigmoid) or tanh (for -1 to 1), and scale labels accordingly. Binary Classification: Logistic (sigmoid). Multiclass Classification (exclusive classes): Softmax. Number of Iterations (Epochs) (Page 327):\nIn most cases, this doesn\u0026rsquo;t actually need to be tweaked directly. Just use early stopping instead. Set a large number of epochs and let early stopping find the optimal point. Important Note on Interacting Hyperparameters (Bird Icon, page 327): The optimal learning rate often depends on other hyperparameters, especially the batch size. If you modify any hyperparameter, make sure to re-evaluate/retune the learning rate.\nFurther Reading (Page 327): The book recommends an excellent 2018 paper by Leslie Smith, \u0026ldquo;A Disciplined Approach to Neural Network Hyper-Parameters,\u0026rdquo; for more best practices.\nConclusion of Chapter 10: This overview of hyperparameter tuning gives you a strategic approach to building effective neural networks. It\u0026rsquo;s less about finding one \u0026ldquo;magic number\u0026rdquo; and more about understanding the trade-offs and having a systematic way to explore the possibilities, often starting simple and iteratively adding complexity or using smart search strategies and regularization.\nThis pretty much wraps up the core conceptual learning from Chapter 10. We\u0026rsquo;ve journeyed from the biological neuron to the idea of MLPs, backpropagation, how to design them for different tasks, and now, critically, how to approach the art and science of tuning them.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-10/","tags":null,"title":"Chapter 10: Introduction to Artificial Neural Networks"},{"categories":null,"content":"This chapter covers the most complex generative AI challenge: creating coherent videos from textual descriptions, involving temporal consistency and motion modeling.\nKey Concepts Temporal Consistency: Maintaining coherence across video frames Motion Modeling: Generating realistic movement and transitions Computational Scaling: Managing the extreme computational requirements Main Topics Covered Text-to-video system architecture Temporal modeling and frame consistency Motion prediction and interpolation Distributed training and inference strategies Video quality assessment and metrics System Design Considerations Handling massive computational and memory requirements Ensuring temporal consistency across long sequences Balancing video quality with generation time Managing storage and bandwidth for video outputs (Your detailed notes for Chapter 11 go here\u0026hellip;)\n","permalink":"https://deepskandpal.github.io/bookshelf/generative-ai-system-design-interview/chapter-11/","tags":null,"title":"Chapter 11: Text-to-Video Generation"},{"categories":null,"content":"Link to the conversation with LLM\nChapter 10 introduced ANNs, and we even notionally \u0026ldquo;trained\u0026rdquo; some (though they were relatively shallow, with just a few hidden layers). But what happens when you need to tackle really complex problems, like detecting hundreds of object types in high-resolution images?\nYou might need a much deeper DNN (10+ layers, hundreds of neurons per layer, hundreds of thousands or millions of connections). Training such a deep DNN is not a walk in the park. The chapter immediately highlights several common problems you could run into:\nVanishing or Exploding Gradients:\nThis is a critical issue. As gradients flow backward through the network during backpropagation (from the output layer to the input layer), they can either: Vanish: Get smaller and smaller, until the gradients for the lower layers (those closer to the input) are tiny. This means the weights of these lower layers barely get updated, and the network effectively stops learning in those early layers. Explode: Get larger and larger, until the weight updates for lower layers become insanely huge, causing the training algorithm to diverge. Both make lower layers very hard to train effectively. Not Enough Training Data:\nLarge, deep networks have many parameters (weights and biases). They often require vast amounts of training data to learn these parameters well and generalize without overfitting. Getting enough labeled data can be very costly and time-consuming. Extremely Slow Training:\nMore layers, more neurons, more connections, more data = much more computation per epoch. Training can take days, weeks, or even longer. Risk of Overfitting:\nA model with millions of parameters can easily memorize the training set (especially if the dataset isn\u0026rsquo;t proportionally huge or is noisy), leading to poor performance on new, unseen data. Chapter Goals: This chapter will go through each of these problems and present techniques to solve them:\nVanishing/Exploding Gradients: We\u0026rsquo;ll look at their causes and popular solutions (like better weight initialization, non-saturating activation functions, Batch Normalization, Gradient Clipping). Lack of Labeled Data / Complex Tasks: We\u0026rsquo;ll explore transfer learning and unsupervised pretraining. Slow Training: We\u0026rsquo;ll discuss various faster optimizers. Overfitting: We\u0026rsquo;ll cover popular regularization techniques for large neural networks. The Vanishing/Exploding Gradients Problems This is a fundamental challenge in training deep networks.\nThe Core Issue:\nBackpropagation involves propagating the error gradient backward from the output layer to the input layer. As these gradients are passed from layer to layer, they are multiplied by the weights of those layers (and the derivatives of activation functions). If weights (or activation function derivatives) are consistently small, the gradients shrink exponentially as they go backward -\u0026gt; Vanishing Gradients. The lower layers learn very slowly or not at all. If weights (or derivatives) are consistently large, the gradients grow exponentially -\u0026gt; Exploding Gradients. Training becomes unstable and diverges. More generally, deep networks suffer from unstable gradients, where different layers may learn at vastly different speeds. Historical Context:\nThis unstable gradient behavior was observed long ago and was a key reason deep NNs were largely abandoned in the early 2000s. The cause wasn\u0026rsquo;t entirely clear until a landmark 2010 paper by Xavier Glorot and Yoshua Bengio. Glorot and Bengio\u0026rsquo;s Findings (2010 Paper - Footnote 1): They identified key culprits:\nLogistic Sigmoid Activation Function: Figure 11-1 shows the logistic sigmoid. When its inputs become large (positive or negative), the function saturates at 0 or 1. In these saturated regions, the derivative of the sigmoid function is extremely close to 0. During backpropagation, when gradients are passed through a saturated sigmoid neuron, they get multiplied by this near-zero derivative, effectively getting squashed. If many layers have saturated sigmoids, the gradient can get diluted to almost nothing by the time it reaches the lower layers. Traditional Weight Initialization: At the time, weights were often initialized from a normal distribution with mean 0 and standard deviation 1. Variance Imbalance: Glorot and Bengio showed that with this combination (sigmoid activation + standard normal initialization), the variance of the outputs of each layer is much greater than the variance of its inputs. As the signal flows forward, the variance keeps increasing layer by layer. This pushes the inputs to the activation functions of the upper layers into their saturated regions (where derivatives are ~0). The problem is worsened because the logistic function has a mean of 0.5, not 0 (tanh, with mean 0, behaves slightly better). The Goal for Proper Signal Flow (Glorot \u0026amp; Bengio\u0026rsquo;s Insight - Page 333): For a signal (activations forward, gradients backward) to flow properly without dying out or exploding:\nThe variance of the outputs of each layer should be (roughly) equal to the variance of its inputs. The gradients should have (roughly) equal variance before and after flowing through a layer in the reverse direction. The analogy (footnote 2) is excellent: a chain of microphone amplifiers. Each needs to be set correctly so your voice comes out clearly at the end, with consistent amplitude through the chain. It\u0026rsquo;s hard to guarantee both conditions 1 \u0026amp; 2 simultaneously unless a layer has an equal number of inputs (fan-in) and outputs/neurons (fan-out).\nXavier/Glorot Initialization (Equation 11-1, Page 334): Glorot and Bengio proposed a practical compromise for weight initialization that works well:\nInitialize connection weights randomly. The distribution should have a mean of 0. The variance σ² should depend on fan_in and fan_out: σ² = 1 / fan_avg where fan_avg = (fan_in + fan_out) / 2. (For a uniform distribution between -r and r, r = sqrt(3 / fan_avg)). What this initialization is ultimately trying to achieve: It aims to keep the variance of activations and backpropagated gradients roughly constant across layers, preventing them from vanishing or exploding. It helps the signal propagate properly. This significantly speeds up training and was a key factor in the success of Deep Learning. LeCun Initialization (Page 334):\nAn earlier strategy by Yann LeCun (1990s). If you replace fan_avg with just fan_in in Equation 11-1, you get LeCun initialization. Equivalent to Glorot when fan_in = fan_out. He Initialization (Kaiming He et al., 2015 - Footnote 3, Page 334):\nGlorot initialization works well for sigmoid, tanh, and softmax. For ReLU and its variants (which became very popular), a different initialization is needed because ReLU behaves differently (it kills half the activations, which changes variance). He Initialization uses: σ² = 2 / fan_in (For a uniform distribution, r = sqrt(6 / fan_in)). Table 11-1 summarizes initialization strategies for different activation functions: Glorot: For None (linear output), tanh, logistic, softmax. Variance 1 / fan_avg. He: For ReLU and variants. Variance 2 / fan_in. LeCun: For SELU (we\u0026rsquo;ll see this soon). Variance 1 / fan_in. Keras Implementation (Page 334):\nKeras uses Glorot initialization with a uniform distribution by default for its Dense layers. You can change it using kernel_initializer: keras.layers.Dense(10, activation=\u0026quot;relu\u0026quot;, kernel_initializer=\u0026quot;he_normal\u0026quot;) keras.layers.Dense(10, activation=\u0026quot;relu\u0026quot;, kernel_initializer=\u0026quot;he_uniform\u0026quot;) For He initialization with uniform distribution based on fan_avg (instead of fan_in): he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform') keras.layers.Dense(..., kernel_initializer=he_avg_init) These initialization strategies are crucial first steps to combat unstable gradients. They are all trying to set the initial weights to a \u0026ldquo;sensible\u0026rdquo; scale so that the signal (activations and gradients) can propagate through many layers without becoming too small or too large too quickly.\nNonsaturating Activation Functions We\u0026rsquo;ve seen that a poor choice of activation function (like the traditional sigmoid) combined with older initialization methods was a major cause of unstable gradients. The Glorot \u0026amp; Bengio paper highlighted this.\nThe Problem with Saturating Activation Functions (like Sigmoid/Tanh):\nSaturation: Functions like sigmoid and tanh \u0026ldquo;saturate\u0026rdquo; – their output flattens out and approaches a fixed value (0 or 1 for sigmoid, -1 or 1 for tanh) when the input z becomes very large (positive or negative). Vanishing Gradients: In these saturated regions, the derivative of the activation function is extremely close to zero. (Look at Figure 11-1 again for sigmoid). Impact on Backpropagation: During backpropagation, the error gradient from the layer above gets multiplied by the local derivative of the activation function. If this derivative is tiny (due to saturation), the gradient being passed back is also tiny. Chain Reaction: If many layers have neurons operating in their saturated regions, this \u0026ldquo;gradient squashing\u0026rdquo; effect compounds as it goes backward through the network. The gradients reaching the early layers become vanishingly small, and those layers learn very slowly or not at all. The Solution: Nonsaturating Activation Functions\nThe insight was to use activation functions that don\u0026rsquo;t saturate as easily, especially for positive input values.\nReLU (Rectified Linear Unit) (Page 335):\nReLU(z) = max(0, z) Key Property: For positive values (z \u0026gt; 0), ReLU does not saturate. Its output is just z, and its derivative is 1. This means if a neuron is active (its input z \u0026gt; 0), the gradient can pass through it backward unchanged (multiplied by 1). This greatly helps prevent the vanishing gradient problem for positive activations. It\u0026rsquo;s also very fast to compute. The \u0026ldquo;Dying ReLUs\u0026rdquo; Problem: ReLU is not perfect. If a neuron\u0026rsquo;s weights get adjusted such that the weighted sum of its inputs (z) becomes negative for all instances in the training set, that neuron will always output 0. Since the derivative of ReLU is 0 for z \u0026lt; 0, no gradient will flow back through this \u0026ldquo;dead\u0026rdquo; neuron, and its weights will never be updated again. It effectively dies. This can happen if the learning rate is too large or due to poor initialization. Footnote 4 mentions a dead neuron might sometimes come back to life if neurons in previous layers change their outputs enough to make its input z positive again, but it\u0026rsquo;s not guaranteed. Leaky ReLU and its Variants (Page 335):\nGoal: To solve the \u0026ldquo;dying ReLUs\u0026rdquo; problem. Leaky ReLU: LeakyReLU_α(z) = max(αz, z) α (alpha) is a small hyperparameter (e.g., 0.01 or 0.2) that defines the slope for z \u0026lt; 0. What it\u0026rsquo;s ultimately trying to achieve: It allows a small, non-zero gradient to flow back even when the unit is not active (i.e., z \u0026lt; 0). This ensures the neuron never truly \u0026ldquo;dies\u0026rdquo; – it can always recover. A 2015 paper (footnote 5) found that leaky variants always outperformed strict ReLU, and a larger \u0026ldquo;leak\u0026rdquo; (α = 0.2) was better than a small one (α = 0.01). Randomized Leaky ReLU (RReLU): α is picked randomly from a given range during training and fixed to an average value during testing. Acted as a regularizer. Parametric Leaky ReLU (PReLU): α is learned during training (it becomes another parameter of the network, updated by backpropagation) instead of being a fixed hyperparameter. Reported to outperform ReLU on large image datasets but risks overfitting on smaller ones. ELU (Exponential Linear Unit) (Page 336):\nProposed in a 2015 paper by Clevert et al. (footnote 6). Equation 11-2 \u0026amp; Figure 11-3: ELU_α(z) = { α(exp(z) - 1) if z \u0026lt; 0 } { z if z ≥ 0 } Reported to outperform all ReLU variants in their experiments: reduced training time and better test set performance. Key Differences/Advantages over ReLU (Page 337): Takes on negative values when z \u0026lt; 0: This allows the units to have an average output closer to 0, which can help alleviate the vanishing gradients problem (similar to how tanh is sometimes better than sigmoid because its outputs are centered around 0). The hyperparameter α defines the value ELU approaches for large negative z (usually α=1). Nonzero gradient for z \u0026lt; 0: Avoids the dying neurons problem. The derivative for z\u0026lt;0 is α*exp(z). Smooth everywhere (if α=1): Including around z=0. This helps Gradient Descent converge faster as it doesn\u0026rsquo;t \u0026ldquo;bounce\u0026rdquo; as much around z=0 compared to ReLU/Leaky ReLU whose slope changes abruptly. Main Drawback: Slower to compute than ReLU (due to the exp(z) function). However, its faster convergence rate during training often compensates for this. At test time, an ELU network will be slower than a ReLU network. SELU (Scaled ELU) (Page 337):\nIntroduced in a 2017 paper by Klambauer et al. (footnote 7). A scaled variant of ELU. Amazing Property: Self-Normalization! If you build a network composed exclusively of a stack of dense layers, and all hidden layers use SELU activation, and a few other conditions are met, the network will self-normalize. What self-normalization achieves: The output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training. This solves the vanishing/exploding gradients problem! SELU often significantly outperforms other activation functions for such deep, dense networks. Conditions for Self-Normalization: Input features must be standardized (mean 0, std dev 1). Every hidden layer\u0026rsquo;s weights must be initialized with LeCun normal initialization (in Keras: kernel_initializer=\u0026quot;lecun_normal\u0026quot;). Network architecture must be sequential. Does not guarantee self-normalization for non-sequential architectures (like RNNs or networks with skip connections like Wide \u0026amp; Deep). The paper guarantees it if all layers are dense, but some researchers note it can improve performance in CNNs too. Which Activation Function to Use for Hidden Layers? (General Guideline - Lizard Icon, page 338):\nGeneral Order of Preference: SELU \u0026gt; ELU \u0026gt; Leaky ReLU (and its variants) \u0026gt; ReLU \u0026gt; tanh \u0026gt; logistic. If network architecture prevents self-normalization (e.g., RNNs, skip connections): ELU might perform better than SELU (since SELU isn\u0026rsquo;t smooth at z=0). Runtime Latency is Critical: Leaky ReLU might be preferred (simpler to compute than ELU/SELU). Simplicity/Don\u0026rsquo;t want to tune α: Use default α values (Keras uses 0.3 for Leaky ReLU). Spare Time/Compute for CV: Evaluate RReLU (if overfitting) or PReLU (if huge training set). Absolute Speed Priority: ReLU is the most used, so many libraries and hardware have specific optimizations for it. It might still be the best choice if raw speed is paramount. Keras Implementation (Page 338):\nLeaky ReLU: Add a keras.layers.LeakyReLU(alpha=0.2) layer after the Dense layer (if the Dense layer doesn\u0026rsquo;t have an activation specified). model.add(keras.layers.Dense(10, kernel_initializer=\u0026#34;he_normal\u0026#34;)) # No activation here model.add(keras.layers.LeakyReLU(alpha=0.2)) PReLU: keras.layers.PReLU() (can be added similarly). SELU: Set directly in the Dense layer: keras.layers.Dense(10, activation=\u0026quot;selu\u0026quot;, kernel_initializer=\u0026quot;lecun_normal\u0026quot;) Key Takeaway from this Section: The choice of activation function is critical for mitigating unstable gradients in deep networks.\nSaturating functions (sigmoid, tanh) are generally problematic for deep hidden layers due to vanishing gradients when neurons saturate. ReLU was a big step forward because it doesn\u0026rsquo;t saturate for positive inputs, but it can suffer from \u0026ldquo;dying neurons.\u0026rdquo; Leaky ReLU, PReLU, ELU address the dying ReLU problem by allowing a small gradient for negative inputs. ELU also offers smoothness and outputs closer to zero mean. SELU offers self-normalization under specific conditions, often leading to the best performance for deep stacks of dense layers. What all these newer activation functions are ultimately trying to achieve: Maintain a healthy flow of gradients during backpropagation, allowing all layers in a deep network to learn effectively. This understanding of activation functions and their impact on gradients, combined with proper weight initialization, forms the first line of defense against the vanishing/exploding gradients problem.\nBatch Normalization - The Concept Even with good initialization and activation functions, the vanishing/exploding gradients problem might not be completely solved, or it might reappear during training as weights get updated.\nThe Problem Batch Normalization (BN) Addresses:\nInternal Covariate Shift: As the parameters of preceding layers change during training, the distribution of each layer\u0026rsquo;s inputs also changes. This makes it harder for the current layer to learn, as it\u0026rsquo;s constantly adapting to a moving target. Vanishing/Exploding Gradients (still a concern). Batch Normalization (BN) - Proposed by Ioffe and Szegedy (2015 - Footnote 8):\nWhat it is: A technique that adds an operation in the model, typically just before or after the activation function of each hidden layer. What it does at each layer during training (for each mini-batch): Zero-centers and normalizes its inputs (makes them have mean 0 and standard deviation 1). Then, it scales and shifts the result using two new learnable parameter vectors per layer: γ (gamma): for scaling. β (beta): for shifting. What Batch Normalization is ultimately trying to achieve (the high-level goals): Reduce Internal Covariate Shift: By normalizing the inputs to each layer, it ensures that the distribution of these inputs is more stable throughout training, making it easier for each layer to learn. Combat Vanishing/Exploding Gradients: By keeping the inputs to activation functions in a more controlled range (around mean 0, std dev 1 before scaling/shifting), it helps prevent them from saturating (for sigmoid/tanh) or dying (for ReLU, by potentially shifting inputs to be positive more often). Allow higher learning rates: Training becomes more stable, often permitting the use of larger learning rates, which speeds up convergence. Act as a regularizer: It adds a bit of noise to each layer\u0026rsquo;s inputs (due to mini-batch statistics varying), which can have a slight regularizing effect, sometimes reducing the need for other regularization like dropout. Reduce sensitivity to weight initialization. How BN Works - The Algorithm (Equation 11-3, Page 339): For each input feature (or activation from a previous neuron) within a mini-batch B:\nμ_B = (1/m_B) * Σᵢ x⁽ⁱ⁾: Calculate the mean (μ_B) of that input feature over the current mini-batch B (which has m_B instances). σ_B² = (1/m_B) * Σᵢ (x⁽ⁱ⁾ - μ_B)²: Calculate the variance (σ_B²) of that input feature over the current mini-batch. x̂⁽ⁱ⁾ = (x⁽ⁱ⁾ - μ_B) / sqrt(σ_B² + ε): Normalize the input x⁽ⁱ⁾ for each instance. ε (epsilon) is a small smoothing term to avoid division by zero (e.g., 10⁻⁵). Now x̂⁽ⁱ⁾ has mean ~0 and std dev ~1 over this mini-batch. z⁽ⁱ⁾ = γ ⊗ x̂⁽ⁱ⁾ + β: Scale (γ) and shift (β) the normalized input. γ (gamma) and β (beta) are learnable parameters for this BN layer (one γ and one β per input feature to the layer). They are learned via backpropagation just like weights and biases of dense layers. What γ and β are ultimately trying to achieve: They allow the network to learn the optimal scale and mean for the inputs to the next layer. While the previous steps normalize to mean 0 / std dev 1, maybe the next activation function performs better if its inputs have, say, mean 0.5 and std dev 2. γ and β let the network learn this. If γ = sqrt(σ_B² + ε) and β = μ_B, the BN layer effectively undoes the normalization for that batch. BN at Test Time (Inference) (Page 340):\nDuring training, μ_B and σ_B are computed per mini-batch. At test time, you might be making predictions for individual instances, so there\u0026rsquo;s no mini-batch to compute a mean/std dev from. Even with a batch, it might be too small or not representative. Solution: During training, BN layers estimate the \u0026ldquo;final\u0026rdquo; population mean μ and standard deviation σ for each input feature using an exponential moving average of the mini-batch μ_B\u0026rsquo;s and σ_B\u0026rsquo;s. At test time, these final μ and σ are used in step 3 instead of batch-specific μ_B and σ_B. The learned γ and β are always used. Keras handles this automatically. Benefits Demonstrated by Ioffe and Szegedy (Page 340):\nBN considerably improved all deep networks they experimented with. Huge improvement on ImageNet classification. Vanishing gradients problem strongly reduced, allowing use of saturating activations like tanh/sigmoid. Less sensitive to weight initialization. Able to use much larger learning rates, significantly speeding up learning. Acts as a regularizer, reducing need for other methods like dropout. Complexity and Runtime Penalty (Page 341):\nBN adds complexity to the model. There\u0026rsquo;s a runtime penalty during inference due to extra computations at each layer. Optimization: It\u0026rsquo;s often possible to fuse the BN layer with the preceding Dense layer after training. The weights and biases of the Dense layer are updated to directly produce outputs of the appropriate scale and offset, effectively incorporating the BN operation. TFLite\u0026rsquo;s optimizer can do this automatically. Training Time (Wall Clock): Although each epoch takes longer with BN, convergence is much faster (fewer epochs needed), so overall \u0026ldquo;wall clock\u0026rdquo; training time is usually shorter. (Page 341-344: Implementing Batch Normalization with Keras)\nSimple Implementation:\nAdd a keras.layers.BatchNormalization() layer before or after each hidden layer\u0026rsquo;s activation function. Optionally, add a BN layer as the very first layer (after Flatten, if using Flatten). If you do this, you might not need to standardize your input data with StandardScaler, as the BN layer will handle normalization (approximately, as it uses mini-batch stats). Example with BN after activation (page 342): model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.BatchNormalization(), # As first layer keras.layers.Dense(300, activation=\u0026#34;elu\u0026#34;, kernel_initializer=\u0026#34;he_normal\u0026#34;), keras.layers.BatchNormalization(), # After first hidden layer\u0026#39;s activation keras.layers.Dense(100, activation=\u0026#34;elu\u0026#34;, kernel_initializer=\u0026#34;he_normal\u0026#34;), keras.layers.BatchNormalization(), # After second hidden layer\u0026#39;s activation keras.layers.Dense(10, activation=\u0026#34;softmax\u0026#34;) ]) For shallow networks, BN might not have a huge impact, but for deeper ones, it can be a game-changer. Parameters Added by BN Layer (Page 342):\nEach BN layer adds 4 parameters per input feature it processes: γ (gamma - scale): Trainable (learned by backprop). β (beta - shift): Trainable. μ (moving_mean): Non-trainable (estimated using moving average). σ (moving_variance): Non-trainable (estimated using moving average). So, if a BN layer receives 784 inputs, it adds 4 * 784 = 3136 parameters. 2 * 784 are trainable (γ, β), 2 * 784 are non-trainable (μ, σ). BN Before or After Activation? (Page 343):\nThe original BN paper suggested adding BN before the activation function. The example above added it after. There\u0026rsquo;s some debate; what\u0026rsquo;s preferable seems to depend on the task and dataset. Experiment to see what works best. To add BN before activation: Remove the activation from the Dense layer (activation=None or just omit it). Add the BatchNormalization layer. Add an keras.layers.Activation(\u0026quot;elu\u0026quot;) layer separately. Also, since BN includes a shift parameter β per input, you can remove the bias term from the preceding Dense layer by setting use_bias=False. This is slightly more efficient. model.add(keras.layers.Dense(300, kernel_initializer=\u0026#34;he_normal\u0026#34;, use_bias=False)) model.add(keras.layers.BatchNormalization()) model.add(keras.layers.Activation(\u0026#34;elu\u0026#34;)) BatchNormalization Hyperparameters (Page 343):\nmomentum: Used for updating the exponential moving averages for μ and σ. Given new batch stats v_batch, running average v̂ is updated as: v̂ ← v̂ * momentum + v_batch * (1 - momentum). A good value is typically close to 1 (e.g., 0.9, 0.99, 0.999). More 9s for larger datasets/smaller mini-batches. Default is 0.99. axis: Determines which axis should be normalized. Defaults to -1 (last axis). For a Dense layer, inputs are usually (batch_size, features). axis=-1 normalizes each feature independently across the batch. If BN is applied to 3D inputs like (batch_size, height, width) (e.g., before Flatten, or in CNNs), axis=-1 would normalize along the width dimension. If you want to normalize each of the height*width pixels independently, you might set axis=[1,2]. call() Method and training Argument (Page 344):\nThe BatchNormalization layer\u0026rsquo;s call() method has a training argument. model.fit() automatically sets this to True during training (so BN uses mini-batch stats and updates moving averages). model.evaluate() and model.predict() automatically set it to False (so BN uses the final moving averages). This is important if you write custom layers that need to behave differently during training vs. inference. BN as a Standard (Page 344):\nBN became so popular that it\u0026rsquo;s often assumed to be used after every (or most) layers in deep networks, sometimes even omitted in diagrams. However, recent research (e.g., \u0026ldquo;Fixup Initialization\u0026rdquo; by Zhang et al., 2019 - footnote 11) has shown it\u0026rsquo;s possible to train very deep networks without BN by using novel weight initialization techniques. This is bleeding-edge, so for now, BN remains a very strong default. Key Takeaway for Batch Normalization: BN is a powerful technique that normalizes the inputs to each layer during training, then scales/shifts them with learnable parameters.\nUltimately, it aims to: Stabilize and speed up training (allowing higher learning rates). Reduce the vanishing/exploding gradient problem. Make the network less sensitive to weight initialization. Provide a slight regularization effect. It\u0026rsquo;s a very common and effective component in modern deep neural networks. Great! Batch Normalization is indeed a very impactful technique.\nGradient Clipping While Batch Normalization helps with unstable gradients generally, sometimes gradients can still become excessively large, especially in certain types of networks like Recurrent Neural Networks (RNNs, Chapter 15). This is the exploding gradients problem.\nThe Problem: If gradients become huge, the parameter updates during Gradient Descent will also be huge (θ ← θ - η * large_gradient). This can cause the algorithm to overshoot the optimal solution wildly, leading to divergence (loss goes to infinity) or very unstable training.\nGradient Clipping - The Technique:\nA popular technique to mitigate exploding gradients. What it\u0026rsquo;s ultimately trying to achieve: To prevent the gradients from becoming too large by imposing a threshold on them during backpropagation. If a gradient exceeds this threshold, it\u0026rsquo;s \u0026ldquo;clipped\u0026rdquo; (scaled down) to the threshold value. This is most often used in RNNs, as Batch Normalization can be tricky to apply effectively in those architectures. For other types of networks (like the MLPs we\u0026rsquo;ve been discussing), Batch Normalization is usually sufficient to handle unstable gradients. Implementing Gradient Clipping in Keras: It\u0026rsquo;s done by setting an argument when creating an optimizer. Keras supports two main types of clipping:\nClipping by Value: optimizer = keras.optimizers.SGD(clipvalue=1.0)\nThis will clip every component (each partial derivative) of the gradient vector to be within a specific range. In this example, between -1.0 and +1.0. So, if any ∂Loss/∂wᵢⱼ is, say, 3.5, it will be clipped to 1.0. If it\u0026rsquo;s -2.0, it will be clipped to -1.0. Caveat: Clipping each component individually may change the orientation (direction) of the overall gradient vector. For instance, if the original gradient was [0.9, 100.0] (pointing mostly along the second axis), after clipping by value to 1.0, it becomes [0.9, 1.0] (pointing roughly diagonally). Despite this, it often works well in practice. The clipvalue is a hyperparameter you can tune. Clipping by Norm: optimizer = keras.optimizers.SGD(clipnorm=1.0)\nThis method ensures that the direction of the gradient vector is preserved. It calculates the ℓ₂ norm of the entire gradient vector ||∇Loss||₂. If this norm is greater than the clipnorm threshold (e.g., 1.0), the entire gradient vector is scaled down so that its norm equals the threshold. gradient ← gradient * (clipnorm / ||gradient||₂) For example, if clipnorm=1.0 and the original gradient is [0.9, 100.0] (whose norm is sqrt(0.9² + 100.0²) ≈ 100.004), it will be scaled down to something like [0.00899964, 0.9999595]. The direction is preserved, but the magnitude is reduced to 1.0. This can be gentler as it doesn\u0026rsquo;t distort the gradient direction, but it might almost eliminate components that were originally small if one component was huge. When to Use:\nIf you observe that gradients are exploding during training (you can track gradient norms using TensorBoard, for example, or just see your loss skyrocket), you might want to try gradient clipping. Experiment with both clipping by value and clipping by norm, and different threshold values, to see which works best for your specific problem and network. Key Takeaway for Gradient Clipping: It\u0026rsquo;s a straightforward technique primarily used to prevent the exploding gradients problem by limiting the maximum size of the gradients used in the parameter updates. While BN is often preferred for feedforward networks, clipping is a valuable tool, especially for RNNs.\nThis technique directly addresses one of the \u0026ldquo;pain points\u0026rdquo; of training deep networks. Next, the chapter moves on to strategies for dealing with another major challenge: not having enough labeled training data, by discussing Reusing Pretrained Layers (Transfer Learning).\nSounds good! Gradient clipping is indeed a practical fix for a very real problem.\nNow, let\u0026rsquo;s move on to a very powerful and widely used set of techniques for training deep neural networks, especially when you don\u0026rsquo;t have massive amounts of labeled data: Reusing Pretrained Layers, which is the core idea behind Transfer Learning (Pages 345-349).\nReusing Pretrained Layers - The Concept of Transfer Learning The Problem: Training a very large Deep Neural Network (DNN) from scratch generally requires a huge amount of training data. What if you don\u0026rsquo;t have that much data for your specific task?\nThe Solution: Transfer Learning\nCore Idea: It\u0026rsquo;s generally not a good idea to train a very large DNN from scratch if you can avoid it. Instead, you should almost always try to find an existing neural network that was trained on a large dataset to accomplish a task similar to yours. Then, you reuse the lower layers of this pretrained network for your own new model. What transfer learning is ultimately trying to achieve: Speed up training considerably: Your model doesn\u0026rsquo;t have to learn low-level features from scratch. Require significantly less training data for your specific task: The pretrained layers have already learned general features from a large dataset, which are often useful for your new task. Often achieve better performance than training from scratch, especially with limited data. Why it Works (Hierarchical Feature Learning):\nAs we discussed in Chapter 10 (and mentioned in this chapter on page 323), deep neural networks learn features in a hierarchical fashion. Lower layers (closer to the input) tend to learn low-level features (e.g., edges, corners, simple textures in images; basic phonetic sounds in speech). These low-level features are often generic and useful across many different tasks. Upper hidden layers learn more complex, task-specific features by combining the low-level features from earlier layers (e.g., parts of objects like \u0026ldquo;wheel\u0026rdquo; or \u0026ldquo;eye\u0026rdquo; in images; words or phrases in speech). Output layer learns to combine these high-level features to make the final prediction for the original task. Applying Transfer Learning (Figure 11-4): Suppose you have a DNN (Model A) pretrained on a large dataset for a general task (e.g., classifying images into 100 categories like animals, plants, vehicles). You now want to train a DNN (Model B) for a new, more specific task (e.g., classifying only specific types of vehicles), and you have limited data for this new task. Steps: Reuse Lower Layers: Take the lower layers from Model A (which learned general features) and use them as the initial layers for your Model B. Replace or Retrain Upper Layers: The upper hidden layers and the output layer of Model A are more specific to its original task. The output layer of Model A must usually be replaced because it\u0026rsquo;s likely not useful for Model B (e.g., different number of classes, different types of outputs). The upper hidden layers of Model A might be useful, or they might be too specific. You need to decide how many of these to reuse. Train Model B: Train your new model (with the reused lower layers and new/modified upper layers) on your smaller dataset for Task B. Preprocessing Input (Bird Icon, page 346):\nIf the input data for your new task (Task B) doesn\u0026rsquo;t have the same size/format as the data Model A was trained on, you\u0026rsquo;ll usually need to add a preprocessing step to resize/reformat your new data to match what the pretrained layers expect. Transfer learning works best when the inputs for the new task have similar low-level features to the original task. How Many Layers to Reuse? (Page 346 \u0026amp; Snake Icon, page 347):\nThe more similar the tasks are, the more layers you can (and probably should) reuse. For very similar tasks, you might reuse all hidden layers and just replace the output layer. Strategy for Fine-Tuning: Freeze Reused Layers Initially: When you first start training Model B, it\u0026rsquo;s often a good idea to freeze the weights of the reused layers (make them non-trainable so Gradient Descent won\u0026rsquo;t modify them). Why? Your new output layer (and any new upper hidden layers) will have randomly initialized weights. If you train everything at once, the large error gradients from these random layers could propagate back and wreck the carefully tuned weights of the pretrained lower layers. Train the model for a few epochs with the reused layers frozen. This allows the new layers to learn reasonable weights without damaging the pretrained ones. Unfreeze Some Layers and Fine-Tune: After the new layers have settled a bit, you can unfreeze some or all of the reused layers (typically starting with the upper ones) and continue training. This allows backpropagation to fine-tune these reused weights for your specific new task. Reduce Learning Rate: When you unfreeze reused layers, it\u0026rsquo;s usually beneficial to use a much smaller learning rate. This is to make only small adjustments to the already good pretrained weights, rather than drastically changing them. Iterate: If you still can\u0026rsquo;t get good performance and have little data, try dropping some of the top reused hidden layers and freezing all remaining reused layers again. If you have plenty of training data, you might try replacing the top hidden layers or even adding more new hidden layers on top of the reused ones. Transfer Learning with Keras (Page 347-348): The book gives an example:\nModel A: Trained on Fashion MNIST for 8 classes (excluding sandal and shirt). Achieved \u0026gt;90% accuracy. Task B: Binary classifier for shirt (positive) vs. sandal (negative). Small dataset (200 labeled images). Training a new model (same architecture as A) from scratch on Task B gets ~97.2% accuracy. Can transfer learning do better? Load Model A and Create Model B: model_A = keras.models.load_model(\u0026quot;my_model_A.h5\u0026quot;) model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) # Reuse all layers except output model_B_on_A.add(keras.layers.Dense(1, activation=\u0026quot;sigmoid\u0026quot;)) # New binary output layer Cloning (Important!): The code above makes model_B_on_A share layers with model_A. If you train model_B_on_A, it will change the weights in model_A as well! To avoid this, clone model_A first if you want to keep model_A intact: model_A_clone = keras.models.clone_model(model_A) model_A_clone.set_weights(model_A.get_weights()) Then build model_B_on_A using model_A_clone.layers[:-1]. Freeze Reused Layers: for layer in model_B_on_A.layers[:-1]: layer.trainable = False model_B_on_A.compile(...) (You must recompile after changing trainable status). Train the New Layer(s): history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, ...) (Train for a few epochs). Unfreeze and Fine-Tune: for layer in model_B_on_A.layers[:-1]: layer.trainable = True optimizer = keras.optimizers.SGD(learning_rate=1e-4) # Use a lower learning rate model_B_on_A.compile(..., optimizer=optimizer, ...) history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, ...) Result: The example shows the transfer learning model achieved 99.25% accuracy, reducing the error rate from 2.8% (model from scratch) to ~0.7% – a factor of four improvement! Caveat (\u0026ldquo;Torturing the Data\u0026rdquo;): The author admits to \u0026ldquo;cheating\u0026rdquo; a bit by trying many configurations to find one that showed a strong improvement for this specific example. Transfer learning doesn\u0026rsquo;t always give such dramatic gains for small dense networks (they learn few, specific patterns). It works best with deep convolutional neural networks (CNNs) which learn more general feature detectors, especially in lower layers. (This will be revisited in Chapter 14). (Page 349-350: Unsupervised Pretraining)\nWhat if you want to tackle a complex task, you don\u0026rsquo;t have much labeled data, AND you can\u0026rsquo;t find a pretrained model trained on a similar task?\nThe Idea: If you can gather plenty of unlabeled training data (which is often cheap and easy), you can use it to train an unsupervised model first. Examples of unsupervised models: Autoencoders, Generative Adversarial Networks (GANs) (Chapter 17). The Process (Figure 11-5, page 350): Train an unsupervised model (e.g., an autoencoder) on your large unlabeled dataset. What this unsupervised training is trying to achieve: The lower layers of this model will learn to detect useful low-level features and patterns present in your unlabeled data. Reuse the lower layers of this pretrained unsupervised model. Add an output layer suitable for your actual supervised task on top. Fine-tune this final network using your small amount of labeled training data. Historical Significance: This technique (often with Restricted Boltzmann Machines - RBMs, Appendix E) was crucial for the revival of neural networks and the success of Deep Learning from 2006 until around 2010. After the vanishing gradients problem was better understood and mitigated (with better initializers, activations, etc.), purely supervised training of DNNs became more common when large labeled datasets were available. Modern Relevance: Unsupervised pretraining (now typically using autoencoders or GANs) is still a very good option when: You have a complex task. No similar pretrained model is available. You have little labeled data but plenty of unlabeled data. Greedy Layer-Wise Pretraining (Old Technique - Figure 11-5, top part): In the early days, training deep models was very hard. So, people would train one unsupervised layer (e.g., RBM) at a time, freeze it, add another on top, train that new layer, freeze it, and so on. Nowadays, things are simpler: people generally train the full unsupervised model (e.g., a deep autoencoder) in one shot (Figure 11-5, starting at step three). (Page 350-351: Pretraining on an Auxiliary Task)\nOne last option if you don\u0026rsquo;t have much labeled data for your main task:\nThe Idea: Find an auxiliary task for which you can easily obtain or generate a lot of labeled training data. Train a first neural network on this auxiliary task. Reuse the lower layers of this network (which learned feature detectors relevant to the auxiliary task) for your actual main task. What this is ultimately trying to achieve: Hope that the features learned for the auxiliary task are also somewhat relevant and useful for your main task. Examples: Face Recognition: If you have few pictures of each specific person you want to recognize (main task), you could first gather lots of pictures of random people from the web and train a network to detect whether two different pictures feature the same person (auxiliary task – a \u0026ldquo;Siamese network\u0026rdquo; setup). This network would learn good general face feature detectors. Natural Language Processing (NLP): Download millions of text documents. Automatically generate labeled data by, for example, randomly masking out some words and training a model to predict the missing words (this is a form of self-supervised learning). A model that performs well on this task (like BERT or GPT) learns a lot about language structure. Its lower layers can then be reused and fine-tuned for a specific NLP task (like sentiment analysis) for which you might have less labeled data. (More in Chapter 15). Self-Supervised Learning (Bird Icon, page 351): When you automatically generate labels from the data itself (like masking words), it\u0026rsquo;s called self-supervised learning. Since no human labeling is needed, it\u0026rsquo;s often classified as a form of unsupervised learning. Key Takeaway for Reusing Layers/Pretraining: These techniques are all about leveraging knowledge learned from one task/dataset to help with another, especially when labeled data for the target task is scarce.\nTransfer Learning: Reuse layers from a model trained on a similar supervised task. Unsupervised Pretraining: Train on unlabeled data first to learn general features, then fine-tune for a supervised task. Pretraining on an Auxiliary Task: Train on a related task where labeled data is abundant, then transfer. These are powerful strategies to build effective deep learning models more efficiently.\nFaster Optimizers The book states, \u0026ldquo;Training a very large deep neural network can be painfully slow.\u0026rdquo; So far, we\u0026rsquo;ve seen four ways to speed up training (and potentially reach a better solution):\nApplying a good initialization strategy for connection weights (Glorot, He). Using a good activation function (ReLU, ELU, SELU). Using Batch Normalization. Reusing parts of a pretrained network (transfer learning). Another huge speed boost comes from using a faster optimizer than the regular (Mini-batch) Gradient Descent optimizer we\u0026rsquo;ve mostly considered so far. This section presents the most popular advanced optimization algorithms.\nWhat all these faster optimizers are ultimately trying to achieve: They aim to converge to a good solution more quickly than standard Gradient Descent by being smarter about the direction and size of the steps taken in the parameter space to minimize the loss function. They often do this by incorporating information about past gradients or by adapting the learning rate for different parameters.*\nMomentum Optimization (Page 351-352):\nProposed by Boris Polyak in 1964. Analogy: Imagine a bowling ball rolling down a gentle slope. It starts slowly but picks up momentum and reaches a terminal velocity. Regular Gradient Descent (GD) just takes small, regular steps, like someone carefully walking down. Core Idea: Momentum optimization cares a great deal about previous gradients. At each iteration, it subtracts the current local gradient from a momentum vector m. It then updates the weights θ by adding this momentum vector m. The gradient is used for acceleration, not directly for speed. Friction Mechanism: To prevent momentum from growing too large, a hyperparameter β (beta), called the momentum, is introduced (between 0 for high friction and 1 for no friction). A typical value is 0.9. Equation 11-4: Momentum algorithm m ← βm - η∇_θJ(θ) (Update momentum vector: previous momentum is decayed by β, and then the current negative gradient scaled by learning rate η is added/subtracted). θ ← θ + m (Update parameters using the momentum vector). Benefits: Faster Convergence: If the gradient remains constant, the terminal velocity (max update size) is η * gradient / (1-β). If β=0.9, this is 10 times faster than standard GD! Escapes Plateaus Faster: Standard GD moves very slowly on flat regions. Momentum helps \u0026ldquo;roll\u0026rdquo; through them. Helps with Elongated Bowls: In cost functions that look like elongated bowls (common when input features have different scales and no Batch Norm), GD goes down the steep slope quickly but then slowly navigates the valley. Momentum helps roll down the valley faster. Can Roll Past Local Optima: The momentum can sometimes carry the optimization past small local optima. Drawback (Bird Icon, page 352): May overshoot the minimum, then come back, oscillate a few times before stabilizing. The friction β helps dampen these oscillations. Adds another hyperparameter (β) to tune (though 0.9 often works well). Keras Implementation: Use the SGD optimizer and set its momentum hyperparameter: optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9) Nesterov Accelerated Gradient (NAG) (Page 353):\nProposed by Yurii Nesterov in 1983. Almost always faster than vanilla momentum optimization. Core Idea: Instead of calculating the gradient at the current position θ (like vanilla momentum), NAG calculates the gradient slightly ahead in the direction of the momentum, at θ + βm. Equation 11-5: Nesterov Accelerated Gradient algorithm m ← βm - η∇_θJ(θ + βm) (Calculate gradient at the \u0026ldquo;look-ahead\u0026rdquo; position). θ ← θ + m Why it\u0026rsquo;s better (Figure 11-6): The momentum vector m generally points towards the optimum. So, the gradient measured slightly ahead in that direction (∇₂ in the figure) is a more accurate estimate of the \u0026ldquo;true\u0026rdquo; gradient towards the optimum than the gradient at the original position (∇₁). When momentum pushes weights across a valley, ∇₁ (original gradient) might continue to push it further across. ∇₂ (look-ahead gradient) will start to push back towards the bottom of the valley sooner. This helps reduce oscillations and converge faster. Keras Implementation: Set nesterov=True in the SGD optimizer: optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True) AdaGrad (Adaptive Gradient Algorithm) (Page 354-355):\nAddresses the \u0026ldquo;elongated bowl\u0026rdquo; problem differently. GD goes down steep slopes quickly but then slows down in gentle-sloped valleys. AdaGrad tries to correct its direction earlier to point more towards the global optimum. Core Idea: It scales down the gradient vector along the steepest dimensions (i.e., it \u0026ldquo;dampens\u0026rdquo; movement in directions where the gradient has been consistently large). This is an adaptive learning rate method – different learning rates for different parameters. Equation 11-6: AdaGrad algorithm s ← s + ∇_θJ(θ) ⊗ ∇_θJ(θ) (Accumulate the square of the gradients into vector s. ⊗ is element-wise multiplication. So sᵢ ← sᵢ + (∂J/∂θᵢ)²). θ ← θ - η ∇_θJ(θ) ⊘ sqrt(s + ε) (⊘ is element-wise division, ε is a small smoothing term to avoid division by zero). How it works: If the cost function is steep along dimension i, (∂J/∂θᵢ)² will be large, so sᵢ will accumulate quickly. In step 2, the update for θᵢ is divided by sqrt(sᵢ + ε). If sᵢ is large, this division scales down the learning rate specifically for θᵢ. So, learning rate decays faster for steep dimensions and slower for dimensions with gentler slopes. Benefits (Figure 11-7): Points updates more directly towards the global optimum. Requires less tuning of the learning rate η. Drawback: Often stops too early when training neural networks. The learning rate gets scaled down so much (as s keeps accumulating) that the algorithm halts before reaching the global optimum. Keras: keras.optimizers.Adagrad(). Not recommended for deep neural networks due to premature stopping, but understanding it helps with RMSProp and Adam. RMSProp (Root Mean Square Propagation) (Page 355-356):\nAddresses AdaGrad\u0026rsquo;s problem of stopping too early. Core Idea: Instead of accumulating all past squared gradients in s, RMSProp accumulates only the gradients from the most recent iterations by using an exponential decay in the first step. Equation 11-7: RMSProp algorithm s ← βs + (1-β)∇_θJ(θ) ⊗ ∇_θJ(θ) (Exponentially decaying average of squared gradients. β is a decay rate, e.g., 0.9). θ ← θ - η ∇_θJ(θ) ⊘ sqrt(s + ε) (Same update rule as AdaGrad, using the new s). The decay rate β (typically 0.9) is a new hyperparameter, but the default often works well. Performance: Almost always performs much better than AdaGrad. It was a preferred optimizer until Adam came along. Keras: keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9) (where rho corresponds to β). Adam (Adaptive Moment Estimation) and Nadam (Page 356-358):\nAdam (Kingma \u0026amp; Ba, 2014 - Footnote 17): Combines the ideas of Momentum optimization and RMSProp.\nKeeps an exponentially decaying average of past gradients (like momentum, this is the \u0026ldquo;first moment,\u0026rdquo; m). Keeps an exponentially decaying average of past squared gradients (like RMSProp, this is the \u0026ldquo;second moment,\u0026rdquo; s). Equation 11-8: Adam algorithm\nm ← β₁m + (1-β₁)∇_θJ(θ) (Update biased first moment estimate) s ← β₂s + (1-β₂)∇_θJ(θ) ⊗ ∇_θJ(θ) (Update biased second moment estimate) m̂ ← m / (1 - β₁ᵗ) (Bias-corrected first moment estimate, t is iteration number) ŝ ← s / (1 - β₂ᵗ) (Bias-corrected second moment estimate) θ ← θ - η m̂ ⊘ (sqrt(ŝ) + ε) (Update parameters) Steps 3 \u0026amp; 4 are technical details to correct for the fact that m and s are initialized at 0 and would be biased towards 0 early in training.\nHyperparameters:\nη (learning rate): Typically 0.001. β₁ (momentum decay): Typically 0.9. β₂ (scaling decay for squared gradients): Typically 0.999. ε (smoothing term): Typically 10⁻⁷. Performance: Adam is an adaptive learning rate algorithm, so it often requires less tuning of the learning rate η. Often very easy to use and a good default.\nKeras: keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\nAdaMax (Variant of Adam - Page 357):\nInstead of using the ℓ₂ norm (square root of sum of squares) of time-decayed gradients to scale updates (like Adam effectively does via sqrt(ŝ)), AdaMax uses the ℓ∞ norm (the maximum value). Can be more stable than Adam in some cases, but Adam generally performs better. Nadam (Page 358):\nAdam optimization + Nesterov trick (calculates gradient using the \u0026ldquo;look-ahead\u0026rdquo; momentum m̂). Often converges slightly faster than Adam. Timothy Dozat\u0026rsquo;s report (footnote 19) found Nadam generally outperforms Adam but can sometimes be outperformed by RMSProp. Caveat for Adaptive Optimizers (Scorpion Icon, page 358):\nAdaptive methods (RMSProp, Adam, Nadam) are often great and converge fast. However, a 2017 paper (Wilson et al. - footnote 20) showed they can generalize poorly on some datasets compared to simpler methods like SGD with Nesterov momentum. Practical Advice: If disappointed by your model\u0026rsquo;s performance with an adaptive optimizer, try plain Nesterov Accelerated Gradient. Your dataset might be \u0026ldquo;allergic\u0026rdquo; to adaptive gradients. Research is ongoing and moving fast. Second-Order Partial Derivatives (Hessians - Page 358):\nAll optimizers discussed so far use first-order partial derivatives (Jacobians/gradients). Optimization literature also has algorithms using second-order partial derivatives (Hessians – derivatives of the gradients). These are very hard to apply to DNNs because the number of Hessians is huge (n² per output, where n is number of parameters), so they often don\u0026rsquo;t fit in memory, and computing them is too slow. Training Sparse Models (Sidebar, page 359):\nIf you need a fast model at runtime or one that uses less memory, you might want a sparse model (most parameters are zero). Optimizers discussed produce dense models. Achieving sparsity: Train as usual, then set tiny weights to zero (pruning). Often doesn\u0026rsquo;t lead to very sparse models and can degrade performance. Apply strong ℓ₁ regularization during training (pushes optimizer to zero out weights, like Lasso). Use tools like TensorFlow Model Optimization Toolkit (TF-MOT), which has pruning APIs. Table 11-2: Optimizer Comparison (Page 359): A nice summary table rating convergence speed and quality for SGD, SGD with momentum, Adagrad, RMSProp, Adam, Nadam, AdaMax.\nSGD: Slow, but can reach good quality. Momentum/NAG: Faster convergence, good quality. Adagrad: Fast convergence, but often stops too early (poor quality for DNNs). RMSProp, Adam, Nadam, AdaMax: Generally fast convergence and good quality. Adam/Nadam are often excellent starting points. Key Takeaway for Faster Optimizers: Standard Gradient Descent can be slow. Advanced optimizers use techniques like momentum, adaptive learning rates per parameter, or a combination, to navigate the loss landscape more efficiently and converge faster to a good solution. Adam or Nadam are often good defaults, but if they don\u0026rsquo;t generalize well, try SGD with Nesterov momentum.\nThis section provides a fantastic toolkit of optimizers beyond basic SGD. They are essential for training deep networks in a reasonable amount of time.\nAlright, let\u0026rsquo;s keep the momentum going! We\u0026rsquo;ve covered optimizers that try to take smarter steps. Now, let\u0026rsquo;s talk about how to adjust the size of those steps during training: Learning Rate Scheduling (Pages 359-363).\nLearning Rate Scheduling - The Why and How We know that finding a good learning rate η is crucial:\nToo high: Training may diverge (loss explodes).\nToo low: Training converges eventually, but takes a very long time.\nSlightly too high: May make quick initial progress but then dance around the optimum, never settling. (See Figure 11-8). Finding a Good Constant Learning Rate (Recap from Chapter 10):\nTrain for a few hundred iterations, starting with a very small η and exponentially increasing it. Plot loss vs. η (log scale for η). Pick a learning rate slightly lower than where the loss starts shooting back up. Reinitialize and train with this constant η. Doing Better Than a Constant Learning Rate:\nIf you start with a large learning rate and then reduce it once training stops making fast progress, you can often reach a good solution faster than with the single best constant learning rate. These strategies to reduce the learning rate during training are called learning schedules. It can also be beneficial to start with a low learning rate, increase it (warm-up), then drop it again. What learning rate scheduling is ultimately trying to achieve: To speed up convergence and potentially reach a better final solution by dynamically adjusting the step size of the optimizer during training. Large steps early on can help navigate flat regions or escape poor local minima quickly, while smaller steps later can help fine-tune the solution around the optimum.*\nCommonly Used Learning Schedules (Page 360-361):\nPower Scheduling:\nη(t) = η₀ / (1 + t/s)ᶜ t: Iteration number. η₀: Initial learning rate. s: A hyperparameter determining how many steps it takes for η to drop significantly. c: Power (typically set to 1). Behavior: Learning rate drops at each step. After s steps, η ≈ η₀ / 2. After another s steps, η ≈ η₀ / 3, then η₀ / 4, etc. Drops quickly at first, then more and more slowly. Requires tuning η₀ and s (and possibly c). Keras Implementation (Easiest - Page 362): Set the decay hyperparameter when creating an SGD optimizer. decay is the inverse of s (and c is assumed to be 1). optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4) (This means s = 1 / 1e-4 = 10000. So after 10000 steps, η will be roughly 0.01/2). Exponential Scheduling:\nη(t) = η₀ * 0.1^(t/s) Behavior: Learning rate drops by a factor of 10 every s steps. More aggressive reduction than power scheduling (keeps slashing by a constant factor). Keras Implementation (Page 362): Define a function that takes the current epoch and returns the learning rate: def exponential_decay_fn(epoch): return 0.01 * 0.1**(epoch / 20) # Or a configurable one: # def exponential_decay(lr0, s): # def exponential_decay_fn(epoch): # return lr0 * 0.1**(epoch / s) # return exponential_decay_fn # exponential_decay_fn = exponential_decay(lr0=0.01, s=20) Create a LearningRateScheduler callback and pass it to fit(): lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn) history = model.fit(..., callbacks=[lr_scheduler]) This updates η at the beginning of each epoch. If you want updates at every step (more frequent), you can write a custom callback. The schedule function can optionally take the current lr as a second argument if it needs to know the previous learning rate. Important if using epoch in schedule function: If you save and then continue training, the epoch argument resets to 0. You might need to set fit()\u0026rsquo;s initial_epoch argument to avoid this. Piecewise Constant Scheduling:\nUse a constant η₀ for e₀ epochs, then a smaller η₁ for e₁ epochs, and so on. Can work very well but requires fiddling to find the right sequence of rates and durations. Keras Implementation (Page 363): Use a schedule function with LearningRateScheduler callback, similar to exponential scheduling: def piecewise_constant_fn(epoch): if epoch \u0026lt; 5: return 0.01 elif epoch \u0026lt; 15: return 0.005 else: return 0.001 Performance Scheduling:\nMeasure validation error every N steps (like early stopping). Reduce η by a factor λ when the error stops dropping. Keras Implementation (Page 363): Use the ReduceLROnPlateau callback. lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5) This will multiply η by 0.5 whenever the best validation loss does not improve for 5 consecutive epochs. 1cycle Scheduling (Leslie Smith, 2018 - Footnote 21, Page 361): A more recent and often very effective schedule.\nBehavior: Start with an initial learning rate η₀. Linearly increase η up to a maximum η₁ about halfway through training. Linearly decrease η back down to η₀ during the second half. In the last few epochs, drop η by several orders of magnitude (still linearly). η₁ (max rate) is chosen using the \u0026ldquo;find optimal LR\u0026rdquo; method (like in Chapter 10). η₀ is often set ~10x lower. Momentum with 1cycle: Often, momentum is also varied: start high (e.g., 0.95), drop to lower (e.g., 0.85) during first half, then back up to high (0.95) for second half. Benefits: Smith showed this can significantly speed up training and achieve better performance. (e.g., CIFAR10: 91.9% accuracy in 100 epochs vs. 90.3% in 800 epochs with standard approach). Keras Implementation (Page 364): Requires a custom callback to modify self.model.optimizer.learning_rate (or .lr for older Keras/TF) at each iteration. tf.keras Schedulers (Page 363): tf.keras (but not the standalone Keras API) offers an alternative: define learning_rate using schedules from keras.optimizers.schedules. Example: keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1) (where s is number of steps for decay). Pass this learning_rate object directly to an optimizer: optimizer = keras.optimizers.SGD(learning_rate=learning_rate_schedule_object). Advantage: Updates η at each step (not just epoch). The schedule state is saved with the model. Summary (Page 364): Exponential decay, performance scheduling, and 1cycle can considerably speed up convergence. They are worth trying!\nKey Takeaway for Learning Rate Scheduling: A constant learning rate is often suboptimal. Dynamically adjusting the learning rate during training—starting larger for faster initial progress and exploration, then reducing it for finer convergence—can lead to faster training and better final models. Many schedules exist, with 1cycle being a particularly promising recent one.\nGreat! It\u0026rsquo;s good that the concept of dynamically adjusting the learning rate makes sense – it\u0026rsquo;s a very practical and effective way to improve training.\nNow, let\u0026rsquo;s shift our focus from speeding up training and stabilizing gradients to another critical challenge in deep learning: Avoiding Overfitting Through Regularization (Pages 364-371).\nAvoiding Overfitting Through Regularization - Introduction The chapter opens with a famous quote often attributed to John von Neumann: \u0026ldquo;With four parameters I can fit an elephant and with five I can make him wiggle his trunk.\u0026rdquo;\nThe Problem with Deep Neural Networks: They typically have tens of thousands, sometimes millions, of parameters. This gives them incredible flexibility to fit a huge variety of complex datasets. The Downside of Flexibility: This same flexibility makes them very prone to overfitting the training set, especially if the amount of training data isn\u0026rsquo;t proportionally massive or if the data is noisy. The model might learn the noise and specific quirks of the training data instead of the underlying generalizable patterns. We need regularization.\nTechniques Already Covered: Early Stopping (Chapter 10): A very effective regularization technique. Batch Normalization: While designed to solve unstable gradients, it also acts as a pretty good regularizer (due to the slight noise from mini-batch statistics). This section will explore other popular regularization techniques specifically for neural networks:\nℓ₁ and ℓ₂ regularization (which we saw for linear models in Chapter 4). Dropout. Max-norm regularization. What all these regularization techniques are ultimately trying to achieve: To constrain the learning algorithm in some way to prevent it from fitting the training data too perfectly, thereby improving its ability to generalize to new, unseen data.*\n(Page 364-365: ℓ₁ and ℓ₂ Regularization)\nThese are familiar from linear models and can be applied to neural network connection weights.\nℓ₂ Regularization (Weight Decay):\nHow it works: Adds a penalty term to the loss function proportional to the sum of the squares of the connection weights (usually excluding biases). For a Keras layer, you\u0026rsquo;d apply it to the kernel (weights). J_regularized(W,b) = J_original(W,b) + λ * (1/2) * Σ w² What it\u0026rsquo;s ultimately trying to achieve: It encourages the network to learn smaller weights. Smaller weights generally lead to simpler, smoother functions that are less likely to overfit by fitting the noise in the training data. It \u0026ldquo;decays\u0026rdquo; the weights towards zero unless the data strongly justifies a larger weight. Keras Implementation: layer = keras.layers.Dense(100, activation=\u0026quot;elu\u0026quot;, kernel_initializer=\u0026quot;he_normal\u0026quot;, kernel_regularizer=keras.regularizers.l2(0.01)) keras.regularizers.l2(0.01) creates a regularizer object with a regularization factor (lambda λ) of 0.01. This regularizer is called at each step during training to compute the regularization loss, which is then added to the main loss. ℓ₁ Regularization:\nHow it works: Adds a penalty term proportional to the sum of the absolute values of the weights. J_regularized(W,b) = J_original(W,b) + λ * Σ |w| What it\u0026rsquo;s ultimately trying to achieve: It also encourages smaller weights, but it has the added property of tending to drive many weights to exactly zero. This results in a sparse model (many connections are effectively removed). This can be useful for feature selection or creating more compact models. Keras Implementation: Use keras.regularizers.l1(0.01). ℓ₁ and ℓ₂ Regularization (Elastic Net):\nYou can use both simultaneously with keras.regularizers.l1_l2(l1=0.01, l2=0.01). Applying to Multiple Layers (Page 365):\nYou typically want to apply the same regularizer, activation function, and initialization strategy to all hidden layers. Repeating these arguments makes code ugly and error-prone. Solution: Use Python\u0026rsquo;s functools.partial() to create a \u0026ldquo;pre-configured\u0026rdquo; layer type: from functools import partial RegularizedDense = partial(keras.layers.Dense, activation=\u0026#34;elu\u0026#34;, kernel_initializer=\u0026#34;he_normal\u0026#34;, kernel_regularizer=keras.regularizers.l2(0.01)) model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), RegularizedDense(300), RegularizedDense(100), RegularizedDense(10, activation=\u0026#34;softmax\u0026#34;, # Override for output kernel_initializer=\u0026#34;glorot_uniform\u0026#34;) # Override for output ]) This makes the code cleaner and easier to manage. Dropout This is one of the most popular and effective regularization techniques for deep neural networks.\nProposed by Geoffrey Hinton et al. (2012) and detailed by Srivastava et al. (2014).\nEven state-of-the-art networks often get a 1-2% accuracy boost from adding dropout. This is significant when accuracy is already high (e.g., 95% -\u0026gt; 97% is a 40% reduction in error rate).\nThe Algorithm (Fairly Simple - Figure 11-9, page 366):\nAt every training step: Every neuron (including input neurons, but always excluding output neurons) has a probability p (the dropout rate) of being temporarily \u0026ldquo;dropped out.\u0026rdquo; \u0026ldquo;Dropped out\u0026rdquo; means the neuron is entirely ignored during this training step – it doesn\u0026rsquo;t produce any output (or its output is considered to be 0). This means for each training step, a different, thinned version of the network is effectively being trained. After training (at test/inference time): Neurons are no longer dropped. All neurons are active. Typical Dropout Rate p: Between 10% and 50%. 20-30% for Recurrent Neural Networks. 40-50% for Convolutional Neural Networks. Why Does This Destructive Technique Work? (Page 366): It\u0026rsquo;s surprising at first!\nForces Neurons to Be More Robust: Neurons trained with dropout cannot co-adapt too much with their neighboring neurons (because those neighbors might be dropped out at any time). Each neuron has to learn to be as useful as possible on its own. They cannot rely excessively on just a few input neurons (as those inputs might be dropped). They must pay attention to each of their inputs more broadly. This makes them less sensitive to slight changes in the inputs. The result is a more robust network that generalizes better. Ensemble of Smaller Networks: Another way to understand dropout is to realize that at each training step, a unique neural network architecture is generated (since each neuron can be present or absent). With N droppable neurons, there are 2^N possible networks – a huge number! After, say, 10,000 training steps, you\u0026rsquo;ve essentially trained 10,000 different (though overlapping, as they share weights) neural networks, each on a single training instance (or batch). The resulting final network (used at test time with no dropout) can be seen as an averaging ensemble of all these thinned networks. Important Technical Detail (Scaling - Page 367):\nSuppose p = 50% (dropout rate). During training, a neuron is connected (on average) to half as many input neurons as it will be during testing (when no neurons are dropped). To compensate for this, so that the total input signal to a neuron at test time is roughly the same scale as it was during training, we need to adjust. Two ways: Multiply each neuron\u0026rsquo;s input connection weights by the keep probability (1-p) after training. Or (more commonly): Divide each neuron\u0026rsquo;s output by the keep probability (1-p) during training (only for the neurons that were not dropped). Keras (and most libraries) use the second method. Implementing Dropout in Keras (Page 367):\nUse the keras.layers.Dropout layer. During training, it randomly drops some inputs (sets them to 0) and scales the remaining inputs by 1 / (1 - rate). After training (during inference), it does nothing; it just passes the inputs through. model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dropout(rate=0.2), # Dropout after flatten keras.layers.Dense(300, activation=\u0026#34;elu\u0026#34;, kernel_initializer=\u0026#34;he_normal\u0026#34;), keras.layers.Dropout(rate=0.2), # Dropout after first hidden keras.layers.Dense(100, activation=\u0026#34;elu\u0026#34;, kernel_initializer=\u0026#34;he_normal\u0026#34;), keras.layers.Dropout(rate=0.2), # Dropout after second hidden keras.layers.Dense(10, activation=\u0026#34;softmax\u0026#34;) ]) The bird icon (page 367) mentions you can usually apply dropout only to neurons in the top 1-3 hidden layers (excluding output). Impact on Training/Validation Loss (Scorpion Icon, page 367):\nSince dropout is only active during training, comparing training loss and validation loss directly can be misleading. A model might be overfitting, yet training and validation losses appear similar because training loss is \u0026ldquo;artificially inflated\u0026rdquo; by dropout. To get a true sense of training loss, evaluate the model on the training set with dropout turned off (e.g., after training is complete). Tuning Dropout Rate (Page 367):\nIf model is overfitting: Increase dropout rate. If model is underfitting: Decrease dropout rate. Can also use higher rates for large layers, lower for small ones. Many state-of-the-art architectures only use dropout after the last hidden layer. Dropout and Convergence Speed (Page 368):\nDropout tends to significantly slow down convergence. But it usually results in a much better model if tuned properly. It\u0026rsquo;s often worth the extra time. AlphaDropout (Page 368):\nIf regularizing a self-normalizing network using SELU activation, use AlphaDropout. Regular dropout would break self-normalization. AlphaDropout is a variant that preserves the mean and standard deviation of its inputs. Monte Carlo (MC) Dropout (Yarin Gal \u0026amp; Zoubin Ghahramani, 2016 - Page 368-370): A powerful technique that gives dropout even more utility.\nBayesian Connection: Established a profound link between dropout networks and approximate Bayesian inference, giving dropout a solid mathematical justification. MC Dropout Technique: Boosts performance of any trained dropout model without retraining. Provides better measures of model uncertainty. Simple to implement: To make a prediction for a new instance (or batch), run the instance through the trained model multiple times (e.g., 100 times), but this time, keep dropout active (i.e., set training=True when calling the model or its layers). Each of these 100 predictions will be slightly different because different neurons are dropped each time. Average these 100 predictions to get the final MC Dropout prediction. # Assuming X_test_scaled is your test data y_probas_mc = np.stack([model(X_test_scaled, training=True) for sample in range(100)]) y_proba_mc_avg = y_probas_mc.mean(axis=0) # Average over the 100 samples What MC Dropout is ultimately trying to achieve: By averaging predictions from many slightly different \u0026ldquo;thinned\u0026rdquo; versions of the network (created by active dropout), it\u0026rsquo;s performing a form of ensemble prediction. This often leads to more robust and accurate predictions than a single pass with dropout turned off. Uncertainty Estimation: The predictions y_probas_mc will vary across the 100 samples. The average y_proba_mc_avg gives the final probability (e.g., for classification). The standard deviation of these probabilities across the samples (y_probas_mc.std(axis=0)) gives an estimate of the model\u0026rsquo;s uncertainty for each prediction. If the standard deviation is high, the model is very uncertain. This is incredibly useful for risk-sensitive applications (e.g., medical, finance). If the model is 99% confident but has high uncertainty (large std dev from MC Dropout), you should treat the prediction with caution. The example on page 369 shows a model being 99% sure about an ankle boot with dropout off, but MC Dropout reveals only 62% confidence with significant variance, suggesting hesitation with \u0026ldquo;sandal\u0026rdquo; or \u0026ldquo;sneaker.\u0026rdquo; MC Dropout can also give a small accuracy boost. Using MCDropout class (page 370): If your model has other layers that behave differently during training (like Batch Normalization), you shouldn\u0026rsquo;t force the whole model into training=True mode for MC Dropout. Instead, you should replace Dropout layers with a custom MCDropout layer that always keeps dropout active. class MCDropout(keras.layers.Dropout): def call(self, inputs): return super().call(inputs, training=True) In short: MC Dropout is fantastic for boosting performance and getting uncertainty estimates from already trained dropout models. It acts like a regularizer during training (because it is regular dropout then) and an ensemble/uncertainty tool at inference. (Page 370-371: Max-Norm Regularization)\nAnother regularization technique popular for neural networks.\nThe Idea: For each neuron, constrain the ℓ₂ norm of the vector of its incoming connection weights w such that ||w||₂ ≤ r. r is the max-norm hyperparameter. How it\u0026rsquo;s Implemented: It does not add a regularization loss term to the overall loss function. Instead, after each training step (after weights are updated by the optimizer), it computes ||w||₂ for each neuron. If ||w||₂ \u0026gt; r, the weight vector w for that neuron is rescaled: w ← w * (r / ||w||₂). What max-norm regularization is ultimately trying to achieve: It keeps the incoming weights for each neuron bounded, preventing them from growing too large. This helps reduce overfitting. It can also help alleviate unstable gradients if you\u0026rsquo;re not using Batch Normalization. Reducing r increases the amount of regularization. Keras Implementation: Set the kernel_constraint argument of a Dense layer (or other weight-bearing layers) to keras.constraints.max_norm(r). keras.layers.Dense(..., kernel_constraint=keras.constraints.max_norm(1.0)) After each training iteration, Keras will call this constraint object, which rescales the weights if necessary. max_norm() has an axis argument (defaults to 0 for Dense layers, meaning it constrains the weights of each neuron\u0026rsquo;s input vector independently). For convolutional layers (Chapter 14), you\u0026rsquo;d set axis appropriately (e.g., ``). Excellent! We\u0026rsquo;re on the home stretch for Chapter 11. We\u0026rsquo;ve covered the major challenges in training deep neural networks: unstable gradients, lack of data, slow training, and overfitting, along with a powerful toolkit of solutions like smart initializations, non-saturating activations, Batch Normalization, faster optimizers, learning rate scheduling, transfer learning, and regularization techniques like Dropout and Max-Norm.\nNow, let\u0026rsquo;s look at the Summary and Practical Guidelines (Pages 371-372) this chapter offers, which attempt to distill all this information into actionable advice.\nSummary and Practical Guidelines The chapter acknowledges that we\u0026rsquo;ve covered a wide range of techniques, and you might be wondering which ones to use. The answer depends on the specific task, and there\u0026rsquo;s no single clear consensus for every situation. However, the book provides a default DNN configuration that it has found to work well in most cases, without requiring excessive hyperparameter tuning.\nTable 11-3: Default DNN Configuration (General Purpose)\nKernel Initializer: He initialization (This is a good default, especially if using ReLU-family activations). Activation Function: ELU (Generally performs well, smoother than ReLU, avoids dying neurons, outputs closer to zero mean). Normalization: None if shallow (few hidden layers). Batch Norm if deep. (BN adds a lot of stability for deeper networks). Regularization: Early stopping (Almost always a good idea). ℓ₂ regularization if needed (can add a bit more control over overfitting if early stopping isn\u0026rsquo;t quite enough or if you want to encourage smaller weights generally). Optimizer: Momentum optimization (e.g., Nesterov) or RMSProp or Nadam/Adam. (Adam/Nadam are often good starting points due to their adaptive nature, but as noted, plain Nesterov Momentum can sometimes generalize better). Learning Rate Schedule: 1cycle (This is a strong recommendation from recent research for fast convergence and good performance). What this default configuration is ultimately trying to achieve: It provides a robust and high-performing starting point for many deep learning tasks. It incorporates solutions for unstable gradients (He init, ELU, BN for deep nets), ways to speed up training (good optimizer, 1cycle LR schedule), and basic regularization (early stopping, optional ℓ₂).\nTable 11-4: DNN Configuration for a Self-Normalizing Net (Page 372)\nIf your network architecture meets the specific conditions for self-normalization (a simple stack of dense layers):\nKernel Initializer: LeCun initialization (Required for SELU). Activation Function: SELU (The key to self-normalization). Normalization: None (because the network self-normalizes; Batch Norm would interfere). Regularization: Alpha Dropout if needed (Regular dropout breaks SELU\u0026rsquo;s self-normalization properties). Optimizer: Momentum optimization (or RMSProp/Nadam). Learning Rate Schedule: 1cycle. What this self-normalizing configuration is ultimately trying to achieve: To create a network where the activations in each layer automatically maintain a mean of 0 and standard deviation of 1 throughout training, thus alleviating vanishing/exploding gradients without needing Batch Normalization, and often leading to excellent performance for suitable architectures.\nGeneral Advice (Page 372):\nNormalize Input Features! Don\u0026rsquo;t forget this crucial preprocessing step. Try to Reuse Pretrained Layers (Transfer Learning): If you can find a network trained on a similar task, reuse its parts. Unsupervised Pretraining / Auxiliary Task Pretraining: Good options if you have lots of unlabeled data (or can generate labeled data for an auxiliary task) but little labeled data for your main task. Exceptions and Specific Cases (Page 372):\nWhile the default guidelines cover most cases, there are exceptions:\nNeed a Sparse Model? Use ℓ₁ regularization (and optionally zero out tiny weights after training). For even sparser models, use TensorFlow Model Optimization Toolkit (TF-MOT) for pruning. If you use ℓ₁ or TF-MOT, self-normalization (SELU) will likely be broken, so revert to the general default configuration (Table 11-3 with ELU, Batch Norm if deep, etc.). Need a Low-Latency Model (Lightning-Fast Predictions)? Use fewer layers. Fold Batch Normalization layers into the previous Dense layers after training (some tools like TFLite optimizer do this automatically). This removes the BN computation at inference time. Possibly use a faster activation function like Leaky ReLU or just ReLU (they are computationally cheaper than ELU/SELU). Having a sparse model also helps. Consider reducing float precision (e.g., 32-bit to 16-bit or even 8-bit) for deployment on mobile/embedded devices (more on this in Chapter 19 under \u0026ldquo;Deploying a Model to a Mobile or Embedded Device\u0026rdquo;). Check out TF-MOT for quantization tools. Building a Risk-Sensitive Application / Inference Latency Not Critical? Use MC Dropout to: Boost performance. Get more reliable probability estimates. Get uncertainty estimates for predictions. Final Words of the Chapter (Page 372):\nWith these guidelines and techniques, you should be ready to train very deep nets. Keras provides a lot of power and flexibility. However, there might be times when you need even more control (e.g., custom loss functions, tweaking the training algorithm itself). For such cases, you\u0026rsquo;ll need to use TensorFlow\u0026rsquo;s lower-level API, which is the topic of the next chapter (Chapter 12). Key Takeaway from the Summary and Guidelines: This chapter provides a powerful set of tools and strategies to overcome the main difficulties in training deep neural networks. There\u0026rsquo;s no one-size-fits-all recipe, but the provided default configurations (especially the one in Table 11-3) are excellent starting points. The process often involves experimentation, understanding the trade-offs of different techniques, and iteratively refining your approach based on the specific problem and dataset.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-11/","tags":null,"title":"Chapter 11: Training Deep Neural Networks"},{"categories":null,"content":"Chapter 12 Overview: Hash Tables Hash tables (or hash maps) are one of the most useful and versatile data structures in computer science. They provide a mechanism to store key-value pairs and allow for extremely fast average-case lookups, insertions, and deletions. In Python, the dict and set types are implemented using hash tables.\nKey Hash Table Concepts: Hash Function: A function that maps a key to an integer, which is then used as an index (or \u0026ldquo;hash code\u0026rdquo;) in an underlying array (often called buckets or slots). A good hash function should distribute keys uniformly across the array to minimize collisions. Key Properties: Keys must be \u0026ldquo;hashable,\u0026rdquo; which generally means they must be immutable. An object is hashable if it has a hash value that never changes during its lifetime. Buckets/Slots: The underlying array that stores the data. The hash code determines which bucket a key-value pair belongs to. Collision: When two different keys map to the same bucket index. This is inevitable unless the hash table is very large and the hash function is perfect. Collision Resolution: The strategy for handling collisions. Separate Chaining (or Chaining): Each bucket is a pointer to a linked list (or another data structure) that holds all the key-value pairs that hashed to that index. This is a very common method. Open Addressing: If a collision occurs, the algorithm probes for the next empty slot in the array according to a fixed sequence. Examples include Linear Probing (checking the next slot, then the next, etc.), Quadratic Probing, and Double Hashing. Performance:\nAverage Case: O(1) for lookups, insertions, and deletions. Worst Case: O(n) for lookups, insertions, and deletions. This happens when all keys hash to the same bucket, effectively turning the hash table into a single linked list. Section 12.1: Test for Palindromic Permutations The Problem: Given a string, determine if any permutation of its characters can form a palindrome.\nExample:\n\u0026ldquo;edified\u0026rdquo; -\u0026gt; True (a permutation is \u0026ldquo;deified\u0026rdquo;) \u0026ldquo;level\u0026rdquo; -\u0026gt; True (it\u0026rsquo;s already a palindrome) \u0026ldquo;aabbc\u0026rdquo; -\u0026gt; True (a permutation is \u0026ldquo;abcba\u0026rdquo;) \u0026ldquo;abc\u0026rdquo; -\u0026gt; False Key Insight: A string can be permuted into a palindrome if and only if at most one of its characters appears an odd number of times. All other characters must appear an even number of times.\nIf the string length is even, all character counts must be even. If the string length is odd, exactly one character count must be odd (the one that will be the center of the palindrome). Algorithm (Using a Hash Table/Set):\nInitialize a hash table (or hash set) to track the counts of characters that have appeared an odd number of times so far. Let\u0026rsquo;s call it odd_chars. Iterate through each character c in the input string: If c is already in odd_chars, it means we\u0026rsquo;ve now seen this character an even number of times. Remove it from odd_chars. If c is not in odd_chars, it means we\u0026rsquo;ve now seen it an odd number of times. Add it to odd_chars. After iterating through the whole string, check the size of odd_chars. If len(odd_chars) \u0026lt;= 1, then a palindromic permutation is possible. Return True. Otherwise, return False. Complexity:\nTime: O(n), where n is the number of characters in the string, because we iterate through it once. Space: O(c), where c is the number of unique characters in the character set (e.g., 26 for lowercase English letters, 128 for ASCII). This is effectively O(1) if the character set is fixed and finite. Section 12.2: Is an Anonymous Letter Constructible? The Problem: You are given two strings, the letter_text and the magazine_text. Write a function to determine if the letter can be written using characters from the magazine. Each character in the magazine can only be used once.\nAlgorithm (Using a Hash Table for Character Counts):\nCreate a hash table (e.g., a collections.Counter in Python or a simple dict) to store the counts of all characters available in the magazine_text. Iterate through magazine_text and populate the hash table. Iterate through each character c in the letter_text: Check if c is in the hash table and if its count is greater than 0. If c is not in the table or its count is 0, it means we don\u0026rsquo;t have this character available in the magazine. Return False. If the character is available, decrement its count in the hash table to \u0026ldquo;use it up\u0026rdquo;. If the loop completes, it means all characters required for the letter were found in the magazine. Return True. Complexity:\nTime: O(m + n), where m is the length of the magazine text and n is the length of the letter text. Space: O(c), where c is the number of unique characters in the magazine\u0026rsquo;s character set. Section 12.4: Implement an LRU Cache The Problem: Design and implement a Least Recently Used (LRU) cache. It should support two operations: insert and lookup. The cache has a fixed capacity. When a new item is inserted and the cache is full, the least recently used item must be evicted.\nKey Requirements:\nlookup(key): If the key exists, return its value and mark it as recently used. insert(key, value): Insert or update the key-value pair. Mark it as recently used. If insertion causes the cache to exceed capacity, evict the LRU item. Both operations should be as fast as possible, ideally O(1). Algorithm (Hash Table + Doubly Linked List): This is the classic solution that combines the strengths of two data structures to achieve O(1) performance.\nHash Table (dict in Python): Stores key -\u0026gt; Node mappings. This provides O(1) average time complexity for lookups. The Node it points to is a node in our doubly linked list. Doubly Linked List: Stores the items in order of use. The most recently used item is at the head (or tail), and the least recently used item is at the other end. A doubly linked list is crucial because it allows for O(1) removal of any node if we have a direct pointer to it. The hash table gives us this pointer. Operations:\nlookup(key): Check if key is in the hash table. If not, return None (cache miss). If it exists, get the Node from the hash table. Move this Node to the head of the linked list to mark it as most recently used. Return the Node\u0026rsquo;s value. insert(key, value): Check if key is already in the hash table. If yes, update the value in its corresponding Node and move that Node to the head of the list. If key is not in the hash table (a new insertion): If the cache is full (len(hash_table) == capacity): Get the lru_node from the tail of the linked list. Remove lru_node.key from the hash table. Remove the lru_node from the linked list. Create a new Node with the given key and value. Add the new Node to the head of the linked list. Add the key -\u0026gt; new_Node mapping to the hash table. Section 12.8: Find the Smallest Subarray Covering a Set of Keywords The Problem: Given a long text (an array of words) and a set of keywords, find the smallest subarray of the text that contains all the keywords.\nExample:\nText: [\u0026quot;apple\u0026quot;, \u0026quot;banana\u0026quot;, \u0026quot;apple\u0026quot;, \u0026quot;apple\u0026quot;, \u0026quot;dog\u0026quot;, \u0026quot;cat\u0026quot;, \u0026quot;apple\u0026quot;, \u0026quot;dog\u0026quot;, \u0026quot;banana\u0026quot;, \u0026quot;apple\u0026quot;, \u0026quot;cat\u0026quot;, \u0026quot;dog\u0026quot;] Keywords: {\u0026quot;banana\u0026quot;, \u0026quot;cat\u0026quot;} Result: [\u0026quot;cat\u0026quot;, \u0026quot;apple\u0026quot;, \u0026quot;dog\u0026quot;, \u0026quot;banana\u0026quot;] (length 4, from index 6 to 9) Algorithm (Sliding Window with Hash Table): This is a classic sliding window problem. We expand a window from the right and shrink it from the left.\nCreate a hash table keyword_counts to store the count of each keyword we need to find. Initialize it from the input set of keywords. Initialize left = 0, result = (-1, -1) (to store the start/end of the best subarray), and remaining_to_find = len(keywords). Use a hash table window_counts to track counts of keywords within the current window [left, right]. Iterate with a right pointer from 0 to len(text) - 1: Let word = text[right]. If word is one of the keywords: Increment its count in window_counts. If window_counts[word] is now equal to keyword_counts[word], it means we have found enough of this specific keyword. Decrement remaining_to_find. Shrink the window from the left: While remaining_to_find == 0: We have a valid window [left, right]. Check if it\u0026rsquo;s smaller than the best result found so far. If so, update result. Let left_word = text[left]. If left_word is a keyword: Decrement its count in window_counts. If its window_counts is now less than its keyword_counts, we no longer have all required keywords in the window. Increment remaining_to_find. Move the left boundary: left += 1. Return the subarray corresponding to the final result indices. ","permalink":"https://deepskandpal.github.io/bookshelf/elements-of-programming/chapter-12/","tags":null,"title":"Chapter 12: Hash Tables"},{"categories":null,"content":"Recursion Core Idea of Recursion: At its simplest, recursion means a function calls itself to solve a smaller version of the same problem.\nPrompt Link(my private discussion for this chapter using ai studio and this chapters main system prompt along with the pdf of this chapter)\nTwo Key Ingredients :\nBase Cases: These are the simplest instances of the problem that the function can solve directly, without further recursion. Intuition: \u0026ldquo;When do I know the answer without asking for more help?\u0026rdquo; Progress (Recursive Call with different arguments): The function must call itself with arguments that move it closer to a base case. If it doesn\u0026rsquo;t make progress, it\u0026rsquo;ll loop forever. Recursion: The \u0026ldquo;Ask a Mini-Me\u0026rdquo; Approach:\nImagine you\u0026rsquo;re a kid and your parent asks you to clean your entire room. It\u0026rsquo;s a disaster. Toys everywhere. You (the function): \u0026ldquo;Ugh, this is too much!\u0026rdquo;\nYour thought process:\nBase Case (Simplest Job): \u0026ldquo;Is there just ONE toy left on the floor?\u0026rdquo; If yes, you pick it up. Done! Easy. Recursive Step (Making it Simpler): \u0026ldquo;Okay, there\u0026rsquo;s more than one toy. What if I pick up just one toy and put it away? Now the room is slightly cleaner. The remaining mess is a smaller version of the original problem.\u0026rdquo; The \u0026ldquo;Call a Mini-Me\u0026rdquo;: \u0026ldquo;Hey, Mini-Me (you call yourself again), can you clean this slightly cleaner room?\u0026rdquo; Trusting Mini-Me: You assume Mini-Me will somehow get the job done. What you do after Mini-Me is done: Nothing! You already did your one piece of work (picking up one toy). Why is recursion useful?\nWhen the input itself is recursive (like file system directories, or a computer grammar). For problems like searching, enumeration (listing all possibilities), and divide-and-conquer. Divide-and-Conquer vs. Recursion:\nDivide-and-Conquer: A strategy. It breaks a problem into independent smaller subproblems of the same type, solves them, and combines their solutions. Merge Sort is a classic. Recursion: A technique. It\u0026rsquo;s how you often implement divide-and-conquer. But recursion is more general. You might have only one subproblem (like binary search or factorial), or the subproblems might not be independent (hello, Dynamic Programming!). Example: Counting Down\nLet\u0026rsquo;s say you want to write a function that prints numbers from N down to 1. countdown(N):\nJob: Print N, then N-1, then N-2, \u0026hellip;, down to 1.\ndef countdown(n): if n == 0: # Base case: stop when we reach 0 (or 1, if you prefer to print 1) print(\u0026#34;Blast off!\u0026#34;) return # This is important! Stop the recursion. print(n) # Do a small piece of work countdown(n - 1) # Ask a \u0026#34;mini-me\u0026#34; to do the rest (a smaller version) countdown(3) # Output: # 3 # 2 # 1 # Blast off! 1. Recursion boot camp Euclidean Algorithm for GCD (Greatest Common Divisor)\nProblem: Find the largest number that divides both x and y without a remainder. Core Idea (from the book): GCD(x, y) is the same as GCD(y, x % y). (Assuming x \u0026gt; y, initially it might be GCD(x-y, y) repeatedly, which simplifies to GCD(x % y, y) and then swapping to keep the first argument larger or just using GCD(y, x%y)). Let\u0026rsquo;s think recursively: def gcd(x, y):\ndef gcd(x, y): # Base case: if y is 0, then x is the GCD. # Think: GCD(12, 0) -\u0026gt; what\u0026#39;s the largest number that divides 12 and 0? It\u0026#39;s 12. return x if y == 0 else gcd(y, x % y) The \u0026ldquo;Work\u0026rdquo;: The % (modulo) operation is the work that makes the problem smaller. \u0026ldquo;Smaller\u0026rdquo;: The numbers y and x % y are generally smaller than x and y. Specifically, x % y is guaranteed to be less than y. Base Case: y == 0. When the second number is zero, the first number is the GCD. GCD(12, 0) = 12. 2. Mutilated Chessboard (Page 218) This is a fantastic example of divide-and-conquer.\n🧩 Problem Cover an 8x8 board with one square missing (mutilated) using L-shaped triominoes (3 squares each).\nTotal squares: 63 Required triominoes: 21 💡 EPI\u0026rsquo;s Reasoning Don’t think n → n+1. That’s a dead end.\nInstead, think:\nIf I can solve for an n x n Mboard, can I solve for a 2n x 2n Mboard?\n✨ The \u0026ldquo;Aha!\u0026rdquo; Insight (Figure 15.1(b)) Imagine a 2n x 2n board. Divide it into four n x n quadrants. One quadrant contains the original missing square. This is already an n x n Mboard (mutilated board) — by hypothesis, we can tile it! The other three quadrants are not Mboards yet. 🧠 Clever Step Place one triomino in the center of the 2n x 2n board. Align it such that it covers one corner square from each of the three \u0026ldquo;full\u0026rdquo; n x n quadrants. This action creates a new missing square in each of the three quadrants, turning them into Mboards! By hypothesis, we can now tile all four n x n Mboards — problem solved!\n3. 📘 Table 15.1: Top Tips for Recursion (Page 219) 1. 🔁 Recursive Rules Input If the problem description sounds recursive, then recursion is a natural fit.\nExample: \u0026ldquo;A directory contains files and other directories.\u0026rdquo;\n2. 🧭 Search, Enumerate, Divide-and-Conquer These problem types are prime candidates for recursion:\n🔍 Search:\n\u0026ldquo;Is the item in the left half? Or the right half?\u0026rdquo;\n→ Example: Binary Search\n📋 Enumerate:\n\u0026ldquo;What if I pick this? What are the options then? What if I don’t pick this?\u0026rdquo;\n→ Example: Generating all subsets\n🧩 Divide-and-Conquer:\nBreak the problem into smaller subproblems of the same type.\n3. 🔄 Alternative to Nested Loops (of Undefined Depth) When you don’t know how many nested loops you’ll need (e.g., problems with variable segments like IP address parsing),\nrecursion gracefully handles varying depth.\n4. 🧱 Removing Recursion (Mimic the Call Stack) If you\u0026rsquo;re asked to make a recursive solution iterative:\nUse your own stack data structure to keep track of “what to do next.”\n5. 🎯 Tail Recursion If the recursive call is the last operation in the function, it can often be converted into a loop.\nNote: Python does not optimize tail recursion, but understanding it is still valuable.\n6. 💾 Caching Repeated Calls (Memoization) If your recursive function recomputes the same result multiple times:\nStore the result (e.g., fib(3) is needed by both fib(5) and fib(4)) This is the gateway to Dynamic Programming! 🧠 Problem 15.1: The Towers of Hanoi (Page 219) This is a classic recursion problem.\nIf you understand this one, you\u0026rsquo;ve got a solid grasp of basic recursive thinking.\n🏗️ The Setup Three pegs:\nP1 → Source P2 → Destination P3 → Auxiliary / Helper n disks of decreasing sizes stacked on P1:\nLargest at the bottom, smallest at the top Goal:\nMove all n disks from P1 to P2.\n📏 Rules Only one disk can be moved at a time. A disk can only be moved if it is the top disk on a peg. A larger disk can never be placed on top of a smaller disk. 🧾 Task Write a program that prints the sequence of operations\nrequired to move all n disks from P1 to P2, obeying the rules.\nThe recursive solution has a pattern:\nIf n \u0026gt; 0:\nHanoi(n-1, src, aux, dest) Move disk n from src to dest Hanoi(n-1, aux, dest, src) Define the recursive function:\ncompute_tower_hanoi_steps(num_rings_to_move, from_peg, to_peg, use_peg)\nBase Case:\nIf num_rings_to_move == 0: do nothing. If num_rings_to_move == 1: move the single disk from from_peg to to_peg. Note: The EPI book uses an implicit base case with if num_rings_to_move \u0026gt; 0:. This works because:\nWhen num_rings_to_move == 1, the first recursive call is with 0 disks (no action), then it moves the disk, then a second call with 0 disks (again, no action). This pattern is correct and avoids explicitly writing a base case. However, using num_rings_to_move == 1 explicitly can improve clarity when learning recursion. Recursive Step (if num_rings_to_move \u0026gt; 1):\nMove (n - 1) disks from from_peg to use_peg using to_peg as a temporary: compute_tower_hanoi_steps(num_rings_to_move - 1, from_peg, use_peg, to_peg) Move the nth disk (the largest in this subproblem) from from_peg to to_peg: print(\u0026ldquo;Move disk\u0026rdquo;, num_rings_to_move, \u0026ldquo;from\u0026rdquo;, from_peg, \u0026ldquo;to\u0026rdquo;, to_peg) (In EPI: pegs[to_peg].append(pegs[from_peg].pop()); result.append(\u0026hellip;)) Move (n - 1) disks from use_peg to to_peg using from_peg as a temporary: compute_tower_hanoi_steps(num_rings_to_move - 1, use_peg, to_peg, from_peg) Backtracking and Recursion Pre-requiste What is Backtracking? It\u0026rsquo;s a Problem-Solving Technique (not a specific formula).\nThink of backtracking as a systematic way to explore all possible solutions to a problem, especially when the problem involves a sequence of choices.\nImagine you\u0026rsquo;re trying to find your way out of a maze:\nYou reach a junction (a decision point). You have multiple paths to choose from. You CHOOSE one path. You EXPLORE that path. If this path leads to the exit (a solution!), great! You might record it. If this path leads to a dead end (not a solution, or violates a rule), or if you\u0026rsquo;ve explored all it has to offer\u0026hellip; You BACKTRACK: You return to the junction where you made your last choice. You UNDO your previous choice (mentally or actually \u0026ldquo;un-take\u0026rdquo; that path). You CHOOSE a different, unexplored path from that same junction and explore it. If all paths from a junction have been explored (and possibly led to dead ends), you backtrack even further to the previous junction. Core Components of Backtracking:\nProblem Representation: The problem can be broken down into a sequence of decisions. The set of all possible decision sequences forms a \u0026ldquo;search space\u0026rdquo; or \u0026ldquo;state-space tree.\u0026rdquo;\nN-Queens: Decision at each row = which column to place the queen. Permutations: Decision at each position = which available item to place. Power Set: Decision for each item = include it or exclude it. Recursive Structure: Backtracking is almost always implemented using recursion because it naturally handles the \u0026ldquo;explore deeper\u0026rdquo; and \u0026ldquo;return to previous state\u0026rdquo; aspects.\nA function call represents making a set of choices up to a certain point and then trying to make the next choice. The \u0026ldquo;Try, Recurse, Undo\u0026rdquo; Pattern (The Heart of Backtracking): Inside a recursive backtracking function, for the current decision point:\nIterate through all possible CHOICES for the current step. For each choice: a. Validate the choice: Can I make this choice given the previous choices? (e.g., Is it safe to place a queen here? Is this letter already used?). b. If choice is valid: i. MAKE the choice: Update the current state (e.g., place the queen, add the letter to the word). ii. EXPLORE (Recurse): Call the function recursively to make the next set of choices, building upon the current choice. iii. UNDO the choice (Backtrack): Revert the state to what it was before making this choice. This is CRUCIAL. It allows the loop to correctly try the next possible choice for the current decision point without being affected by the exploration of the previous choice. Base Cases:\nSolution Found: The sequence of choices has led to a valid, complete solution. (e.g., N queens placed, word of K length formed). Record it. Dead End / Invalid Path: The current path of choices cannot possibly lead to a solution, or it violates a fundamental constraint. (e.g., no safe spot for the current queen). The function returns, triggering backtracking in the caller. Why is it called \u0026ldquo;Backtracking\u0026rdquo;? Because when a path doesn\u0026rsquo;t lead to a solution (or all solutions down that path have been found), the algorithm \u0026ldquo;backtracks\u0026rdquo; up the decision tree (i.e., recursive calls return) to a previous decision point to try a different option.\nIs it a formula? No, it\u0026rsquo;s more like a template or a strategy for designing algorithms. The specific implementation details (how state is stored, how choices are made, how validity is checked, how \u0026ldquo;undo\u0026rdquo; happens) vary from problem to problem.\nWhen to Think \u0026ldquo;Backtracking\u0026rdquo;:\nYou need to generate all possible configurations/solutions (permutations, combinations, subsets, placements). The problem involves a sequence of decisions. You can check the validity of a partial solution or a next choice. You need to explore many possibilities, but some paths can be identified as \u0026ldquo;dead ends\u0026rdquo; early. Backtracking is the general search technique. The \u0026ldquo;undo\u0026rdquo; step is what makes it backtracking rather than just a one-way greedy search. Without the \u0026ldquo;undo,\u0026rdquo; you\u0026rsquo;d explore one path and get stuck with its consequences. The \u0026ldquo;undo\u0026rdquo; allows you to say, \u0026ldquo;Okay, that path (or sub-path) is done; let\u0026rsquo;s rewind and try something else from my last major decision point.\u0026rdquo;\nGoal Find all 2-letter \u0026ldquo;words\u0026rdquo; using letters from \u0026ldquo;ABC\u0026rdquo;, where each letter is used at most once.\nExpected Output: \u0026ldquo;AB\u0026rdquo;, \u0026ldquo;AC\u0026rdquo;, \u0026ldquo;BA\u0026rdquo;, \u0026ldquo;BC\u0026rdquo;, \u0026ldquo;CA\u0026rdquo;, \u0026ldquo;CB\u0026rdquo;\nRecursive Function Let our recursive function be:\nfind_words(current_word, used_letters)\nBase Case If len(current_word) == 2:\nWe\u0026rsquo;ve built a 2-letter word. Add it to the list of solutions. Return. Recursive Step Iterate through each letter in \u0026quot;ABC\u0026quot;:\nCheck if the letter is already in used_letters. If the letter is not used:\na. Place / Choose Add the letter to current_word Add the letter to used_letters\nb. Recurse Call find_words(current_word, used_letters)\nc. Backtrack / Undo Remove the letter from current_word Remove the letter from used_letters Backtracking ensures the state is clean when trying the next available letter.\nVisualization of This Simpler Problem\nImagine you have 3 slots to pick from: A, B, C. You are building a 2-letter word.\n1st letter choice:\nPick \u0026lsquo;A\u0026rsquo;\n→ Now need 1 more letter.\n→ Used: {\u0026lsquo;A\u0026rsquo;}\n→ Word so far: \u0026ldquo;A\u0026rdquo;\n2nd letter choice (can\u0026rsquo;t be \u0026lsquo;A\u0026rsquo;):\nPick \u0026lsquo;B\u0026rsquo;\n→ Word is \u0026ldquo;AB\u0026rdquo; — Done! (Base case)\nBacktrack: Forget \u0026lsquo;B\u0026rsquo;\n→ Word so far: \u0026ldquo;A\u0026rdquo;\n→ Used: {\u0026lsquo;A\u0026rsquo;}\nPick \u0026lsquo;C\u0026rsquo;\n→ Word is \u0026ldquo;AC\u0026rdquo; — Done! (Base case)\nBacktrack: Forget \u0026lsquo;C\u0026rsquo;\n→ Word so far: \u0026ldquo;A\u0026rdquo;\n→ Used: {\u0026lsquo;A\u0026rsquo;}\nBacktrack: Forget \u0026lsquo;A\u0026rsquo;\n→ Word so far: \u0026quot;\u0026quot;\n→ Used: {}\nPick \u0026lsquo;B\u0026rsquo;\n→ Now need 1 more letter.\n→ Used: {\u0026lsquo;B\u0026rsquo;}\n→ Word so far: \u0026ldquo;B\u0026rdquo;\n2nd letter choice (can\u0026rsquo;t be \u0026lsquo;B\u0026rsquo;):\nPick \u0026lsquo;A\u0026rsquo;\n→ Word is \u0026ldquo;BA\u0026rdquo; — Done!\nBacktrack: Forget \u0026lsquo;A\u0026rsquo;\n→ Word so far: \u0026ldquo;B\u0026rdquo;\n→ Used: {\u0026lsquo;B\u0026rsquo;}\nPick \u0026lsquo;C\u0026rsquo;\n→ Word is \u0026ldquo;BC\u0026rdquo; — Done!\nBacktrack: Forget \u0026lsquo;C\u0026rsquo;\n→ Word so far: \u0026ldquo;B\u0026rdquo;\n→ Used: {\u0026lsquo;B\u0026rsquo;}\nBacktrack: Forget \u0026lsquo;B\u0026rsquo;\n→ Word so far: \u0026quot;\u0026quot;\n→ Used: {}\nPick \u0026lsquo;C\u0026rsquo;\n→ Now need 1 more letter.\n→ Used: {\u0026lsquo;C\u0026rsquo;}\n→ Word so far: \u0026ldquo;C\u0026rdquo;\n2nd letter choice (can\u0026rsquo;t be \u0026lsquo;C\u0026rsquo;):\nPick \u0026lsquo;A\u0026rsquo;\n→ Word is \u0026ldquo;CA\u0026rdquo; — Done!\nBacktrack: Forget \u0026lsquo;A\u0026rsquo;\n→ Word so far: \u0026ldquo;C\u0026rdquo;\n→ Used: {\u0026lsquo;C\u0026rsquo;}\nPick \u0026lsquo;B\u0026rsquo;\n→ Word is \u0026ldquo;CB\u0026rdquo; — Done!\nBacktrack: Forget \u0026lsquo;B\u0026rsquo;\n→ Word so far: \u0026ldquo;C\u0026rdquo;\n→ Used: {\u0026lsquo;C\u0026rsquo;}\nBacktrack: Forget \u0026lsquo;C\u0026rsquo;\n→ Word so far: \u0026quot;\u0026quot;\n→ Used: {}\nHere the Undo part ( Backtrack ) is EXPLICIT when we do \u0026ldquo;FORGET\u0026rdquo; aplhabet (A/B/C)\nLet\u0026rsquo;s re-look at the Alphabet example with the \u0026ldquo;Backtracking Template\u0026rdquo; in mind: find_words(current_word_list, used_letters_set) (Decision: What letter to pick next?)\nBase Case (Solution Found): len(current_word_list) == K. Iterate through choices: for letter in all_available_letters: Validate: if letter not in used_letters_set: Make Choice: current_word_list.append(letter), used_letters_set.add(letter) Explore (Recurse): find_words(next_state...) Undo Choice: used_letters_set.remove(letter), current_word_list.pop() Key Characteristics/Benefits of Backtracking:\nFinds All Solutions: If you let it run its course without early termination, it will systematically find all solutions. Handles Constraints: The \u0026ldquo;validate choice\u0026rdquo; step allows you to prune the search space by not exploring paths that violate problem constraints. Conceptually Simple (once grasped): The \u0026ldquo;try all options, and for each, recursively try all further options\u0026rdquo; is a natural way to think about exhaustive search. Often a starting point: For many hard problems (NP-complete problems), backtracking is a fundamental approach. Optimizations (like memoization in dynamic programming, or better pruning heuristics) can then be built on top of it. When is the O(N²) Simple Iterative Solution \u0026ldquo;Good Enough\u0026rdquo;? Preferred When:\nThe depth of choices is small and fixed (e.g., K = 2). The state between choices is minimal. Raw performance is critical and recursion’s function call overhead, though usually minor, matters. Mental Model:\nIterative Nested Loops:\nBest for fixed, shallow decision trees. Like building with a fixed number of Lego blocks in a specific sequence. Recursive Backtracking:\nBest for variable-depth or complex decision trees. Like exploring a maze with unknown turns and constraints. BACKTRACKING HELPS FIND ALL POSSIBLE SOLUTIONS ♛ 15.2 GENERATE ALL NONATTACKING PLACEMENTS OF N-QUEENS (Page 221) Goal: Place N queens on an N×N chessboard such that no two queens attack each other.\n🧩 The Problem: Given an N × N chessboard. Place N queens on the board such that no two queens attack each other. ⚔️ Attack Rules: Queens can attack horizontally, vertically, and diagonally. 🎯 Goal: Return all distinct configurations (valid placements) of these N queens. 🧾 Task Exact problem: Place exactly one queen per row. Find ALL Possible combinations\nRecusrion Backtracking: \u0026ldquo;choose, explore, undo\u0026rdquo; pattern\nTLDR;\nHow it happens in N-Queens (EPI style): You are trying to place a queen in Row X. You try putting it in Column A. board_config[X] = A. You recursively try to solve for Row X+1 and beyond. When that recursion finishes and returns, you are still in the loop for Row X. Next, you try putting the queen for Row X in Column B. The \u0026ldquo;Undo\u0026rdquo;: board_config[X] = B overwrites board_config[X] = A. The choice of A for Row X is now gone for future explorations from Row X. Recursive Function Idea: solve_queens(row_to_place_in, current_board_config)\n\u0026ldquo;Building the solution piece by piece\u0026rdquo; (like building the word letter by letter):\nIn N-Queens: We are trying to decide the column for the queen in the row_to_place_in. This is like deciding the next letter for our word. \u0026ldquo;What choices do I have for the current piece?\u0026rdquo;\nAlphabet problem: Choose any unused letter from \u0026lsquo;A\u0026rsquo;, \u0026lsquo;B\u0026rsquo;, \u0026lsquo;C\u0026rsquo;. N-Queens: For the current row_to_place_in, try placing the queen in column 0, then column 1, then column 2, \u0026hellip;, up to column N-1. solve_queens(row, placement) (where placement[r] stores the column of the queen in row r)\nBase Case (Solution Found):\nAlphabet: if len(current_word) == K_target_length: Solution found. N-Queens: if row == N: We have successfully placed queens in row 0 to N-1. All N queens are on the board. This is a valid complete solution. Action: Add a copy of placement to our list of results. Return. Recursive Step (Trying to place a queen in the current row):\nLoop through all possible choices for this step: Alphabet: for each_letter in available_letters:\nN-Queens: for col_choice from 0 to N-1: (This is trying each column for the queen in the current row)\nCheck if the choice is valid/allowed:\nAlphabet: if each_letter not in used_letters: N-Queens: if is_safe_to_place_queen_at(row, col_choice, placement_so_far): The is_safe_to_place_queen_at function checks: Is col_choice already taken by a queen in a previous row (placement[prev_row] == col_choice)? Is (row, col_choice) on a diagonal with any queen in a previous row\n(abs(placement[prev_row] - col_choice) == abs(prev_row - row))? If the choice IS VALID/SAFE: a. \u0026ldquo;Place\u0026rdquo; / Make the Choice:\nAlphabet: current_word.append(each_letter), used_letters.add(each_letter) N-Queens: placement[row] = col_choice (We’ve decided the queen for the current row goes in col_choice) b. \u0026ldquo;Explore\u0026rdquo; / Recurse (Try to complete the rest of the solution based on this choice):\nAlphabet: find_words(current_word, used_letters) (which internally will try to fill the next letter position) N-Queens: solve_queens(row + 1, placement) (Try to place queens for all subsequent rows, starting with the very next row) c. \u0026ldquo;Backtrack\u0026rdquo; / Undo the Choice (CRUCIAL for exploring other options for the current step):\nAlphabet: current_word.pop(), used_letters.remove(each_letter) N-Queens: This is where the EPI code is a bit more implicit. When the solve_queens(row + 1, placement) call returns (meaning it has fully explored all possibilities from placing a queen at (row, col_choice), or it hit a dead end), the execution comes back to the for col_choice ... loop. If the loop continues to the next col_choice for the same current row, the line placement[row] = new_col_choice will simply overwrite the previous col_choice. This acts as the \u0026ldquo;undo\u0026rdquo; for the specific assignment to placement[row]. If the for col_choice ... loop finishes for the current row (all columns tried), the solve_queens(row, placement) function itself returns. This signifies that all paths starting with the configuration of queens up to row-1 (that led to this call) have been explored for this particular row. The state placement[0...row-1] remains untouched by this returning function, allowing the caller (which was trying to place a queen in row-1) to potentially try a different column for row-1. Q. so what if we find all the valid placements in the first go itself . Then don\u0026rsquo;t you think our return and the loop for placement[row] = new_col_choice iterating further undoing our setup wasted when we Have a valid placement? You\u0026rsquo;re asking: If a particular choice for placement[row] (say, col_A) leads to one or more valid complete solutions through the solve(row+1, ...) call, why does the for loop for row continue to try other columns (like col_B) for that same row? Isn\u0026rsquo;t that wasted effort if we\u0026rsquo;ve already found solutions?\nThe answer depends on the goal of the problem:\nGoal: Find ALL distinct non-attacking placements (as in N-Queens).\nIf the goal is to find all possible solutions, then no, it\u0026rsquo;s not wasted. We must continue exploring.\nJust because placing a queen at (row, col_A) led to some solutions doesn\u0026rsquo;t mean that placing a queen at (row, col_B) (if safe) won\u0026rsquo;t also lead to other, different valid solutions. The N-Queens problem specifically asks for all distinct nonattacking placements. To achieve this, the backtracking algorithm must systematically explore every possible valid path in the decision tree. When solve(row+1, ...) returns after exploring the consequences of placement[row] = col_A, it has done its job for that branch. The for loop for row then must try placement[row] = col_B (and col_C, etc.) to see if those choices also lead to valid complete solutions.\nEach of these will be a distinct branch. Goal: Find ANY ONE non-attacking placement (or determine if one exists).\nIf the problem was modified to \u0026ldquo;find just one solution and then stop,\u0026rdquo; then yes, you could optimize.\nIn this scenario, your recursive function could return a boolean: true if a solution was found down that path, false otherwise. 15.3 GENERATE PERMUTATIONS (Page 222) Problem: Given an array of distinct integers, generate all possible orderings (permutations) of those integers. No permutation should appear more than once.\nCore Idea (Recursive Backtracking): To form a permutation of N items:\nChoose one item to be the first item in the permutation. Then, generate all permutations of the remaining N-1 items. Prepend the chosen first item to each of these N-1 item permutations. Repeat this by choosing every possible item as the \u0026ldquo;first\u0026rdquo; item. Simplified Recursive Logic: generate_perms(index, current_array)\nindex: The current position in the permutation we are trying to fill (from 0 to N-1). current_array: The array whose elements are being swapped to form permutations. Base Case:\nIf index == N-1 (or index == N depending on how you structure it): We\u0026rsquo;ve fixed elements for all positions from 0 to N-1. The current_array now holds one complete permutation. Action: Add a copy of current_array to the results. Return. Recursive Step (For the current index): Iterate i from index to N-1 (these are the elements available to be placed at the current index).\nChoose/Place: Swap current_array[index] with current_array[i]. This brings the element originally at current_array[i] into the index-th position. Explore/Recurse: Call generate_perms(index + 1, current_array). This will generate all permutations for the rest of the array (from index + 1 onwards), given the choice made for index. Undo/Backtrack: Swap current_array[index] with current_array[i] again. This restores the array to its state before this iteration\u0026rsquo;s choice, allowing the next iteration of the for loop (for i) to correctly pick a different element for the index-th position. This is crucial. Example: A = [1,2,3], directed_permutations_recursive(0)\nidx_to_fill = 0: (Think of idx_to_fill as \u0026ldquo;locking down\u0026rdquo; elements from left to right. Everything to the left of idx_to_fill is considered \u0026ldquo;fixed\u0026rdquo; for the current branch of recursion.) i = 0: A=[1,2,3]. Swap A[0],A[0] (no change). Recurse d_p(1) with A=[1,2,3]. idx_to_fill = 1: (Processing A=[1,_,_]) i = 1: A=[1,2,3]. Swap A[1],A[1]. Recurse d_p(2) with A=[1,2,3]. idx_to_fill = 2. Base Case. Add [1,2,3] to result. Return. Undo swap A[1],A[1]. A=[1,2,3]. i = 2: A=[1,2,3]. Swap A[1],A[2] -\u0026gt; A=[1,3,2]. Recurse d_p(2) with A=[1,3,2]. idx_to_fill = 2. Base Case. Add [1,3,2] to result. Return. Undo swap A[1],A[2] -\u0026gt; A=[1,2,3]. d_p(1) returns. Undo swap A[0],A[0]. A=[1,2,3]. i = 1: A=[1,2,3]. Swap A[0],A[1] -\u0026gt; A=[2,1,3]. Recurse d_p(1) with A=[2,1,3]. idx_to_fill = 1: (Processing A=[2,_,_]) i = 1: A=[2,1,3]. Swap A[1],A[1]. Recurse d_p(2) with A=[2,1,3]. Add [2,1,3] to result. Undo. i = 2: A=[2,1,3]. Swap A[1],A[2] -\u0026gt; A=[2,3,1]. Recurse d_p(2) with A=[2,3,1]. Add [2,3,1] to result. Undo. d_p(1) returns. Undo swap A[0],A[1] -\u0026gt; A=[1,2,3]. i = 2: A=[1,2,3]. Swap A[0],A[2] -\u0026gt; A=[3,2,1]. Recurse d_p(1) with A=[3,2,1]. (generates [3,2,1] and [3,1,2]) Undo. d_p(0) returns. Complexity:\nThere are N! permutations. Each permutation is of length N. Generating it and copying it takes O(N). The recursion tree has roughly N! leaf nodes. The path to each leaf involves N levels of recursion. The work done at non-leaf nodes involves a loop and swaps. Time: O(N * N!) because we spend O(N) work (loop and swaps) along the path to each of the N! permutations, and then O(N) to copy the result. More precisely, the number of recursive calls C(N) is N * C(N-1), leading to N! calls roughly. Each call involves a loop. The book derives it as sum(N! / (N-k)!) calls, which is O(N!). Space: O(N) for the recursion call stack. O(N * N!) if storing all results. The \u0026ldquo;swap, recurse, swap back\u0026rdquo; is the classic pattern for generating permutations in place.\n15.4 GENERATE THE POWER SET (Page 224) Problem: Given a set S (represented as a list/array for ordering purposes), return its power set. The power set is the set of all possible subsets of S, including the empty set and S itself.\nExample: If S = {0, 1, 2}, its power set is: {{}, {0}, {1}, {2}, {0,1}, {0,2}, {1,2}, {0,1,2}}\nCore Idea (Recursive Backtracking - \u0026ldquo;Take it or Leave it\u0026rdquo;): For each element in the input set, we have two choices:\nInclude the element in the current subset we are building. Do NOT include the element in the current subset we are building. We explore both choices recursively.\nRecursive Logic: generate_power_set_recursive(index_of_element_to_consider, current_subset_being_built)\nindex_of_element_to_consider: The index of the element in the input set S that we are currently deciding whether to include or not. current_subset_being_built: The subset formed so far based on decisions for elements S[0] through S[index-1]. Base Case:\nIf index_of_element_to_consider == len(S): We have made a decision (include/exclude) for every element in the input set S. Action: The current_subset_being_built is now one complete subset of S. Add a copy of it to our list of all subsets (the power set). Return. Recursive Step (For S[index_of_element_to_consider]):\nLet element = S[index_of_element_to_consider]. Choice 1: EXCLUDE element Don\u0026rsquo;t add element to current_subset_being_built. Recursively call generate_power_set_recursive(index_of_element_to_consider + 1, current_subset_being_built). (Move to decide for the next element). Choice 2: INCLUDE element \u0026ldquo;Place\u0026rdquo;/Choose: Add element to current_subset_being_built. Recursively call generate_power_set_recursive(index_of_element_to_consider + 1, current_subset_being_built). (Move to decide for the next element, with the current one included). \u0026ldquo;Undo\u0026rdquo;/Backtrack: Remove element from current_subset_being_built. This is crucial so that when the \u0026ldquo;EXCLUDE\u0026rdquo; path for the next higher level of recursion (or other branches) is taken, current_subset_being_built is in the correct state. EPI\u0026rsquo;s directed_power_set(to_be_selected, selected_so_far) (Page 225):\nto_be_selected: Corresponds to my index_of_element_to_consider. selected_so_far: Corresponds to my current_subset_being_built. Example: input_set = [0, 1], directed_power_set_recursive(0, [])\ndps(0, []): (Deciding for element 0) Exclude 0: Call dps(1, []) dps(1, []): (Deciding for element 1) Exclude 1: Call dps(2, []) dps(2, []): idx(2) == len(2). Base Case! Add [] to results. Return. Back in dps(1, []). Include 1: current_subset.append(1) -\u0026gt; current_subset is [1] Call dps(2, [1]) dps(2, [1]): idx(2) == len(2). Base Case! Add [1] to results. Return. current_subset.pop() -\u0026gt; current_subset is [] dps(1, []) returns. Back in dps(0, []). current_subset is still []. Include 0: current_subset.append(0) -\u0026gt; current_subset is [0] Call dps(1, [0]) dps(1, [0]): (Deciding for element 1) Exclude 1: Call dps(2, [0]) dps(2, [0]): idx(2) == len(2). Base Case! Add [0] to results. Return. Back in dps(1, [0]). Include 1: current_subset.append(1) -\u0026gt; current_subset is [0, 1] Call dps(2, [0, 1]) dps(2, [0, 1]): idx(2) == len(2). Base Case! Add [0, 1] to results. Return. current_subset.pop() -\u0026gt; current_subset is [0] dps(1, [0]) returns. current_subset.pop() -\u0026gt; current_subset is [] dps(0, []) returns. Results: [ [], [1], [0], [0,1] ] (Order might vary based on recursion path, but all subsets are there).\nComplexity:\nThere are 2^N subsets for a set of N elements. Each subset can have up to N elements. Copying a subset takes O(N). The recursion tree has 2^N leaf nodes (each representing a subset). The depth is N. Time: O(N * 2^N) because we generate 2^N subsets, and for each, we might do O(N) work (e.g., appending to list, copying at the end). The number of nodes in the decision tree is 2^(N+1) - 1. Space: O(N) for the recursion call stack (depth N) and O(N) for current_subset. If storing all results, then O(N * 2^N). Alternative Iterative Approach (Bit Manipulation - Page 225):\nIf N is small (e.g., \u0026lt;= 64), each integer from 0 to 2^N - 1 can represent a subset. The k-th bit of the integer corresponds to the k-th element of the set. If the bit is 1, include the element; if 0, exclude. Example: S = {a,b,c} (N=3) 0 (000_2) -\u0026gt; {} 1 (001_2) -\u0026gt; {c} (assuming c is 0th, b is 1st, a is 2nd if mapping right to left) 2 (010_2) -\u0026gt; {b} \u0026hellip; 7 (111_2) -\u0026gt; {a,b,c} This is often very fast in practice. Complexity is O(N * 2^N) because you iterate 2^N numbers, and for each, you might iterate up to N bits to construct the subset. This \u0026ldquo;take it or leave it\u0026rdquo; recursive pattern is fundamental for many subset, combination, and related problems.\n15.5 GENERATE ALL SUBSETS OF SIZE k (Page 226) Problem: Given n and k, compute all subsets of size k from the set {1, 2, ..., n}. (The book uses {1,2,...,n} for the example, but the code can be adapted for any input set if needed by passing the set and an offset). This is also known as generating combinations: \u0026ldquo;n choose k\u0026rdquo;.\nExample: n=5, k=2. Subsets of size 2 from {1,2,3,4,5}: {{1,2}, {1,3}, {1,4}, {1,5}, {2,3}, {2,4}, {2,5}, {3,4}, {3,5}, {4,5}}\nCore Idea (Recursive Backtracking - Focused \u0026ldquo;Take it or Leave it\u0026rdquo;): We want to build a combination of size k. We can iterate through the numbers from 1 to n. For each number, we decide:\nDo we include this number in our current combination? If we include it, we then need to find k-1 more numbers from the remaining available numbers (i.e., numbers greater than the current one to avoid duplicates and maintain order). If we don\u0026rsquo;t include it, we need to find k numbers from the remaining available numbers. To make it more structured and avoid duplicate combinations (like {1,2} and {2,1} which are the same set), we can enforce that the numbers in our partial_combination are always added in increasing order.\nRecursive Logic: generate_combinations_recursive(start_number_to_consider, current_combination)\nstart_number_to_consider: The smallest number we can currently pick to add to our combination. This ensures we pick numbers in increasing order. current_combination: The list of numbers picked so far for the current combination. Base Case 1 (Combination Complete):\nIf len(current_combination) == k: We have found a combination of the desired size k. Action: Add a copy of current_combination to our list of results. Return. Base Case 2 (Not enough elements left to form a combination of size k):\nIf start_number_to_consider \u0026gt; n and len(current_combination) \u0026lt; k: We\u0026rsquo;ve run out of numbers to pick from, but haven\u0026rsquo;t reached size k. This path is a dead end. Return. More precisely, if the number of remaining elements (n - start_number_to_consider + 1) is less than the number of elements we still need (k - len(current_combination)), then we can\u0026rsquo;t complete the combination. This check can prune branches earlier. (EPI code handles this implicitly with loop bounds). Recursive Step (For numbers starting from start_number_to_consider):\nIterate num_to_add from start_number_to_consider up to n. (Optimization from EPI: num_to_add should not go so far that there aren\u0026rsquo;t enough remaining elements to complete a k-sized combination. The loop in EPI is while i \u0026lt;= n and num_remaining \u0026lt;= n - i + 1: which captures this). a. \u0026ldquo;Place\u0026rdquo;/Choose: Add num_to_add to current_combination. b. Explore/Recurse: Call generate_combinations_recursive(num_to_add + 1, current_combination). We pass num_to_add + 1 as the next start_number_to_consider to ensure subsequent numbers are greater, maintaining sorted order and preventing duplicate combinations. c. \u0026ldquo;Undo\u0026rdquo;/Backtrack: Remove num_to_add from current_combination. This is crucial to allow the for loop to try the next num_to_add for the current position in the combination, or for previous recursive calls to explore different branches. EPI\u0026rsquo;s directed_combinations(offset, partial_combination) (Page 226):\noffset: Corresponds to my start_number_to_consider. (EPI uses 1-based indexing for numbers, so offset starts at 1). partial_combination: Corresponds to my current_combination. Example: n=4, k=2, directed_combinations_recursive(1, [])\ndcr(1, []): needed=2. end_loop_at = 4-2+1 = 3. Loop num_to_add from 1 to 3. num_to_add = 1: combo=[1]. Call dcr(2, [1]). dcr(2, [1]): needed=1. end_loop_at = 4-1+1 = 4. Loop num_to_add_inner from 2 to 4. num_to_add_inner = 2: combo=[1,2]. Call dcr(3, [1,2]). dcr(3, [1,2]): len==k. Base! Add [1,2]. Return. combo.pop() -\u0026gt; [1]. num_to_add_inner = 3: combo=[1,3]. Call dcr(4, [1,3]). dcr(4, [1,3]): len==k. Base! Add [1,3]. Return. combo.pop() -\u0026gt; [1]. num_to_add_inner = 4: combo=[1,4]. Call dcr(5, [1,4]). dcr(5, [1,4]): len==k. Base! Add [1,4]. Return. combo.pop() -\u0026gt; [1]. Loop ends. dcr(2, [1]) returns. combo.pop() -\u0026gt; []. num_to_add = 2: combo=[2]. Call dcr(3, [2]). dcr(3, [2]): needed=1. end_loop_at = 4-1+1 = 4. Loop num_to_add_inner from 3 to 4. num_to_add_inner = 3: combo=[2,3]. Call dcr(4, [2,3]). Add [2,3]. combo.pop() -\u0026gt; [2]. num_to_add_inner = 4: combo=[2,4]. Call dcr(5, [2,4]). Add [2,4]. combo.pop() -\u0026gt; [2]. Loop ends. dcr(3, [2]) returns. combo.pop() -\u0026gt; []. num_to_add = 3: combo=[3]. Call dcr(4, [3]). dcr(4, [3]): needed=1. end_loop_at = 4-1+1 = 4. Loop num_to_add_inner from 4 to 4. num_to_add_inner = 4: combo=[3,4]. Call dcr(5, [3,4]). Add [3,4]. combo.pop() -\u0026gt; [3]. Loop ends. dcr(4, [3]) returns. combo.pop() -\u0026gt; []. Loop ends. dcr(1, []) returns. Results: [[1,2], [1,3], [1,4], [2,3], [2,4], [3,4]]\nComplexity:\nNumber of combinations is \u0026ldquo;n choose k\u0026rdquo;, denoted C(n,k) = n! / (k! * (n-k)!). Each combination has k elements. Time: O(k * C(n,k)) because we generate C(n,k) combinations, and forming/copying each takes about O(k). The recursion depth is k. Space: O(k) for the recursion stack and current_combo. O(k * C(n,k)) if storing all results. This pattern is very similar to the power set, but with the added constraint on the size k and the optimization of only picking numbers greater than the previously picked one to ensure unique combinations.\n","permalink":"https://deepskandpal.github.io/bookshelf/elements-of-programming/chapter-15/","tags":null,"title":"Chapter 15: Recursion"},{"categories":null,"content":"The World of Sequences The chapter opens with a vivid example: an outfielder catching a ball. This involves:\nAnticipating the future: Predicting the ball\u0026rsquo;s trajectory. Adapting to new information: Tracking the ball and adjusting movements. This ability to process information over time and predict what comes next is something humans do constantly (finishing a friend\u0026rsquo;s sentence, smelling coffee brewing). This chapter introduces Recurrent Neural Networks (RNNs) as a class of neural networks designed for precisely this kind of task.\nWhat RNNs are good for:\nAnalyzing time series data (e.g., stock prices, weather patterns). Anticipating trajectories (e.g., in autonomous driving). Working on sequences of arbitrary lengths (unlike the fixed-sized inputs we\u0026rsquo;ve seen so far with MLPs and standard CNNs). This is a key differentiator. Processing sentences, documents, or audio samples. Hugely important for Natural Language Processing (NLP) tasks like automatic translation or speech-to-text. Chapter Focus (for Chapter 15):\nFundamental concepts of RNNs. How to train them using backpropagation through time (BPTT). Using RNNs to forecast a time series. Exploring two main difficulties RNNs face: Unstable gradients (vanishing/exploding, as discussed in Chapter 11), and techniques to alleviate them (recurrent dropout, recurrent layer normalization). Limited short-term memory, and how to extend it using LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) cells. Alternatives to RNNs for Sequential Data (Page 497, bottom):\nThe chapter acknowledges that RNNs aren\u0026rsquo;t the only way to handle sequential data. For small sequences, a regular dense network (MLP) might suffice. For very long sequences (audio, long text), Convolutional Neural Networks (CNNs) can also work surprisingly well. The chapter will discuss this and implement WaveNet, a CNN architecture for long sequences. (Chapter 16 will continue with RNNs for NLP and then move to attention mechanisms). What this chapter is ultimately trying to achieve: To introduce a new type of neural network architecture capable of understanding and making predictions based on ordered sequences of data, where the order and context over time are crucial.*\nRecurrent Neurons and Layers Up to now, we\u0026rsquo;ve focused on feedforward neural networks, where activations flow in one direction: input -\u0026gt; hidden layers -\u0026gt; output.\nRecurrent Neural Network (RNN) Difference: An RNN looks like a feedforward network, but it also has connections pointing backward. This creates a loop, allowing information to persist.\nSimplest RNN: A Single Recurrent Neuron (Figure 15-1, page 498):\nLeft diagram (rolled): A single neuron receives an input x, produces an output y, and importantly, sends its own output back to itself as an input for the next step. At each time step t (also called a frame): The recurrent neuron receives the current external input x(t). It also receives its own output from the previous time step, y(t-1). This y(t-1) is the \u0026ldquo;memory\u0026rdquo; or \u0026ldquo;state\u0026rdquo; from the past. At the very first time step (t=0), there\u0026rsquo;s no previous output, so y(-1) is generally initialized to 0. Right diagram (unrolled through time): This is a crucial visualization. It shows the same recurrent neuron being represented at different points in time (t-3, t-2, t-1, t). At time t, the neuron takes x(t) and the output from itself at t-1 (which is y(t-1)) to produce y(t). This \u0026ldquo;unrolling\u0026rdquo; helps visualize how information flows and how gradients will be calculated. What this recurrent connection is ultimately trying to achieve: It allows the neuron to maintain a \u0026ldquo;state\u0026rdquo; or \u0026ldquo;memory\u0026rdquo; of past inputs, influencing its current output based not just on the current input but also on what it has seen before. Layer of Recurrent Neurons (Figure 15-2, page 499):\nYou can have a whole layer of these recurrent neurons.\nAt each time step t:\nEvery neuron in the layer receives the entire input vector x(t). Every neuron also receives the entire output vector y(t-1) from all neurons in this same layer at the previous time step. Weights: Each recurrent neuron (or the layer as a whole) now has two sets of weights:\nWₓ: Weights for the current inputs x(t). Wᵧ: Weights for the outputs from the previous time step y(t-1). Equation 15-1: Output of a recurrent layer for a single instance at time t: y(t) = φ( Wₓᵀx(t) + Wᵧᵀy(t-1) + b )\nφ is the activation function (e.g., tanh is common, ReLU can be used but needs care as mentioned in Ch 11). b is the bias vector. What this equation is ultimately trying to achieve: It formalizes how the current output y(t) is a function of both the current input x(t) and the collective state/output of the layer from the previous time step y(t-1). Equation 15-2: Output for a mini-batch: Y(t) = φ( X(t)Wₓ + Y(t-1)Wᵧ + b ) This is the vectorized form for a whole mini-batch X(t).\nY(t): Matrix of outputs for all instances in the batch at time step t. X(t): Matrix of inputs for all instances at time step t. Y(t-1): Matrix of outputs from the previous time step. The equation also shows a common convention: Wₓ and Wᵧ are often concatenated into a single weight matrix W, and inputs [X(t) Y(t-1)] are concatenated before multiplication. Memory and State (Page 500):\nSince the output Y(t) depends on all inputs from X(0) up to X(t), the RNN has a form of memory. A part of a neural network that preserves some state across time steps is called a memory cell (or simply a \u0026ldquo;cell\u0026rdquo;). A single recurrent neuron or a layer of them is a very basic cell, typically capable of learning only short patterns (e.g., ~10 steps). More complex cells (LSTM, GRU) can learn longer patterns. Cell State h(t): In general, a cell\u0026rsquo;s state at time t is denoted h(t) (\u0026ldquo;h\u0026rdquo; for hidden state). It\u0026rsquo;s a function of the previous state h(t-1) and the current input x(t): h(t) = f(h(t-1), x(t)). Output y(t): The cell\u0026rsquo;s output y(t) is also a function of the previous state and current inputs. Figure 15-3: For basic recurrent cells like SimpleRNN in Keras, the output y(t) is simply equal to the hidden state h(t). However, for more complex cells like LSTMs (which we\u0026rsquo;ll see later), the hidden state h(t) and the output y(t) can be different. The cell might maintain an internal state that is more complex than what it chooses to output at each time step. Input and Output Sequences (Figure 15-4, page 501): RNNs can handle various types of input/output sequence configurations:\nSequence-to-Sequence (Top-Left): Input: A sequence of vectors X(0), X(1), ..., X(T). Output: A sequence of vectors Y(0), Y(1), ..., Y(T). Example: Predicting stock prices (input last N days\u0026rsquo; prices, output prices shifted one day into the future). What it\u0026rsquo;s ultimately trying to achieve: Transform an input sequence into an output sequence, often for prediction or transduction. Sequence-to-Vector (Top-Right): Input: A sequence of vectors. Output: A single vector (usually from the last time step\u0026rsquo;s output). All other outputs are ignored. Example: Sentiment analysis of a movie review (input sequence of words, output a single sentiment score like -1 to +1). What it\u0026rsquo;s ultimately trying to achieve: Summarize or classify an entire input sequence into a single vector representation or decision. Vector-to-Sequence (Bottom-Left): Input: A single fixed vector, fed repeatedly at each time step (or just at the first time step with subsequent inputs being zero). Output: A sequence of vectors. Example: Image captioning (input an image or its CNN feature vector, output a sequence of words forming a caption). What it\u0026rsquo;s ultimately trying to achieve: Generate a sequence based on a static input. Encoder-Decoder (Bottom-Right): A sequence-to-vector network (the encoder) is followed by a vector-to-sequence network (the decoder). Example: Neural Machine Translation (NMT). Encoder reads an input sentence (e.g., English) and compresses it into a single vector representation (often called the \u0026ldquo;context vector\u0026rdquo; or \u0026ldquo;thought vector\u0026rdquo;). Decoder takes this context vector and generates an output sentence (e.g., French), word by word. This two-step model generally works much better for tasks like translation than a single sequence-to-sequence RNN because the meaning of the whole input sentence needs to be captured before starting to generate the translation (the end of an English sentence can affect the beginning of its French translation). (More complex Encoder-Decoder details in Chapter 16). The core idea so far is that recurrent neurons have a \u0026ldquo;loop\u0026rdquo; that allows them to pass their previous output (their state) as an input to their next computation, enabling them to process sequences and maintain a memory of past information.\nThe next logical step in the chapter is Training RNNs (Page 502).\nTraining RNNs - Backpropagation Through Time (BPTT) So, we have this recurrent structure. How do we train it? How do we adjust Wₓ, Wᵧ, and b?\nThe Trick: Unroll and Use Regular Backpropagation:\nThe core idea is surprisingly simple: Unroll the RNN through time for the length of your input sequences (as we saw in Figure 15-1 and 15-2). When you unroll it, it looks just like a very deep feedforward network, where each \u0026ldquo;time slice\u0026rdquo; of the RNN becomes a layer in this unrolled network. Crucially, the weights (Wₓ, Wᵧ, b) are shared across all these time-step \u0026ldquo;layers\u0026rdquo; in the unrolled network. The Wₓ used at time t=1 is the same Wₓ used at t=2, t=3, etc. Then, you simply apply regular backpropagation (as discussed in Chapter 10) to this unrolled network. This strategy is called Backpropagation Through Time (BPTT). The BPTT Process (Figure 15-5, page 503):\nForward Pass: Feed the input sequence through the unrolled network, time step by time step (dashed arrows in the figure). At each time step, calculate the outputs Y(0), Y(1), ..., Y(T) (where T is the maximum time step or sequence length). Evaluate Loss: The output sequence Y(0)...Y(T) is evaluated using a cost function C(Y(0), ..., Y(T)). Important Note: The cost function might ignore some outputs. For example: In a sequence-to-vector RNN (like sentiment analysis), the cost function might only care about the very last output Y(T). In Figure 15-5, the cost function C(Y(2), Y(3), Y(4)) uses only the last three outputs. Backward Pass: The gradients of the cost function are then propagated backward through the unrolled network (solid arrows in the figure). Gradients flow backward through all the outputs that were used by the cost function. In Figure 15-5, gradients flow from Y(4), Y(3), and Y(2). They do not flow from Y(1) or Y(0) because those were not part of the cost calculation in this specific example. Update Parameters: Since the same parameters Wₓ, Wᵧ, and b are used at each time step in the unrolled network, the gradients calculated for these parameters at each time step are summed up (or averaged) across all time steps. These aggregated gradients are then used to update Wₓ, Wᵧ, and b using a standard gradient descent step. What this summing of gradients across time achieves: It ensures that the parameter updates reflect how those parameters contributed to the error across the entire sequence processing, not just at one particular time step. Keras Handles the Complexity:\nThe good news is that tf.keras takes care of all this BPTT complexity for you when you use its recurrent layers! You define the recurrent layer, and during fit(), Keras handles the unrolling, BPTT, and parameter updates. Key Takeaway for BPTT: BPTT is the standard way to train RNNs. It cleverly transforms the problem of training a network with loops into training a deep feedforward network (the unrolled version) where weights are shared across \u0026ldquo;layers\u0026rdquo; (time steps). Regular backpropagation can then be applied, with the understanding that gradients for shared weights are aggregated across all time steps.\nThis is a foundational concept for training any kind of recurrent network. Next, the chapter moves into a practical example: forecasting a time series.\nHow does the idea of \u0026ldquo;unrolling in time\u0026rdquo; and then applying standard backpropagation feel as a strategy for training RNNs?\nExcellent! It\u0026rsquo;s good that BPTT makes conceptual sense. It\u0026rsquo;s a clever way to adapt a known algorithm (backpropagation) to a new type of network architecture (RNNs).\nNow, let\u0026rsquo;s dive into the first practical application in Chapter 15: Forecasting a Time Series (Pages 503-511). This section will also involve Keras implementations, but we\u0026rsquo;ll continue to focus on the \u0026ldquo;what are we trying to achieve\u0026rdquo; for the RNN-specific concepts and the data handling.\nForecasting a Time Series - The Task What is a Time Series? A sequence of one or more values per time step. Univariate time series: Single value per time step (e.g., daily temperature in your city, active users per hour on a website). Multivariate time series: Multiple values per time step (e.g., a company\u0026rsquo;s quarterly revenue, debt, profit, etc.). Common Tasks: Forecasting: Predicting future values. Imputation: Predicting (or \u0026ldquo;postdicting\u0026rdquo;) missing values from the past. Example for this section (Figure 15-6, page 504): Three univariate time series, each 50 time steps long. Goal: Forecast the value at the next time step (the \u0026lsquo;X\u0026rsquo; in the figure) for each of them. Generating Time Series Data For simplicity, the book uses a generate_time_series() function to create synthetic data.\nIt creates batch_size time series, each of length n_steps. Each series is univariate (one value per time step). Each series is a sum of two sine waves with random frequencies and phases, plus some noise. The function returns a NumPy array of shape [batch_size, time_steps, 1] (the last dimension is for the number of features per time step, which is 1 for univariate). Input Data Shape for RNNs (Bird Icon, page 504):\nWhen dealing with sequences, input features are generally represented as 3D arrays: [batch_size, time_steps, dimensionality] dimensionality: 1 for univariate time series, more for multivariate. Creating Train/Validation/Test Sets:\nn_steps = 50 Generate 10,000 series, each n_steps + 1 long (the extra step is for the target). series = generate_time_series(10000, n_steps + 1) Split the data: X_train: First 7000 series, first n_steps time steps each [:7000, :n_steps] y_train: First 7000 series, the last time step each [:7000, -1] (this is the value we want to predict after seeing n_steps). Similarly for X_valid (next 2000) and X_test (last 1000). Shapes: X_train is . `y_train` is . Baseline Metrics Before building complex RNNs, it\u0026rsquo;s crucial to establish some simple baselines.\nWhat baselines are ultimately trying to achieve: They give you a simple reference point. If your fancy RNN can\u0026rsquo;t beat a very basic model, something is wrong, or the task might be too simple for an RNN, or the basic model is surprisingly effective. Naive Forecasting:\nPredict the last observed value in each series as the next value. y_pred_naive = X_valid[:, -1, :] (Take the last time step from each validation sequence). Calculate Mean Squared Error (MSE): np.mean(keras.losses.mean_squared_error(y_valid, y_pred_naive)) Result: ~0.020 MSE. Simple Linear Regression Model:\nUse a fully connected network (a Dense layer) with a linear activation. Since Dense layers expect a flat list of features, add a Flatten layer first. model_linear = keras.models.Sequential([ keras.layers.Flatten(input_shape=[50, 1]), # Input: 50 time steps, 1 feature each keras.layers.Dense(1) # Output: 1 predicted value ]) Compile with MSE loss and Adam optimizer. Train for 20 epochs. Result: ~0.004 MSE. Much better than naive forecasting! Implementing a Simple RNN Now let\u0026rsquo;s try a very basic RNN. Can it beat the linear model?\nmodel_simple_rnn = keras.models.Sequential([ keras.layers.SimpleRNN(1, input_shape=[None, 1]) # Single layer, single neuron ]) keras.layers.SimpleRNN(1, ...): Creates a single recurrent layer with just one recurrent neuron.\ninput_shape=[None, 1]:\nNone: We don\u0026rsquo;t need to specify the length of the input sequences (n_steps). RNNs can process sequences of arbitrary length. This is a major advantage. 1: Dimensionality of the input at each time step (univariate). Default Behavior of SimpleRNN:\nUses tanh (hyperbolic tangent) as the activation function. Returns only the final output: By default, recurrent layers in Keras only return the output of the last time step. If our input sequence has 50 steps (y(0) to y(49)), this layer will output y(49). (Bird icon, page 506). This is exactly what we want for this forecasting task (predicting the value at the next step after seeing n_steps). How it works (conceptually, as in Figure 15-1):\nInitial hidden state h_init is set to 0. For t=0: Neuron takes x(0) and h_init, computes h(0) (which is also y(0) for SimpleRNN). For t=1: Neuron takes x(1) and h(0), computes h(1) (which is also y(1)). \u0026hellip; For t=49: Neuron takes x(49) and h(48), computes h(49) (which is also y(49)). The layer outputs this final y(49). Training: Compile with MSE, Adam optimizer, train for 20 epochs.\nResult: MSE of ~0.014.\nBetter than naive forecasting (0.020). But worse than the simple linear model (0.004)! Why is it worse? Parameters:\nThe linear model had 50 inputs * 1 weight_per_input + 1 bias = 51 parameters. A simple RNN neuron has: Weights for current input x(t) (vector Wₓ, if x(t) is a vector). Here, x(t) is scalar, so 1 weight. Weights for previous hidden state h(t-1) (vector Wᵧ, if h(t-1) is a vector). Here, h(t-1) is scalar (output of one neuron), so 1 weight. One bias term. Total: 3 parameters for this single SimpleRNN(1) neuron. With only 3 parameters, it\u0026rsquo;s much less powerful than the 51-parameter linear model for this task. Trend and Seasonality (Sidebar, page 506):\nTraditional time series models (like ARIMA) often require you to manually remove trends (e.g., 10% monthly growth) and seasonality (e.g., sunscreen sales peak in summer) from the data before training. After prediction, you add them back. With RNNs, this is generally not necessary. The RNN can learn trends and seasonality if they exist in the data. However, preprocessing to remove them might sometimes improve performance, as the model then has a simpler pattern to learn. Deep RNNs The simple RNN with one neuron was too simple. Let\u0026rsquo;s try stacking multiple layers of cells – a deep RNN (Figure 15-7, page 507).\nKeras Implementation: Just stack recurrent layers.\nmodel_deep_rnn = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20, return_sequences=True), # Second layer keras.layers.SimpleRNN(1) # Output layer ]) First SimpleRNN(20, return_sequences=True, ...): 20 recurrent neurons (units). return_sequences=True: This is crucial! By default, a recurrent layer only outputs the hidden state of the last time step. For stacking, the next recurrent layer needs to receive a full sequence of outputs from the current layer (one output vector per time step). Setting return_sequences=True makes it output the hidden state at every time step. So, if input is 50 steps, output is 50 steps (each step being a 20-dim vector). Second SimpleRNN(20, return_sequences=True): Also 20 units. Also returns sequences to feed into what would be the next layer. (The scorpion icon on page 507 emphasizes setting return_sequences=True for all recurrent layers except possibly the last one if you only care about its final output). Third SimpleRNN(1): The final layer has 1 unit (to predict a single value). It does not have return_sequences=True (or it\u0026rsquo;s False by default), so it will only output the value from the very last time step. Training: Compile with MSE, Adam optimizer, train.\nResult: MSE of ~0.003! We finally beat the linear model (0.004).\nImproving the Output Layer (Page 507):\nThe last SimpleRNN(1) layer is not ideal: Its hidden state is just a single number, which isn\u0026rsquo;t much memory. The model likely relies more on the hidden states of the previous 20-unit layers. SimpleRNN uses tanh by default, so predictions are bounded between -1 and 1. What if our time series values go outside this range? Better approach: Replace the final recurrent layer with a Dense layer. model_deep_rnn_dense_output = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20), # Now this is the last recurrent layer, so no return_sequences=True keras.layers.Dense(1) ]) The second SimpleRNN now only outputs its final state (a 20-dim vector). The Dense(1) layer takes this 20-dim vector and outputs a single value. Benefits: Dense layer is often faster. No tanh constraint on the output value (unless you add an activation to the Dense layer). This model converges faster and performs just as well or better. Forecasting Several Time Steps Ahead So far, we\u0026rsquo;ve only predicted the very next value (t+1). What if we want to predict multiple steps ahead (e.g., t+1 to t+10)?\nOption 1: Iterative Predictions (Predict one step, use it as input for next, etc. - Page 508)\nUse one of the models trained to predict just one step ahead. To predict 10 steps: Feed the last n_steps of known data to the model, get prediction for step n_steps+1. Take this prediction, append it to your known sequence (as if it were a true value). Feed the new last n_steps (which now includes the first prediction) to the model to get prediction for n_steps+2. Repeat 10 times. Code Example: # series is [1, n_steps + 10, 1] # X_new is initial sequence [1, n_steps, 1] # Y_new is target future sequence [1, 10, 1] X = X_new for step_ahead in range(10): y_pred_one = model.predict(X[:, step_ahead:]) # Predict one step X = np.concatenate([X, y_pred_one[:, np.newaxis, :]], axis=1) # Append prediction Y_pred_iterative = X[:, n_steps:] # The 10 appended predictions Problem: Errors can accumulate. If the prediction for step 1 is slightly off, it makes the input for predicting step 2 slightly off, and so on. (Figure 15-8, page 509, shows this). Result: MSE on validation set is ~0.029. Much higher than previous models, but it\u0026rsquo;s a harder task. Naive forecast (predict last value for all 10 future steps) gives MSE ~0.223 (terrible). A simple linear model (trained to predict 10 steps ahead) gives MSE ~0.0188 – much better than this iterative RNN approach and faster to train. When it might work: For a few steps ahead, or for complex tasks where linear models fail, this iterative approach might be okay. Option 2: Train an RNN to Predict All Future Values at Once (Sequence-to-Vector - Page 509)\nChange the targets: Instead of y_train being the value at n_steps+1, make Y_train a vector of the 10 values from n_steps+1 to n_steps+10. Y_train = series[:7000, -10:, 0] (if series has n_steps + 10 length). The last dimension 0 is to flatten it if the series had multiple features per step. If it\u0026rsquo;s already [batch, 10], no need. The book actually means the targets are series[:7000, n_steps:n_steps+10, 0]. Let\u0026rsquo;s assume the targets are reshaped to [batch_size, 10]. Modify the model: Output layer now needs 10 units (instead of 1). model_seq_to_vec_10 = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20), # Last RNN outputs its final state (20-dim vector) keras.layers.Dense(10) # Dense layer predicts 10 values ]) Result: MSE for next 10 steps is ~0.008. Much better than the linear model (0.0188)! Option 3: Train an RNN to Predict Future Values at Each Time Step (Sequence-to-Sequence - Page 509-510) This is even better. Instead of only forecasting the next 10 values at the very last time step, train the model to forecast the next 10 values at each and every time step of the input.\nTargets Y (Page 510):\nFor each input sequence X(0)...X(n_steps-1), the target Y needs to be a sequence of the same length. At time step t of the input, the target Y(t) is a vector of the actual future values [actual(t+1), actual(t+2), ..., actual(t+10)]. The code snippet shows how to construct this Y array of shape [batch_size, n_steps, 10]. Causal Model (Bird Icon, page 510):\nThe targets Y_train will contain values that also appear in X_train (e.g., X_train[0, t+1] is part of the target for X_train[0,t]). Is this cheating? No, because at each time step t, the model only knows about past time steps up to t. It cannot look ahead in the input X to see X(t+1) when predicting the target for Y(t). It\u0026rsquo;s a causal model. Model Architecture (Sequence-to-Sequence - Page 510):\nAll recurrent layers must have return_sequences=True (even the last recurrent one). The output Dense(10) layer must be applied at every time step. Keras offers a TimeDistributed layer wrapper for this. model_seq_to_seq = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20, return_sequences=True), # Both RNNs return sequences keras.layers.TimeDistributed(keras.layers.Dense(10)) # Apply Dense(10) at each time step ]) TimeDistributed(Dense(10)): Wraps the Dense(10) layer. It takes the input sequence from the previous SimpleRNN (shape [batch_size, time_steps, 20]). It reshapes it to [batch_size * time_steps, 20], applies the Dense(10) layer (getting [batch_size * time_steps, 10]), and then reshapes it back to [batch_size, time_steps, 10]. The footnote 2 on page 510 mentions that a Dense layer itself can often handle sequence inputs correctly (applying to the last dimension independently across time steps), making TimeDistributed(Dense(...)) sometimes redundant. However, TimeDistributed makes the intent very clear. Loss and Evaluation (Page 511):\nDuring training, all outputs are needed, so MSE over all outputs is fine. For prediction and final evaluation, often only the output at the last time step is useful (i.e., given the full input sequence X(0)...X(n_steps-1), what\u0026rsquo;s the forecast for X(n_steps)...X(n_steps+9)?). A custom metric last_time_step_mse is defined to compute MSE only on the output at the last time step. model.compile(loss=\u0026quot;mse\u0026quot;, optimizer=\u0026quot;adam\u0026quot;, metrics=[last_time_step_mse]) Result: Validation MSE (for the last time step\u0026rsquo;s 10-step forecast) is ~0.006. This is 25% better than the sequence-to-vector model (0.008)!\nWhy it\u0026rsquo;s better:\nThe loss includes terms for the RNN\u0026rsquo;s output at every time step. This means many more error gradients flow through the model during BPTT. Gradients don\u0026rsquo;t just flow through time (horizontally in unrolled view); they also flow from the output of each time step (vertically). This stabilizes and speeds up training. Generating Arbitrarily Long Sequences (Page 511): You can combine this sequence-to-sequence approach (predicting next 10 values) with the iterative approach (feed predictions back as input) to generate very long sequences (e.g., for music or text generation, Chapter 16). May not be accurate for long-term forecasting but fine for generation.\nError Bars with MC Dropout (Bird Icon, page 511): For forecasting, it\u0026rsquo;s useful to have error bars. MC Dropout (from Chapter 11) can be used: add MC Dropout layers within recurrent cells, then at inference, run the model many times with dropout active and compute mean/std dev of predictions.\nHandling Long Sequences - The Problems The chapter states: \u0026ldquo;Simple RNNs can be quite good at forecasting time series or handling other kinds of sequences, but they do not perform as well on long time series or sequences.\u0026rdquo;\nWhy? To train an RNN on long sequences, we must run it over many time steps. This means the unrolled RNN becomes a very deep network. Problems with Deep Unrolled RNNs: Unstable Gradients Problem (Vanishing/Exploding): Just like any deep neural network (as discussed in Chapter 11), it may suffer from gradients becoming too small or too large as they propagate back through many time steps. This makes training very slow or unstable. Forgetting Early Inputs (Limited Short-Term Memory): As an RNN processes a long sequence, the information from the earlier time steps has to be carried through many transformations in the hidden state. Due to these repeated transformations (and often the \u0026ldquo;squashing\u0026rdquo; nature of activation functions like tanh), the information from the initial inputs tends to get diluted or lost. The RNN\u0026rsquo;s state effectively \u0026ldquo;forgets\u0026rdquo; what happened much earlier in the sequence. This section will address both these problems.\nFighting the Unstable Gradients Problem in RNNs Many tricks from Chapter 11 for deep feedforward nets can also be used for RNNs:\nGood parameter initialization (Glorot, He). Faster optimizers (Adam, Nadam). Dropout (with caveats). However, some techniques behave differently or have limitations with RNNs:\nNonsaturating Activation Functions (e.g., ReLU):\nMay not help as much with unstable gradients in RNNs and can even make them more unstable. Why? The same weights (Wₓ, Wᵧ) are applied at every time step. If GD updates these weights in a way that slightly increases the outputs at time step 1, this effect can compound at time step 2, then time step 3, and so on, potentially leading to outputs exploding. A nonsaturating activation function (like ReLU) doesn\u0026rsquo;t prevent this explosion for positive values. Solution/Preference: A saturating activation function like tanh is often the default and preferred choice for simple RNN cells because its bounded output (-1 to 1) helps control the explosion of activations. The vanishing gradient issue (due to tanh saturation) is then tackled by other means (like LSTMs/GRUs or a smaller learning rate). If training is unstable (gradients exploding), monitor gradient sizes (e.g., with TensorBoard) and perhaps use Gradient Clipping (as discussed in Chapter 11). Batch Normalization (BN) with RNNs:\nCannot be used as efficiently with RNNs as with deep feedforward nets. You cannot use standard BN between time steps (i.e., to normalize the hidden state h(t) before it\u0026rsquo;s fed into the next time step t+1 along with x(t+1)). This is because BN\u0026rsquo;s moving averages for mean/std dev are calculated per feature across a batch, and the statistics of the hidden state can vary significantly from time step to time step. Applying the same BN parameters (learned across all time steps) to each step doesn\u0026rsquo;t work well. Where it can be used: It is technically possible to add a BN layer inside a memory cell so it\u0026rsquo;s applied at each time step (to both x(t) and h(t-1) inputs to the cell). However, the same BN parameters (γ, β, and the moving averages μ, σ) would be used at each time step, regardless of the actual scale/offset of inputs at that step. A 2015 paper by Laurent et al. (footnote 3) found this didn\u0026rsquo;t yield good results when applied to hidden states, only slightly beneficial when applied to the inputs x(t). What works (somewhat): Apply BN between recurrent layers (i.e., \u0026ldquo;vertically\u0026rdquo; in the unrolled view of Figure 15-7, not \u0026ldquo;horizontally\u0026rdquo; across time steps). You can do this in Keras by adding a BatchNormalization layer before each recurrent layer. Don\u0026rsquo;t expect miracles, but it might help a bit. Layer Normalization (LN) with RNNs (Page 512):\nIntroduced by Ba et al. in a 2016 paper (footnote 4). Often works better than BN with RNNs.\nKey Difference from BN:\nBN normalizes across the batch dimension (for each feature, calculate mean/std over all instances in the batch). LN normalizes across the features dimension (for each instance, calculate mean/std over all features/units at the current time step). Advantages for RNNs:\nLN can compute its required statistics (mean/std for normalization) on the fly at each time step, independently for each instance. It doesn\u0026rsquo;t need to average over a batch or estimate population stats with moving averages. This means it behaves the same way during training and testing. Like BN, LN learns scale (γ) and offset (β) parameters per layer.\nIn an RNN, it\u0026rsquo;s typically used right after the linear combination of inputs x(t) and hidden states h(t-1), before the activation function.\nImplementing Layer Normalization in a Custom Keras Cell (Page 513): The book shows how to create a custom RNN cell (LNSimpleRNNCell) that incorporates Layer Normalization.\nIt inherits from keras.layers.Layer. The constructor (__init__) sets up: self.state_size and self.output_size (both equal to units for a simple RNN cell). An internal keras.layers.SimpleRNNCell(units, activation=None) (note: activation=None because LN will happen before the final activation). A keras.layers.LayerNormalization() layer. The desired final activation function (e.g., keras.activations.get(\u0026quot;tanh\u0026quot;)). The call(self, inputs, states) method (which defines the cell\u0026rsquo;s logic for one time step): outputs, new_states = self.simple_rnn_cell(inputs, states): Get the linear combination from the internal simple RNN cell. (outputs and new_states are the same here). norm_outputs = self.layer_norm(outputs): Apply Layer Normalization to these pre-activation values. activated_norm_outputs = self.activation(norm_outputs): Apply the final activation. Return activated_norm_outputs, [activated_norm_outputs] (output and new state are the same). Using the Custom Cell (Page 514): Wrap this custom cell in a keras.layers.RNN layer: model.add(keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, ...)) The RNN layer handles the unrolling through time, feeding states, etc., using the logic defined in your custom cell. Dropout in RNNs (Page 514):\nYou can apply dropout to the inputs fed to the recurrent layer (at each time step). You can also apply dropout to the hidden states as they are passed between time steps. This is often called recurrent dropout. Most Keras recurrent layers (like SimpleRNN, LSTM, GRU, but not the generic keras.layers.RNN wrapper unless the cell implements it) have two dropout hyperparameters: dropout: Dropout rate for the inputs to the cell. recurrent_dropout: Dropout rate for the hidden states. No need for a custom cell just for dropout if using built-in LSTM/GRU layers. With these techniques (proper initialization, careful choice of activation, Layer Normalization, recurrent dropout, Gradient Clipping), you can significantly alleviate the unstable gradients problem and train RNNs much more effectively, even fairly deep ones.\nKey Takeaway for Fighting Unstable Gradients in RNNs: The same weights being applied repeatedly across time steps makes RNNs particularly susceptible to unstable gradients.\nStandard solutions like good initialization and optimizers help. Nonsaturating activations like ReLU need care; saturating ones like tanh are often safer defaults for basic RNN cells. Batch Normalization is tricky to apply effectively within the recurrence (horizontally); Layer Normalization is generally better. Recurrent dropout and gradient clipping are also valuable tools. What these techniques are ultimately trying to achieve: Create a stable learning environment where gradients can flow effectively through many time steps without vanishing or exploding, allowing the RNN to learn dependencies across time. Tackling the Short-Term Memory Problem - Introduction The Problem:\nWhen an RNN processes a sequence, the data goes through many transformations as the hidden state is updated at each time step. Due to these repeated transformations (and often the squashing effect of activation functions like tanh), information from the early parts of the sequence tends to get diluted or completely lost by the time the RNN reaches later time steps. The RNN\u0026rsquo;s state h(t) might contain virtually no trace of the first inputs x(0), x(1), ... if t is large. This is a showstopper for tasks requiring understanding long-range dependencies (e.g., understanding the beginning of a long paragraph to make sense of the end, or translating a long sentence where the first word in the source affects the last word in the target). The book uses the analogy of Dory the fish from \u0026ldquo;Finding Nemo\u0026rdquo; trying to translate a long sentence – by the time she finishes reading it, she has no clue how it started. The Solution: Cells with Long-Term Memory:\nTo tackle this, various types of recurrent cells with mechanisms for long-term memory have been introduced. These have proven so successful that basic SimpleRNN cells are not used much anymore for complex sequence tasks. We\u0026rsquo;ll look at the most popular ones: LSTM and GRU cells. LSTM Cells - Long Short-Term Memory History: Proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber, and gradually improved by others (Alex Graves, Haşim Sak, Wojciech Zaremba).\nAs a Black Box (Page 515):\nFrom the outside, an LSTM cell can be used much like a basic SimpleRNN cell. You can just replace keras.layers.SimpleRNN with keras.layers.LSTM. Benefits: Performs much better. Training converges faster. Can detect long-term dependencies in the data. Keras Implementation: model = keras.models.Sequential([ keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]), keras.layers.LSTM(20, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10)) ]) Or, using the generic RNN layer with an LSTMCell (though the specialized LSTM layer is usually preferred as it can use optimized GPU implementations, see Chapter 19): model = keras.models.Sequential([ keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True, input_shape=[None, 1]), # ... ]) How Does an LSTM Cell Work? (The \u0026ldquo;Internals\u0026rdquo; - Figure 15-9, page 516): This is where the magic happens.\nSplit State: An LSTM cell\u0026rsquo;s state is split into two vectors: h(t) (hidden state): The short-term state. This is also the output y(t) of the cell at time step t. c(t) (cell state or memory cell): The long-term state. Core Idea: The network can learn what to store in the long-term state c(t), what to throw away from c(t-1), and what to read from c(t) to influence the current output h(t). Data Flow (Left to Right for c(t)): The previous long-term state c(t-1) first goes through a forget gate. Some new memories (selected by an input gate) are added to it via an addition operation. The result is the new long-term state c(t), which is sent straight out (without further transformation) to be the long-term state for the next time step. Output Path (for h(t)): After the addition operation (that produced c(t)), the long-term state c(t) is copied. This copy is passed through a tanh function. The result is then filtered by an output gate. This produces the short-term state h(t) (which is also the cell\u0026rsquo;s output y(t)). The Gates (The Key Components - Page 516): The \u0026ldquo;forget,\u0026rdquo; \u0026ldquo;input,\u0026rdquo; and \u0026ldquo;output\u0026rdquo; gates are the controllers.\nThe current input vector x(t) and the previous short-term state h(t-1) are fed into four different fully connected layers (these are internal to the LSTM cell, each with its own weights and biases). 1. Main Layer (outputs g(t)): This layer analyzes x(t) and h(t-1). Its output g(t) (often passed through tanh) represents candidate values/memories to be potentially added to the long-term state. In a basic RNN, this g(t) would directly become h(t) and y(t). In an LSTM, its most important parts are stored in c(t). 2, 3, 4. Gate Controller Layers (output f(t), i(t), o(t)): These three layers use the logistic (sigmoid) activation function, so their outputs range from 0 to 1. These outputs are then used in element-wise multiplication operations, acting like gates: Output near 0: \u0026ldquo;Closes\u0026rdquo; the gate (blocks information flow). Output near 1: \u0026ldquo;Opens\u0026rdquo; the gate (lets information through). Forget Gate (controlled by f(t)): c(t) ← f(t) ⊗ c(t-1) + ... It decides which parts of the previous long-term state c(t-1) should be erased or kept. If an element of f(t) is 0, the corresponding part of c(t-1) is forgotten. Input Gate (controlled by i(t)): c(t) ← ... + i(t) ⊗ g(t) It decides which parts of the candidate memories g(t) (from the main layer) should be added to the long-term state. Output Gate (controlled by o(t)): h(t) = y(t) = o(t) ⊗ tanh(c(t)) It decides which parts of the current long-term state c(t) (after passing through tanh) should be read out and output as the short-term state h(t) and the cell output y(t). Equation 15-3: LSTM Computations (Page 517): Summarizes the math for a single instance:\ni(t) = σ(Wₓᵢᵀx(t) + Wₕᵢᵀh(t-1) + bᵢ) (Input gate) f(t) = σ(Wₓfᵀx(t) + Wₕfᵀh(t-1) + b_f) (Forget gate - TensorFlow initializes b_f to 1s to prevent forgetting everything at start) o(t) = σ(Wₓₒᵀx(t) + Wₕₒᵀh(t-1) + bₒ) (Output gate) g(t) = tanh(Wₓgᵀx(t) + Wₕgᵀh(t-1) + b_g) (Main layer transforming inputs/prev_state) c(t) = f(t) ⊗ c(t-1) + i(t) ⊗ g(t) (New long-term state: forget old, add new) y(t) = h(t) = o(t) ⊗ tanh(c(t)) (Output/new short-term state: filter long-term state) Each gate and the main layer have their own weight matrices for x(t) (e.g., Wₓᵢ) and h(t-1) (e.g., Wₕᵢ), and their own bias terms (e.g., bᵢ). These are all learned. What the LSTM cell is ultimately trying to achieve: By having these gate mechanisms, the LSTM can learn:\nWhen to let new important information in (input gate). What old information to forget (forget gate). What information to preserve in its long-term memory cell c(t) over many time steps. What part of its current memory to output or use for its short-term state h(t) (output gate). This allows it to capture long-term patterns and dependencies much more effectively than simple RNNs. Peephole Connections (Felix Gers \u0026amp; Jürgen Schmidhuber, 2000 - Page 518):\nAn LSTM variant where the gate controllers can also \u0026ldquo;peek\u0026rdquo; at the long-term state c(t-1) (for input/forget gates) or c(t) (for output gate) in addition to x(t) and h(t-1). This can sometimes improve performance, but not always. Standard Keras LSTMCell doesn\u0026rsquo;t support them, but tf.keras.experimental.PeepholeLSTMCell does. 1. Why Sigmoid (Logistic) for the Gates (f(t), i(t), o(t))?\nWhat a Gate Does: A gate\u0026rsquo;s purpose is to control the flow of information. It needs to decide \u0026ldquo;how much\u0026rdquo; of some signal should be allowed to pass through. This is like a valve that can be fully open, fully closed, or partially open.\nSigmoid Output Range (0 to 1): The sigmoid function σ(z) = 1 / (1 + exp(-z)) outputs values strictly between 0 and 1.\nOutput near 0: \u0026ldquo;Close the gate\u0026rdquo; – block most/all of the information. Output near 1: \u0026ldquo;Open the gate\u0026rdquo; – let most/all of the information pass through. Output around 0.5: \u0026ldquo;Partially open the gate\u0026rdquo; – let some proportion of the information pass. Use in Element-wise Multiplication: The outputs of these sigmoid gates are then used in element-wise multiplications (the ⊗ symbol in Equation 15-3):\nf(t) ⊗ c(t-1): How much of the old long-term state to keep (if f(t) is near 1) or forget (if f(t) is near 0). i(t) ⊗ g(t): How much of the new candidate memory g(t) to let in. o(t) ⊗ tanh(c(t)): How much of the (processed) long-term state to output. Differentiability: Sigmoid is differentiable, which is essential for backpropagation to learn the weights of the gate controller layers.\nWhat if we used Tanh for Gates?\nTanh outputs values between -1 and 1. If a gate outputted -1, and you multiplied it by a value, it would invert the sign of that value and also scale it. This isn\u0026rsquo;t really what \u0026ldquo;gating\u0026rdquo; or \u0026ldquo;controlling flow\u0026rdquo; means in an intuitive sense. We want to scale information between \u0026ldquo;none of it\u0026rdquo; (0) and \u0026ldquo;all of it\u0026rdquo; (1). It might be possible to design a system where tanh is used and its output is then rescaled (e.g., (tanh(z) + 1) / 2 to map it to 0-1), but sigmoid naturally provides the 0-1 range needed for this multiplicative gating effect. What if we used ReLU for Gates?\nReLU outputs 0 for negative inputs and z for positive inputs (unbounded positive output). An output of 0 would effectively \u0026ldquo;close the gate.\u0026rdquo; An output \u0026gt; 1 would not just \u0026ldquo;open the gate\u0026rdquo; but also amplify the signal passing through. This could lead to exploding values and instability, which is precisely what LSTMs are trying to manage better than simple RNNs. The gating mechanism generally needs a bounded \u0026ldquo;how much\u0026rdquo; signal, and sigmoid\u0026rsquo;s 0-1 range is perfect for this. 2. Why Tanh for the Main Layer (g(t)) and for Processing c(t) before the Output Gate?\nMain Layer g(t): This layer g(t) = tanh(W_xgᵀx(t) + W_hgᵀh(t-1) + b_g) computes the \u0026ldquo;candidate values\u0026rdquo; to be potentially added to the long-term cell state c(t).\nProcessing c(t) for Output: The output y(t) = o(t) ⊗ tanh(c(t)). The long-term state c(t) is passed through tanh before being gated by o(t).\nTanh Output Range (-1 to 1):\nThe tanh function squashes its input to a range between -1 and 1. What this achieves for g(t) (candidate values): It keeps the potential updates to the cell state bounded. This helps prevent the cell state c(t) from growing uncontrollably large (exploding). What this achieves for tanh(c(t)) before output: It ensures that the values being considered for output are also in a bounded range before the output gate decides how much of it to pass through. Zero-Centered Output: The output of tanh is roughly centered around 0. This can be beneficial for learning in subsequent layers or time steps, as discussed in Chapter 10 (it\u0026rsquo;s a property that sometimes helps with convergence speed compared to sigmoid which is centered at 0.5).\nDifferentiability: Tanh is also differentiable.\nWhat if we used Sigmoid for g(t) or tanh(c(t))?\nSigmoid outputs 0 to 1. If you continually add positive values (from sigmoid) to the cell state, the cell state could still grow very large without bound (unless the forget gate is very aggressive). Tanh allowing for both positive and negative values (and being zero-centered) gives the network more flexibility to increment or decrement values in the cell state c(t) and to output values that are centered around zero. This can lead to more stable dynamics. What if we used ReLU for g(t) or tanh(c(t))?\nFor g(t): If ReLU were used, g(t) could be unbounded positive. If the input gate i(t) is open, this could lead to the cell state c(t) growing indefinitely, causing exploding values. LSTMs are designed to prevent this kind of instability that plagues simple RNNs with ReLU. For tanh(c(t)) before output: Using ReLU here might also be problematic if c(t) can take on large values, as the output h(t) could then become unbounded before being used by subsequent layers or calculations. The tanh here provides a final \u0026ldquo;squashing\u0026rdquo; to keep things in a controlled range. In Summary:\nSigmoid for Gates (0 to 1 range): Ultimately trying to achieve: A probabilistic-like control over information flow – \u0026ldquo;block\u0026rdquo; (0), \u0026ldquo;pass through\u0026rdquo; (1), or \u0026ldquo;partially pass through\u0026rdquo; (between 0 and 1). This is ideal for multiplicative gating. Tanh for Candidate Values and Pre-Output Processing (-1 to 1 range): Ultimately trying to achieve: Keep the values that modify the cell state and the values considered for output bounded and roughly zero-centered. This contributes to the stability of the LSTM cell and helps prevent the cell state from exploding, which is a key part of solving the long-term memory problem. The choice of these specific activation functions is a result of careful design and empirical evidence showing that this combination allows LSTMs to effectively learn, store, and retrieve information over long sequences while maintaining training stability. They are not arbitrary choices but rather integral to the LSTM\u0026rsquo;s mechanism for managing information flow and memory.\nGRU Cells - Gated Recurrent Unit Proposed by Kyunghyun Cho et al. in 2014 (in the same paper that introduced the Encoder-Decoder architecture we briefly touched upon).\nWhat it is: A simplified version of the LSTM cell.\nPerformance: It seems to perform just as well as LSTMs on many tasks (footnote 12 refers to a 2015 paper by Greff et al. that found LSTM variants perform roughly the same). This, along with its simplicity, explains its growing popularity.\nMain Simplifications Compared to LSTM (Figure 15-10, page 519):\nMerged State Vectors: Both state vectors (c(t) for long-term and h(t) for short-term in LSTM) are merged into a single state vector h(t) in GRU. Single \u0026ldquo;Update\u0026rdquo; Gate z(t) controls both Forget and Input: LSTM has a separate forget gate f(t) and an input gate i(t). GRU has a single update gate z(t). If z(t) is close to 1 (gate \u0026ldquo;open\u0026rdquo; for update), it means \u0026ldquo;forget the previous state and update with the new candidate state.\u0026rdquo; Specifically, h(t) = z(t) ⊗ h(t-1) + (1-z(t)) ⊗ g(t). If z(t) ≈ 1: h(t) ≈ 1 ⊗ h(t-1) + 0 ⊗ g(t) ≈ h(t-1) (keep old state, ignore candidate). Correction: The equation in the book and standard GRU is h(t) = (1-z(t)) ⊗ h(t-1) + z(t) ⊗ g(t). So if z(t) ≈ 1 (update gate is \u0026ldquo;active for new candidate\u0026rdquo;), then h(t) ≈ 0 ⊗ h(t-1) + 1 ⊗ g(t) ≈ g(t) (take the new candidate). If z(t) ≈ 0, then h(t) ≈ 1 ⊗ h(t-1) + 0 ⊗ g(t) ≈ h(t-1) (keep old state). What the update gate z(t) is ultimately trying to achieve: It learns to decide how much of the previous state h(t-1) to keep versus how much of the new candidate state g(t) to incorporate. No Output Gate: The full state vector h(t) is output at every time step. (In LSTM, the output gate o(t) controlled which parts of tanh(c(t)) were output as h(t)). New \u0026ldquo;Reset\u0026rdquo; Gate r(t): There\u0026rsquo;s a new gate controller r(t) (also using sigmoid, outputting 0 to 1). This gate controls which part of the previous state h(t-1) will be shown to the main layer g(t) when calculating the candidate state. The candidate state is g(t) = tanh(W_xgᵀx(t) + W_hgᵀ(r(t) ⊗ h(t-1)) + b_g). If r(t) is close to 0 for some components, those components of h(t-1) are effectively ignored when computing the new candidate g(t). What the reset gate r(t) is ultimately trying to achieve: It allows the cell to \u0026ldquo;forget\u0026rdquo; or ignore parts of its previous state that are deemed irrelevant for computing the next candidate state, before deciding how to update the state with that candidate. Equation 15-4: GRU Computations (Page 520): Summarizes the math for a single instance.\nz(t) = σ(W_xzᵀx(t) + W_hzᵀh(t-1) + b_z) (Update gate) r(t) = σ(W_xrᵀx(t) + W_hrᵀh(t-1) + b_r) (Reset gate) g(t) = tanh(W_xgᵀx(t) + W_hgᵀ(r(t) ⊗ h(t-1)) + b_g) (Candidate hidden state) h(t) = (1-z(t)) ⊗ h(t-1) + z(t) ⊗ g(t) (Final hidden state for the time step) Parameters to Learn: W_xz, W_hz, b_z, W_xr, W_hr, b_r, and W_xg, W_hg, b_g. Notice fewer weight matrices than LSTM because there are fewer gates and only one state vector. Keras Implementation:\nKeras provides a keras.layers.GRU layer (based on keras.layers.GRUCell). Using it is just a matter of replacing SimpleRNN or LSTM with GRU in your Sequential model. Why LSTMs and GRUs are Successful:\n\u0026ldquo;LSTM and GRU cells are one of the main reasons behind the success of RNNs.\u0026rdquo; They can tackle much longer sequences than simple RNNs. However, they still have fairly limited short-term memory and can have a hard time learning long-term patterns in sequences of 100 time steps or more (e.g., very long audio samples, long time series, long sentences). One way to help LSTMs/GRUs handle even longer sequences is to shorten the input sequences they directly see, for example, by using 1D convolutional layers first.\nKey Takeaway for GRUs: GRUs are a streamlined version of LSTMs that often perform just as well but are computationally simpler (fewer parameters, one state vector).\nThey use an update gate (to decide how much of the past state to keep vs. new candidate state) and a reset gate (to decide how much of the past state influences the current candidate state). What GRUs are ultimately trying to achieve: Similar to LSTMs, they aim to capture dependencies over longer sequences by using gating mechanisms to control information flow and memory, but with a more compact architecture. This covers the main advanced recurrent cell types (LSTM and GRU) designed to combat the short-term memory problem of simple RNNs. The next step in the chapter is to see how to combine these with other types of layers, like 1D convolutions, to process sequences even more effectively.\nUsing 1D Convolutional Layers to Process Sequences While LSTMs and GRUs are much better than simple RNNs at handling longer sequences, they can still struggle with very long dependencies (e.g., hundreds or thousands of time steps, like in raw audio or very long text documents).\nThe Idea: One way to solve this is to shorten the input sequences before feeding them to the recurrent layers. How? By using 1D convolutional layers.\nRecall 2D Convolutional Layers (from Chapter 14, though not yet covered by us in detail):\nThey work by sliding small kernels (filters) across an image (a 2D grid of pixels). They produce multiple 2D feature maps (one per kernel). Each kernel learns to detect a specific local pattern (e.g., an edge, a texture). 1D Convolutional Layers for Sequences:\nSimilarly, a 1D convolutional layer slides several kernels (filters) across an input sequence (a 1D array of features over time). It produces a 1D feature map per kernel. What each 1D kernel is ultimately trying to achieve: It learns to detect a single, very short sequential pattern (no longer than the kernel size). For example, if processing text character by character, a kernel of size 3 might learn to detect common trigrams like \u0026ldquo;the\u0026rdquo; or \u0026ldquo;ing\u0026rdquo;. If you use, say, 10 kernels, the layer\u0026rsquo;s output will be composed of 10 1-dimensional sequences (feature maps). You can view this output as a single sequence where each time step now has 10 features (instead of, say, 1 feature if it was a univariate time series). Building Hybrid Networks:\nThis means you can build neural networks composed of a mix of: Recurrent layers (LSTMs, GRUs). 1D convolutional layers. Even 1D pooling layers (which downsample the sequence, similar to 2D pooling for images). Impact on Sequence Length:\nIf a 1D convolutional layer uses a stride of 1 and \u0026quot;same\u0026quot; padding, the output sequence will have the same length as the input sequence. If it uses \u0026quot;valid\u0026quot; padding or a stride greater than 1, then the output sequence will be shorter than the input sequence. This downsampling of the sequence by the convolutional layer is the key benefit here. The convolutional layer can learn to preserve useful local information while dropping unimportant details, effectively compressing the sequence. The subsequent recurrent layers then have a shorter sequence to process, making it easier for them to detect longer-range patterns within that compressed representation. Keras Example (Page 521): The book shows modifying the time series forecasting model to include a 1D convolutional layer at the beginning.\nmodel = keras.models.Sequential([ keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\u0026#34;valid\u0026#34;, input_shape=[None, 1]), # Input is [batch, time_steps, features] keras.layers.GRU(20, return_sequences=True), keras.layers.GRU(20, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10)) # Assuming seq-to-seq for 10-step forecast ]) keras.layers.Conv1D(...): filters=20: It will learn 20 different short patterns. kernel_size=4: Each pattern is 4 time steps long. strides=2: The kernel slides by 2 steps each time, effectively downsampling the input sequence by a factor of 2. padding=\u0026quot;valid\u0026quot;: No padding, so output sequence length will be (input_length - kernel_size) / strides + 1. input_shape=[None, 1]: Takes sequences of any length, with 1 feature per time step. Target Adjustment: Because the Conv1D layer shortens the sequence (and the first output corresponds to input steps 0-3), the target sequences Y_train need to be adjusted (cropped and downsampled) to match the output length and alignment of the Conv1D layer. The example Y_train[:, 3::2] does this: 3::: Skips the first 3 time steps in the target (because the first Conv1D output depends on inputs 0-3, so it effectively predicts starting from a later point relative to the original targets). ::2: Downsamples the targets by a factor of 2 to match the strides=2. Result: The book states that if you train and evaluate this model, \u0026ldquo;you will find that it is the best model so far. The convolutional layer really helps.\u0026rdquo;\nIt even mentions that it\u0026rsquo;s possible to use only 1D convolutional layers (and drop recurrent layers entirely) for sequence processing, which leads to the WaveNet architecture.\nKey Takeaway for 1D Convolutions in Sequence Processing: 1D convolutional layers can be used as a preprocessing step for RNNs (or even as a replacement for them in some cases like WaveNet).\nWhat they are ultimately trying to achieve when used with RNNs: Feature Extraction: Learn to detect relevant local patterns in the input sequence. Downsampling/Shortening Sequences: By using strides, they can reduce the length of the sequence fed into the RNN layers. This allows the RNNs (like LSTMs/GRUs) to focus their memory capacity on longer-range dependencies within a more compressed, feature-rich representation of the original sequence. (Page 521-522: WaveNet)\nThis is an example of a powerful architecture that uses only 1D convolutional layers (no recurrent layers) to process very long sequences.\nIntroduced in a 2016 paper by van den Oord et al. (DeepMind) for generative modeling of raw audio.\nArchitecture (Figure 15-11, page 522):\nIt stacks 1D convolutional layers. Crucially, it doubles the dilation rate at every layer. Dilation Rate: Controls how spread apart the kernel\u0026rsquo;s input taps are. Layer 1: Dilation rate 1 (looks at, say, 2 adjacent time steps). Layer 2: Dilation rate 2 (looks at inputs 2 steps apart, e.g., time t and t-2). Its receptive field is wider. Layer 3: Dilation rate 4 (looks at inputs 4 steps apart). And so on (1, 2, 4, 8, 16, \u0026hellip;). What doubling dilation rates achieves: The receptive field of the network (how far back in time it can \u0026ldquo;see\u0026rdquo;) grows exponentially with depth. This allows the network to capture very long-range dependencies efficiently. Lower layers learn short-term patterns. Higher layers learn long-term patterns by combining features from lower layers that already cover wider time spans. The paper stacked blocks of 10 such convolutional layers (dilation rates 1, 2, \u0026hellip;, 512), and repeated these blocks. Used \u0026ldquo;causal\u0026rdquo; padding to ensure convolutions don\u0026rsquo;t peek into the future. Keras Implementation of a Simplified WaveNet (Page 522):\nmodel = keras.models.Sequential() model.add(keras.layers.InputLayer(input_shape=[None, 1])) for rate in (1, 2, 4, 8) * 2: # Two blocks of these dilation rates model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\u0026#34;causal\u0026#34;, activation=\u0026#34;relu\u0026#34;, dilation_rate=rate)) model.add(keras.layers.Conv1D(filters=10, kernel_size=1)) # Output layer padding=\u0026quot;causal\u0026quot;: Ensures that the output at time t only depends on inputs up to time t (no future peeking). The output layer is a Conv1D with kernel_size=1. This is like applying a Dense layer independently at each time step to the features extracted by the dilated convolutions. Performance:\nThe book states that these last two models (RNN with initial Conv1D, and the simplified WaveNet) offer the best performance so far on the time series forecasting task. The original WaveNet achieved state-of-the-art on audio generation (text-to-speech, music), handling tens of thousands of time steps per second of audio. This is something LSTMs/GRUs would struggle immensely with. Key Takeaway for WaveNet: WaveNet demonstrates that stacked 1D convolutional layers with exponentially increasing dilation rates can be extremely effective at capturing long-range dependencies in sequences, offering an alternative to RNNs for very long sequences.\nWhat it\u0026rsquo;s ultimately trying to achieve: Efficiently model long-term patterns by having a hierarchy of convolutions that look at progressively larger time scales, without the computational cost of very large kernels or the memory limitations of very deep unrolled RNNs. This concludes the main content of Chapter 15! It\u0026rsquo;s taken us from the basic concept of recurrence to sophisticated cells like LSTMs and GRUs, and finally to using CNNs (either with RNNs or alone like WaveNet) for sequence processing.\nThe core theme has been how to enable neural networks to effectively \u0026ldquo;remember\u0026rdquo; and utilize information from previous parts of a sequence to make current predictions or classifications.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-15/","tags":null,"title":"Chapter 15: Processing Sequences Using RNNs and CNNs"},{"categories":null,"content":"(Page 609: Introduction - The Excitement of RL)\nThe chapter starts by highlighting that Reinforcement Learning (RL) is one of the most exciting fields in Machine Learning today, yet it\u0026rsquo;s also one of the oldest, with roots in the 1950s.\nHistorical Applications: It has seen interesting applications over the years, particularly in games (like TD-Gammon, a Backgammon program) and machine control, but it didn\u0026rsquo;t often make headline news. The DeepMind Revolution (2013 onwards): A major turning point occurred in 2013 when researchers from DeepMind (a British startup, later bought by Google) demonstrated a system that could learn to play almost any Atari game from scratch, using only raw pixel inputs and no prior knowledge of the game rules. It eventually outperformed humans in most of them. This was followed by a series of amazing feats, most famously AlphaGo defeating legendary Go player Lee Sedol in 2016 and world champion Ke Jie in 2017. This was a monumental achievement, as Go is an incredibly complex game. The \u0026ldquo;Simple\u0026rdquo; Idea Behind DeepMind\u0026rsquo;s Success: With hindsight, the core idea was to apply the power of Deep Learning (which we\u0026rsquo;ve been studying) to the field of Reinforcement Learning. The combination proved to be incredibly effective. Current State: The field of RL is now \u0026ldquo;boiling with new ideas\u0026rdquo; and has a wide range of potential applications. Chapter Goals:\nExplain what Reinforcement Learning is and what it\u0026rsquo;s good at. Present two of the most important techniques in Deep Reinforcement Learning (DRL): Policy Gradients (PG) Deep Q-Networks (DQNs) This will include a discussion of Markov Decision Processes (MDPs), which provide a formal framework for RL problems. Use these techniques to train models for classic RL tasks (e.g., balancing a pole on a cart). Introduce the TF-Agents library (a TensorFlow library for RL) to build powerful RL systems and train an agent to play the Atari game Breakout. Briefly look at some of the latest advances in the field. What this chapter is ultimately trying to achieve: To give you a solid understanding of the fundamental principles of Reinforcement Learning, how deep learning is used to enhance it, and how to start building and training RL agents.*\n(Page 610-611: Learning to Optimize Rewards)\nThis is the core definition of what RL is all about.\nThe Setup: A software agent (our learning algorithm or program). Makes observations from an environment. Takes actions within that environment. In return, it receives rewards (or penalties). The Objective of the Agent: To learn to act in a way that will maximize its expected cumulative rewards over time. Anthropomorphic View: Positive rewards = \u0026ldquo;pleasure.\u0026rdquo; Negative rewards = \u0026ldquo;pain.\u0026rdquo; The agent learns by trial and error to maximize its pleasure and minimize its pain. Broad Applicability: This is a very general framework. Examples (Figure 18-1, page 611): a. Robotics: Agent is the robot\u0026rsquo;s control program. Environment is the real world. Observations from sensors. Actions are motor commands. Rewards for reaching a target, penalties for wasting time/going wrong. b. Ms. Pac-Man: Agent is the game-playing program. Environment is the Atari game simulation. Actions are joystick moves. Observations are game screenshots. Rewards are game points. c. Go Player: Similar to Ms. Pac-Man, but for a board game. d. Smart Thermostat: Doesn\u0026rsquo;t have to be a \u0026ldquo;moving\u0026rdquo; agent. Agent learns to anticipate human needs. Rewards for target temperature and energy saving, penalties if humans have to adjust it. e. Automatic Stock Trader: Observes market prices, decides to buy/sell. Rewards are monetary gains/losses. Nature of Rewards: Rewards don\u0026rsquo;t have to be positive. An agent in a maze might get a negative reward at every time step, so its goal is to find the exit as quickly as possible (to minimize total negative reward). Other Applications: Self-driving cars, recommender systems, ad placement, controlling where an image classification system should focus its attention. What \u0026ldquo;Learning to Optimize Rewards\u0026rdquo; is ultimately trying to achieve: It\u0026rsquo;s about training an agent to develop a strategy (a \u0026ldquo;policy\u0026rdquo;) that leads to the best possible long-term outcomes, as measured by the sum of rewards it receives from the environment.* This is different from supervised learning (where you have explicit correct labels) and unsupervised learning (where you\u0026rsquo;re finding structure in unlabeled data). In RL, the \u0026ldquo;supervision\u0026rdquo; comes from the sparse and often delayed reward signals.\n(Page 612-613: Policy Search)\nWhat is a Policy?\nThe algorithm or strategy a software agent uses to determine its actions is called its policy. What a policy is ultimately trying to achieve: It\u0026rsquo;s the agent\u0026rsquo;s \u0026ldquo;brain\u0026rdquo; or decision-making function. Given an observation from the environment, the policy outputs an action to take. The policy could be a simple rule-based system, a lookup table, or, very commonly in Deep RL, a neural network (Figure 18-2). The neural network would take observations as input and output the action (or probabilities for actions). Types of Policies:\nDeterministic: For a given observation, always outputs the same action. Stochastic: For a given observation, outputs a probability distribution over possible actions. The action is then sampled from this distribution. Example: A robotic vacuum cleaner\u0026rsquo;s policy might be: Move forward with probability p each second. Randomly rotate left or right (with probability 1-p) by a random angle between -r and +r. This randomness (stochasticity) ensures the robot explores its environment. The question is, what values of p and r will maximize the dust collected in 30 minutes? Policy Search:\nThe process of finding the best policy parameters (like p and r in the vacuum example, or the weights of a neural network policy). What policy search is ultimately trying to achieve: To find the settings for the agent\u0026rsquo;s decision-making process that lead to the maximum cumulative reward. Methods for Policy Search:\nBrute Force (Figure 18-3, page 613):\nTry out many different values for the policy parameters. Pick the combination that performs best. This is like searching for a needle in a haystack if the policy space (the set of all possible parameter combinations) is large, which it usually is. Genetic Algorithms (Page 612):\nAn evolutionary approach. Start with a first generation of, say, 100 randomly created policies. Try them all out in the environment. \u0026ldquo;Kill\u0026rdquo; the worst-performing ones (e.g., the bottom 80). Let the survivors \u0026ldquo;reproduce\u0026rdquo; to create the next generation. Offspring are copies of their parent(s) plus some random variation (mutation). Iterate through generations until a good policy is found. (Footnote 8 mentions NEAT - NeuroEvolution of Augmenting Topologies - as an interesting example). Optimization Techniques (Policy Gradients - Page 613):\nThis is a more direct approach that we\u0026rsquo;ll focus on later in the chapter. Evaluate the gradients of the rewards with respect to the policy parameters. Tweak the parameters by following these gradients towards higher rewards. This is called Gradient Ascent (like Gradient Descent, but we want to maximize rewards, not minimize a cost). Example (vacuum robot): Slightly increase p (probability of moving forward). Does it pick up more dust? If yes, increase p more. If no, reduce p. This approach is called Policy Gradients (PG). The Need for an Environment to Train In:\nBefore we can implement Policy Gradients (or most RL algorithms), we need an environment for the agent to interact with. This is a major challenge in RL.\nFor Atari games, you need an Atari game simulator. For a walking robot, the environment is the real world (with limitations: can\u0026rsquo;t easily undo mistakes, can\u0026rsquo;t speed up time, expensive to run many robots in parallel). So, simulated environments are crucial, at least for initial \u0026ldquo;bootstrap\u0026rdquo; training. Libraries like PyBullet or MuJoCo are used for 3D physics simulation. (Page 613-616: Introduction to OpenAI Gym)\nThis is where OpenAI Gym comes in.\nWhat it is: A toolkit that provides a wide variety of simulated environments for RL.\nAtari games, board games, 2D and 3D physical simulations, etc. It\u0026rsquo;s a standard platform to train agents, compare different RL algorithms, or develop new ones. Setting up OpenAI Gym (Page 614):\nActivate your virtual environment (if using one). pip install -U gym May need extra libraries for rendering some environments (e.g., Mesa OpenGL Utility on Ubuntu). Using OpenAI Gym - Example with CartPole (Page 614): The CartPole environment (Figure 18-4, page 615) is a classic RL \u0026ldquo;hello world.\u0026rdquo;\nA cart can move left or right. A pole is attached to the cart. Goal: Keep the pole balanced upright for as long as possible. Create an environment: import gym env = gym.make(\u0026quot;CartPole-v1\u0026quot;) Reset the environment: Must be done before starting an episode. Returns the initial observation. obs = env.reset() print(obs) might give array([-0.0125..., -0.0015..., 0.0420..., -0.0018...]) Observations: Depend on the environment. For CartPole: A 1D NumPy array with 4 floats: Cart horizontal position (0.0 = center). Cart velocity (positive = right). Pole angle (0.0 = vertical). Pole angular velocity (positive = clockwise). Render the environment (optional, for visualization - page 615): env.render() (Opens a window showing the cart and pole). If on a headless server (no screen), rendering will fail unless you use a fake X server like Xvfb. env.render(mode=\u0026quot;rgb_array\u0026quot;) returns the image as a NumPy array. Check Possible Actions (Page 615): print(env.action_space) gives Discrete(2). This means there are 2 discrete actions: integers 0 and 1. For CartPole: 0 = accelerate left, 1 = accelerate right. Other environments might have more actions, or continuous action spaces (e.g., apply force between -1.0 and 1.0). Take a Step in the Environment (Page 616): action = 1 # accelerate right obs, reward, done, info = env.step(action) The step() method executes the chosen action and returns four values: obs: The new observation after the action. reward: The reward received for taking that action in the previous state. For CartPole: You get a reward of 1.0 at every step, no matter what you do. What this reward structure ultimately trying to achieve: The agent\u0026rsquo;s goal is to keep the episode running as long as possible (to accumulate as many +1 rewards as possible). done: A boolean. True if the episode is over. For CartPole, this happens if: Pole tilts too much. Cart goes off the screen. After 200 steps (in this version, a limit often exists). In this case, you\u0026rsquo;ve \u0026ldquo;won\u0026rdquo; the episode. After done is True, you must call env.reset() before using it again. info: An environment-specific dictionary with extra information (often empty, but can be useful for debugging or specific tasks, e.g., number of lives left in a game). Close the Environment: When done: env.close() to free resources. Simple Hardcoded Policy Example (Page 617): Let\u0026rsquo;s try a very simple policy: if pole angle (obs) \u0026lt; 0 (leaning left), accelerate left (action 0). If angle ≥ 0 (leaning right or vertical), accelerate right (action 1).\ndef basic_policy(obs): angle = obs[2] return 0 if angle \u0026lt; 0 else 1 totals = [] for episode in range(500): episode_rewards = 0 obs = env.reset() for step in range(200): # Max 200 steps per episode for CartPole-v1 action = basic_policy(obs) obs, reward, done, info = env.step(action) episode_rewards += reward if done: break totals.append(episode_rewards) Run this for 500 episodes. Result: np.mean(totals) is around 41-42. np.max(totals) might be around 68. This means this simple policy can\u0026rsquo;t even keep the pole up for the full 200 steps on average. Not great. The cart tends to oscillate more and more until the pole falls. Key Takeaway from OpenAI Gym Introduction: OpenAI Gym provides a standardized way to interact with various simulated environments, which is essential for developing and testing RL algorithms. The core interaction loop is: reset -\u0026gt; loop (get action from policy, step, observe reward/done) -\u0026gt; close.\n(Page 617-618: Neural Network Policies)\nWe just saw a simple, hardcoded policy for CartPole. Now, let\u0026rsquo;s create a policy using a neural network.\nInput: The neural network will take an observation from the environment as input.\nOutput: It will output the action to be executed.\nStochastic Policy (More Precisely): The neural network will usually estimate a probability for each possible action. We then select an action randomly according to these estimated probabilities (Figure 18-5, page 618).\nCartPole Example: Two possible actions: 0 (left) and 1 (right). We only need one output neuron in our neural network. This neuron will output p, the probability of action 0 (going left). The probability of action 1 (going right) will then be 1 - p. If the network outputs p = 0.7, we\u0026rsquo;ll pick action 0 with 70% probability and action 1 with 30% probability. Why Pick Randomly (Exploration vs. Exploitation - Page 618)?\nWhy not just always pick the action with the highest estimated probability (greedy approach)? This is the classic exploration vs. exploitation dilemma in RL. Exploitation: Choosing actions that are known to give good rewards based on current knowledge. Exploration: Trying out new actions to discover potentially even better rewards or to learn more about the environment. If you only exploit, you might get stuck with a suboptimal policy because you never try actions that could lead to better long-term outcomes. (Analogy: always ordering your favorite dish at a restaurant and never trying others that might be even better). Sampling actions based on their probabilities allows the agent to strike a balance: mostly picking actions it thinks are good, but occasionally trying out less probable ones. Considering Past Actions/Observations (State Representation - Page 618):\nCartPole Simplicity: In the CartPole environment, each observation (cart position, cart velocity, pole angle, pole angular velocity) contains the environment\u0026rsquo;s full state. Past actions and observations can be safely ignored because the current observation tells you everything you need to know about the current situation. Hidden State in the Environment: If the environment had some hidden state (i.e., the observation doesn\u0026rsquo;t fully describe the situation), then the agent might need to consider past actions and observations to infer the true state. Example: If the environment only revealed cart position but not velocity, the agent would need to look at the current and previous positions to estimate velocity. Example: If observations are noisy, an agent might want to average a few past observations to get a more stable estimate of the current state. What this means for policy design: For simple, fully observable environments like CartPole, a policy that only looks at the current observation is sufficient. For partially observable environments, the policy (or the agent\u0026rsquo;s architecture) might need some form of memory (like an RNN, or by explicitly feeding sequences of observations). Building the Neural Network Policy with tf.keras (Page 619):\nimport tensorflow as tf from tensorflow import keras n_inputs = 4 # For CartPole: env.observation_space.shape[0] model = keras.models.Sequential([ keras.layers.Dense(5, activation=\u0026#34;elu\u0026#34;, input_shape=[n_inputs]), keras.layers.Dense(1, activation=\u0026#34;sigmoid\u0026#34;) # Output probability of action 0 (left) ]) Input Layer: Implicitly defined by input_shape=[n_inputs] in the first Dense layer. n_inputs is 4 for CartPole. Hidden Layer: A single Dense layer with 5 neurons and \u0026ldquo;elu\u0026rdquo; activation. This is a simple problem, so a small hidden layer is likely sufficient. Output Layer: A single Dense neuron with \u0026ldquo;sigmoid\u0026rdquo; activation. Why sigmoid? Sigmoid outputs a value between 0 and 1, which is perfect for representing the probability p of choosing action 0 (left). If there were more than two possible actions (e.g., up, down, left, right), we would use one output neuron per action and a softmax activation function in the output layer to get a probability distribution over all actions. We now have a neural network policy! It takes observations and outputs action probabilities. The Big Question: But how do we train it? We don\u0026rsquo;t have explicit \u0026ldquo;correct action\u0026rdquo; labels like in supervised learning. This is where the \u0026ldquo;credit assignment problem\u0026rdquo; comes in.\nKey Takeaway for Neural Network Policies: Neural networks provide a flexible and powerful way to define an agent\u0026rsquo;s policy, mapping observations to action probabilities. Using probabilities allows for a natural way to balance exploration and exploitation. The next step is to figure out how to adjust the network\u0026rsquo;s weights based on the rewards received.\nPerfect! That concept of a neural network outputting action probabilities, which then guide the agent\u0026rsquo;s choices (allowing for exploration), is fundamental to many RL approaches.\nNow we hit a core challenge in Reinforcement Learning: Evaluating Actions: The Credit Assignment Problem (Pages 619-620).\n(Page 619: Evaluating Actions: The Credit Assignment Problem)\nWe have our neural network policy that can suggest actions. But how do we train it?\nIf we knew the best action at each step: We could just do supervised learning. We\u0026rsquo;d train the network to output a high probability for the \u0026ldquo;correct\u0026rdquo; action and low probabilities for others, likely by minimizing cross-entropy between its output distribution and the target distribution (which would be a one-hot vector for the correct action).\nThe RL Reality: The only guidance the agent gets is through rewards, and these rewards are typically:\nSparse: You don\u0026rsquo;t get a reward after every single action. You might only get a reward at the very end of an episode (e.g., winning or losing a game). Delayed: The action that truly led to a future reward might have occurred many steps earlier. The Credit Assignment Problem:\nWhen an agent receives a reward (positive or negative), it\u0026rsquo;s hard for it to know which of its past actions actually contributed to that reward (or penalty). Example: If the CartPole agent balances the pole for 100 steps and then it falls, was the 100th action solely responsible? Probably not. Some earlier actions were good (kept it balanced), and some (perhaps the last few) were bad. Analogy: Rewarding a dog hours after it behaved well. Will the dog understand what it\u0026rsquo;s being rewarded for? Unlikely. What the credit assignment problem is ultimately trying to solve: How do we assign \u0026ldquo;credit\u0026rdquo; or \u0026ldquo;blame\u0026rdquo; to individual actions in a sequence when the feedback (reward) is sparse and delayed? Common Strategy: Sum of Discounted Future Rewards (Returns)\nTo tackle this, a common strategy is to evaluate an action based on the sum of all the rewards that come after it in that episode. Usually, we apply a discount factor γ (gamma) at each step. This sum of discounted rewards is called the return for that action. Figure 18-6 (page 620) illustrates this: Agent takes action A₁ -\u0026gt; gets R₁ = +10 Agent takes action A₂ -\u0026gt; gets R₂ = 0 Agent takes action A₃ -\u0026gt; gets R₃ = -50 Assume discount factor γ = 0.8. Return for action A₁: G₁ = R₁ + γ*R₂ + γ²*R₃ = 10 + (0.8 * 0) + (0.8² * -50) = 10 + 0 + (0.64 * -50) = 10 - 32 = -22. Return for action A₂: G₂ = R₂ + γ*R₃ = 0 + (0.8 * -50) = -40. Return for action A₃: G₃ = R₃ = -50. Discount Factor γ: A value between 0 and 1. If γ is close to 0: Future rewards don\u0026rsquo;t count for much compared to immediate rewards. The agent becomes very \u0026ldquo;short-sighted.\u0026rdquo; If γ is close to 1: Rewards far into the future count almost as much as immediate rewards. The agent becomes more \u0026ldquo;far-sighted.\u0026rdquo; Typical values: 0.9 to 0.99. For CartPole, actions have fairly short-term effects, so γ = 0.95 is suggested as reasonable. (With γ=0.95, rewards 13 steps ahead are discounted by about half: 0.95¹³ ≈ 0.5). From Returns to Action Advantage (Page 620, bottom):\nA good action might be followed by a sequence of unlucky bad actions by the agent, leading to a low return for that initially good action. However, if we run many episodes, on average, good actions will tend to get higher returns than bad ones. We want to estimate how much better or worse an action is compared to other possible actions on average. This is called the action advantage. Estimating Advantage: Run many episodes. For each action taken in each episode, calculate its return (sum of discounted future rewards for that episode). Normalize all these action returns across all episodes (e.g., by subtracting the mean of all returns and dividing by their standard deviation). After normalization: Actions with a positive advantage were probably good. Actions with a negative advantage were probably bad. The \u0026ldquo;Perfect\u0026rdquo; Moment: Now that we have a way to evaluate each action (by its normalized advantage), we are ready to train our first agent using Policy Gradients.\nKey Takeaway for Credit Assignment and Returns: The core challenge is linking delayed rewards back to the actions that caused them.\nThe return (sum of discounted future rewards) is a way to estimate the long-term value of taking an action from a certain state. By running many episodes and normalizing these returns into advantages, we get a good signal about which actions were \u0026ldquo;good\u0026rdquo; (positive advantage) and which were \u0026ldquo;bad\u0026rdquo; (negative advantage). What this process is ultimately trying to achieve: To generate a learning signal for each action taken, even if the explicit reward from the environment is sparse or delayed. This learning signal (the advantage) will tell the policy gradient algorithm whether to make that action more or less likely in the future. This concept of discounted returns is fundamental to many RL algorithms. It\u0026rsquo;s how we assign value to actions that don\u0026rsquo;t necessarily yield immediate rewards.\n","permalink":"https://deepskandpal.github.io/bookshelf/hands-on-ml/chapter-18/","tags":null,"title":"Chapter 18: Reinforcement Learning"},{"categories":null,"content":"Problem Description Given a string s containing only digits, return all possible valid IP addresses that can be formed by inserting three dots into s.\nA valid IP address consists of exactly four integers separated by single dots. Each integer is between 0 and 255 (inclusive) and cannot have leading zeros (e.g., \u0026ldquo;01\u0026rdquo; is invalid, but \u0026ldquo;0\u0026rdquo; is valid).\nFor example, given s = \u0026quot;25525511135\u0026quot;, the valid IP addresses are [\u0026quot;255.255.11.135\u0026quot;].\nSolution Approach This problem can be solved using a backtracking approach. We need to split the string into four valid parts. A \u0026ldquo;valid part\u0026rdquo; is a number between 0 and 255, without any leading zeros unless the number is just \u0026ldquo;0\u0026rdquo;.\nThe core idea is to use a recursive helper function, say backtrack(start_index, path), where start_index is the current position in the input string s, and path is a list of the IP address parts we have found so far.\nThe base cases for the recursion are:\nIf we have found 4 parts (len(path) == 4) and we have consumed the entire string (start_index == len(s)), we have found a valid IP address. We join the parts with dots and add it to our results. If we have found 4 parts but haven\u0026rsquo;t consumed the string, or if we have consumed the string but have fewer than 4 parts, we have reached an invalid state, so we backtrack. In the recursive step, we try to form the next part of the IP address by taking a substring of length 1, 2, or 3 starting from start_index. For each potential part, we check if it\u0026rsquo;s valid:\nThe substring must not have leading zeros (unless it\u0026rsquo;s just \u0026ldquo;0\u0026rdquo;). The integer value of the substring must be between 0 and 255. If the part is valid, we add it to our current path and make a recursive call with an updated start_index. After the recursive call returns, we remove the part from the path to explore other possibilities (this is the \u0026ldquo;backtracking\u0026rdquo; step).\nSince an IP address has a fixed structure (4 parts, with each part being at most 3 digits), the length of the input string s that can form a valid IP is limited (4 to 12 characters). This means the recursion depth and branching factor are small and constant, leading to a constant time complexity.\nComplexity Analysis Time: O(1). The length of a valid IP address string is bounded between 4 and 12 characters. The backtracking algorithm has a fixed recursion depth (at most 4 levels) and a fixed branching factor (at most 3 branches for lengths 1, 2, 3). Therefore, the total number of explorations is constant, regardless of the input string length (within the valid range). Space: O(1). The recursion stack depth is at most 4. The path list stores at most 4 parts. The space required for the result list is not typically counted for space complexity, but even if it were, the number of possible valid IP addresses is bounded. Notes \u0026amp; Learnings This is a classic example of a backtracking problem where we need to find all valid combinations by partitioning a string. Careful handling of constraints is key: checking for leading zeros, the 0-255 range, and the final length of the path and consumed string. Pruning the search space is important for efficiency. For instance, we can stop early if the remaining characters are too many or too few to form the remaining parts of the IP address. My solution does this implicitly. For example, if start_index + length \u0026gt; n: stops us going out of bounds. ","permalink":"https://deepskandpal.github.io/dsa-log/restore-ip-addresses/","tags":["Backtracking","Python"],"title":"Restore Ip Addresses"},{"categories":["Embeddings"],"content":"","permalink":"https://deepskandpal.github.io/papershelf/currently-reading/vec2vec-embedding-translation/","tags":null,"title":"Harnessing the Universal Geometry of Embeddings"},{"categories":["Maths and Science"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/fundamentals-of-physics/","tags":null,"title":"Fundamentals of Physics"},{"categories":null,"content":"Problem Description (Optional Summary) The Problem: Write a function that takes such an array A representing integer D and updates it in-place to represent D + 1. Handle potential carries, including the case where the number of digits increases (like 99 + 1 = 100).\nSolution Approach Brute-Force (and why it’s often not allowed/intended):\nConvert the array [1, 2, 9] into the integer 129. Add 1: 129 + 1 = 130. Convert 130 back into an array [1, 3, 0]. Limitation: This fails if the integer D is larger than the maximum value the language’s built-in integer type can hold (this isn’t an issue for Python’s runtime integers, but the problem setup often simulates fixed-precision constraints or asks you to avoid this conversion). It also doesn’t modify the array in-place directly.\nThe Grade-School Algorithm Approach: Think about how you add 1 to a number on paper, like 129 + 1:\nStart from the rightmost digit (Least Significant Digit - LSB). Add 1 to the last digit: 9 + 1 = 10. Write down the 0, carry-over the 1. Move to the next digit to the left (2). Add the carry: 2 + 1 = 3. Write down the 3. No carry-over this time (carry is 0). Move to the next digit to the left (1). Add the carry: 1 + 0 = 1. Write down the 1. No carry-over. No more digits, result is 130. Now consider 99 + 1:\nStart rightmost: 9 + 1 = 10. Write 0, carry 1. Next digit: 9 + 1 (carry) = 10. Write 0, carry 1. No more digits, but we still have a carry. This means the result needs an extra digit. The result is 100.\nSimulating on the Array:\nWe operate directly on the array A. Increment the last element: A[n-1] += 1. Iterate from right-to-left (from n-1 down to 1). If A[i] == 10 (meaning we had a carry into this position): Set A[i] = 0. Increment the digit to the left: A[i-1] += 1. If A[i] is not 10 after adding the potential carry, then the carry propagation stops, and we can break the loop early. Special Case: After the loop, check if the first digit A[0] became 10. If it did, it means we had a carry out of the most significant digit. Set A[0] = 1. Append a 0 to the end of the array to accommodate the new least significant digit. (The book uses a slightly different but equivalent way: set A[0]=1, A.append(0)).\nComplexity Analysis Time: O(n) Space: O(1) Notes \u0026amp; Learnings learned how to do reverse loop for i in range(len(A)-1, -1, -1) when checking for carry over also add condition that the pointer has to be greater than 0 for carry ( to avoid appending 0 at the start) if A[i] == 10 and i\u0026gt;0: ","permalink":"https://deepskandpal.github.io/dsa-log/arbitrary_sum/","tags":["Pointers"],"title":"Arbitrary_sum"},{"categories":["Maths and Science"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/calculus-made-easy/","tags":null,"title":"Calculus Made Easy"},{"categories":["Biographies"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/walt-disney/","tags":null,"title":"Walt Disney: The Triumph of the American Imagination"},{"categories":["AI"],"content":"At its heart, \u0026ldquo;The Illusion of Thinking,\u0026rdquo; is about trying to genuinely understand how well these new \u0026ldquo;Large Reasoning Models\u0026rdquo; (LRMs) actually reason. You\u0026rsquo;ve probably heard about models that show their \u0026ldquo;thinking steps\u0026rdquo; before giving an answer, like with Chain-of-Thought. They often do better on benchmarks, which is exciting. But we felt that just looking at the final answer on standard math or coding tests wasn\u0026rsquo;t telling the whole story.\nThe Big Problem We Saw: Imagine you\u0026rsquo;re teaching a student. If they just spit out the right answer, you don\u0026rsquo;t know if they truly understood the method or just got lucky, or maybe even saw the answer somewhere before (which is a bit like \u0026ldquo;data contamination\u0026rdquo; in AI benchmarks). We wanted to look inside the \u0026ldquo;thinking process\u0026rdquo; and also test these models in a way where we could be sure they hadn\u0026rsquo;t just memorized the solutions.\nOur Approach: Puzzles! Instead of standard benchmarks, we turned to classic puzzles – things like the Tower of Hanoi, Checkers Jumping, River Crossing, and Blocks World (you can see these in Figure 3 on page 6). Why puzzles?\nControllable Complexity: We can make these puzzles a little harder or a little easier just by changing a small thing (like adding one more disk in Tower of Hanoi). This lets us see exactly when and how the models start to struggle. No Cheating: It\u0026rsquo;s highly unlikely these models have seen the exact step-by-step solutions to, say, a 10-disk Tower of Hanoi problem in their training data, especially in the specific format we used. Clear Rules, Clear Steps: The logic is all there. They don\u0026rsquo;t need outside knowledge, just the rules we give them. This helps us see if they can follow algorithmic steps. We Can Check Their Work: We can use simulators to see if every single step in their \u0026ldquo;thinking\u0026rdquo; is correct, not just the final answer. (See the top part of Figure 1 on page 2 for how we analyze both the \u0026ldquo;thoughts\u0026rdquo; and the final answer). What We Found (The \u0026ldquo;Illusion of Thinking\u0026rdquo; Part): There\u0026rsquo;s a Wall – Complete Accuracy Collapse: As we made the puzzles harder, every LRM we tested eventually hit a wall. Beyond a certain complexity, their accuracy just plummeted to zero. They couldn\u0026rsquo;t solve it at all, no matter how many tries we gave them (within a generous token budget). (You can see this in Figure 4 on page 7, where accuracy drops off a cliff for all puzzles as complexity increases).\nThey Stop Trying So Hard (Counter-intuitive Scaling): This was really interesting. You\u0026rsquo;d think that as a problem gets harder, the model would \u0026ldquo;think\u0026rdquo; more (i.e., use more computational steps or \u0026ldquo;tokens\u0026rdquo;). They do, up to a point. But right around where they start to fail catastrophically, they actually start reducing their reasoning effort, even if they have plenty of \u0026ldquo;thinking time\u0026rdquo; (token budget) left. It’s like they get overwhelmed and just give up prematurely. (Look at Figure 1, bottom middle graph on page 2, or Figure 6 on page 9 – the token usage goes up, then down).\nThe Three Regimes (When \u0026ldquo;Thinking\u0026rdquo; Helps, and When It Doesn\u0026rsquo;t):\nEasy Problems: Surprisingly, standard LLMs (without the fancy \u0026ldquo;thinking\u0026rdquo; steps) often did better or were more efficient than the LRMs. It seems the extra \u0026ldquo;thinking\u0026rdquo; by LRMs was sometimes unnecessary or even confusing for simple tasks. (See Figure 5 on page 8, the \u0026ldquo;low complexity\u0026rdquo; section). Medium Problems: This is where LRMs shone. The extra \u0026ldquo;thinking\u0026rdquo; steps helped them solve problems that the standard LLMs struggled with. (Figure 5, \u0026ldquo;medium complexity\u0026rdquo;). Hard Problems: Both the LRMs and the standard LLMs eventually failed completely. The \u0026ldquo;thinking\u0026rdquo; could delay the failure, but it couldn\u0026rsquo;t prevent it. (Figure 5, \u0026ldquo;high complexity\u0026rdquo;). Peeking Inside the \u0026ldquo;Thoughts\u0026rdquo; (Figure 1, bottom right, and Figure 7a on page 10):\nOverthinking Simple Stuff: For easy puzzles, we saw models find the correct solution path early in their \u0026ldquo;thoughts\u0026rdquo; but then continue to explore, sometimes making mistakes later or just wasting effort. Struggling with Harder Stuff: For more complex problems (but before they completely collapsed), they\u0026rsquo;d often explore a lot of wrong paths before stumbling upon the correct one, if they found it at all. Giving Up: For the really hard problems, they just couldn\u0026rsquo;t find a correct solution path, no matter how long their \u0026ldquo;thoughts\u0026rdquo; were. They Can\u0026rsquo;t Always Follow Directions: Even when we gave the models the exact algorithm to solve a puzzle (like Tower of Hanoi, see Figure 8a and 8b on page 11), they still failed around the same complexity level! This tells us they have trouble just executing a known series of logical steps, not just finding the steps.\nInconsistent Reasoning: A model might be pretty good at Tower of Hanoi up to a certain number of disks, which involves many steps. But then it might fail miserably at a River Crossing problem that actually requires fewer steps but has different types of constraints (Figure 8c and 8d on page 11). This suggests they aren\u0026rsquo;t learning a general, abstract reasoning skill but are perhaps more sensitive to the specific structure or \u0026ldquo;feel\u0026rdquo; of a problem, maybe due to what they saw in training.\nSo, What\u0026rsquo;s the \u0026ldquo;Illusion\u0026rdquo;? The \u0026ldquo;illusion\u0026rdquo; is that while these models appear to be thinking and reasoning like humans when they produce step-by-step traces, their underlying capabilities have hard limits. Their reasoning isn\u0026rsquo;t as robust or generalizable as it might seem from their performance on some benchmarks. It seems they are very good at pattern matching and interpolating from things they\u0026rsquo;ve seen, but when faced with problems that require them to systematically apply rules in novel or increasingly complex ways (algorithmic complexity), they break down.\nWhy Does This Matter? Our findings suggest we need to be cautious about how much we trust the \u0026ldquo;reasoning\u0026rdquo; of current LRMs, especially for complex, high-stakes tasks. It also points towards areas where these models need to improve, like developing more robust algorithmic execution and generalization capabilities.\n","permalink":"https://deepskandpal.github.io/papershelf/read/illusion-of-thinking/","tags":null,"title":"The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity"},{"categories":["Thoughts"],"content":"So I watched or rather tried to watch this weeks movie releases. I had 2 options one was House full 5 and the other was a few weeks old but new to OTT bhul chuk maaf. Now I personally never found any housefull movie funny not even the first one. Except that one dialogue by Ritesh Deshmukh regarding \u0026ldquo;Biwi\u0026rdquo; and \u0026ldquo;Behen\u0026rdquo; in the first one I have mostly cringed. Pardon me if my sense of humour is not keeping up with the times but I grew up watching comedies of Priya Darshan and Akshay kumar. Raju hirani, and too some extent I find Rohit shetty older films also funny with Chennai Express being my favorite from his Filmography. So I would think of myself as someone who under stands what \u0026ldquo;Brain rot comedy\u0026rdquo; is in Hindi Cinema context. I don\u0026rsquo;t really understand why we have moved on to accept absolutely mediocre films with sub par production/acting/writing/production-design/music/dance in the name of \u0026ldquo;oH bUt iTs tImE pASS dON\u0026rsquo;t tHinK tOO mUch\u0026rdquo; . If i pay money for something I am liable for getting what is expected in return. Let me draw a parallel. If you go to a restaurant to have creamy shahi paneer and the waiter serves you the Now known to everyone \u0026ldquo;Analogue\u0026rdquo; Paneer. Which we all know is not real paneer and also bad for your health but it tastes almost like an original paneer. Would you accept my argument \u0026ldquo;But bro itne paise mai aisa hi paneer milega restaurant ko bhi toh paisa banane hai\u0026rdquo;. Won\u0026rsquo;t you feel cheated that in the name of paneer garbage was fed to you? ( a seemingly paneer looking garbage but still gargbage)\nSame I think is the case with today\u0026rsquo;s hindi films especially in the last few years more specifically post covid. It feels as if they are lost. We do get a gem once in a while but a good hindi movie which entertains you even for time pass is now an exception rather than a norm.\nWhy does it sound as if I am pissed well because I am. So pissed that I am trying very hard with each word as I type to not use any abuse ( I abuse a lot IRL but I normally try to keep a calm composure while writing) but its getting harder and harder to keep it at bay since I was trying to watch bhul chuk maaf for 2 reasons.\nI cannot could not bring myself to buy a ticket for HOUSEFULL 5A/B whatever. My wife is a huge housefull fan but even she was not interested in watching the movie after seeing the trailer Bhul chuk maaf was available in prime video we were like great we will have a good time plus the expectation is was a bit higher for me since I didn\u0026rsquo;t have any from housefull AND OH MY GOD my blood just boiled with each passing frame. I know sounds like an over reaction but its good that we stopped watching after 21 minutes. My wife went to sleep and I switched to Netflix to put on Oppenheimer something which I couldn\u0026rsquo;t get a chance to watch earlier and was looking for an opportunity to catch up. And within 5 min my blood stopped boiling and I was feeling pleasant again.\nNow I know it is NOT AT ALL fair to compare Oppenheimer with any of these movies . And I am not going to . I am not stupid. But while was watching it I realized something really important. Christoper Nolan cares about his movies.\nIn Bollywood atleast for now it feels nobody cares anymore.\nWhy am I generalizing the whole hindi film industry because its pretty clear that the care factor has gone for a toss at least at the top layer (the yrfs , dharmas, maddocks etc). You pick up randomly any 5 movies from these production houses in the last 5 years and you compare them against any random 5 moves from pre covid era say 2011-2018. And keep a base metric of IMBD ratings and reviews you would see that they have fallen. I have done a very adhoc analysis but If needed I will do a full fledge one to draw some statistically significant conclusions but the high level conclusion is this\n\u0026ldquo;Movies have become expensive over the last 5 years but the quality has declined or has been in decline\u0026rdquo; for the quantification of quality a simple sentiment analysis of the reviews would be easiest way to do it. Now I know its very \u0026ldquo;scientific\u0026rdquo; it feels more like a feeler but this can be a good basis for where to look for directionally to get more data , better methods to prove this. But I think its a waste of time . I don\u0026rsquo;t need to ask a Michelin star chef wether the omelette I am having has too much salt I can eat it and tell it. Similarly we have over complicated movie going experience as well. The film is bad if people who are watching it don\u0026rsquo;t like it , if it doesn\u0026rsquo;t do a good box office, if ( and this is new) no OTT wants it. Its just that simple.\nPeople just think \u0026ldquo;Oh the film viewing is very subjective look Anupama chopra found befikre nice.\u0026rdquo; well true it can happen but it \u0026ldquo;rarely happens\u0026rdquo; that is the whole point of exceptions. Exceptions show case that there is a general pattern and for that general pattern there are a few Anomalies. So when I feel that hindi cinema is going through a low time I am not just saying after watching ONLY 21 minutes of the latest comedy movie.\nIn my specific case I have watched, stree2, singham again, bhool bhulaiya 3, dunki, jawan, pathaan, dhrishyam2, shaitaan , viky vida , fighter, sky force and many many more\nout of this I have only loved dhrishyam2 and some what liked fighter in contrast if i compare them with other movies that i have watched recently like , MI7 , MI8, die hard 3, maharaja ,the menu, Nosferatu, bhramayugam, CRAZZY, american made, KGF Chapter 2 , kantara, Leo, godfather (telegu), the movie that i loved the most was MI7 , American made, maharaja , kantara , KGF Chapter 2 and then i liked Leo, die hard 3, MI8 , and I hated the menu and godfather.\nSo clearly for me the hindi cinema has gone down just by this simple comparison. And I am sure a lot us feel the same way.\nAt the end of they day what I am trying to say is 2 things\nI want my money\u0026rsquo;s worth , If you are comedy movie make me fucking laugh, if you are making an action movie keep me at the edge of the movie. I should be able to pick your films (yes you hindi film directors) over others and yes you have very very tough competition especially in today\u0026rsquo;s world.\nDo not justify mediocrity do not produce mediocre products and don\u0026rsquo;t give me this bullshit \u0026ldquo;FiLm MakInG iS hARD\u0026rdquo; bitch every job is hard my job is also hard for you ( yes you, you fanatic fans , hindi film apologists), I shouldn\u0026rsquo;t need Data or logic to conclude that shit is bad for you hence you shouldn\u0026rsquo;t eat it. Sometimes its implied.\nI just hope that as film fan I continue to find hindi cinema as a viable movie going experience\n","permalink":"https://deepskandpal.github.io/thoughts/bollywood/","tags":["Bollywood"],"title":"Bollywood needs a hard reset"},{"categories":["Non Fiction"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/fabric-of-reality/","tags":null,"title":"Fabric of Reality"},{"categories":["Non Fiction"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/thinking-systems/","tags":null,"title":"Thinking in Systems"},{"categories":["Business"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/creativity-inc/","tags":null,"title":"Creativity Inc"},{"categories":["Maths and Science"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/elements/","tags":null,"title":"The Elements of Statistical Learning"},{"categories":["Maths and Science"],"content":"TO BE READ ","permalink":"https://deepskandpal.github.io/bookshelf/linear-done-right/","tags":null,"title":"Linear Done Right"},{"categories":["Maths and Science"],"content":"To be Read\n","permalink":"https://deepskandpal.github.io/bookshelf/machine-learning-prob-approach/","tags":null,"title":"Machine Learning: A Probabilistic Perspective"},{"categories":["Programming"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/lsr/","tags":null,"title":"Introduction to Algorithms, fourth edition "},{"categories":["Data Science"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/statistical-rethinking/","tags":null,"title":"Statistical Rethinking: A Bayesian Course with Examples in R and Stan"},{"categories":["Data Science"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/applied-predictive/","tags":null,"title":"Applied Predictive"},{"categories":["Data Science"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/statistical-test/","tags":null,"title":"Statistical Test"},{"categories":["Data Science"],"content":"Notes / Summary for the Book (Your general notes or summary about the book go here. For chapter-specific notes, create separate .md files within this book\u0026rsquo;s folder, e.g., chapter-1.md, introduction.md etc. These will be automatically listed on the book\u0026rsquo;s page.)\n","permalink":"https://deepskandpal.github.io/bookshelf/marketing-analytics/","tags":null,"title":"Marketing Analytics"},{"categories":["Thoughts"],"content":"In Indian Twitter (currently known as X) lingo, any kind of verbal skirmish is termed as e-lafda (fight). The latest one is between YouTuber Mohan Mangal and ANI over the use of copyrighted clips. Mohak claims that he has used 5-10 seconds of clips in a few of his recent videos from ANI coverage regarding recent events such as Operation Sindoor in his videos covering the same, and ANI has applied a copyright strike on his YouTube channel because of this. His claim is that when he contacted ANI for the reason and the removal of the copyright strike, he was informed that it would only happen if he pays somewhere in the range of 40-50 lakhs + GST to ANI for the use of those clips in his videos (they claim they have applied for 8 copyright strikes, so there are more to come). Failure to do so will result in getting 3 strikes (2 of which he has received already) that will lead to an automatic channel deletion as per YouTube policy.\nMohak obviously given his commentary on the topic wants to take up the fight but the real questions to ask are\nCan ANI do this? if yes will they succeed Should ANI do this will this become a norm. Now copy right strikes have been for a while seen a \u0026ldquo;weapon\u0026rdquo; to be used against any content creator with reasons to be used can range from legal infringment to take down attempts, Something which I would not like to get into. I believe there are some universal truths and post that everything is up for debate. Right now the debate around this issue is pretty interesting.\nPeople on the left are completely behind Mohak on this; supportingn him telling him that he is right and sharing his message accross twitter and other media platforms. Left leaning political leaders have also come out in support of him. Obviously the people who are opposite to him are the people on the Right. It helps because ANI is often seen as a right leaning news organization but from my perspective they are centre right. Most of the time they try to be neutral or have a moral stance but the Lean towards BJP and right wing is pretty clear. So natrually as per the rules of any good ecosystem the right has come and supported ANI accussing Mohak and everybody supporting him or even trying to debate with them of being \u0026ldquo;left cronies\u0026rdquo; or \u0026ldquo;sadak chap\u0026rdquo; or whatever fancy insult they would like to call them. I personally don\u0026rsquo;t really worry or care about these personal attacks. It usually means the person resorting to these has no substantial argument left in them so they are trying to reach the bottom of the barrel. More than what the arguement is being made about I like to disect how well the arguement is formed in support or against stance. So I\u0026rsquo;ll try to do both here\nPro Mohak side\nYoutube has a fair use policy that allows copyright material avaliable on the platform to be used for commentary and educational purpose. If there is a problem with the copyright infrigment there are better ways to deal with revenue sharing of the video. Demonetization. Prior intimation of the copyright strike asking for fair compensation of the clips used. The way ANI handled it, Its pretty obvious they don\u0026rsquo;t want to go a sustainable business model route where everyone can win. The idea of using copyright strike as a gun to shoot down the whole household for ransom. Yes if this is how they want to play it then lets play it in court see how it turns. Probably some judgement around this would help. For the people who are supporting ANI they should know tomorrow they can be in the same place sometime in the future. Pro ANI side\nOur clip our right we can do whatever the hell we want. We spend money on stringers, reporters sources equipments, work hard verify news produce clippings. So why shouldn\u0026rsquo;t we get compensated for it. News channels pay for our clips why can\u0026rsquo;t youtubers do the same. We offered mohak a way to resolve this with money. We just want fair compensation today youtube is also a revenue generating platform . If a content creator is making revenue, paying for a team , equipements , resources licesning of softwares and other media , why can\u0026rsquo;t they pay us? Its our perogative to decide who we charge money whom we don\u0026rsquo;t. We are not obliged to favour anyone specific because they support ur or our in our good books. That being said we give concessions when we want to ; to whoever we want to. I beleive that this was one of the few ways we can look at it but twitter as usual barring some accounts believes in serving the lowest common denominator and rage baiting. I hate this part of the site. Because majority of people either are low IQ or behave like a low IQ for engagement farming. After all ins\u0026rsquo;t all this about this about money\n","permalink":"https://deepskandpal.github.io/thoughts/copyright/","tags":[],"title":"Copyright: right to intellectual property or Tool for Extortation?"},{"categories":["Engineering"],"content":"The Tyranny of the Top Rank Ever feel like your favorite music app is playing DJ Déjà Vu, stuck on an endless loop of your top-played genres or artists you already know and love? Or, if you\u0026rsquo;re an artist, have you wondered how your fresh, amazing track can cut through the noise and find new ears? This isn\u0026rsquo;t just a feeling; it\u0026rsquo;s a common challenge in any system that ranks and recommends items – whether it\u0026rsquo;s songs, movies, products, or even internal company resources.\nMost recommendation algorithms are designed to be highly effective at finding what they think is the absolute \u0026ldquo;best\u0026rdquo; for you based on your past behavior and similar users. This deterministic precision is great for relevance, but it can lead to a predictable experience. We call this the tyranny of the top rank. Users might get trapped in filter bubbles, missing out on diverse and potentially delightful new discoveries. Recommendations can become stale, and for content creators, it means items ranked just slightly lower than the top might never see the light of day.\nBut what if there was a simple way to shake things up, to inject a bit of serendipity without completely abandoning relevance? What if a tiny, controlled \u0026rsquo;nudge\u0026rsquo; – a bit of strategic randomness – could unlock a richer, more diverse experience?\nThat\u0026rsquo;s exactly what we\u0026rsquo;re going to explore. We\u0026rsquo;ll look at how introducing a small amount of \u0026ldquo;jitter\u0026rdquo; or using \u0026ldquo;probabilistic sampling\u0026rdquo; on an item\u0026rsquo;s relevance score can significantly improve the diversity of recommendations, making them more engaging and useful in the long run.\nThe Core Idea: Controlled Randomness At the heart of our approach is the idea of controlled randomness. Instead of always picking the item with the highest calculated relevance score, we slightly perturb these scores randomly before making the final selection. This manages the classic exploration-exploitation trade-off:\nExploitation: Picking the known best (high deterministic score). Exploration: Trying out other potentially good options to discover new value and provide variety. Our goal is to lean heavily on exploitation (we still want relevant recommendations!) but introduce just enough exploration to make things interesting.\nOur Journey: From Internal Challenge to Broader Solution Our journey into this started with a similar challenge: how to ensure a wide range of valuable internal training resources were being utilized, rather than just the same few top-scorers. We found that even with well-defined scoring for relevance and effectiveness, a deterministic approach led to a predictable and limited selection. This prompted us to explore how controlled randomness could improve the diversity of assigned trainings. This experience highlighted a universal issue in ranked selections, leading us to generalize these techniques for broader application.\nOur Case Study: Crafting a \u0026ldquo;Discovery Weekly\u0026rdquo; Playlist Imagine we\u0026rsquo;re building a feature like Spotify\u0026rsquo;s \u0026ldquo;Discover Weekly.\u0026rdquo; We have a pool of newly released or under-the-radar songs. For a given user, our main recommendation engine has already calculated a UserMatchScore (0-100) for each song, indicating how well it fits their known taste profile.\nIf we always pick the songs with the top 10 UserMatchScores, the playlist might be highly relevant but could lack variety, potentially just offering more of what the user already listens to. We want to introduce songs that are still a good match but might be from slightly different sub-genres or a bit less obvious.\nThis is where our randomization techniques come in. We\u0026rsquo;ll apply them to the UserMatchScore.\nTwo Simple Techniques for Introducing Diversity We explored two main methods:\nSimple Jitter (Uniform Distribution Jitter):\nConcept: We take each song\u0026rsquo;s UserMatchScore and add a small, random \u0026ldquo;nudge\u0026rdquo; to it. This nudge can be positive-only (only boosting scores) or positive-negative (allowing scores to be randomly increased or decreased). How it works: We define a Jitter_Coefficient (JC). For the current set of candidate songs, we find the range of their UserMatchScores (Max Score - Min Score). The maximum possible nudge is JC * score_range. For each song, a random factor (e.g., between -1 and 1 for positive-negative jitter) is multiplied by this maximum nudge to get the actual jitter amount. Jittered_Score = UserMatchScore + Actual_Jitter_Amount. The song with the highest Jittered_Score is selected. Effect: This gives a chance for songs with slightly lower UserMatchScores to be selected if they get a \u0026ldquo;lucky\u0026rdquo; positive nudge while a higher-scoring song gets an \u0026ldquo;unlucky\u0026rdquo; negative nudge (or a smaller positive one). Probabilistic Normal Sampling:\nConcept: We treat each song\u0026rsquo;s UserMatchScore not as a fixed point, but as the average (mean) of a bell curve (Normal distribution) representing its potential \u0026ldquo;appeal\u0026rdquo; in any given instance. How it works: We define a Standard_Deviation_Coefficient (SDC). The standard deviation (std_dev) for each song\u0026rsquo;s Normal distribution is SDC * score_range (score range of current candidates). For each song, we draw one random sample from its specific Normal distribution: Sampled_Score = random_sample_from_Normal(mean=UserMatchScore, std_dev=std_dev). The song with the highest Sampled_Score is selected. Effect: Songs with higher UserMatchScores are statistically more likely to produce the highest sample, but there\u0026rsquo;s a \u0026ldquo;smoother\u0026rdquo; probability of songs with slightly lower scores winning. The chance of an upset depends on how much the bell curves of different songs overlap. Simulating the Impact To see these techniques in action, we ran simulations. For a chosen set of candidate songs for our \u0026ldquo;Discovery Weekly,\u0026rdquo; we repeatedly selected songs using different JC and SDC values and tracked:\nHow often the original #1 song (by UserMatchScore) was still selected. How often the selection stayed within the Top-K original ranks (e.g., Top-10). How many different songs were selected over time (a measure of diversity/exploration). What proportion of the available candidate songs were \u0026ldquo;touched\u0026rdquo; by the selection process. (Here, you would embed your generic plots, similar to the ones we discussed, showing lines for \u0026ldquo;Avg Prob(Select Top 1)\u0026rdquo; decreasing and \u0026ldquo;Avg Distinct Items Selected\u0026rdquo; increasing as JC/SDC increases. Label axes generically, e.g., \u0026ldquo;Randomization Coefficient (JC/SDC)\u0026rdquo; and \u0026ldquo;Selection Probability\u0026rdquo; or \u0026ldquo;Number of Distinct Items\u0026rdquo;.)\nExample Plot Caption: Fig 1: As the randomization coefficient (JC or SDC) increases, the probability of selecting the deterministically top-ranked item typically decreases (blue line), while the number of unique items selected over many trials tends to increase (green line), indicating greater exploration.\nKey Findings from Simulation Our simulations (even with simplified scores) consistently showed:\nIncreased Diversity: Both methods successfully increased the variety of items selected compared to a purely deterministic approach. Controllability: The JC and SDC parameters provided effective control over the trade-off. Small values resulted in minor deviations from the top ranks, while larger values promoted more significant exploration. Simple Jitter (positive_negative type): Tended to explore more broadly for a given relative coefficient, sometimes selecting items further down the deterministic rank if their \u0026ldquo;luck factor\u0026rdquo; was high. Normal Sampling: Provided a \u0026ldquo;smoother\u0026rdquo; exploration, where items with scores closer to the top had a proportionally better chance of being selected during an \u0026ldquo;upset\u0026rdquo; than items ranked much lower. The probability of selection decreased more gradually. The \u0026ldquo;best\u0026rdquo; coefficient depends on the specific goals: if aggressive discovery is desired, a higher coefficient might be chosen. If maintaining very high relevance is paramount, a lower coefficient is better. Benefits \u0026amp; Considerations Pros of these simple randomization techniques:\nSimplicity: Easy to understand and implement on top of existing ranking scores. Effective Diversity Boost: Can significantly improve the variety of recommendations. Improved User Experience: Can reduce recommendation fatigue and increase serendipitous discoveries. Low Computational Cost: Adds minimal overhead to the selection process. Cons/Considerations:\nNot Adaptive: These methods don\u0026rsquo;t \u0026ldquo;learn\u0026rdquo; from user feedback (e.g., if a jitter-selected song is consistently skipped). They rely on the quality of the underlying DeterministicScore. Offline Tuning: The JC or SDC parameter needs to be tuned, often through offline simulations like ours, to find the right balance for your specific application. Potential for \u0026ldquo;Sub-optimal\u0026rdquo; (by one metric): By design, you will sometimes recommend items that aren\u0026rsquo;t the absolute #1 by your primary deterministic score. This is the trade-off for diversity. Conclusion: Embrace the (Controlled) Chaos! Always showing the \u0026ldquo;best\u0026rdquo; isn\u0026rsquo;t always best for long-term user engagement or content discovery. Introducing a small, controlled amount of randomness, like the Simple Jitter or Probabilistic Normal Sampling techniques we explored, can be a surprisingly powerful and easy-to-implement strategy. It helps break out of predictability, surfaces a wider range of relevant items, and can make your recommendation system feel more dynamic and intelligent.\nSo, next time you\u0026rsquo;re building a ranking system, consider adding a little \u0026ldquo;jitter bug\u0026rdquo; – your users (and maybe even your content providers) might thank you for the delightful new discoveries!\nTutorial: Adding Diversity to Rankings with Jitter \u0026amp; Normal Sampling Below is a self-contained Python tutorial demonstrating how to add controlled randomness to ranking scores using Simple Jitter and Probabilistic Normal Sampling.\nColab tutorial can be found here 1. Setup and Sample Song Data import pandas as pd import numpy as np import random import matplotlib.pyplot as plt # --- Generate Synthetic Song Data --- num_songs = 100 # Candidate songs for our discovery playlist genres = [\u0026#34;Indie Pop\u0026#34;, \u0026#34;Electro Funk\u0026#34;, \u0026#34;Ambient\u0026#34;, \u0026#34;Folk Rock\u0026#34;, \u0026#34;Lo-fi Beats\u0026#34;, \u0026#34;Synthwave\u0026#34;] subgenres = { \u0026#34;Indie Pop\u0026#34;: [\u0026#34;Dream Pop\u0026#34;, \u0026#34;Bedroom Pop\u0026#34;], \u0026#34;Electro Funk\u0026#34;: [\u0026#34;Nu-Disco\u0026#34;], \u0026#34;Ambient\u0026#34;: [\u0026#34;Drone\u0026#34;, \u0026#34;Chillout\u0026#34;], \u0026#34;Folk Rock\u0026#34;: [\u0026#34;Indie Folk\u0026#34;], \u0026#34;Lo-fi Beats\u0026#34;: [\u0026#34;Chillhop\u0026#34;], \u0026#34;Synthwave\u0026#34;: [\u0026#34;Retrowave\u0026#34;] } song_data = [] for i in range(num_songs): genre = np.random.choice(genres) base_score = 60 if genre in [\u0026#34;Indie Pop\u0026#34;, \u0026#34;Lo-fi Beats\u0026#34;]: base_score += np.random.randint(20, 35) else: base_score += np.random.randint(0, 20) user_match_score = np.clip(base_score + np.random.normal(0, 8), 40, 100) song_data.append({ \u0026#39;SongID\u0026#39;: f\u0026#34;Song_{i+1:03}\u0026#34;, \u0026#39;Title\u0026#39;: f\u0026#34;{genre} Track {i+1}\u0026#34;, \u0026#39;Artist\u0026#39;: f\u0026#34;Artist_{np.random.randint(1, 20)}\u0026#34;, \u0026#39;Genre\u0026#39;: genre, \u0026#39;UserMatchScore\u0026#39;: round(user_match_score, 2) }) songs_df = pd.DataFrame(song_data) print(\u0026#34;Sample Candidate Songs:\u0026#34;) print(songs_df.head()) print(\u0026#34;\\nTop 5 Songs (Deterministic by UserMatchScore):\u0026#34;) print(songs_df.sort_values(\u0026#39;UserMatchScore\u0026#39;, ascending=False).head()) 2. Implementing the Selection Functions A. Simple Jitter Function def apply_simple_jitter_selection(df_candidates, score_column, jc_value, jitter_type=\u0026#39;positive_negative\u0026#39;, id_column=\u0026#39;SongID\u0026#39;): if df_candidates.empty: return None if len(df_candidates) == 1: return df_candidates[id_column].iloc[0] temp_df = df_candidates.copy() max_score = temp_df[score_column].max() min_score = temp_df[score_column].min() score_range = max_score - min_score if score_range \u0026gt; 0 and jc_value \u0026gt; 0: n_rows = len(temp_df) if jitter_type == \u0026#39;positive_negative\u0026#39;: random_factors = np.random.uniform(-1.0, 1.0, size=n_rows) elif jitter_type == \u0026#39;positive_only\u0026#39;: random_factors = np.random.rand(n_rows) else: random_factors = np.zeros(n_rows) jitter_amount = random_factors * jc_value * score_range temp_df[\u0026#39;Jittered_Score\u0026#39;] = temp_df[score_column] + jitter_amount else: temp_df[\u0026#39;Jittered_Score\u0026#39;] = temp_df[score_column] max_jittered_score = temp_df[\u0026#39;Jittered_Score\u0026#39;].max() top_songs = temp_df[ np.isclose(temp_df[\u0026#39;Jittered_Score\u0026#39;], max_jittered_score) ][id_column].tolist() return random.choice(top_songs) if top_songs else None B. Probabilistic Normal Sampling Function def apply_normal_sampling_selection(df_candidates, score_column, sdc_value, id_column=\u0026#39;SongID\u0026#39;): if df_candidates.empty: return None if len(df_candidates) == 1: return df_candidates[id_column].iloc[0] temp_df = df_candidates.copy() max_score = temp_df[score_column].max() min_score = temp_df[score_column].min() score_range = max_score - min_score std_dev = max(0, sdc_value * score_range) if score_range \u0026gt; 0 else 0 if std_dev \u0026gt; 0: temp_df[\u0026#39;Sampled_Score\u0026#39;] = np.random.normal( loc=temp_df[score_column], scale=std_dev ) else: temp_df[\u0026#39;Sampled_Score\u0026#39;] = temp_df[score_column] max_sampled_score = temp_df[\u0026#39;Sampled_Score\u0026#39;].max() top_songs = temp_df[ np.isclose(temp_df[\u0026#39;Sampled_Score\u0026#39;], max_sampled_score) ][id_column].tolist() return random.choice(top_songs) if top_songs else None 3. Running a Small Simulation N_PLAYLISTS_TO_SIMULATE = 100 PLAYLIST_SIZE = 10 TUTORIAL_JC = 0.7 TUTORIAL_JITTER_TYPE = \u0026#39;positive_negative\u0026#39; TUTORIAL_SDC = 0.3 all_jitter_selections = [] all_normal_selections = [] candidate_songs_df = songs_df.copy() for _ in range(N_PLAYLISTS_TO_SIMULATE): for _ in range(PLAYLIST_SIZE): selected_jitter = apply_simple_jitter_selection( candidate_songs_df, \u0026#39;UserMatchScore\u0026#39;, TUTORIAL_JC, TUTORIAL_JITTER_TYPE ) if selected_jitter: all_jitter_selections.append(selected_jitter) for _ in range(PLAYLIST_SIZE): selected_normal = apply_normal_sampling_selection( candidate_songs_df, \u0026#39;UserMatchScore\u0026#39;, TUTORIAL_SDC ) if selected_normal: all_normal_selections.append(selected_normal) jitter_counts = pd.Series(all_jitter_selections).value_counts(normalize=True) normal_counts = pd.Series(all_normal_selections).value_counts(normalize=True) print(\u0026#34;\\n--- Simple Jitter Selections (Top 10) ---\u0026#34;) print(pd.merge( jitter_counts.head(10).rename(\u0026#34;Prop_Jitter\u0026#34;), songs_df, left_index=True, right_on=\u0026#39;SongID\u0026#39; )[[\u0026#39;SongID\u0026#39;,\u0026#39;Title\u0026#39;,\u0026#39;Genre\u0026#39;,\u0026#39;UserMatchScore\u0026#39;,\u0026#39;Prop_Jitter\u0026#39;]]) print(\u0026#34;\\n--- Normal Sampling Selections (Top 10) ---\u0026#34;) print(pd.merge( normal_counts.head(10).rename(\u0026#34;Prop_Normal\u0026#34;), songs_df, left_index=True, right_on=\u0026#39;SongID\u0026#39; )[[\u0026#39;SongID\u0026#39;,\u0026#39;Title\u0026#39;,\u0026#39;Genre\u0026#39;,\u0026#39;UserMatchScore\u0026#39;,\u0026#39;Prop_Normal\u0026#39;]]) 4. Visualizing the Diversity # Prepare genre distributions df_jitter = pd.merge(jitter_counts.rename(\u0026#34;Prob\u0026#34;), songs_df[[\u0026#39;SongID\u0026#39;,\u0026#39;Genre\u0026#39;]], left_index=True, right_on=\u0026#39;SongID\u0026#39;) jitter_genre_dist = df_jitter.groupby(\u0026#39;Genre\u0026#39;)[\u0026#39;Prob\u0026#39;].sum().sort_values(ascending=False) df_normal = pd.merge(normal_counts.rename(\u0026#34;Prob\u0026#34;), songs_df[[\u0026#39;SongID\u0026#39;,\u0026#39;Genre\u0026#39;]], left_index=True, right_on=\u0026#39;SongID\u0026#39;) normal_genre_dist = df_normal.groupby(\u0026#39;Genre\u0026#39;)[\u0026#39;Prob\u0026#39;].sum().sort_values(ascending=False) # Plotting fig, axes = plt.subplots(1, 2, figsize=(14, 6)) jitter_genre_dist.plot(kind=\u0026#39;bar\u0026#39;, ax=axes[0]) axes[0].set_title(f\u0026#39;Simple Jitter (JC={TUTORIAL_JC})\u0026#39;) axes[0].set_ylabel(\u0026#39;Selection Probability\u0026#39;) normal_genre_dist.plot(kind=\u0026#39;bar\u0026#39;, ax=axes[1]) axes[1].set_title(f\u0026#39;Normal Sampling (SDC={TUTORIAL_SDC})\u0026#39;) axes[1].set_ylabel(\u0026#39;Selection Probability\u0026#39;) plt.tight_layout() plt.show() # Deterministic baseline deterministic = songs_df.sort_values(\u0026#39;UserMatchScore\u0026#39;, ascending=False).head( N_PLAYLISTS_TO_SIMULATE * PLAYLIST_SIZE ) deterministic_dist = deterministic[\u0026#39;Genre\u0026#39;].value_counts(normalize=True) plt.figure(figsize=(7, 6)) deterministic_dist.plot(kind=\u0026#39;bar\u0026#39;) plt.title(\u0026#39;Deterministic Top Scores\u0026#39;) plt.ylabel(\u0026#39;Proportion\u0026#39;) plt.show() ","permalink":"https://deepskandpal.github.io/tech-writings/jitter-bug/","tags":["Recommendations Sytems"],"title":"The Jitter Bug - How a Little Randomness Makes Your Recommendations Smarter (and More Fun!)"},{"categories":["Business"],"content":"Notes / Summary for the Book ","permalink":"https://deepskandpal.github.io/bookshelf/polyster-prince/","tags":null,"title":"Polyster Prince"},{"categories":[],"content":"Absolutely! That 6-step process you\u0026rsquo;ve outlined is GOLD. It\u0026rsquo;s precisely how you build deep, intuitive understanding, especially for recursive problems which can feel like black magic otherwise. You\u0026rsquo;re essentially becoming the debugger and the CPU, tracing the logic yourself.\nFor Chapter 6 (Strings) in EPI Python, here are a few \u0026ldquo;keystone\u0026rdquo; problems that lend themselves exceptionally well to your 6-step deep dive method. These problems either involve recursion (like your Towers of Hanoi example) or intricate iterative logic where hand-simulation is invaluable.\nTop Keystone Problems for Your 6-Step Method:\n6.7 Compute All Mnemonics for a Phone Number (Page 74)\nWhy it\u0026rsquo;s Keystone: This is a classic recursion/backtracking problem. You\u0026rsquo;re exploring a tree of possibilities. Your 6-step method is perfect for this. Understanding how the partial_mnemonic is built up and how the digit_index drives the recursion is key. Fits your 6 Steps: High-level: For each digit, try all its possible letters. Parameters: Current digit index being processed, the partially built mnemonic string. Base Case: All digits processed. Hand-simulation: Crucial for N=2 or N=3 digits to see the call stack unfold. Pseudocode: Will clearly show the loop for letters and the recursive call. 6.10 Compute All Valid IP Addresses (Page 77)\nWhy it\u0026rsquo;s Keystone: Another excellent recursion/backtracking problem. It\u0026rsquo;s about partitioning a string into 4 valid parts. Slightly more complex than mnemonics due to segment validation rules (0-255, no leading zeros for multi-digit segments). Fits your 6 Steps: High-level: Try to make 3 cuts in the string to form 4 segments. Parameters: Index in the input string to start forming the current segment, number of segments already formed, list of current segments. Base Case: 4 segments formed AND the entire input string is consumed. Hand-simulation: With a short string like \u0026ldquo;19216811\u0026rdquo; or even \u0026ldquo;2552551111\u0026rdquo;, trying to place the dots. 6.2 Base Conversion (Page 69) - Specifically, the construct_from_base part, or the \u0026ldquo;integer to string in base b2\u0026rdquo; logic.\nWhy it\u0026rsquo;s Keystone: While the overall problem has two parts (string_base1 -\u0026gt; int, then int -\u0026gt; string_base2), the second part is often naturally recursive. It\u0026rsquo;s a simpler recursion than mnemonics but beautifully illustrates the \u0026ldquo;divide problem, solve subproblem, combine result\u0026rdquo; pattern. The variant in 6.3 (Integer to Spreadsheet Column ID) is very similar. Fits your 6 Steps (for int to string_base_b2): High-level: To convert N to base B, get the last digit (N % B), then convert N // B to base B and append the digit. Parameters: The integer to convert, the target base. Base Case: Integer is 0. Hand-simulation: Convert 10 (decimal) to base 2. construct_from_base(10, 2). 6.4 Replace and Remove (Page 71) - For iterative deep dive.\nWhy it\u0026rsquo;s Keystone (Iterative): This problem is a gem for understanding multi-pass algorithms and in-place manipulation using two (or more) pointers within an array (or list of characters). It\u0026rsquo;s not recursive, but your hand-simulation step (Step 2) and pseudocode (Step 3) are vital for getting the pointer logic correct. Fits your 6 Steps: High-level: First pass to remove \u0026lsquo;b\u0026rsquo;s and count \u0026lsquo;a\u0026rsquo;s (to know final size). Second pass (backwards) to do the \u0026lsquo;a\u0026rsquo; -\u0026gt; \u0026lsquo;dd\u0026rsquo; replacement. Parameters (for helper functions or main logic): The character array/list, current read index, current write index, count of \u0026lsquo;a\u0026rsquo;s. Base Case (for loops): Reaching end of array / start of array. Hand-simulation: Absolutely critical! Track read_idx, write_idx, a_count on paper with an example like (a,c,d,b,b,c,a). Recommendation:\nStart with 6.7 Compute All Mnemonics for a Phone Number. It\u0026rsquo;s the purest example of recursion in this list that directly mirrors the \u0026ldquo;Towers of Hanoi\u0026rdquo; style of thinking (exploring choices at each step).\nLet\u0026rsquo;s imagine you pick 6.7 Compute All Mnemonics for a Phone Number. Here\u0026rsquo;s how you might approach it with your 6 steps, and I can guide you through it:\nYour Step 1: Go through my teaching / EPI explanation.\nMy Teaching (High Level): The goal is to generate all possible letter sequences for a given phone number string (e.g., \u0026ldquo;23\u0026rdquo;). Digit \u0026lsquo;2\u0026rsquo; maps to \u0026lsquo;A\u0026rsquo;, \u0026lsquo;B\u0026rsquo;, \u0026lsquo;C\u0026rsquo;. Digit \u0026lsquo;3\u0026rsquo; maps to \u0026lsquo;D\u0026rsquo;, \u0026lsquo;E\u0026rsquo;, \u0026lsquo;F\u0026rsquo;. We need to combine these: \u0026ldquo;AD\u0026rdquo;, \u0026ldquo;AE\u0026rdquo;, \u0026ldquo;AF\u0026rdquo;, \u0026ldquo;BD\u0026rdquo;, \u0026ldquo;BE\u0026rdquo;, \u0026ldquo;BF\u0026rdquo;, \u0026ldquo;CD\u0026rdquo;, \u0026ldquo;CE\u0026rdquo;, \u0026ldquo;CF\u0026rdquo;. This sounds like we make a choice for the first digit, then for that choice, we make a choice for the second digit, and so on. This \u0026ldquo;and so on\u0026rdquo; hints at recursion! Parameters (for the recursive helper function): digit_index: Which digit of the phone number are we currently trying to find a letter for? partial_mnemonic: A list (or array) of characters representing the mnemonic built so far up to digit_index - 1. Base Case: What\u0026rsquo;s the simplest state? When we\u0026rsquo;ve processed all digits! If digit_index is equal to the length of the phone number, it means we\u0026rsquo;ve successfully chosen a letter for every digit. So, we add the partial_mnemonic (joined into a string) to our list of results. Your Step 2 (NEW - Hand-Simulate with Pen and Paper FOR A SMALL CASE):\nLet phone_number = \u0026quot;23\u0026quot;. MAPPING = {..., '2':\u0026quot;ABC\u0026quot;, '3':\u0026quot;DEF\u0026quot;, ...}. Let results = [], partial_mnemonic = [_, _] (length of phone_number). mnemonic_helper(digit_index=0, partial_mnemonic): Current digit is phone_number[0] which is \u0026lsquo;2\u0026rsquo;. Letters are \u0026lsquo;A\u0026rsquo;, \u0026lsquo;B\u0026rsquo;, \u0026lsquo;C\u0026rsquo;. Try \u0026lsquo;A\u0026rsquo;: partial_mnemonic[0] = 'A'. Call mnemonic_helper(digit_index=1, partial_mnemonic_now_is_['A',_]). Current digit is phone_number[1] which is \u0026lsquo;3\u0026rsquo;. Letters are \u0026lsquo;D\u0026rsquo;, \u0026lsquo;E\u0026rsquo;, \u0026lsquo;F\u0026rsquo;. Try \u0026lsquo;D\u0026rsquo;: partial_mnemonic[1] = 'D'. Call mnemonic_helper(digit_index=2, partial_mnemonic_now_is_['A','D']). digit_index (2) == len(\u0026quot;23\u0026quot;) (2). Base Case! Add \u0026ldquo;AD\u0026rdquo; to results. Return. Try \u0026lsquo;E\u0026rsquo;: partial_mnemonic[1] = 'E'. Call mnemonic_helper(digit_index=2, partial_mnemonic_now_is_['A','E']). Base Case! Add \u0026ldquo;AE\u0026rdquo; to results. Return. Try \u0026lsquo;F\u0026rsquo;: partial_mnemonic[1] = 'F'. Call mnemonic_helper(digit_index=2, partial_mnemonic_now_is_['A','F']). Base Case! Add \u0026ldquo;AF\u0026rdquo; to results. Return. Done with letters for digit \u0026lsquo;3\u0026rsquo; (when first digit was \u0026lsquo;A\u0026rsquo;). Return from this call. Try \u0026lsquo;B\u0026rsquo;: partial_mnemonic[0] = 'B'. Call mnemonic_helper(digit_index=1, partial_mnemonic_now_is_['B',_]). \u0026hellip; (similar logic, will generate \u0026ldquo;BD\u0026rdquo;, \u0026ldquo;BE\u0026rdquo;, \u0026ldquo;BF\u0026rdquo;) Try \u0026lsquo;C\u0026rsquo;: partial_mnemonic[0] = 'C'. Call mnemonic_helper(digit_index=1, partial_mnemonic_now_is_['C',_]). \u0026hellip; (similar logic, will generate \u0026ldquo;CD\u0026rdquo;, \u0026ldquo;CE\u0026rdquo;, \u0026ldquo;CF\u0026rdquo;) Done with letters for digit \u0026lsquo;2\u0026rsquo;. Return from initial call. Wow! Doing this on paper, you actually see the depth-first traversal of the decision tree. Your Step 3 (NEW - Pseudocode/Skeleton Code):\nMAPPING = {\u0026#39;0\u0026#39;:\u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;:\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;:\u0026#39;ABC\u0026#39;, ...} // Global or passed in function generate_mnemonics(phone_number_string): results_list = [] current_partial_mnemonic = array_of_chars of size len(phone_number_string) mnemonic_recursive_helper(digit_idx = 0, current_partial_mnemonic, results_list, phone_number_string) return results_list function mnemonic_recursive_helper(digit_idx, partial_mnemonic_chars, all_results, original_phone_num): if digit_idx == len(original_phone_num): // Base Case add join(partial_mnemonic_chars) to all_results return current_digit_char = original_phone_num[digit_idx] possible_letters_for_digit = MAPPING[current_digit_char] for each letter L in possible_letters_for_digit: partial_mnemonic_chars[digit_idx] = L mnemonic_recursive_helper(digit_idx + 1, partial_mnemonic_chars, all_results, original_phone_num) // No cleanup needed for partial_mnemonic_chars[digit_idx] here because the next iteration // of the loop (or the call from a higher stack frame) will overwrite it. Your Step 4: Translate to Actual Code. Now you\u0026rsquo;d try to write the Python code. You\u0026rsquo;d initialize MAPPING, the main function, and the helper. The partial_mnemonic in Python is often a list of characters which you then \u0026quot;\u0026quot;.join() in the base case.\nYour Step 5 (CRUCIAL - Debug and Test): Run with \u0026ldquo;2\u0026rdquo;, then \u0026ldquo;23\u0026rdquo;. Add print() statements inside the helper:\nprint(f\u0026#34;Helper called: digit_idx={digit_idx}, partial_mnemonic_so_far={\u0026#39;\u0026#39;.join(partial_mnemonic_chars[:digit_idx])}\u0026#34;) print(f\u0026#34; Current digit: {original_phone_num[digit_idx]}, Letters: {possible_letters_for_digit}\u0026#34;) # In the base case: print(f\u0026#34; Base Case Reached! Adding: {\u0026#39;\u0026#39;.join(partial_mnemonic_chars)}\u0026#34;) This tracing is invaluable.\nYour Step 6: Reflect and Solidify.\n\u0026ldquo;Base case was when digit_idx reached the end of the phone number.\u0026rdquo; \u0026ldquo;Problem got smaller by incrementing digit_idx, focusing on the next digit.\u0026rdquo; \u0026ldquo;Arguments changed: digit_idx increased. partial_mnemonic_chars had one more character filled in.\u0026rdquo; \u0026ldquo;The loop iterates through choices for the current digit. The recursive call solves the problem for the rest of the digits given that choice.\u0026rdquo; Core Concept 1: What a Number Represents (Positional Notation)\nWhen we see a number like 345 (in our everyday base 10):\nIt\u0026rsquo;s NOT \u0026ldquo;three and four and five.\u0026rdquo; It IS: 3 hundreds (3 * 10²) 4 tens (4 * 10¹) 5 ones (5 * 10⁰) The position of a digit tells us what power of the base it\u0026rsquo;s multiplying. The base here is 10 because we use 10 unique symbols (0, 1, 2, 3, 4, 5, 6, 7, 8, 9).\nGeneralizing to any base B: If you have a number string d_k ... d_2 d_1 d_0 in base B (where each d_i is a digit valid for that base, i.e., 0 to B-1): Its value in base 10 is: d_k * (B^k) + ... + d_2 * (B^2) + d_1 * (B^1) + d_0 * (B^0)\nExample: \u0026ldquo;101\u0026rdquo; in base 2 (binary) Base B = 2. Digits are 0, 1. \u0026ldquo;101\u0026rdquo; means:\n1 * (2²) = 1 * 4 = 4 0 * (2¹) = 0 * 2 = 0 1 * (2⁰) = 1 * 1 = 1 Total value in base 10 = 4 + 0 + 1 = 5. Example: \u0026ldquo;1A\u0026rdquo; in base 16 (hexadecimal) Base B = 16. Digits are 0-9, A(10), B(11), C(12), D(13), E(14), F(15). \u0026ldquo;1A\u0026rdquo; means:\n1 * (16¹) = 1 * 16 = 16 A (which is 10) * (16⁰) = 10 * 1 = 10 Total value in base 10 = 16 + 10 = 26. Core Concept 2: Converting a String in Base B to its Base 10 Integer Value\nThis is what we did in:\nstring_to_int (for base 10) Part 1 of convert_base (for generic b1) ss_decode_col_id (a variation for base 26 with digits 1-26) The Algorithm (Iterative, Left-to-Right):\nInitialize value_in_base10 = 0. For each digit_char in the input string (from left to right): a. Convert digit_char to its integer digit_value (e.g., \u0026lsquo;A\u0026rsquo; -\u0026gt; 10). b. value_in_base10 = value_in_base10 * B + digit_value. The final value_in_base10 is your answer. Why does value_in_base10 = value_in_base10 * B + digit_value work? Let the string be d_2 d_1 d_0 in base B.\nInitially: value = 0 Process d_2: value = 0 * B + d_2_val = d_2_val Process d_1: value = d_2_val * B + d_1_val Process d_0: value = (d_2_val * B + d_1_val) * B + d_0_val = d_2_val * B^2 + d_1_val * B + d_0_val This exactly matches the formula from Core Concept 1! Core Concept 3: Converting a Base 10 Integer to a String in Base B\nThis is what we did in:\nint_to_string (for base 10) Part 2 of convert_base (the construct_from_base part, for generic b2) The \u0026ldquo;integer to spreadsheet column\u0026rdquo; variant. The Algorithm (Iterative, gets digits in reverse order):\nLet num be the integer value in base 10. Initialize an empty list result_digits_reversed. While num \u0026gt; 0: a. remainder = num % B (This remainder is the last digit in base B). b. Convert remainder to its char_representation (e.g., 10 -\u0026gt; \u0026lsquo;A\u0026rsquo;). c. Add char_representation to result_digits_reversed. d. num = num // B (Integer division; effectively removes the last digit we just processed). If result_digits_reversed is empty (meaning original num was 0), the answer is \u0026ldquo;0\u0026rdquo;. Otherwise, reverse result_digits_reversed and join the characters to form the final string. Why does num % B give the last digit and num // B give the rest? Consider 345 (base 10) converting to base 10 (just for illustration). B=10.\nnum = 345 345 % 10 = 5 (last digit) 345 // 10 = 34 (the rest of the number without the last digit) num = 34 34 % 10 = 4 (new last digit) 34 // 10 = 3 (the rest) num = 3 3 % 10 = 3 3 // 10 = 0 Stop. Digits collected (in reverse): 5, 4, 3. Reversed: 3, 4, 5. String: \u0026ldquo;345\u0026rdquo;. This logic works for any target base B. Example: 26 (base 10) to base 16. B=16.\nnum = 26 26 % 16 = 10 (Digit \u0026lsquo;A\u0026rsquo;) 26 // 16 = 1 Digits list: ['A'] num = 1 1 % 16 = 1 (Digit \u0026lsquo;1\u0026rsquo;) 1 // 16 = 0 Digits list: ['A', '1'] Stop. Reversed list joined: \u0026ldquo;1A\u0026rdquo;. Spreadsheet Column Variation (1-indexed \u0026ldquo;digits\u0026rdquo;):\nString to Int (\u0026ldquo;AA\u0026rdquo; -\u0026gt; 27): This is like Core Concept 2, result = result * 26 + digit_value, but digit_value is 1 for \u0026lsquo;A\u0026rsquo;, 2 for \u0026lsquo;B\u0026rsquo;, \u0026hellip;, 26 for \u0026lsquo;Z\u0026rsquo;. \u0026ldquo;AA\u0026rdquo;: result = 0 \u0026lsquo;A\u0026rsquo; (val 1): result = 0 * 26 + 1 = 1 \u0026lsquo;A\u0026rsquo; (val 1): result = 1 * 26 + 1 = 27\nInt to String (27 -\u0026gt; \u0026ldquo;AA\u0026rdquo;): This is like Core Concept 3, but because the \u0026ldquo;digits\u0026rdquo; are 1-26 (not 0-25), we use a trick. To get a 0-25 index for our character map (\u0026lsquo;A\u0026rsquo; at index 0): (num - 1) % 26. To update num for the next iteration: num = (num - 1) // 26. num = 27: idx = (27-1)%26 = 0 (\u0026lsquo;A\u0026rsquo;) num = (27-1)//26 = 1 List: ['A'] num = 1: idx = (1-1)%26 = 0 (\u0026lsquo;A\u0026rsquo;) num = (1-1)//26 = 0 List: ['A', 'A'] Reversed: \u0026ldquo;AA\u0026rdquo;.\nKey Takeaways for Base Problems:\nUnderstand Positional Value: A digit\u0026rsquo;s worth depends on its place and the base. String to Integer (Any Base B to Base 10): value = 0 Loop: value = value * B + current_digit_numeric_value Integer to String (Base 10 to Any Base B): Loop: remainder = number % B (this is your next digit, in reverse order) number = number // B Collect remainders, then reverse. Character to Digit Value: You need a way to map \u0026lsquo;0\u0026rsquo;-\u0026lsquo;9\u0026rsquo;, \u0026lsquo;A\u0026rsquo;-\u0026lsquo;F\u0026rsquo; to their numeric values (0-9, 10-15). ord() helps. Digit Value to Character: You need a way to map numeric values back to characters. chr() helps. Does this focused revision on the number system concepts themselves make more sense for approaching these types of problems? The string part is often just how you get the digits in or how you format the digits out. The math is the core.\nThat\u0026rsquo;s a great, specific question! That piece of code is a common trick to convert an alphabet character (specifically \u0026lsquo;a\u0026rsquo; through \u0026lsquo;f\u0026rsquo;, or \u0026lsquo;A\u0026rsquo; through \u0026lsquo;F\u0026rsquo; for hexadecimal) into its corresponding integer value (10 through 15).\nLet\u0026rsquo;s break it down piece by piece, assuming char_digit is something like \u0026lsquo;A\u0026rsquo;, \u0026lsquo;b\u0026rsquo;, \u0026lsquo;C\u0026rsquo;, etc.\nchar_digit.lower()\nThis first converts the character to its lowercase equivalent. If char_digit is \u0026lsquo;A\u0026rsquo;, char_digit.lower() becomes \u0026lsquo;a\u0026rsquo;. If char_digit is \u0026lsquo;b\u0026rsquo;, char_digit.lower() remains \u0026lsquo;b\u0026rsquo;. If char_digit is \u0026lsquo;F\u0026rsquo;, char_digit.lower() becomes \u0026lsquo;f\u0026rsquo;. Why? This standardizes the input so we only have to compare against lowercase \u0026lsquo;a\u0026rsquo; through \u0026lsquo;f\u0026rsquo;. It makes the subsequent logic simpler. ord(...)\nThe ord() function in Python returns the Unicode code point (which for common English characters is the same as its ASCII value) of a given character. For example: ord('a') might be 97 (this value can vary slightly by system encoding but the relative differences are constant for ASCII). ord('b') would then be 98. ord('c') would be 99. \u0026hellip; ord('f') would be 102. The key is that alphabetical characters have consecutive ord() values. ord(char_digit.lower()) - ord('a')\nThis part calculates the \u0026ldquo;offset\u0026rdquo; or \u0026ldquo;distance\u0026rdquo; of our character from \u0026lsquo;a\u0026rsquo;. Let\u0026rsquo;s see some examples: If char_digit was \u0026lsquo;A\u0026rsquo; (so char_digit.lower() is \u0026lsquo;a\u0026rsquo;): ord('a') - ord('a') = 97 - 97 = 0 If char_digit was \u0026lsquo;B\u0026rsquo; (so char_digit.lower() is \u0026lsquo;b\u0026rsquo;): ord('b') - ord('a') = 98 - 97 = 1 If char_digit was \u0026lsquo;C\u0026rsquo; (so char_digit.lower() is \u0026lsquo;c\u0026rsquo;): ord('c') - ord('a') = 99 - 97 = 2 \u0026hellip; If char_digit was \u0026lsquo;F\u0026rsquo; (so char_digit.lower() is \u0026lsquo;f\u0026rsquo;): ord('f') - ord('a') = 102 - 97 = 5 So, this expression gives us: 0 if the character is \u0026lsquo;a\u0026rsquo; or \u0026lsquo;A\u0026rsquo; 1 if the character is \u0026lsquo;b\u0026rsquo; or \u0026lsquo;B\u0026rsquo; \u0026hellip; 5 if the character is \u0026lsquo;f\u0026rsquo; or \u0026lsquo;F\u0026rsquo; ... + 10\nFinally, we add 10 to the result from step 3. Why 10? In hexadecimal (base 16) or other bases greater than 10: The digits 0-9 have their usual values. \u0026lsquo;A\u0026rsquo; represents the value 10. \u0026lsquo;B\u0026rsquo; represents the value 11. \u0026hellip; \u0026lsquo;F\u0026rsquo; represents the value 15. So, continuing our examples: For \u0026lsquo;A\u0026rsquo; (offset 0): 0 + 10 = 10 For \u0026lsquo;B\u0026rsquo; (offset 1): 1 + 10 = 11 For \u0026lsquo;C\u0026rsquo; (offset 2): 2 + 10 = 12 \u0026hellip; For \u0026lsquo;F\u0026rsquo; (offset 5): 5 + 10 = 15 In summary, ord(char_digit.lower()) - ord('a') + 10 is a formula to: Take a character that is expected to be one of \u0026lsquo;A\u0026rsquo;/\u0026lsquo;a\u0026rsquo;, \u0026lsquo;B\u0026rsquo;/\u0026lsquo;b\u0026rsquo;, \u0026hellip;, \u0026lsquo;F\u0026rsquo;/\u0026lsquo;f\u0026rsquo;, and convert it to its corresponding integer value 10, 11, \u0026hellip;, 15.\nThis is typically used when converting a number string from a base like hexadecimal (base 16) into an integer. The part of the code that uses this would first check if char_digit is a numeric digit (\u0026lsquo;0\u0026rsquo;-\u0026lsquo;9\u0026rsquo;). If it\u0026rsquo;s not, it then assumes it\u0026rsquo;s one of these letter-digits and uses this formula to get its value.\nFor example, in string_in_base_b1_to_int_base10('1A', 16):\nFor \u0026lsquo;1\u0026rsquo;: int('1') gives 1. For \u0026lsquo;A\u0026rsquo;: This formula would be used: ord('a') - ord('a') + 10 = 0 + 10 = 10. This makes it a compact way to handle the A-F characters without a long if/elif chain or a dictionary lookup for just these 6 characters.\n","permalink":"https://deepskandpal.github.io/dsa-concepts/strings/","tags":[],"title":"Strings"},{"categories":[],"content":"","permalink":"https://deepskandpal.github.io/dsa-concepts/trees/","tags":[],"title":"Trees"},{"categories":null,"content":"Problem Description Given an array A and an index pivot_idx, rearrange A such that all elements less than A[pivot_idx] (the pivot value) come first, followed by all elements equal to the pivot, and finally all elements greater than the pivot. This must be done in-place.\nSolution Approach Let A = [0, 1, 2, 0, 2, 1, 1] pivot_idx = 1 pivot_value = A[pivot_idx] = 1 We maintain three pointers (our “fingers”): smaller: end of the “LESS THAN pivot” zone. Anything to its left is \u0026lt; pivot_value. Starts at 0. equal: the current “inspector” pointer. Points to the element under examination. Starts at 0. larger: start of the “GREATER THAN pivot” zone (from the right end). Anything at or to its right is \u0026gt; pivot_value. Starts at len(A) (one past the end). A[0...smaller-1] = LESS zone (empty) A[smaller...equal-1] = EQUAL zone (empty) A[equal...larger-1] = UNCLASSIFIED (the whole array) A[larger...end] = GREATER zone (empty) We loop while equal \u0026lt; larger: while equal \u0026lt; larger: if A[equal] \u0026lt; pivot_value: swap(A[smaller], A[equal]) smaller += 1 equal += 1 elif A[equal] == pivot_value: # Already in the EQUAL zone equal += 1 else: # A[equal] \u0026gt; pivot_value larger -= 1 swap(A[equal], A[larger]) # do not increment `equal`! Step-by-Step Walkthrough\nIteration smaller equal larger A Action 1 0 0 7 [0, 1, 2, 0, 2, 1, 1] A[0]=0 \u0026lt; 1: swap(0↔0), smaller=1, equal=1 2 1 1 7 [0, 1, 2, 0, 2, 1, 1] A[1]=1 == 1: equal=2 3 1 2 7 [0, 1, 2, 0, 2, 1, 1] A[2]=2 \u0026gt; 1: larger=6; swap(A[2], A[6]) → [0, 1, 1, 0, 2, 1, 2] 4 1 2 6 [0, 1, 1, 0, 2, 1, 2] A[2]=1 == 1: equal=3 5 1 3 6 [0, 1, 1, 0, 2, 1, 2] A[3]=0 \u0026lt; 1: swap(A[1], A[3]) → [0, 0, 1, 1, 2, 1, 2]; smaller=2, equal=4 6 2 4 6 [0, 0, 1, 1, 2, 1, 2] A[4]=2 \u0026gt; 1: larger=5; swap(A[4], A[5]) → [0, 0, 1, 1, 1, 2, 2] 7 2 4 5 [0, 0, 1, 1, 1, 2, 2] A[4]=1 == 1: equal=5 Stop — 5 5 [0, 0, 1, 1, 1, 2, 2] equal == larger → loop ends Final partition: LESS (A[0...1]): [0, 0] EQUAL (A[2...4]): [1, 1, 1] GREATER (A[5...6]): [2, 2] Code Check leet code Complexity Analysis Time: Each element is examined at most once and swapped at most once → O(n). Space: In-place partitioning → O(1) extra space.\nNotes \u0026amp; Learnings Key insight: when swapping a “greater” element to the right, don’t advance equal—you must re-examine the swapped-in element.\n(Key takeaways, patterns, common pitfalls, related problems)\n","permalink":"https://deepskandpal.github.io/dsa-log/dutch-national-flag/","tags":["Three pointers"],"title":"Dutch National Flag"},{"categories":null,"content":"","permalink":"https://deepskandpal.github.io/dsa-log/gcd/","tags":["Three pointers"],"title":"Greatest common division"},{"categories":["Maths"],"content":"The Big Picture Goal: We want our machine learning models to make good predictions. To do this, we define a loss function (or cost function) that measures how \u0026ldquo;bad\u0026rdquo; our model\u0026rsquo;s predictions are compared to the true values.\nTraining = Minimizing Loss: Training a model means finding the model parameters (like weights w and biases b) that make this loss function as small as possible. Calculus as a Tool for Minimization: Derivatives (and their extensions like gradients) tell us the rate of change or the slope of a function. If we know the slope of our loss function with respect to our model parameters, we know which \u0026ldquo;direction\u0026rdquo; to tweak those parameters to decrease the loss. This is the essence of Gradient Descent. So, all this calculus is ultimately about finding an efficient way to \u0026ldquo;walk downhill\u0026rdquo; on our loss function surface to find the parameter values that give the lowest error.\nNow, let\u0026rsquo;s look at the paper.\n(Section 1 - Introduction) The paper acknowledges that derivatives are critical for ML, especially deep neural networks, which are trained by optimizing a loss function.\nIt mentions that while modern libraries have \u0026ldquo;automatic differentiation,\u0026rdquo; understanding the underlying matrix calculus helps to \u0026ldquo;grok academic papers\u0026rdquo; and understand what\u0026rsquo;s happening \u0026ldquo;under the hood.\u0026rdquo;\nIt gives an example of a single neuron\u0026rsquo;s activation:\nz(x) = w · x + b (affine function: dot product of weights w with input x, plus a bias b). Followed by an activation function, e.g., max(0, z(x)) (ReLU). Key statement on training: \u0026ldquo;Training this neuron means choosing weights w and bias b so that we get the desired output for all N inputs x. To do that, we minimize a loss function\u0026hellip; All of those [gradient descent methods] require the partial derivative (the gradient) of activation(x) with respect to the model parameters w and b. Our goal is to gradually tweak w and b so that the overall loss function keeps getting smaller\u0026hellip;\u0026rdquo;\nWhat this is ultimately trying to achieve: Use the gradient (which calculus helps us find) to iteratively adjust w and b to reduce the model\u0026rsquo;s error. The introduction to the paper\u0026rsquo;s goal: \u0026ldquo;\u0026hellip;rederive and rediscover some key matrix calculus rules in an effort to explain them\u0026hellip; There aren’t dozens of new rules to learn; just a couple of key concepts.\u0026rdquo; This is encouraging!\nLet\u0026rsquo;s proceed to Section 2: Review: Scalar derivative rules (Page 4 of the paper). This should be a good refresher. Are you ready to look at these basic rules? We can take it slow and make sure each one makes sense.\nAlright, let\u0026rsquo;s dive into Section 2: Review: Scalar derivative rules from the Parr \u0026amp; Howard paper (page 4, extending to the table on page 5).\nThis section is a quick refresher of the basic derivative rules you likely encountered in a first calculus course. The core idea of a derivative for a function of a single variable, f(x), is to find its instantaneous rate of change or the slope of the tangent line at any given point x.\nWhat we are ultimately trying to achieve with derivatives (in an ML context): We want to know how a small change in an input (like a model parameter θ or an input feature x) will affect the output of a function (like the loss function J(θ) or an activation function a(z)). If the derivative is large and positive, a small increase in the input leads to a large increase in the output. If it\u0026rsquo;s negative, an increase in input leads to a decrease in output. If it\u0026rsquo;s zero, the function is momentarily flat at that point (which could indicate a minimum, maximum, or a plateau).\nLet\u0026rsquo;s look at the rules presented in the table on page 5:\nConstant Rule:\nRule: f(x) = c (where c is a constant) Scalar derivative notation: d/dx (c) = 0 Example: d/dx (99) = 0 What it\u0026rsquo;s ultimately trying to achieve: A constant function doesn\u0026rsquo;t change, no matter what x is. So, its rate of change (slope) is always zero. Think of a flat horizontal line – its slope is 0. Multiplication by Constant Rule:\nRule: f(x) = cf(x) (actually, it should be g(x) = c * f(x)) Scalar derivative notation: d/dx (c * f(x)) = c * (df/dx) Example: d/dx (3x) = 3 * (d/dx (x)) = 3 * 1 = 3 What it\u0026rsquo;s ultimately trying to achieve: If you scale a function by a constant, its rate of change (slope) at any point is also scaled by that same constant. The shape of the change is the same, just amplified or shrunk. Power Rule:\nRule: f(x) = xⁿ Scalar derivative notation: d/dx (xⁿ) = nxⁿ⁻¹ Example: d/dx (x³) = 3x³⁻¹ = 3x² What it\u0026rsquo;s ultimately trying to achieve: This rule tells us how functions involving powers of x change. For x³, the slope isn\u0026rsquo;t constant; it changes depending on x. At x=1, slope is 3. At x=2, slope is 3*(2)² = 12. Sum Rule:\nRule: h(x) = f(x) + g(x) Scalar derivative notation: d/dx (f(x) + g(x)) = (df/dx) + (dg/dx) Example: d/dx (x² + 3x) = (d/dx x²) + (d/dx 3x) = 2x + 3 What it\u0026rsquo;s ultimately trying to achieve: The rate of change of a sum of functions is the sum of their individual rates of change. If one part is changing quickly and another slowly, the sum changes according to the combined effect. Difference Rule:\nRule: h(x) = f(x) - g(x) Scalar derivative notation: d/dx (f(x) - g(x)) = (df/dx) - (dg/dx) Example: d/dx (x² - 3x) = (d/dx x²) - (d/dx 3x) = 2x - 3 What it\u0026rsquo;s ultimately trying to achieve: Similar to the sum rule, the rate of change of a difference is the difference of the rates of change. Product Rule:\nRule: h(x) = f(x)g(x) Scalar derivative notation: d/dx (f(x)g(x)) = f(x)(dg/dx) + (df/dx)g(x) Example: d/dx (x² * x) (where f(x)=x², g(x)=x) = x² * (d/dx x) + (d/dx x²) * x = x² * 1 + 2x * x = x² + 2x² = 3x² (This is consistent with d/dx (x³) = 3x²) What it\u0026rsquo;s ultimately trying to achieve: When two functions are multiplied, a change in x affects both. The overall rate of change depends on how g(x) changes (weighted by f(x)) plus how f(x) changes (weighted by g(x)). Chain Rule (for single variable nested functions):\nRule: h(x) = f(g(x)) (a function f applied to the output of another function g) Scalar derivative notation: d/dx f(g(x)) = (df/du) * (du/dx), where u = g(x). The paper writes this as df(u)/du * du/dx. Example: d/dx ln(x²) Let u = g(x) = x². Then f(u) = ln(u). du/dx = d/dx (x²) = 2x. df/du = d/du (ln(u)) = 1/u. So, d/dx ln(x²) = (1/u) * (2x) = (1/x²) * (2x) = 2x/x² = 2/x. What it\u0026rsquo;s ultimately trying to achieve: It tells us how to find the rate of change of a composite function. How much does the outer function f change with respect to its input u (df/du), multiplied by how much that inner input u changes with respect to the original variable x (du/dx). It \u0026ldquo;chains\u0026rdquo; the rates of change together. This rule is extremely important for neural networks because they are essentially deeply nested functions, and backpropagation is an application of the chain rule. The paper also mentions (bottom of page 4, top of page 5):\nThe notation d/dx can be thought of as an operator that takes a function f(x) and gives you its derivative df/dx. This operator view is helpful because the operator is distributive (d/dx (f+g) = df/dx + dg/dx) and lets you pull out constants (d/dx (cf) = c * df/dx). This simplifies taking derivatives of complex expressions, like their example: d/dx (9(x + x²)) = 9 * d/dx (x + x²) = 9 * (d/dx x + d/dx x²) = 9 * (1 + 2x) = 9 + 18x. This review of scalar derivative rules is foundational. For deep learning and training models with Gradient Descent, we are constantly asking: \u0026ldquo;If I wiggle this weight/bias a tiny bit, how much does my loss function change?\u0026rdquo; Derivatives give us the answer to that question.\nSection 3: Introduction to vector calculus and partial derivatives This section transitions us from functions of a single variable (f(x)) to functions of multiple variables (e.g., f(x, y)). Neural network layers often have many inputs (features) and many parameters (weights, biases), so we need to be able to talk about how the output changes when we wiggle one specific input or parameter, while keeping others constant.\nFunctions of Multiple Parameters:\nInstead of f(x), we now consider f(x, y). For example, the product xy. If we want to know its derivative, the question is \u0026ldquo;derivative with respect to what?\u0026rdquo; With respect to x, or with respect to y? The change in xy will be different depending on which variable we \u0026ldquo;wiggle.\u0026rdquo; Partial Derivatives:\nWhen we have a function of multiple variables, we compute partial derivatives. The notation changes from d/dx to ∂/∂x (using a stylized \u0026rsquo;d\u0026rsquo;, often called \u0026ldquo;del\u0026rdquo; or \u0026ldquo;curly d\u0026rdquo;). ∂f(x,y)/∂x: This is the partial derivative of f(x,y) with respect to x. What it\u0026rsquo;s ultimately trying to achieve: It tells us the rate of change of the function f if we change x by a tiny amount, while holding all other variables (in this case, y) constant. You treat y as if it were just a number. ∂f(x,y)/∂y: This is the partial derivative of f(x,y) with respect to y. What it\u0026rsquo;s ultimately trying to achieve: It tells us the rate of change of f if we change y by a tiny amount, while holding x constant. Example from the paper: f(x, y) = 3x²y\nPartial derivative with respect to x (∂f/∂x): Treat y (and 3) as constants. ∂/∂x (3x²y) = 3y * (∂/∂x x²) = 3y * (2x) = 6yx. Intuition: If y is fixed, say y=2, then f(x,2) = 6x². The derivative d/dx (6x²) = 12x. Our partial derivative 6yx gives 6(2)x = 12x. It matches! Partial derivative with respect to y (∂f/∂y): Treat x (and 3x²) as constants. ∂/∂y (3x²y) = 3x² * (∂/∂y y) = 3x² * 1 = 3x². Intuition: If x is fixed, say x=1, then f(1,y) = 3y. The derivative d/dy (3y) = 3. Our partial derivative 3x² gives 3(1)² = 3. It matches! The Gradient (∇f):\nFor a function of multiple variables, like f(x,y), we can organize all its partial derivatives into a vector. This vector is called the gradient of f, denoted by ∇f (nabla f). For f(x,y) = 3x²y, the paper writes: ∇f(x, y) = [∂f/∂x, ∂f/∂y] = [6yx, 3x²] What the gradient is ultimately trying to achieve: The gradient vector ∇f at a specific point (x₀, y₀) points in the direction of the steepest ascent (fastest increase) of the function f at that point. Its magnitude ||∇f|| tells you the rate of increase in that direction. In machine learning, our loss function J(θ) depends on many parameters θ₁, θ₂, ..., θₙ. The gradient ∇J(θ) will be a vector [∂J/∂θ₁, ∂J/∂θ₂, ..., ∂J/∂θₙ]. Gradient Descent uses this: it calculates ∇J(θ) and then takes a step in the opposite direction (-∇J(θ)) to go \u0026ldquo;downhill\u0026rdquo; and reduce the loss. This section essentially extends the concept of a derivative (rate of change/slope) from single-variable functions to multi-variable functions by considering the rate of change with respect to each variable individually, holding others constant. The gradient then bundles all these partial rates of change into a single vector that gives us the \u0026ldquo;overall\u0026rdquo; direction of steepest increase.\nSection 4: Matrix calculus This is where things get a bit more generalized. So far:\nScalar derivative: df/dx (function of one variable, derivative is a scalar). Vector calculus (gradient): ∇f(x,y) = [∂f/∂x, ∂f/∂y] (function of multiple scalar variables x, y, output is a scalar f, gradient is a vector of partials). Matrix calculus deals with derivatives when:\nThe input is a vector (e.g., x = [x₁, x₂]). The output can also be a vector (e.g., y = [f₁(x), f₂(x)]). From One Function to Many Functions: The paper keeps f(x, y) = 3x²y from the previous section. It introduces another function g(x, y) = 2x + y⁸. We can find the gradient of g just like we did for f: ∂g/∂x = ∂/∂x (2x + y⁸) = 2 (treating y⁸ as constant) ∂g/∂y = ∂/∂y (2x + y⁸) = 8y⁷ (treating 2x as constant) So, ∇g(x, y) = [2, 8y⁷]. The Jacobian Matrix (J):\nWhen we have multiple functions (say, f and g), each of which can depend on multiple input variables (say, x and y), we can organize their gradients into a matrix. This matrix is called the Jacobian matrix (or just the Jacobian).\nIf we have two functions f(x,y) and g(x,y), the Jacobian J (using the paper\u0026rsquo;s \u0026ldquo;numerator layout\u0026rdquo; where gradients are rows) is formed by stacking their gradient vectors: J = [ ∇f(x, y) ] = [ ∂f/∂x ∂f/∂y ] [ ∇g(x, y) ] [ ∂g/∂x ∂g/∂y ]\nSo for f(x,y) = 3x²y and g(x,y) = 2x + y⁸, the Jacobian is: J = [ 6yx 3x² ] [ 2 8y⁷ ]\nWhat the Jacobian is ultimately trying to achieve: It captures all the first-order partial derivatives of a vector-valued function (a function that outputs a vector of values) with respect to a vector of input variables.\nEach row i tells you how function fᵢ changes with respect to each input variable. Each column j tells you how all output functions change with respect to input variable xⱼ. Essentially, if you have m functions and n input variables, the Jacobian is an m x n matrix where the entry Jᵢⱼ = ∂fᵢ/∂xⱼ. Layouts (Numerator vs. Denominator):\nThe paper notes: \u0026ldquo;there are multiple ways to represent the Jacobian.\u0026rdquo; They use \u0026ldquo;numerator layout\u0026rdquo; (where ∇f is a row vector, and the Jacobian stacks these row vectors). Other sources might use \u0026ldquo;denominator layout,\u0026rdquo; which is essentially the transpose of the numerator layout Jacobian. It\u0026rsquo;s important to be aware of this when consulting different texts, as the shapes of matrices in equations will change accordingly. The paper sticks to numerator layout. Example given (page 7, top): Transpose of our Jacobian above is [ 6yx 2 ] [ 3x² 8y⁷ ] This Jacobian matrix is a fundamental tool when dealing with transformations between vector spaces in calculus, and it plays a key role in the chain rule for vector functions, which is vital for backpropagation in neural networks with multiple layers and multiple neurons per layer.\nThe main idea here is organization:\nGradient: Organizes partial derivatives of a single scalar-output function with respect to its multiple inputs into a vector. Jacobian: Organizes gradients of multiple scalar-output functions (or equivalently, partial derivatives of a single vector-output function) into a matrix. Great! Understanding the Jacobian as an organized collection of partial derivatives is a key step.\nSection 4.1: Generalization of the Jacobian (page 7 of the Parr \u0026amp; Howard paper). This section formalizes the Jacobian for a general case where you have a vector of functions, y = f(x), where x is an input vector and y is an output vector.\nVector Notation:\nInput vector x has n elements: x = [x₁, x₂, ..., xₙ]ᵀ (they assume column vectors by default). Output vector y has m scalar-valued functions: y = [f₁(x), f₂(x), ..., fₘ(x)]ᵀ. Each fᵢ(x) takes the whole vector x as input and returns a scalar. The Jacobian Matrix ∂y/∂x: The paper defines the Jacobian as a stack of m gradients (one for each output function fᵢ). Since they use numerator layout (where each gradient ∇fᵢ(x) is a row vector of partials with respect to the components of x), the Jacobian becomes an m x n matrix:\n∂y/∂x = [ ∇f₁(x) ] = [ ∂f₁/∂x₁ ∂f₁/∂x₂ ... ∂f₁/∂xₙ ] [ ∇f₂(x) ] [ ∂f₂/∂x₁ ∂f₂/∂x₂ ... ∂f₂/∂xₙ ] [ ... ] [ ... ... ... ... ] [ ∇fₘ(x) ] [ ∂fₘ/∂x₁ ∂fₘ/∂x₂ ... ∂fₘ/∂xₙ ]\nRows: Each row i of the Jacobian corresponds to one output function fᵢ(x) and contains all the partial derivatives of that specific output function with respect to each of the input variables x₁, x₂, ..., xₙ. So, row i is ∇fᵢ(x). Columns: Each column j of the Jacobian corresponds to one input variable xⱼ and contains all the partial derivatives of each of the output functions f₁, f₂, ..., fₘ with respect to that specific input variable xⱼ. What the Jacobian ∂y/∂x is ultimately trying to achieve: It describes how each component of the output vector y changes in response to a small change in each component of the input vector x. It\u0026rsquo;s a complete map of all the first-order sensitivities between the inputs and outputs.\nVisualizing Jacobian Shapes (diagram on page 8): This is a handy diagram to remember the dimensions based on the nature of input x and output f (or y):\nScalar input x, Scalar output f: Derivative ∂f/∂x is a scalar. (This is standard Calc 1). Vector input x, Scalar output f: Derivative ∂f/∂x (which is ∇f) is a row vector (1 x n) in numerator layout. Scalar input x, Vector output f: Derivative ∂f/∂x is a column vector (m x 1). (Each ∂fᵢ/∂x is a scalar, stacked up). Vector input x, Vector output f: Derivative ∂f/∂x (the Jacobian) is an m x n matrix. Example: Jacobian of the Identity Function (page 8):\nIf y = f(x) = x, then each output component yᵢ is just equal to the corresponding input component xᵢ (so fᵢ(x) = xᵢ). Here, m=n. Let\u0026rsquo;s find ∂fᵢ/∂xⱼ = ∂xᵢ/∂xⱼ: If i = j, then ∂xᵢ/∂xᵢ = 1 (the derivative of x₁ with respect to x₁ is 1). If i ≠ j, then ∂xᵢ/∂xⱼ = 0 (e.g., x₁ does not change when x₂ changes, because they are independent input components, so ∂x₁/∂x₂ = 0). When you assemble these into the Jacobian matrix, you get 1s on the main diagonal and 0s everywhere else. This is the Identity Matrix (I). ∂x/∂x = I What this ultimately means: If you wiggle an input xⱼ by a small amount, only the corresponding output yⱼ wiggles by that same amount, and other outputs yᵢ (where i≠j) don\u0026rsquo;t change at all. This makes perfect sense for the identity function. This section firmly establishes the Jacobian matrix as the way to represent the derivative of a vector function with respect to a vector input. It\u0026rsquo;s the matrix that holds all the individual \u0026ldquo;slopes\u0026rdquo; that connect changes in each input dimension to changes in each output dimension.\nThis general form of the Jacobian will be crucial when we get to the vector chain rule later, which is how backpropagation efficiently calculates gradients through multiple layers of a neural network, where each layer can be seen as a vector function taking a vector input.\nSection 4.2: Derivatives of vector element-wise binary operators (Pages 9-11). The Big Picture of This Section:\nNeural networks involve many operations on vectors: adding an input vector to a bias vector, multiplying activations by weights, etc. Often, these operations are element-wise – meaning the operation is applied independently to corresponding elements of the input vectors to produce an element of the output vector.\nExample: If w = [w₁, w₂] and x = [x₁, x₂], then w + x = [w₁ + x₁, w₂ + x₂]. The first element of the output only depends on the first elements of the inputs, and so on. This section aims to figure out:\nIf we have an output vector y that\u0026rsquo;s a result of an element-wise operation between two input vectors w and x (e.g., y = w + x or y = w * x), how does y change if we wiggle w? (This gives us the Jacobian ∂y/∂w). And how does y change if we wiggle x? (This gives us the Jacobian ∂y/∂x). What we are ultimately trying to achieve here is to find simplified forms for these Jacobians, because for element-wise operations, many of the terms in the full Jacobian matrix will turn out to be zero.\nBreaking it Down (Page 9):\nThe paper starts with the general form: y = f(w) ⊙ g(x).\nw and x are input vectors. f(w) and g(x) are functions that produce vectors of the same size. ⊙ represents any element-wise binary operator (like +, -, element-wise *, element-wise /). y is the output vector. So, yᵢ = fᵢ(w) ⊙ gᵢ(x). The i-th element of y depends only on the i-th element processing of w and x. The full Jacobian ∂y/∂w would be a matrix where element (i,j) is ∂yᵢ/∂wⱼ (how the i-th output element yᵢ changes with respect to the j-th input element wⱼ from vector w). Similarly for ∂y/∂x.\nThe \u0026ldquo;Furball\u0026rdquo; and the Simplification (Page 10):\nThe general Jacobian matrix for such an operation (shown at the bottom of page 9) looks complicated – a \u0026ldquo;furball,\u0026rdquo; as the paper says.\nHowever, because the operations are element-wise, there\u0026rsquo;s a huge simplification:\nyᵢ (the i-th element of the output) only depends on wᵢ (the i-th element of w) and xᵢ (the i-th element of x). yᵢ does not depend on wⱼ if j ≠ i. yᵢ does not depend on xⱼ if j ≠ i. What does this mean for the Jacobian ∂y/∂w? Consider ∂yᵢ/∂wⱼ:\nIf j ≠ i (we are looking at an off-diagonal element of the Jacobian): Since yᵢ does not depend on wⱼ, its derivative ∂yᵢ/∂wⱼ must be zero. If j = i (we are looking at a diagonal element of the Jacobian): Then ∂yᵢ/∂wᵢ will generally be non-zero, and it\u0026rsquo;s just the derivative of the i-th scalar operation fᵢ(wᵢ) ⊙ gᵢ(xᵢ) with respect to wᵢ (treating xᵢ as a constant for this partial derivative). The Result: Diagonal Jacobians! This means that for element-wise operations, the Jacobian matrices ∂y/∂w and ∂y/∂x are diagonal matrices. A diagonal matrix has non-zero values only along its main diagonal, and zeros everywhere else.\nThe paper introduces the \u0026ldquo;element-wise diagonal condition\u0026rdquo;: fᵢ(w) should only access wᵢ and gᵢ(x) should only access xᵢ. This is precisely what happens in simple element-wise vector operations.\nSo, the Jacobian ∂y/∂w simplifies to (middle of page 10): ∂y/∂w = diag( ∂/∂w₁ (f₁(w₁) ⊙ g₁(x₁)), ∂/∂w₂ (f₂(w₂) ⊙ g₂(x₂)), ..., ∂/∂wₙ (fₙ(wₙ) ⊙ gₙ(xₙ)) ) Each term on the diagonal is just a scalar derivative of the i-th component operation.\nWhat this simplification is ultimately trying to achieve: It makes calculating these Jacobians much easier. Instead of a full matrix of derivatives, we only need to calculate n scalar derivatives for the diagonal.\nSpecial Case: f(w) = w (Page 10-11)\nVery often in neural networks, one of the functions in the element-wise operation is just the identity. For example, vector addition y = w + x. Here f(w) = w (so fᵢ(wᵢ) = wᵢ) and g(x) = x (so gᵢ(xᵢ) = xᵢ).\nLet\u0026rsquo;s look at y = w + x, so yᵢ = wᵢ + xᵢ.\n∂y/∂w: The i-th diagonal element is ∂/∂wᵢ (wᵢ + xᵢ). Treating xᵢ as a constant, this derivative is 1. So, ∂(w+x)/∂w = diag(1, 1, ..., 1) = I (the identity matrix). ∂y/∂x: The i-th diagonal element is ∂/∂xᵢ (wᵢ + xᵢ). Treating wᵢ as a constant, this derivative is 1. So, ∂(w+x)/∂x = diag(1, 1, ..., 1) = I (the identity matrix). The paper then lists Jacobians for common element-wise binary operations where f(w) = w (so fᵢ(wᵢ) = wᵢ):\nAddition: y = w + x\n∂y/∂w = I ∂y/∂x = I Intuition: If you wiggle w₁ by a small amount Δ, y₁ changes by Δ, and no other yⱼ changes. Same for x. Subtraction: y = w - x\n∂y/∂w = I ∂y/∂x = -I (because ∂/∂xᵢ (wᵢ - xᵢ) = -1) Intuition: If you wiggle x₁ by Δ, y₁ changes by -Δ. Element-wise Multiplication (Hadamard Product): y = w ⊗ x (so yᵢ = wᵢ * xᵢ)\n∂y/∂w: i-th diagonal element is ∂/∂wᵢ (wᵢ * xᵢ) = xᵢ. So, ∂y/∂w = diag(x). ∂y/∂x: i-th diagonal element is ∂/∂xᵢ (wᵢ * xᵢ) = wᵢ. So, ∂y/∂x = diag(w). Intuition for ∂y/∂w: If you wiggle w₁ by Δ, y₁ changes by x₁ * Δ. The change in y₁ depends on the value of x₁. Element-wise Division: y = w / x (so yᵢ = wᵢ / xᵢ)\n∂y/∂w: i-th diagonal element is ∂/∂wᵢ (wᵢ / xᵢ) = 1/xᵢ. So, ∂y/∂w = diag(1/x₁, 1/x₂, ...). ∂y/∂x: i-th diagonal element is ∂/∂xᵢ (wᵢ / xᵢ) = -wᵢ / xᵢ². So, ∂y/∂x = diag(-w₁/x₁², ...). Key Takeaway from Section 4.2: When dealing with element-wise operations between two vectors w and x to produce y, the Jacobians ∂y/∂w and ∂y/∂x are diagonal matrices. This is a huge simplification. The values on the diagonal are found by simply taking the scalar derivative of the i-th component operation with respect to the i-th component of the input vector. This section provides the rules for common operations like addition, subtraction, and element-wise multiplication/division.\nThis means that when we\u0026rsquo;re backpropagating gradients through such an element-wise layer, the calculations become much simpler than if we had to deal with full Jacobian matrices. We\u0026rsquo;re essentially just scaling the incoming gradients by these diagonal terms.\nSection 4.3: Derivatives involving scalar expansion (Pages 11-12). The Big Picture of This Section:\nIn neural networks, we often perform operations between a vector and a scalar. For example:\nAdding a scalar bias b to every element of a vector z: y = z + b. Multiplying every element of a vector z by a scalar learning rate η: y = η * z. This section explains how to find the derivatives for these types of operations. The trick is to realize that these operations can be viewed as implicit element-wise operations where the scalar is \u0026ldquo;expanded\u0026rdquo; or \u0026ldquo;broadcast\u0026rdquo; into a vector of the same size as the other vector.\nWhat we are ultimately trying to achieve here is to get rules for how the output vector y changes if we wiggle the input vector x, and how y changes if we wiggle the input scalar z.\nBreaking it Down (Page 11-12):\nThe paper uses the example y = x + z, where x is a vector and z is a scalar.\nImplicit Expansion: This is really y = f(x) + g(z) where f(x) = x and g(z) = 1z (a vector where every element is z). So, yᵢ = xᵢ + z. Each element yᵢ is the sum of the corresponding xᵢ and the same scalar z. 1. Derivative with respect to the vector x: ∂y/∂x\nThis fits the \u0026ldquo;element-wise diagonal condition\u0026rdquo; we just discussed. fᵢ(x) = xᵢ only depends on xᵢ. gᵢ(z) = z (for the i-th component) only depends on the scalar z (which is considered independent of x for this partial derivative). The i-th diagonal element of the Jacobian ∂y/∂x is ∂/∂xᵢ (xᵢ + z). Since z is treated as a constant when differentiating with respect to xᵢ, ∂z/∂xᵢ = 0. So, ∂/∂xᵢ (xᵢ + z) = ∂xᵢ/∂xᵢ + ∂z/∂xᵢ = 1 + 0 = 1. Therefore, ∂y/∂x = diag(1, 1, ..., 1) = I (the identity matrix). Intuition: If you wiggle xᵢ by a small amount Δ, only yᵢ changes, and it changes by Δ. This is the definition of the identity matrix\u0026rsquo;s effect. What this is ultimately trying to achieve: It confirms that adding a scalar to a vector shifts all elements equally, so the rate of change of each output element yᵢ with respect to its corresponding input xᵢ is 1, and there\u0026rsquo;s no cross-influence. 2. Derivative with respect to the scalar z: ∂y/∂z This is different! We are differentiating a vector output y with respect to a scalar input z. Based on the table from page 8 (or just thinking about it):\nInput: scalar z Output: vector y The derivative ∂y/∂z should be a column vector (or a vertical vector as the paper terms it). Each element of this column vector will be ∂yᵢ/∂z. Let\u0026rsquo;s find ∂yᵢ/∂z for yᵢ = xᵢ + z. When differentiating with respect to z, we treat xᵢ as a constant. So, ∂xᵢ/∂z = 0. ∂yᵢ/∂z = ∂/∂z (xᵢ + z) = ∂xᵢ/∂z + ∂z/∂z = 0 + 1 = 1. Since this is true for every i (from 1 to n, the dimension of y and x), then ∂y/∂z is a column vector of all ones. The paper writes this as ∂/∂z (x + z) = 1 (where 1 is the vector of ones of appropriate length). Intuition: If you wiggle the single scalar z by Δ, every element yᵢ of the output vector changes by Δ because z is added to every xᵢ. What this is ultimately trying to achieve: It shows how a single scalar change propagates to all elements of the output vector when scalar addition is involved. Now for Scalar Multiplication: y = xz (or y = zx) This is treated as element-wise multiplication: y = x ⊗ 1z. So, yᵢ = xᵢ * z.\n1. Derivative with respect to the vector x: ∂y/∂x\nAgain, element-wise diagonal condition holds. The i-th diagonal element is ∂/∂xᵢ (xᵢ * z). Treat z as constant: ∂/∂xᵢ (xᵢ * z) = z * ∂xᵢ/∂xᵢ = z * 1 = z. So, ∂y/∂x = diag(z, z, ..., z) = zI (scalar z times the identity matrix). The paper writes this as ∂/∂x (xz) = diag(1z) = Iz. (Here, 1z in diag(1z) means a vector of zs, which makes more sense. If diag(z) was meant for a scalar z, it would just be z itself, not a matrix. So diag(1z) is the clearer way to express zI). My interpretation: diag(1z) means a diagonal matrix with z on every diagonal element. This is z * I. Intuition: If you wiggle xᵢ by Δ, yᵢ changes by z * Δ. The effect is scaled by z. 2. Derivative with respect to the scalar z: ∂y/∂z\n∂y/∂z will be a column vector. The i-th element is ∂yᵢ/∂z. yᵢ = xᵢ * z. Treat xᵢ as constant. ∂yᵢ/∂z = ∂/∂z (xᵢ * z) = xᵢ * ∂z/∂z = xᵢ * 1 = xᵢ. So, ∂y/∂z is a column vector whose elements are [x₁, x₂, ..., xₙ]ᵀ, which is just the vector x. The paper writes this as ∂/∂z (xz) = x. Intuition: If you wiggle the scalar z by Δ, the i-th output yᵢ changes by xᵢ * Δ. The amount yᵢ changes depends on the value of xᵢ. What this is ultimately trying to achieve: It shows how a single scalar change z propagates to the output vector y, with each element yᵢ being affected proportionally to the corresponding xᵢ. Key Takeaway from Section 4.3: Operations involving a vector and a scalar (like adding a scalar to a vector, or multiplying a vector by a scalar) can be understood by \u0026ldquo;expanding\u0026rdquo; the scalar into a vector and then applying the rules for element-wise vector operations.\nWhen differentiating with respect to the vector input, the Jacobian is still diagonal. When differentiating with respect to the scalar input, the result is a vector (not a diagonal matrix), because a single change in the scalar affects all components of the output vector. This section helps us build up the rules needed for things like Wx + b in a neural network layer, where b might be a bias vector added element-wise, or a scalar bias broadcasted. The logic applies similarly.\nSection 4.4: Vector sum reduction (Pages 12-13). The Big Picture of This Section:\nIn many machine learning contexts, especially when defining loss functions, we often need to reduce a vector to a single scalar value. The most common way to do this is by summing up all the elements of the vector.\nExample: The Mean Squared Error (MSE) involves summing the squared errors for each instance. If we have a vector of squared errors, we sum them up to get a total squared error, then average. This section focuses on finding the derivative of such a sum:\nIf y = sum(f(x)), where x is an input vector and f(x) produces an output vector whose elements are then summed to get the scalar y, how does this scalar y change if we wiggle the input vector x? This will give us ∂y/∂x, which will be a gradient (a row vector, according to the paper\u0026rsquo;s convention for derivative of a scalar w.r.t. a vector). What we are ultimately trying to achieve here is a rule for how to differentiate through a summation operation. This is critical for backpropagation, as the total loss is often a sum of losses from individual components or instances.\nBreaking it Down (Page 13):\nLet y = sum(f(x)) = Σᵢ fᵢ(x).\nx is an input vector [x₁, x₂, ..., xₙ]. f(x) is a vector-valued function, producing an output vector [f₁(x), f₂(x), ..., fₚ(x)]. (Note: the paper uses n as the dimension of x and also n for the number of terms in the sum, implying p=n. Let\u0026rsquo;s assume the output vector of f(x) has p elements). Each fᵢ(x) could potentially depend on all elements of x (not just xᵢ). This is an important distinction from the element-wise operations earlier. y is the scalar sum of all elements of the vector f(x). We want to find the gradient ∂y/∂x. This will be a row vector: ∂y/∂x = [∂y/∂x₁, ∂y/∂x₂, ..., ∂y/∂xₙ]\nLet\u0026rsquo;s consider one component of this gradient, ∂y/∂xⱼ (how the sum y changes with respect to one input element xⱼ): ∂y/∂xⱼ = ∂/∂xⱼ ( Σᵢ fᵢ(x) )\nBecause the derivative operator is distributive over a sum, we can move it inside: ∂y/∂xⱼ = Σᵢ ( ∂fᵢ(x)/∂xⱼ )\nWhat this means: To find how the total sum y changes when xⱼ changes, we sum up how each individual term fᵢ(x) in the sum changes when xⱼ changes.\nSo, the gradient vector ∂y/∂x becomes: ∂y/∂x = [ Σᵢ(∂fᵢ/∂x₁), Σᵢ(∂fᵢ/∂x₂), ..., Σᵢ(∂fᵢ/∂xₙ) ]\nExample 1: y = sum(x) (Simple sum of input vector elements) Here, f(x) = x, so fᵢ(x) = xᵢ.\n∂y/∂xⱼ = Σᵢ (∂xᵢ/∂xⱼ) ∂xᵢ/∂xⱼ is 1 if i=j, and 0 if i≠j. So, in the sum Σᵢ (∂xᵢ/∂xⱼ), only one term is non-zero: when i=j, the term is 1. Therefore, ∂y/∂xⱼ = 1 for all j. This means ∂y/∂x = [1, 1, ..., 1]. The paper writes this as ∇y = 1ᵀ (a row vector of ones). Intuition: If you have y = x₁ + x₂ + ... + xₙ, and you wiggle x₁ by a small amount Δ, y changes by Δ. If you wiggle x₂ by Δ, y also changes by Δ, and so on. The rate of change of y with respect to any xⱼ is 1. What this result ultimately achieves: It gives us a very simple rule: the derivative of the sum of a vector\u0026rsquo;s elements, with respect to that vector itself, is a vector of ones (transposed to match the gradient convention). Example 2: y = sum(xz) (Sum of a vector multiplied by a scalar) Here, x is a vector, z is a scalar. Let f(x,z) be the vector xz (meaning fᵢ(x,z) = xᵢz). Then y = sum(f(x,z)) = Σᵢ (xᵢz).\nDerivative with respect to the vector x: ∂y/∂x\nThe j-th component of the gradient is ∂y/∂xⱼ = Σᵢ (∂(xᵢz)/∂xⱼ). ∂(xᵢz)/∂xⱼ: If i = j: ∂(xⱼz)/∂xⱼ = z (treating z as constant). If i ≠ j: ∂(xᵢz)/∂xⱼ = 0 (because xᵢz doesn\u0026rsquo;t depend on xⱼ). So, in the sum Σᵢ (∂(xᵢz)/∂xⱼ), only the term where i=j survives, and it is z. Therefore, ∂y/∂xⱼ = z for all j. This means ∂y/∂x = [z, z, ..., z]. Intuition: If y = x₁z + x₂z + ... + xₙz, and you wiggle xⱼ by Δ, then y changes by zΔ. The rate of change is z. Derivative with respect to the scalar z: ∂y/∂z (This is a scalar derivative of a scalar w.r.t a scalar)\ny = Σᵢ (xᵢz) = z * (Σᵢ xᵢ). ∂y/∂z = ∂/∂z ( z * sum(x) ). Since sum(x) is treated as a constant when differentiating w.r.t. z, ∂y/∂z = sum(x). Intuition: If y = z * (x₁ + x₂ + ... + xₙ), and you wiggle z by Δ, then y changes by (x₁ + ... + xₙ)Δ. The rate of change is sum(x). Key Takeaway from Section 4.4: When you differentiate a scalar sum y = sum(f(x)) with respect to the input vector x, the j-th component of the resulting gradient ∂y/∂x is the sum of how each term fᵢ(x) in the original sum changes with respect to xⱼ.\nFor y = sum(x), this simplifies to ∂y/∂x = 1ᵀ. This rule is fundamental for backpropagation because the overall loss of a network is often a sum (or average) of losses per instance, or a sum of terms in a complex loss function. When we take the derivative of this total loss with respect to weights in earlier layers, we\u0026rsquo;ll be implicitly using this idea of differentiating through a sum. The paper uses the notation ∇y when the output y is a scalar and the differentiation is with respect to a vector, resulting in a row vector (the gradient). It uses ∂y/∂x for the more general Jacobian when y could also be a vector.\nSection 4.5: The Chain Rules The Big Picture of This Section:\nNeural networks are essentially deeply nested functions. The output of one layer becomes the input to the next, which then feeds into another, and so on, until we get the final output, and then a loss function is applied. loss = L( activation_L( ... activation_2( activation_1(X, w₁, b₁), w₂, b₂) ... ), y_true )\nTo train the network using gradient descent, we need to calculate how the final loss changes with respect to every single weight and bias in the entire network, even those in the very first layers. This is a monumental task if done naively.\nThe Chain Rule is the mathematical tool that allows us to do this efficiently.\nWhat it\u0026rsquo;s ultimately trying to achieve: It provides a systematic way to calculate the derivative of a composite (nested) function by breaking it down into simpler derivatives of its constituent parts and then \u0026ldquo;chaining\u0026rdquo; them together (usually by multiplication). The paper discusses three variants, from simple to more general:\nSingle-variable chain rule (scalar function of a scalar variable). Single-variable total-derivative chain rule (scalar function of a scalar variable, but with intermediate multivariate functions). Vector chain rule (vector function of a vector variable). This is the most general one for neural networks. 4.5.1 Single-variable chain rule (Pages 14-17)\nThis is the chain rule you likely learned in basic calculus.\nScenario: You have a function nested within another, like y = f(g(x)). For example, y = sin(x²). Here, g(x) = x² (the inner function). And f(u) = sin(u) (the outer function, where u = g(x)). The Rule: dy/dx = dy/du * du/dx du/dx: How the inner function u changes with respect to x. For u=x², du/dx = 2x. dy/du: How the outer function y changes with respect to its direct input u. For y=sin(u), dy/du = cos(u). Then dy/dx = cos(u) * 2x. Substitute back u=x² to get cos(x²) * 2x. The Process (as recommended by the paper): Introduce intermediate variables: Break down the nested expression. For y = sin(x²), let u = x². Then y = sin(u). Compute derivatives of intermediate variables: du/dx = 2x dy/du = cos(u) Combine (multiply) the derivatives: dy/dx = (dy/du) * (du/dx) = cos(u) * 2x Substitute back: dy/dx = cos(x²) * 2x Units Analogy (page 15): The paper gives a nice analogy: if y is miles, u is gallons, and x is tank level, then miles/tank = (miles/gallon) * (gallons/tank). The intermediate unit \u0026ldquo;gallon\u0026rdquo; cancels out. Dataflow Diagram / Abstract Syntax Tree (page 16): Visualizing the chain of operations helps. Changes in x bubble up to u, then to y. The chain rule traces this path. Condition for Single-Variable Chain Rule: There\u0026rsquo;s a single dataflow path from x to y. Intermediate functions (u(x), y(u)) have only one parameter. Deeply Nested Expressions (page 17): The process extends. For y = f₄(f₃(f₂(f₁(x)))), let u₁=f₁(x), u₂=f₂(u₁) etc. Then dy/dx = (dy/du₃) * (du₃/du₂) * (du₂/du₁) * (du₁/dx). Example: y = ln(sin(x³)²) u₁ = x³ u₂ = sin(u₁) u₃ = u₂² y = u₄ = ln(u₃) du₁/dx = 3x² du₂/du₁ = cos(u₁) du₃/du₂ = 2u₂ dy/du₃ = 1/u₃ dy/dx = (1/u₃) * (2u₂) * (cos(u₁)) * (3x²) Substitute back: (1/sin(x³)² ) * (2sin(x³)) * (cos(x³)) * (3x²) = 6x²cos(x³)/sin(x³) What the single-variable chain rule is ultimately trying to achieve: It provides a recipe to find the overall rate of change of a nested scalar function by multiplying the rates of change of its individual components along the chain of dependency. 4.5.2 Single-variable total-derivative chain rule (Pages 18-20)\nThis handles a more complex scenario. What if an intermediate variable depends on the original input x and other intermediate variables that also depend on x?\nScenario: y = f(x) = x + x². The paper rewrites this using intermediate variables to illustrate: u₁(x) = x² y = u₂(x, u₁) = x + u₁ Here, y (which is u₂) depends directly on x and it depends on u₁, which also depends on x. So, x influences y through two paths:\nDirectly (the x term in x + u₁). Indirectly (through u₁ which is x²). The simple chain rule dy/dx = (∂y/∂u₁) * (du₁/dx) would give 1 * 2x = 2x, which is wrong (the derivative of x+x² is 1+2x).\nThe \u0026ldquo;Law\u0026rdquo; of Total Derivatives (page 19): To get dy/dx (the total derivative of y with respect to x), you need to sum up all possible contributions from changes in x to the change in y. For y = u₂(x, u₁(x)): dy/dx = ∂u₂/∂x + (∂u₂/∂u₁) * (du₁/dx) Let\u0026rsquo;s break this down:\n∂u₂/∂x: This is the partial derivative of u₂(x,u₁) with respect to x, treating u₁ as if it were an independent constant for this term. For u₂ = x + u₁, ∂u₂/∂x = 1. (How y changes if x wiggles but u₁ is held fixed). ∂u₂/∂u₁: Partial derivative of u₂(x,u₁) with respect to u₁, treating x as constant. For u₂ = x + u₁, ∂u₂/∂u₁ = 1. (How y changes if u₁ wiggles but x is held fixed). du₁/dx: Total derivative of u₁(x) with respect to x. For u₁ = x², du₁/dx = 2x. (How u₁ changes if x wiggles). So, dy/dx = 1 + (1 * 2x) = 1 + 2x. Correct! General Single-Variable Total-Derivative Chain Rule (page 19): If y = f(x, u₁, u₂, ..., uₙ), and each uᵢ is also a function of x (uᵢ(x)): dy/dx (total) = ∂f/∂x (direct) + Σᵢ (∂f/∂uᵢ) * (duᵢ/dx (total))\n∂f/∂x: Partial derivative of f w.r.t x, holding all uᵢ constant (measures direct effect of x on f). ∂f/∂uᵢ: Partial derivative of f w.r.t uᵢ, holding x and other uⱼ constant (how f changes if only uᵢ changes). duᵢ/dx: Total derivative of uᵢ w.r.t x (how uᵢ itself changes when x changes, which might involve its own chain of dependencies). What this rule is ultimately trying to achieve: It correctly accounts for all paths through which a change in x can affect the final output y, summing up the contributions from the direct path and all indirect paths through intermediate variables uᵢ. Simplified Final Form (page 20): The paper cleverly shows that if you introduce x itself as another intermediate variable (e.g., uₙ₊₁ = x), then the \u0026ldquo;direct\u0026rdquo; ∂f/∂x term can be absorbed into the sum. If y = f(u₁, ..., uₙ, uₙ₊₁) where uᵢ = uᵢ(x) for all i, and uₙ₊₁ = x (so duₙ₊₁/dx = 1), Then dy/dx = Σᵢ (∂f/∂uᵢ) * (duᵢ/dx) (summing over all n+1 intermediate variables). This foreshadows the vector chain rule where the sum looks like a dot product.\nCaution on Terminology: The paper notes that what they call the \u0026ldquo;single-variable total-derivative chain rule\u0026rdquo; is often just called the \u0026ldquo;multivariable chain rule\u0026rdquo; in calculus discussions, which can be misleading because the overall function f(x) is still a scalar function of a single scalar x.\n4.5.3 Vector chain rule (Pages 21-22) - THE PAYOFF!\nNow for the most general case, relevant to neural networks:\ng(x): A function that takes a vector x and outputs an intermediate vector u = g(x).\nf(u): A function that takes the vector u and outputs a final vector y = f(u).\nWe want to find ∂y/∂x, the Jacobian of the composite function y = f(g(x)) with respect to x.\nThe Rule (beautifully simple in vector/matrix form): ∂y/∂x = (∂y/∂u) * (∂u/∂x) Where:\n∂u/∂x: Is the Jacobian of g with respect to x. If x is n x 1 and u is k x 1, this is a k x n matrix. ∂y/∂u: Is the Jacobian of f with respect to u. If u is k x 1 and y is m x 1, this is an m x k matrix. *: This is matrix multiplication. The result ∂y/∂x will be an m x n matrix, which is the correct shape for the Jacobian of y (m-dim) w.r.t. x (n-dim). How the paper rediscovers it (page 21): It starts with a simpler case: y = f(g(x)) where x is a scalar, g(x) is a vector [g₁(x), g₂(x)]ᵀ, and f(g) is a vector [f₁(g), f₂(g)]ᵀ (where f₁ might depend on g₁ and g₂, and f₂ might also depend on g₁ and g₂).\n∂y/∂x will be a column vector [∂y₁/∂x, ∂y₂/∂x]ᵀ. Using the single-variable total-derivative chain rule for each component yᵢ: ∂y₁/∂x = (∂f₁/∂g₁) * (dg₁/dx) + (∂f₁/∂g₂) * (dg₂/dx) ∂y₂/∂x = (∂f₂/∂g₁) * (dg₁/dx) + (∂f₂/∂g₂) * (dg₂/dx) This can be written in matrix form: [ ∂y₁/∂x ] = [ ∂f₁/∂g₁ ∂f₁/∂g₂ ] * [ dg₁/dx ] [ ∂y₂/∂x ] [ ∂f₂/∂g₁ ∂f₂/∂g₂ ] [ dg₂/dx ] This is exactly ∂y/∂x = (∂y/∂g) * (∂g/∂x)! (Here u is called g). The Beauty of the Vector Chain Rule (page 22): It automatically takes care of the total derivative aspect (summing over all intermediate paths) because of how matrix multiplication is defined (sum of products). The full Jacobian components are shown, illustrating the m x k matrix ∂f/∂g multiplying the k x n matrix ∂g/∂x to give an m x n result.\nSimplification for Element-wise Operations (page 22, bottom): If f operates element-wise on g (i.e., yᵢ = fᵢ(gᵢ)) AND g operates element-wise on x (i.e., gᵢ = gᵢ(xᵢ)), then:\n∂f/∂g becomes diag(∂fᵢ/∂gᵢ) ∂g/∂x becomes diag(∂gᵢ/∂xᵢ) Then ∂y/∂x = diag(∂fᵢ/∂gᵢ) * diag(∂gᵢ/∂xᵢ) = diag( (∂fᵢ/∂gᵢ) * (∂gᵢ/∂xᵢ) ). The Jacobian is diagonal, and each diagonal element is just the result of the single-variable chain rule applied to the components. This connects back to the earlier sections. Key Takeaway from Section 4.5: The chain rule, in its various forms, is the fundamental mechanism for calculating derivatives of complex, nested functions.\nFor a simple chain y(u(x)), it\u0026rsquo;s dy/dx = (dy/du)(du/dx). If x can affect y through multiple paths via intermediate variables uᵢ(x), the total derivative involves summing the contributions from each path. For vector functions of vector variables y(u(x)), the Jacobian ∂y/∂x = (∂y/∂u)(∂u/∂x) (matrix multiplication) elegantly captures all these interactions. This is the version most directly applicable to backpropagation in neural networks, where ∂y/∂u is the gradient from the layer above, and ∂u/∂x is the local gradient of the current layer. This vector chain rule is what allows backpropagation to efficiently compute the gradient of the final loss with respect to every single weight in a deep network by \u0026ldquo;chaining\u0026rdquo; these Jacobian multiplications backward through the layers. Each Jacobian ∂(layer_output)/∂(layer_input) tells us how a layer transforms incoming gradient signals.\nSection 5: The gradient of neuron activation The Big Picture of This Section:\nIn the previous section (4.5.3 Vector Chain Rule), the paper showed how to calculate ∂y/∂x = (∂y/∂u) * (∂u/∂x) where u=g(x) and y=f(u). This section is going to calculate one of these crucial Jacobians: the ∂u/∂x part, specifically for a typical neuron\u0026rsquo;s \u0026ldquo;affine function\u0026rdquo; (the weighted sum + bias) before the non-linear activation. Then, it will combine it with the derivative of an activation function (like max(0,z)) using the chain rule.\nEssentially, we want to answer:\nIf a neuron calculates z = w·x + b (affine part) and then activation = A(z) (activation part), how does this activation change if we wiggle the inputs x, the weights w, or the bias b? This is a fundamental building block for backpropagation. When we have the gradient of the loss with respect to a neuron\u0026rsquo;s activation (this would be like ∂L/∂activation, coming from layers above), we\u0026rsquo;ll need ∂activation/∂w and ∂activation/∂b to find out how to update that neuron\u0026rsquo;s weights and bias.\nWhat we are ultimately trying to achieve here is to find the specific derivative expressions (gradients) for a single neuron\u0026rsquo;s output with respect to its inputs and its own parameters (weights and bias). These will be the local gradients used in the backpropagation algorithm.\nBreaking it Down:\nThe typical neuron computation is:\nAffine function: z(w, b, x) = w · x + b (a scalar output if we consider one neuron for now) Activation function: activation(z) = A(z). The paper uses A(z) = max(0, z) (ReLU) as the example. So, the overall neuron activation is activation(x) = max(0, w · x + b).\nStep 1: Focus on the Affine Part z = w · x + b (Page 24, bottom half)\nLet y = w · x + b. (The paper uses y here for the affine part before max, which is a bit confusing as y is often the final output. Let\u0026rsquo;s stick to z for the affine part as the paper does in max(0,z)). So, z = w · x + b. We need ∂z/∂w and ∂z/∂b.\n∂z/∂w (Derivative of w·x + b with respect to vector w):\n∂/∂w (w·x): This is the derivative of a dot product. The paper cleverly rephrases w·x as sum(w ⊗ x) where ⊗ is element-wise multiplication. Let u = w ⊗ x. We know from Section 4.2 that ∂u/∂w = diag(x). Let z' = sum(u). We know from Section 4.4 that ∂z'/∂u = 1ᵀ (a row vector of ones). Using the vector chain rule for ∂z'/∂w = (∂z'/∂u) * (∂u/∂w): ∂(w·x)/∂w = 1ᵀ * diag(x) = xᵀ (a row vector). ∂/∂w (b): Since b is a scalar bias and doesn\u0026rsquo;t depend on w, this derivative is 0ᵀ (a row vector of zeros). So, ∂z/∂w = ∂(w·x + b)/∂w = xᵀ + 0ᵀ = xᵀ. Intuition: How does z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b change if wⱼ changes? It changes by xⱼ. So the gradient vector is [x₁, x₂, ..., xₙ], which is xᵀ. What this is trying to achieve: It tells us that the sensitivity of the pre-activation value z to a change in a weight wⱼ is simply the value of the corresponding input xⱼ. Larger inputs mean that their corresponding weights have a bigger impact. ∂z/∂b (Derivative of w·x + b with respect to scalar b):\n∂/∂b (w·x): This is 0, as w·x doesn\u0026rsquo;t depend on b. ∂/∂b (b): This is 1. So, ∂z/∂b = 0 + 1 = 1. Intuition: If you change the bias b by a small amount Δ, z changes by Δ. The rate of change is 1. What this is trying to achieve: It tells us that the pre-activation value z changes one-to-one with changes in the bias b. Step 2: Tackle the max(0, z) Activation Function (ReLU) (Page 24, bottom, and Page 25, top)\nLet activation = A(z) = max(0, z). We need ∂A/∂z. The max(0,z) function is piecewise:\nIf z ≤ 0, then A(z) = 0. The derivative dA/dz = 0. If z \u0026gt; 0, then A(z) = z. The derivative dA/dz = 1. The paper writes this as: ∂/∂z max(0, z) = { 0 if z ≤ 0; 1 if z \u0026gt; 0 }. (Note: Technically, it\u0026rsquo;s not differentiable at z=0, but in practice, we can assign a subderivative of 0 or 1). The paper then briefly mentions \u0026ldquo;broadcasting\u0026rdquo; for max(0, x) if x is a vector, meaning max is applied element-wise. The derivative ∂/∂xᵢ max(0, xᵢ) would be 0 if xᵢ ≤ 0 and 1 if xᵢ \u0026gt; 0.\nStep 3: Combine using the Chain Rule for activation(x) = max(0, w · x + b) (Page 25)\nLet z(w, b, x) = w · x + b (intermediate scalar variable representing the affine part). Let activation(z) = max(0, z).\nWe want ∂activation/∂w and ∂activation/∂b. Using the vector chain rule (or its scalar adaptation here since z and activation are scalars for a single neuron):\n∂activation/∂w = (∂activation/∂z) * (∂z/∂w)\n∂activation/∂z is 0 if z ≤ 0 and 1 if z \u0026gt; 0. ∂z/∂w is xᵀ. So, ∂activation/∂w = { 0ᵀ if z ≤ 0 (i.e., w·x+b ≤ 0) } { xᵀ if z \u0026gt; 0 (i.e., w·x+b \u0026gt; 0) } What this is ultimately trying to achieve: If the neuron is \u0026ldquo;off\u0026rdquo; (pre-activation z is negative or zero, so ReLU output is 0), then small changes to the weights w have no effect on the output (gradient is 0). If the neuron is \u0026ldquo;on\u0026rdquo; (z \u0026gt; 0, ReLU output is z), then the way the output changes with respect to w is the same as how z changes with respect to w, which is xᵀ. ∂activation/∂b = (∂activation/∂z) * (∂z/∂b)\n∂activation/∂z is 0 if z ≤ 0 and 1 if z \u0026gt; 0. ∂z/∂b is 1. So, ∂activation/∂b = { 0 if z ≤ 0 (i.e., w·x+b ≤ 0) } { 1 if z \u0026gt; 0 (i.e., w·x+b \u0026gt; 0) } What this is ultimately trying to achieve: If the neuron is \u0026ldquo;off\u0026rdquo;, changes to b have no effect on the output. If the neuron is \u0026ldquo;on\u0026rdquo;, changes to b have a one-to-one effect on the output. Key Takeaway from Section 5: This section derived the gradients of a single ReLU neuron\u0026rsquo;s activation with respect to its weights w and its bias b.\n∂(ReLU(w·x+b))/∂w = xᵀ if w·x+b \u0026gt; 0, and 0ᵀ otherwise. ∂(ReLU(w·x+b))/∂b = 1 if w·x+b \u0026gt; 0, and 0 otherwise. These are the local gradients for a single neuron. In backpropagation, these local gradients will be multiplied by the gradient of the loss coming from the layer above (using the chain rule again!) to find out how much that specific neuron\u0026rsquo;s weights and bias contributed to the overall network error. This tells us how to update w and b for that neuron.\nSection 6: The gradient of the neural network loss function The Big Picture of This Section:\nThis is where everything we\u0026rsquo;ve learned about matrix calculus (gradients, Jacobians, chain rules) comes together to explain how a neural network learns by minimizing a loss function.\nWe have:\nA model (our neural network) with parameters (weights w and biases b for all neurons). A loss function C(w, b, X, y) that measures how \u0026ldquo;bad\u0026rdquo; the model\u0026rsquo;s predictions are on a set of training data X with true labels y. For example, Mean Squared Error for regression. An optimization algorithm (Gradient Descent) that needs the gradient of this loss function C with respect to all the model\u0026rsquo;s parameters (w\u0026rsquo;s and b\u0026rsquo;s) to update them and reduce the loss. What we are ultimately trying to achieve in this section is to derive the formulas for ∂C/∂w and ∂C/∂b for a simple neuron, using the chain rule and the neuron activation gradients we found in Section 5. This will show the complete calculation for one neuron\u0026rsquo;s parameter updates.\nSetting the Stage (Page 25, bottom):\nThe paper considers training a single neuron.\nInput: X is a matrix where each row xᵢ is an input vector for one training instance. N is the number of instances. Target: y is a vector where each yᵢ is the scalar target output for instance xᵢ. Loss Function (Mean Squared Error): C(w, b, X, y) = (1/N) * Σᵢ (yᵢ - activation(xᵢ))² where activation(xᵢ) = max(0, w · xᵢ + b). Following the Chain Rule Process (Page 26):\nTo find ∂C/∂w (and later ∂C/∂b), we introduce intermediate variables to break down the loss function calculation for a single instance xᵢ first, and then we\u0026rsquo;ll average over all instances.\nLet\u0026rsquo;s consider the contribution of one instance x (dropping the i superscript for a moment) to the loss: (y_true - activation(x))². The paper uses these intermediate variables:\nu(w, b, x) = max(0, w · x + b) (This is the neuron\u0026rsquo;s activation. Let\u0026rsquo;s call it a for activation to avoid confusion with u often being an intermediate in chain rule). So, a = max(0, w·x+b). v(y_true, a) = y_true - a (This is the error for this instance). L_instance(v) = v² (The squared error for this instance. The total loss C will be the average of these L_instance values). 6.1 The gradient with respect to the weights w (Pages 26-27)\nWe want ∂C/∂w. Since C is an average, ∂C/∂w = (1/N) Σᵢ (∂L_instanceᵢ / ∂w). Let\u0026rsquo;s find ∂L_instance / ∂w for one instance. L_instance = v² v = y_true - a a = max(0, w·x + b)\nUsing the chain rule: ∂L_instance/∂w = (∂L_instance/∂v) * (∂v/∂a) * (∂a/∂w)\n∂L_instance/∂v = ∂(v²)/∂v = 2v ∂v/∂a = ∂(y_true - a)/∂a = -1 ∂a/∂w = ∂(max(0, w·x+b))/∂w. We found this in Section 5! It\u0026rsquo;s xᵀ if w·x+b \u0026gt; 0 (neuron is active). It\u0026rsquo;s 0ᵀ if w·x+b ≤ 0 (neuron is not active). Combining these for one instance: ∂L_instance/∂w = (2v) * (-1) * (xᵀ or 0ᵀ) ∂L_instance/∂w = -2v * (xᵀ or 0ᵀ) Substitute v = y_true - a = y_true - max(0, w·x+b): ∂L_instance/∂w = -2(y_true - max(0, w·x+b)) * (xᵀ or 0ᵀ)\nThe paper simplifies this into a conditional expression (top of page 26, and then expanded middle of page 27):\nIf w·x + b ≤ 0 (neuron not active, max(0,...) = 0): Then ∂L_instance/∂w = -2(y_true - 0) * 0ᵀ = 0ᵀ. (Makes sense, if neuron output is 0, weights don\u0026rsquo;t affect it in this region). If w·x + b \u0026gt; 0 (neuron active, max(0,...) = w·x+b): Then ∂L_instance/∂w = -2(y_true - (w·x+b)) * xᵀ = 2((w·x+b) - y_true) * xᵀ. Now, average over all N instances xᵢ: ∂C/∂w = (1/N) Σᵢ 2 * ( (w·xᵢ+b) - yᵢ ) * xᵢᵀ (This sum is only for instances where w·xᵢ+b \u0026gt; 0). Or, more compactly, as shown in the paper before the \u0026ldquo;To interpret that equation\u0026hellip;\u0026rdquo; part: ∂C/∂w = (2/N) Σᵢ eᵢ xᵢᵀ (where eᵢ = (w·xᵢ+b) - yᵢ is the error, and this sum is for the \u0026ldquo;nonzero activation case,\u0026rdquo; i.e., where w·xᵢ+b \u0026gt; 0).\nInterpretation (Page 27): ∂C/∂w is a weighted average of the input vectors xᵢᵀ (transposed to be row vectors, which when summed give the gradient row vector). The weights for this average are the error terms eᵢ (the difference between the neuron\u0026rsquo;s affine output w·xᵢ+b and the target yᵢ). What this is ultimately trying to achieve: If the error eᵢ for instance xᵢ is large and positive (model predicted too high), the gradient ∂C/∂w will be pushed in the direction of xᵢᵀ. To reduce the error via gradient descent (which subtracts the gradient), the weights w will be adjusted against xᵢᵀ, effectively reducing the output w·xᵢ+b. If the error eᵢ is large and negative (model predicted too low), the gradient ∂C/∂w will be pushed in the direction opposite to xᵢᵀ. Gradient descent will adjust w towards xᵢᵀ, increasing the output. Instances xᵢ that result in larger errors eᵢ have more influence on the gradient (they \u0026ldquo;pull\u0026rdquo; the weights harder). This is exactly what we want for learning! The gradient points in a direction that tells us how to change w to reduce the average error. Gradient Descent Update (Page 27, bottom): w_next = w_current - η * (∂C/∂w) This is the standard gradient descent update rule. Because the gradient ∂C/∂w points towards higher cost, we subtract a small amount of it (scaled by learning rate η) from the current weights to move towards lower cost.\n6.2 The derivative with respect to the bias b (Pages 27-28)\nThe process is very similar. We want ∂C/∂b. ∂C/∂b = (1/N) Σᵢ (∂L_instanceᵢ / ∂b). Using the chain rule: ∂L_instance/∂b = (∂L_instance/∂v) * (∂v/∂a) * (∂a/∂b)\n∂L_instance/∂v = 2v (same as before) ∂v/∂a = -1 (same as before) ∂a/∂b = ∂(max(0, w·x+b))/∂b. We found this in Section 5! It\u0026rsquo;s 1 if w·x+b \u0026gt; 0 (neuron is active). It\u0026rsquo;s 0 if w·x+b ≤ 0 (neuron is not active). Combining these for one instance: ∂L_instance/∂b = (2v) * (-1) * (1 or 0) ∂L_instance/∂b = -2v * (1 or 0) Substitute v = y_true - max(0, w·x+b):\nIf w·x + b ≤ 0: ∂L_instance/∂b = -2(y_true - 0) * 0 = 0. If w·x + b \u0026gt; 0: ∂L_instance/∂b = -2(y_true - (w·x+b)) * 1 = 2((w·x+b) - y_true). Averaging over all N instances xᵢ: ∂C/∂b = (1/N) Σᵢ 2 * ( (w·xᵢ+b) - yᵢ ) (This sum is only for instances where w·xᵢ+b \u0026gt; 0). Or, as the paper shows: ∂C/∂b = (2/N) Σᵢ eᵢ (where eᵢ = (w·xᵢ+b) - yᵢ, summed for the \u0026ldquo;nonzero activation case\u0026rdquo;).\nInterpretation (Page 28): The partial derivative ∂C/∂b is just the average error eᵢ (for activated neurons), scaled by 2. What this is ultimately trying to achieve: If, on average, the neuron is predicting too high (average eᵢ is positive), the gradient ∂C/∂b is positive. Gradient descent b_next = b - η(∂C/∂b) will decrease b, reducing the neuron\u0026rsquo;s output. If, on average, the neuron is predicting too low (average eᵢ is negative), the gradient ∂C/∂b is negative. Gradient descent will increase b, increasing the neuron\u0026rsquo;s output. Combining w and b (Page 29, top): The paper mentions that in practice, it\u0026rsquo;s convenient to combine w and b into a single parameter vector ŵ = [wᵀ, b]ᵀ and augment the input vector x with a 1: ✰ = [xᵀ, 1]ᵀ. Then the affine function is just ŵ · ✰. This simplifies some notation but the underlying derivative calculations are the same.\nSection 7: Summary (Page 29) This section of the paper concludes that we now have the two partials (∂C/∂w and ∂C/∂b) necessary to perform gradient descent and optimize the parameters of a single neuron. It also reminds us that the next step in understanding full backpropagation for a multi-layer network would be to learn about partial derivatives of matrices, not just vectors.\nKey Takeaway from Section 6: This section ties everything together for a single neuron. By applying the chain rule, we derived how the overall Mean Squared Error loss changes with respect to the neuron\u0026rsquo;s weights w and its bias b.\n∂C/∂w is an average of input vectors xᵢᵀ, weighted by the prediction errors eᵢ. ∂C/∂b is an average of the prediction errors eᵢ. These gradients tell gradient descent how to adjust w and b to make the neuron\u0026rsquo;s predictions better on average for the given training data. This is the fundamental step of learning for one neuron. Backpropagation is essentially applying this logic layer by layer, from the output back to the input, using the chain rule at each step. ","permalink":"https://deepskandpal.github.io/papershelf/read/maths/","tags":null,"title":"The Matrix Calculus You Need For Deep Learning"},{"categories":["Horror"],"content":"","permalink":"https://deepskandpal.github.io/stories/ek-kahani-aisibhi/","tags":null,"title":""},{"categories":null,"content":"Hi—I’m Deepanshu Kandpal. Over the past eight years I’ve jumped between e‑commerce, telecom, consulting and cybersecurity, always chasing that moment when clever code becomes real‑world impact. Right now at KnowBe4 I’m building LLM‑powered agents that sniff out threats and automate responses at scale. Before that at Farfetch I helped grow an A/B testing platform to handle 100K+ queries a day across 300 TB of data—learning firsthand how to coordinate agile teams across continents. On the side, I tinker with LangChef, my open‑source framework for LLM experimentation. I actually got my start as an iOS developer at Jio, crafting AI assistants that now handle over 3 million queries daily and take a huge load off call‑center teams.\n","permalink":"https://deepskandpal.github.io/about/","tags":null,"title":"About"},{"categories":["Maths and Science"],"content":"The Essential Foundation for Data Science and AI A comprehensive, freely available linear algebra textbook that provides the mathematical foundation for machine learning, computer graphics, data science, and modern AI systems. Winner of the MAA Solow Award in 2020, this text has been adopted by hundreds of institutions worldwide.\nWhy This Book is Essential for Modern Tech Linear algebra is the mathematical language of:\nMachine Learning: Understanding how neural networks, PCA, and optimization algorithms work Computer Graphics: Transformations, rotations, and 3D rendering Data Science: Dimensionality reduction, feature engineering, and statistical modeling Quantum Computing: State vectors and quantum operations Signal Processing: Fourier transforms and digital signal analysis Connection to Your Technical Knowledge Tree This book provides the mathematical foundation that makes advanced topics accessible:\nDeep Learning: Understanding matrix operations in neural networks Computer Vision: Image transformations and feature detection Natural Language Processing: Vector embeddings and similarity measures Reinforcement Learning: Value functions and policy gradients Optimization: Gradient descent and convex optimization foundations Pedagogical Excellence What makes this textbook exceptional:\nDevelopmental Approach: Builds mathematical maturity gradually Computational Examples: Balances theory with practical applications Extensive Exercises: 700+ problems with complete solutions available Open Source: Freely available with full LaTeX source Active Learning: Includes lab manual using Sage (Python-based) Multimedia Support: Beamer slides and YouTube videos Comprehensive Coverage Standard linear algebra curriculum:\nLinear Systems: Gaussian elimination and matrix operations Vector Spaces: Abstract vector spaces and linear independence Linear Maps and Matrices: Transformations and matrix representations Determinants: Properties and computational methods Eigenvalues and Eigenvectors: Diagonalization and applications Modern Applications Each chapter includes supplemental topics showing real-world applications:\nComputer graphics transformations Markov chains and Google\u0026rsquo;s PageRank algorithm Principal Component Analysis for data reduction Cryptography and coding theory Network analysis and graph theory For Self-Study and Instruction This book excels at:\nSelf-contained learning: No advanced prerequisites beyond calculus Instructor support: Complete teaching materials available Flexible use: Main text, supplement, or reference International adoption: Used in courses worldwide Free access: Removes financial barriers to mathematical education Award Recognition 2020 MAA Solow Award: For impact on undergraduate mathematics education Hundreds of adoptions: Used at major universities globally Open textbook pioneer: Leading example of quality open educational resources This book democratizes access to high-quality mathematical education and provides the essential foundation for anyone working in data science, AI, or computational fields.\n","permalink":"https://deepskandpal.github.io/bookshelf/linear-algebra-hefferon/","tags":null,"title":"Linear Algebra"},{"categories":["Maths and Science"],"content":"The Definitive Guide to Natural Language Processing The authoritative textbook on NLP that provides the linguistic and computational foundation for understanding large language models and transformer architectures. This book bridges the gap between traditional computational linguistics and modern neural language models.\nWhy This Book is Essential for Understanding LLMs Large Language Models are fundamentally natural language processing systems. This book provides the essential background:\nTokenization and Text Processing: How text is converted into tokens that transformers can process Language Modeling: Mathematical foundation for next-token prediction and perplexity Parsing and Syntax: Understanding how language structure relates to transformer attention patterns Semantics and Pragmatics: How meaning emerges from statistical patterns in text Evaluation in NLP: Metrics and methods for evaluating language generation systems Connection to Your GenAI Knowledge Tree Critical NLP concepts that directly apply to your GenAI materials:\nTransformer Architecture: Historical context from RNNs to attention mechanisms Pre-training Objectives: Language modeling and masked language modeling explained Fine-tuning for NLP Tasks: Task-specific adaptation of language models Evaluation Metrics: BLEU, ROUGE, and other metrics for text generation Multimodal NLP: Integration of language with vision and other modalities From Classical NLP to Modern LLMs This book excels at showing the evolution from traditional NLP to neural approaches:\nN-gram Models to Neural Language Models: Evolution of language modeling Rule-based to Statistical to Neural: Paradigm shifts in NLP Symbolic Parsing to Attention: How transformers capture linguistic structure Feature Engineering to Representation Learning: From handcrafted to learned features Practical Applications Understanding covered in this book enables you to:\nDesign better prompts for language models Understand why certain architectures work better for different languages Evaluate language model outputs more effectively Debug issues in text generation systems Design multilingual AI systems Modern Relevance to GenAI The latest edition covers contemporary topics:\nNeural language models and transformers Pre-training and fine-tuning paradigms Ethical considerations in NLP Bias and fairness in language models Applications to conversational AI and text generation For GenAI Practitioners This book is essential for anyone working with:\nLarge language models (GPT, BERT, T5) Text generation and conversational AI Multilingual and cross-lingual systems Evaluation and benchmarking of language models NLP applications in production systems This book provides the linguistic foundation that makes you effective at working with language-based AI systems, not just applying them blindly.\n","permalink":"https://deepskandpal.github.io/bookshelf/speech-language-processing-jurafsky-martin/","tags":null,"title":"Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition"},{"categories":["Machine Learning"],"content":"The Comprehensive Foundation of Artificial Intelligence The most widely used AI textbook that provides the broad conceptual foundation for understanding how GenAI fits into the larger landscape of artificial intelligence. This book contextualizes all the specialized GenAI techniques within the broader AI paradigm.\nWhy This Book Provides Essential Context While specialized books cover specific aspects of GenAI, this text provides the overarching framework that makes everything else make sense:\nProblem Solving and Search: Foundation for understanding how AI agents reason and plan Knowledge Representation: How AI systems represent and manipulate information Reasoning Under Uncertainty: Probabilistic approaches that underlie AI safety and alignment Machine Learning Integration: How GenAI fits into the broader ML landscape AI Safety and Ethics: Early treatment of alignment problems and ethical considerations Connection to Your GenAI Knowledge Tree This book provides the conceptual bridges between different areas of your GenAI materials:\nAI Agents: From simple reflex agents to complex reasoning systems like ChatGPT Multi-Agent Systems: Understanding how multiple AI systems interact Planning and Decision Making: Foundation for AI systems that can plan and execute complex tasks Natural Language Processing: Historical context for how we arrived at transformers Computer Vision: Broader context for multimodal AI systems Bridging Classical and Modern AI Russell and Norvig excel at showing how classical AI techniques connect to modern deep learning:\nSymbolic vs Connectionist AI: Understanding the paradigm shift to neural approaches Expert Systems to Foundation Models: Evolution from rule-based to learned systems Logic and Reasoning: How formal reasoning relates to emergent reasoning in LLMs Game Theory: Foundation for understanding multi-agent interactions and alignment For Comprehensive AI Understanding This book is essential for:\nUnderstanding how GenAI fits into the broader AI landscape Grasping the historical evolution that led to current techniques Seeing connections between different AI subfields Developing a systematic approach to AI problem-solving Understanding AI safety and alignment in broader context Modern Relevance The latest editions address contemporary concerns:\nMachine Learning safety and alignment challenges Ethical implications of powerful AI systems Social and economic impacts of AI Future directions for AI research This book provides the conceptual framework that makes you a complete AI practitioner, not just a specialist in one area.\n","permalink":"https://deepskandpal.github.io/bookshelf/ai-modern-approach-russell-norvig/","tags":null,"title":"Artificial Intelligence: A Modern Approach"},{"categories":["Maths and Science"],"content":"The Information-Theoretic Foundation of AI The definitive text connecting information theory, statistical inference, and machine learning. This book provides the theoretical framework for understanding compression, generalization, and the fundamental limits of learning in AI systems.\nWhy This Book is Essential for GenAI Information theory underlies many fundamental concepts in your GenAI knowledge tree:\nEntropy and Compression: Foundation for understanding tokenization and data efficiency Mutual Information: Mathematical basis for attention mechanisms and representation learning KL Divergence: Core metric used in VAEs, diffusion models, and alignment techniques Channel Capacity: Understanding the fundamental limits of information transfer Coding Theory: Mathematical foundation for efficient model compression Connection to GenAI Systems Critical information-theoretic concepts that appear throughout your materials:\nAttention Mechanisms: Information-theoretic view of what models \u0026ldquo;pay attention\u0026rdquo; to Variational Autoencoders: KL divergence in the loss function comes directly from information theory Model Compression: Information-theoretic bounds on how much models can be compressed Generalization Theory: Information-theoretic bounds on learning and generalization Evaluation Metrics: Perplexity and other information-based evaluation measures Bridging Theory and Practice MacKay\u0026rsquo;s unique approach connects abstract mathematical concepts to practical algorithms:\nBayesian Neural Networks: Information-theoretic view of uncertainty Error-Correcting Codes: Foundation for understanding robustness in AI systems MCMC Methods: Sampling algorithms used in modern AI training Compression Algorithms: Connection between compression and prediction For Advanced AI Practitioners This book provides the theoretical depth to understand:\nWhy certain architectures are more efficient at representing information How to design better evaluation metrics for generative models The fundamental trade-offs between model size, data, and performance Information-theoretic approaches to AI safety and alignment Essential for anyone who wants to understand the fundamental information-processing principles that make AI systems work.\n","permalink":"https://deepskandpal.github.io/bookshelf/information-theory-mackay/","tags":null,"title":"Information Theory, Inference, and Learning Algorithms"},{"categories":["Machine Learning"],"content":"The Foundation of AI Alignment and RLHF The authoritative text on reinforcement learning that provides the theoretical foundation for modern AI alignment techniques. This book is essential for understanding RLHF, Constitutional AI, and human preference learning in GenAI systems.\nWhy This Book is Critical for Modern GenAI RLHF (Reinforcement Learning from Human Feedback) is the key breakthrough that enables models like ChatGPT and Claude to behave helpfully and safely. This book provides the theoretical foundation:\nPolicy Optimization: Mathematical basis for PPO and other algorithms used in RLHF Value Functions: Understanding reward modeling and human preference learning Exploration vs Exploitation: Balancing learning new behaviors vs exploiting known good behaviors Monte Carlo Methods: Sampling techniques used in policy gradient algorithms Temporal Difference Learning: Foundation for reward learning from human feedback Connection to GenAI Alignment Systems Key concepts from your GenAI materials that directly build on this book:\nRLHF Pipeline: Policy optimization techniques for aligning language models Constitutional AI: Using RL principles for self-supervised alignment Reward Modeling: Learning human preferences through value function approximation Safety Training: Exploration strategies that avoid harmful behaviors AI Agents: Multi-step reasoning and planning in language models From Theory to Practice This book bridges the gap between theoretical RL and practical AI alignment:\nUnderstanding why PPO works for language model fine-tuning How reward models capture human preferences Why exploration is crucial for safe AI development How to design reward functions that capture human values For AI Safety and Alignment Essential reading for anyone working on:\nRLHF implementation for language models Machine Learning safety and alignment research Constitutional AI and self-supervised alignment Human preference learning systems This book provides the mathematical foundation that makes safe, aligned AI systems possible.\n","permalink":"https://deepskandpal.github.io/bookshelf/reinforcement-learning-sutton-barto/","tags":null,"title":"Reinforcement Learning: An Introduction"},{"categories":["Maths and Science"],"content":"The Mathematical Foundation of AI Training The definitive text on optimization theory that underlies all modern AI training algorithms. This book provides the mathematical rigor needed to understand why gradient descent, Adam, and other optimization techniques work in large-scale AI systems.\nWhy This Book is Essential for GenAI Training Every aspect of training large language models and generative AI systems relies on optimization principles covered in this text:\nGradient Descent Theory: Mathematical foundations of backpropagation Convex Analysis: Understanding loss landscapes and convergence guarantees Duality Theory: Theoretical underpinnings of many ML algorithms Constrained Optimization: Foundation for techniques like RLHF and constitutional AI Algorithms and Convergence: Why modern optimizers like Adam and AdamW work Connection to GenAI Training Systems Critical concepts from your GenAI training materials trace directly to this book:\nDistributed Training Optimization: Mathematical foundations for parameter sharding Learning Rate Scheduling: Theoretical basis for warmup and decay strategies Memory-Efficient Training: Optimization principles behind gradient checkpointing Parameter-Efficient Methods: Mathematical basis for LoRA and other techniques Safety and Alignment: Constrained optimization approaches to AI alignment For AI Practitioners and Researchers This book provides the theoretical depth needed to:\nUnderstand why certain training techniques work Debug optimization problems in large-scale training Develop new optimization algorithms for AI systems Analyze convergence properties of training procedures The mathematical rigor in this text is what separates AI practitioners who can debug and innovate from those who only apply existing techniques.\n","permalink":"https://deepskandpal.github.io/bookshelf/convex-optimization-boyd/","tags":null,"title":"Convex Optimization"},{"categories":["Maths and Science"],"content":"The Bayesian Foundation of Modern AI The definitive guide to probabilistic approaches in machine learning. This book provides the mathematical rigor and Bayesian thinking that underlies much of modern AI safety, alignment, and uncertainty quantification in GenAI systems.\nWhy This Book is Critical for GenAI Bishop\u0026rsquo;s text covers fundamental probabilistic concepts that are essential for understanding advanced GenAI topics:\nBayesian Inference: Foundation for uncertainty quantification in AI systems Probabilistic Graphical Models: Understanding relationships in complex AI systems Variational Methods: Mathematical foundation for VAEs and variational training Monte Carlo Methods: Sampling techniques used in modern training algorithms Model Selection: Principled approaches to choosing between different architectures Connection to GenAI Systems Key concepts from this book appear throughout your GenAI knowledge tree:\nRLHF and Constitutional AI: Bayesian approaches to preference learning Variational Autoencoders: Direct application of variational inference Uncertainty in AI Safety: Probabilistic approaches to AI alignment Evaluation Metrics: Principled statistical evaluation of model performance Active Learning: Bayesian approaches to efficient data collection Mathematical Rigor for AI Practitioners This book bridges the gap between theoretical foundations and practical AI applications, providing the mathematical sophistication needed to understand why modern GenAI techniques work.\nEssential reading for anyone building production GenAI systems that need to handle uncertainty and safety considerations.\n","permalink":"https://deepskandpal.github.io/bookshelf/pattern-recognition-ml-bishop/","tags":null,"title":"Pattern Recognition and Machine Learning"},{"categories":["Machine Learning"],"content":"The Foundational Text for Deep Learning The definitive reference for deep learning theory and practice. Written by three of the field\u0026rsquo;s most prominent researchers, this book provides the mathematical foundations and conceptual framework that underlies all modern generative AI systems.\nWhy This Book is Essential for GenAI This text covers all the fundamental concepts referenced throughout your GenAI knowledge tree:\nMathematical Foundations: Linear algebra, probability theory, information theory Neural Network Architectures: From MLPs to the building blocks of transformers Optimization Theory: The mathematical principles behind training large models Regularization: Techniques that prevent overfitting in large-scale training Representation Learning: How models learn meaningful representations Connection to GenAI Concepts Every major concept in your GenAI materials traces back to principles established in this book:\nAttention mechanisms build on sequence modeling principles Transformer architectures extend the neural network foundations Training optimization techniques are rooted in the optimization theory presented Generative models (GANs, VAEs) are covered in dedicated sections This is the mathematical and conceptual foundation that makes understanding advanced GenAI possible.\n","permalink":"https://deepskandpal.github.io/bookshelf/deep-learning-goodfellow/","tags":null,"title":"Deep Learning"},{"categories":["LLM Platform","MLOps"],"content":"LangChef: End-to-End LLM Workflow Platform A comprehensive platform for prompt engineering, dataset management, and LLM experimentation that streamlines the entire lifecycle of LLM applications.\n🎯 What is LangChef? LangChef is a production-ready platform designed for teams who need to iterate fast and maintain quality in their AI workflows. It addresses the complete LLM development lifecycle from initial prompt engineering to production evaluation and monitoring.\n🚀 Key Features Prompt Management\nCreate, version, and organize prompts with full lifecycle tracking A/B test prompt variations with statistical significance Template management and reusable prompt components Performance tracking across different prompt versions Dataset Management\nMulti-format uploads (JSON, CSV, JSONL) with schema validation Dataset versioning and quality metrics Automated data preprocessing and validation Integration with popular ML data formats Experimentation Platform\nControlled experiments across prompts, models, and datasets Multi-model comparison (OpenAI, Anthropic, AWS Bedrock) Cost optimization and performance benchmarking Statistical analysis and experiment tracking Interactive Playground\nReal-time testing environment with live model switching Configuration export for production deployment Result visualization and comparison tools Collaborative testing and sharing capabilities 💡 Technical Architecture Full-Stack Implementation\nBackend: FastAPI + SQLAlchemy for high-performance API Frontend: React + Material-UI for modern, responsive interface Database: PostgreSQL for robust data persistence Containerization: Docker + Docker Compose for easy deployment LLM Provider Integration\nMulti-provider support (OpenAI, Anthropic, AWS Bedrock) Unified API abstraction layer Automatic failover and load balancing Cost tracking across different providers Observability \u0026amp; Monitoring\nComplete request tracing for production applications Performance monitoring (latency, token usage, costs) Custom metrics and alerting Production-ready logging and debugging tools 🔧 Skills Demonstrated Full-Stack Development: Modern Python backend with React frontend LLM Integration: Multi-provider AI service architecture MLOps: End-to-end machine learning workflow management System Architecture: Scalable, production-ready platform design DevOps: Docker containerization, CI/CD, and deployment automation 🎨 Key Innovations Unified Workflow Management\nSingle platform for the entire LLM development lifecycle Seamless integration between prompt engineering and experimentation Production deployment pipeline with monitoring Statistical Experimentation\nRigorous A/B testing framework for LLM applications Statistical significance testing and confidence intervals Cost-aware experimentation with budget controls Multi-Provider Abstraction\nVendor-agnostic LLM integration layer Automatic provider selection based on cost/performance Consistent API across different LLM providers 📊 Use Cases Perfect for:\nAI/ML teams building LLM-powered applications Prompt engineers optimizing model performance Product teams running A/B tests on AI features Organizations needing production LLM monitoring Research teams comparing different language models 🔗 Links GitHub: deepskandpal/LangChef License: MIT Open Source Tech Stack: FastAPI, React, PostgreSQL, Docker Requirements: Python 3.11+, Node.js 18+, PostgreSQL 13+ LangChef represents my vision for democratizing LLM development by providing enterprise-grade tools that make prompt engineering, experimentation, and deployment accessible to teams of all sizes.\n","permalink":"https://deepskandpal.github.io/creations/langchef/","tags":["Python","React","FastAPI","LLM","MLOps","Experimentation"],"title":"LangChef - End-to-End LLM Workflow Platform"},{"categories":["Non Fiction"],"content":"About the Book \u0026ldquo;Why Zebras Don\u0026rsquo;t Get Ulcers\u0026rdquo; by Stanford University biologist Robert M. Sapolsky is a groundbreaking exploration of stress and its profound impact on human health. This acclaimed work masterfully combines cutting-edge scientific research with accessible explanations and Sapolsky\u0026rsquo;s characteristic wit.\nKey Themes The Stress Response System Sapolsky explains how our stress response system, designed for acute physical threats, becomes problematic when activated chronically by modern psychological stressors.\nZebras vs. Humans The central metaphor: zebras experience intense but brief stress when fleeing predators, then return to a relaxed state. Humans, however, can activate the same stress response through worry, anticipation, and psychological pressure - and keep it activated for months or years.\nPhysiological Impact Detailed examination of how chronic stress affects:\nCardiovascular system - Heart disease and hypertension Immune system - Increased susceptibility to illness Digestive system - Ulcers and gastrointestinal problems Brain function - Memory, learning, and mental health Reproductive system - Fertility and sexual function Why This Book Matters Scientific Authority Sapolsky brings decades of research experience studying stress in both wild baboons and laboratory settings, providing unique insights into stress biology.\nPractical Applications Beyond explaining the science, the book offers practical strategies for managing stress and building resilience in modern life.\nAccessible Science Communication Complex neurobiological and endocrinological concepts are explained with humor and clarity, making advanced science accessible to general readers.\nKey Insights Chronic activation of acute stress responses leads to disease Psychological stressors can be as damaging as physical ones Social status and control significantly impact stress levels Individual differences in stress susceptibility and coping mechanisms Modern lifestyle factors that exacerbate chronic stress Target Audience Perfect for:\nAnyone interested in understanding the biological basis of stress Health professionals seeking deeper insights into stress-related illness Students of psychology, biology, or neuroscience Readers looking for science-based stress management strategies Personal Learning Goals To be updated as I read through the book:\nUnderstand the neurobiological mechanisms of stress Learn about the evolutionary origins of stress responses Explore the connection between social factors and stress Identify practical applications for stress management Examine the latest research on stress and aging Reading Notes This section will be updated with key insights and reflections as I progress through the book.\nPublication Details:\nPublisher: Holt Paperbacks First Published: 1994 Current Edition: Third Edition (2004) Pages: 560 ISBN: 978-0-8050-7369-0 This book represents essential reading for understanding one of the most significant health challenges of modern life - chronic stress and its wide-ranging effects on human biology and well-being.\n","permalink":"https://deepskandpal.github.io/bookshelf/why-zebras-dont-get-ulcers/","tags":null,"title":"Why Zebras Don't Get Ulcers: The Acclaimed Guide to Stress, Stress-Related Diseases, and Coping"},{"categories":["macOS Development","AI Integration"],"content":"OpenPage: iA Writer + AI for macOS A distraction-free, AI-enhanced writing environment that combines iA Writer\u0026rsquo;s legendary simplicity with intelligent writing assistance.\n🎯 What is OpenPage? OpenPage is a native macOS writing application that seamlessly integrates AI capabilities while maintaining the elegant simplicity that Mac users expect. It demonstrates advanced Swift development skills, multi-provider AI integration, and deep understanding of macOS design principles.\n🚀 Key Features Native macOS Integration\nSwiftUI + AppKit hybrid for optimal text editing performance macOS Keychain integration for secure API key storage Full sandboxed architecture with HIG compliance Privacy-first design with local processing when possible Multi-Provider AI Support\nAnthropic Claude for reasoning and writing assistance OpenAI GPT for creative writing and code generation Google Gemini for multimodal content understanding Smart provider selection with automatic failover Advanced Text Processing\nDual-mode editor: Rich Text ↔ Markdown switching Real-time markdown rendering with syntax highlighting Context-aware AI suggestions that don\u0026rsquo;t interrupt flow Wikilinks support for knowledge management 💡 Technical Highlights Architecture\nClean MVVM + Services architecture SwiftData for modern document persistence Combine for reactive state management Async/await for AI API calls Performance\n\u0026lt; 2 second cold launch time \u0026lt; 100MB memory usage for typical documents Intelligent caching of AI responses Background processing for all AI requests User Experience\niA Writer-style focus modes (sentence/paragraph/zen) Distraction-free interface that disappears during writing Comprehensive keyboard shortcuts for power users Typography optimized for long writing sessions 🔧 Skills Demonstrated macOS Development: Native SwiftUI/AppKit, security frameworks, HIG compliance AI Integration: Multi-provider architecture, privacy-preserving implementation Software Architecture: Clean architecture, reactive programming, modern Swift patterns UX Design: Distraction-free interfaces, intelligent assistance integration 🔗 Links GitHub: deepskandpal/OpenPage License: MIT Open Source Requirements: macOS 14.0+, Xcode 15.0+ OpenPage represents my vision for AI-enhanced writing tools that feel natural and unintrusive while maintaining premium macOS software quality.\n","permalink":"https://deepskandpal.github.io/creations/openpage/","tags":["Swift","SwiftUI","AI","Writing","macOS"],"title":"OpenPage - AI-Enhanced Writing App for macOS"},{"categories":null,"content":"DSA Study Log \u0026amp; Progress Tracker Overall Progress Completed: 3 | Learning: 0 | To Do: 57 %%{init: {'pie': {'textPosition': 0.75}, 'themeVariables': {'pieOuterStrokeWidth': '2px'}}}%% pie showData title Overall GenAI Study Progress \"⚪️ To Do\" : 57 \"✅ Completed\" : 3 This document serves as a centralized dashboard to track my learning progress through the DSA master list. The goal is to master these high-ROI problems and patterns.\nStatus Legend:\n⚪️ To Do: Not yet started. 🟡 Learning: Actively studying the topic. ✅ Completed: Study is complete and validated with a Proof of Study. Tier 1: The Bedrock (Non-Negotiable Fundamentals) Arrays \u0026amp; Hashing Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Arrays \u0026amp; Hashing Group Anagrams ⚪️ (NeetCode) Arrays \u0026amp; Hashing Two Sum ✅ (NeetCode) Two Sum Arrays \u0026amp; Hashing Sort Colors / The Dutch National Flag Problem ✅ (NeetCode / EPI 6.1) Dutch National Flag Arrays \u0026amp; Hashing Test Palindromicity ⚪️ (EPI 7.5) Arrays \u0026amp; Hashing Longest Consecutive Sequence ⚪️ (NeetCode) Linked Lists Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Linked Lists Reverse Linked List ⚪️ (NeetCode) Linked Lists Merge Two Sorted Lists ⚪️ (NeetCode / EPI 8.1) Linked Lists Linked List Cycle / Test for Cyclicity ⚪️ (NeetCode / EPI 8.3) Stacks Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Stacks Valid Parentheses ⚪️ (NeetCode) Stacks Evaluate RPN Expressions ⚪️ (NeetCode / EPI 9.2) Binary Trees (General) Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Binary Trees Maximum Depth of Binary Tree ⚪️ (NeetCode) Binary Trees Binary Tree Level Order Traversal ⚪️ (NeetCode) Binary Trees Invert Binary Tree ⚪️ (NeetCode) Graphs Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Graphs Number of Islands / Search a Maze ⚪️ (NeetCode / EPI 19.1) Graphs Clone a Graph ⚪️ (NeetCode / EPI 19.4) Tier 2: The Differentiators (Senior-Level Proficiency) Advanced Arrays \u0026amp; Hashing Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Advanced Arrays \u0026amp; Hashing Container With Most Water ⚪️ (NeetCode) Advanced Arrays \u0026amp; Hashing Product of Array Except Self ⚪️ (NeetCode) Advanced Arrays \u0026amp; Hashing Best Time to Buy and Sell Stock ✅ (NeetCode / EPI 6.6) Buying Selling Stock Once String Manipulation Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway String Manipulation Valid Anagram ⚪️ (NeetCode) String Manipulation String to Integer (atoi) ⚪️ (NeetCode) Binary Search Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Binary Search Binary Search ⚪️ (NeetCode) Binary Search Search in Rotated Sorted Array ⚪️ (NeetCode) Binary Search Find Minimum in Rotated Sorted Array ⚪️ (NeetCode / EPI) Sliding Window Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Sliding Window Longest Substring Without Repeating Characters ⚪️ (NeetCode) Sliding Window Minimum Window Substring / Find the Smallest Subarray Covering All Values ⚪️ (NeetCode / EPI 13.7) Matrix Problems Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Matrix Problems Spiral Matrix ⚪️ (NeetCode) Matrix Problems Set Matrix Zeroes ⚪️ (NeetCode) Intervals Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Intervals Merge Intervals ⚪️ (NeetCode / EPI) Intervals Insert Interval ⚪️ (NeetCode) Heaps (Priority Queues) Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Heaps Kth Largest Element in an Array ⚪️ (NeetCode) Heaps Find Median from Data Stream ⚪️ (NeetCode) Heaps Merge K Sorted Lists ⚪️ (NeetCode / EPI 11.1) Backtracking (Applied Recursion) Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Backtracking Subsets ⚪️ (NeetCode) Backtracking Combination Sum ⚪️ (NeetCode) Backtracking Permutations ⚪️ (NeetCode / EPI 16.3) Backtracking Word Search ⚪️ (NeetCode) Tries Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Tries Implement Trie (Prefix Tree) ⚪️ (NeetCode) Tries Design Add and Search Words Data Structure ⚪️ (NeetCode) Advanced Trees \u0026amp; BSTs Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Advanced Trees \u0026amp; BSTs Validate Binary Search Tree ⚪️ (NeetCode / EPI 15.1) Advanced Trees \u0026amp; BSTs Lowest Common Ancestor of a Binary Search Tree ⚪️ (NeetCode) Advanced Trees \u0026amp; BSTs Binary Tree Maximum Path Sum ⚪️ (NeetCode) Advanced Trees \u0026amp; BSTs Construct Binary Tree from Preorder and Inorder Traversal ⚪️ (NeetCode / EPI 10.12) Advanced Graphs Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Advanced Graphs Course Schedule ⚪️ (NeetCode) Advanced Graphs Rotting Oranges ⚪️ (NeetCode) Advanced Graphs Word Ladder / Transform One String to Another ⚪️ (NeetCode / EPI 19.7) Dynamic Programming (DP) Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Dynamic Programming Climbing Stairs ⚪️ (NeetCode) Dynamic Programming Coin Change ⚪️ (NeetCode) Dynamic Programming Longest Increasing Subsequence ⚪️ (NeetCode) Dynamic Programming Longest Palindromic Substring ⚪️ (NeetCode) Dynamic Programming Unique Paths ⚪️ (NeetCode) Dynamic Programming Edit Distance / Compute the Levenshtein Distance ⚪️ (NeetCode / EPI 17.2) Tier 3: Advanced Extensions (For Comprehensive FAANG Prep) These additional problems from NeetCode 150 and EPI provide deeper practice on key patterns, focusing on variations that frequently appear in big tech interviews.\nTwo Pointers Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Two Pointers 3Sum ⚪️ (NeetCode / EPI 18.2) Two Pointers Trapping Rain Water ⚪️ (NeetCode) Heaps \u0026amp; Priority Queues Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Heaps K Closest Points to Origin ⚪️ (NeetCode) Heaps Top K Frequent Elements ⚪️ (NeetCode) Trees \u0026amp; Serialization Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Trees Serialize and Deserialize Binary Tree ⚪️ (NeetCode / EPI 10.13) Graphs \u0026amp; Advanced Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Graphs Pacific Atlantic Water Flow ⚪️ (NeetCode) Graphs Network Delay Time (Dijkstra\u0026rsquo;s) ⚪️ (NeetCode) Math \u0026amp; Bit Manipulation Topic Problem Status References Proof of Study (Link) Notes / Key Takeaway Math \u0026amp; Bit Manipulation Number of 1 Bits ⚪️ (NeetCode) Math \u0026amp; Bit Manipulation Reverse Integer ⚪️ (NeetCode) Updated with comprehensive coverage including String Manipulation, Binary Search, Matrix Problems, Intervals, and Math/Bit Manipulation for complete FAANG prep.\n","permalink":"https://deepskandpal.github.io/dsa-log/dsa-study-log/","tags":["DSA"],"title":"DSA Study Log \u0026 Progress Tracker"},{"categories":["Fitness"],"content":" Day Meal Time Meal Description Notes / Katori Guide Day 1 (e.g., Monday) Early Morning (Pre-Workout) 1 Banana OR Small handful of almonds (5-6) Breakfast 3-4 Egg Omelette (with onions, tomatoes, capsicum, spinach) + 1 Whole Wheat Roti OR 1 slice Whole Wheat Toast Mid-Morning Snack 1 MK Sprouts Salad (moong, chana) with lemon juice \u0026amp; chaat masala MK: ~150ml Lunch 1 LK Chicken Curry (focus on chicken pieces) OR 1 LK Paneer Bhurji/Curry + 1 LK Mixed Vegetable Sabzi (e.g., Gobi, Carrot, Peas) + 1 MK Cooked Brown Rice OR 2 Whole Wheat Rotis + Generous Green Salad LK: ~240ml, MK: ~150ml Evening Snack 1 Apple OR Pear + Small handful of Roasted Chana (unsalted) Dinner 1 LK Baked/Grilled Fish OR 1 LK Tofu Stir-fry with plenty of vegetables + 1 MK Dal (e.g., Moong Dal) + 1 LK Lauki/Bhindi Sabzi + 1 Whole Wheat Roti (optional) LK: ~240ml, MK: ~150ml Day 2 (e.g., Tuesday) Early Morning (Pre-Workout) Small sweet potato (boiled) OR 1 small apple Breakfast 2 Moong Dal Chillas OR Besan Chillas (medium size) stuffed with 1 SK grated paneer/vegetables + 1 SK Mint-coriander chutney (no sugar) SK: ~110ml Mid-Morning Snack 1 Orange OR 1 MK Papaya MK: ~150ml Lunch 1 LK Rajma Curry OR Chana Masala + 1 LK Palak Sabzi (spinach preparation) + 1 MK Quinoa OR 2 Millets (Bajra/Jowar) Rotis + Generous Green Salad LK: ~240ml, MK: ~150ml Evening Snack 1-2 Boiled Eggs (if you eat eggs) OR Handful of peanuts (approx. 20-25) Dinner 1 LK Paneer Tikka (grilled/baked) OR 1 LK Soy Chunk Curry + 1 MK Vegetable Raita (plain yogurt) OR Large mixed bean salad + 1 LK Baingan Bharta OR Cabbage Sabzi LK: ~240ml, MK: ~150ml. Avoid sugary/mayo-based Raita. Day 3 (e.g., Wednesday) Breakfast 1 MK Vegetable Poha (with peas, carrots, onions) + 1 SK roasted peanuts sprinkled on top + 1 Glass Unsweetened Lassi OR Buttermilk (Chaas) MK: ~150ml, SK: ~110ml Mid-Morning Snack 1 Guava OR 1 Pomegranate Lunch 1 LK Mixed Dal (e.g., Toor + Masoor) + 1 LK Methi Aloo Sabzi (more methi, less aloo) + 1 MK Cooked Rice OR 2 Whole Wheat Rotis + Generous Green Salad LK: ~240ml, MK: ~150ml Evening Snack 1 MK Fruit Salad (mixed fruits) MK: ~150ml Dinner Vegetable Clear Soup (large bowl) + 1 LK Mushroom \u0026amp; Capsicum Sabzi OR Stir-fried Mixed Vegetables + 1-2 Besan or Moong Dal Chillas (plain or minimal veggie stuffing) LK: ~240ml Day 4 (e.g., Thursday) Early Morning (Pre-Workout) Black coffee + 4-5 Almonds \u0026amp; 1-2 Walnuts Breakfast 3-Egg Scramble with 1 SK chopped chicken/turkey ham (if non-veg) or tofu/paneer cubes + 1 Whole Wheat Roti OR 1 slice Whole Wheat Toast SK: ~110ml Mid-Morning Snack 1 MK Steamed Edamame (if available) OR 1 MK Sprouts MK: ~150ml Lunch 1 LK Fish Curry OR 1 LK Matar Paneer (Peas \u0026amp; Paneer) + 1 LK Thoran/Poriyal (South Indian dry veg) + 1 MK Brown Rice OR 2 Rotis + Generous Salad LK: ~240ml, MK: ~150ml Evening Snack 1 Apple with 1 tablespoon Peanut Butter (unsweetened) Dinner 1 LK Chicken Stir-fry with lots of colorful vegetables OR 1 LK Paneer \u0026amp; Bell Pepper Stir-fry + Small portion (1 SK) of Quinoa or Brown Rice (optional) + Large bowl Clear Broth LK: ~240ml, SK: ~110ml Day 5 (e.g., Friday) Early Morning (Pre-Workout) 1 small Banana + Black Coffee Breakfast 1 LK Paneer Bhurji + 2 slices Whole Wheat Toast OR 2 small Uthappams (with lots of onion/tomato topping) LK: ~240ml Mid-Morning Snack 1 Pear OR handful of Grapes Lunch 1 LK Mutton Curry (lean pieces, less oil) OR 1 LK Lobia (Black-eyed peas) Curry + 1 LK Bhindi (Okra) Sabzi + 1 MK Rice OR 2 Rotis + Generous Salad LK: ~240ml, MK: ~150ml Evening Snack Handful of Mixed Nuts \u0026amp; Seeds (almonds, walnuts, pumpkin seeds) Dinner Grilled Chicken Breast (1-2 pieces) OR Large portion (1 LK) of Grilled Paneer/Tofu steaks + 1 LK Steamed or Sautéed Mixed Vegetables + Salad with lemon-herb dressing LK: ~240ml Day 6 (e.g., Saturday) Breakfast Vegetable Idli (3-4) with 1 MK Sambhar (focus on dal \u0026amp; veggies) + Minimal coconut chutney MK: ~150ml Mid-Morning Snack 1 MK Watermelon or Muskmelon MK: ~150ml Lunch Mindful Meal Out/Relaxed Meal: E.g., Tandoori Chicken/Fish (2-3 pieces) with Salad OR Veggie \u0026amp; Paneer Kathi Roll (whole wheat, less sauce) Avoid creamy curries, large biryanis, naan. Evening Snack 1 MK Roasted Makhana (Fox Nuts) MK: ~150ml Dinner 1 LK Dal Tadka (Yellow Dal) + 1 LK Gajar Matar Sabzi (Carrot \u0026amp; Peas) + 2 Whole Wheat Rotis + Side Salad LK: ~240ml Day 7 (e.g., Sunday) Breakfast Paneer Paratha (1-2, less oil, more filling) OR Aloo Paratha (1-2) with 1 SK plain low-fat yogurt (optional) or pickle. SK: ~110ml. If Aloo Paratha, add protein like leftover dal/egg. Mid-Morning Snack Your choice of 1 Fruit Lunch Home-style Chicken Biryani (1 LK, focus on chicken, less rice) with 1 MK Raita (if using yogurt) OR Large Salad. Veg: Veg Biryani/Pulao (1 LK) with 1 MK Dal or Mixed Veg Raita. LK: ~240ml, MK: ~150ml Evening Snack Sweet Potato Chaat (1 MK, boiled sweet potato, chaat masala, lemon) MK: ~150ml Dinner Keep it light: Large Bowl of Mixed Vegetable \u0026amp; Lentil Soup + 1-2 slices Whole Wheat Toast with 1 SK hummus (optional) SK: ~110ml General Reminders for the Diet Plan:\nHydration: Drink 3-4 liters of water daily. Protein Focus: Ensure every main meal has a substantial protein source. Cooking Oil: Use minimal oil (1-2 tsp per person per meal). Your Yogurt Dip: If used, keep portions very small (1-2 tbsp max) as a condiment, not a replacement for plain yogurt servings. Listen to Your Body: Adjust vegetable portions up if hungry. This is a template; adapt with similar food types. ","permalink":"https://deepskandpal.github.io/fitness-log/diet-plan/","tags":["Plan"],"title":"Current Diet Plan"},{"categories":["Fitness"],"content":"Important Notes Before You Start:\nWarm-up: Always perform a 5-10 minute warm-up before each workout (light cardio, dynamic stretches). Cool-down: Always perform a 5-10 minute cool-down after each workout (static stretching). Weight Column: This column is for you to fill in each week. Start with a weight that challenges you to complete the target reps with good form. If you can easily complete the highest number of reps listed (e.g., 12 reps) for all sets, increase the weight slightly the following week. Form is always more important than the amount of weight. Progression (Weeks 1-8): Focus on Progressive Overload. Gradually increase weight, reps, sets, or intensity, or decrease rest time. Listen to Your Body: Stop if you feel pain. Take extra rest days if needed. Workout Plan: Weeks 1-8\nMonday: Upper Body Strength\nExercise Sets Reps Rest (between sets) Notes Bench Press / Push-ups 3 8-12 60-90s Choose variation appropriate for strength Bent-Over Rows (DB or BB) 3 8-12 60-90s Keep back straight, pull towards hips Overhead Press (DB or BB) 3 8-12 60-90s Keep core tight, avoid excessive back arch Lat Pulldowns / Pull-ups 3 8-12 60-90s Use assisted machine or bands if needed Dumbbell Bicep Curls 3 10-12 60s Avoid swinging Triceps Pushdowns / Overhead Ext 3 10-12 60s Choose one based on equipment Progression Note: Increase weight when you can comfortably complete 12 reps for all 3 sets with good form. Tuesday: Active Recovery\nActivity Duration Intensity Notes Walk (e.g., 6k) or Light Cycling/Yoga 45-60 mins+ Low Focus on movement and recovery Wednesday: Lower Body \u0026amp; Core Strength\nExercise Sets Reps / Time Rest (between sets) Notes Squats (Barbell or Goblet) 3 8-12 60-90s Focus on depth and form Romanian Deadlifts / Good Mornings 3 8-12 60-90s Feel the stretch in hamstrings, keep back straight Lunges (Walking or Static) 3 10-12 / leg 60s Bodyweight or hold dumbbells Leg Press / Leg Extensions 3 10-12 60s Choose one based on equipment/preference Hamstring Curls 3 10-12 60s Control the negative (lowering phase) Calf Raises 3 15-20 45-60s Bodyweight or hold dumbbells, focus on full stretch Plank 3 30-60s hold 45s Keep body in a straight line Leg Raises 3 15-20 45s Keep lower back pressed down Russian Twists 3 15-20 / side 45s Bodyweight or hold light weight Progression Note: Increase weight/duration when you can comfortably complete the upper rep/time range for all 3 sets. Thursday: Full Body Functional / Boxing HIIT\nComponent Details Sets / Rounds Work Rest (in circuit) Rest (between circuits) Notes Warm-up Light Cardio + Dynamic Stretches + 5 min Shadow Boxing - - - - Prepare body for intense work Circuit 1 1. Kettlebell Swings 2. Box Jumps / Squat Jumps 3. Burpees 4. Mountain Climbers 3-4 45s 15s 60-90s Focus on explosive power and maintaining pace. Choose jump variation suitable for you. Circuit 2 1. Alt. Reverse Lunge + Knee Drive 2. Med Ball Slams / DB Ground-to-Overhead 3. Shadow Boxing (Intensity) 4. High Knees 3-4 45s 15s 60-90s Focus on intensity and full body engagement. Finisher (Opt.) Shadow Boxing / Heavy Bag Work 3-5 60s 30s - If time and energy permit Core (Opt.) E.g., Bicycle Crunches, V-Ups 2-3 15-20 reps 30s - Add if time permits Progression Note: Weeks 5-8: Increase intensity (more reps per 45s), increase work duration (e.g., 50s work / 10s rest), or add 1 more round to circuits. Friday: Full Body Strength (Compound Focus)\nExercise Sets Reps Rest (between sets) Notes Deadlifts (Conventional/Sumo) 3-4 5-8 90-120s PRIORITIZE FORM. Start lighter, focus on technique. Squats / Leg Press 3-4 5-8 90-120s Focus on heavy but controlled lifts Bench Press / Incline DB Press 3-4 5-8 90-120s Choose one based on preference Overhead Press / Push Press 3-4 5-8 90-120s Push press allows leg drive for slightly heavier weight Bent-Over Rows / T-Bar Rows 3-4 5-8 90-120s Focus on pulling with back muscles Pull-ups / Weighted Pull-ups / Lat Pulldowns 3-4 5-8 90-120s Choose variation allowing 5-8 reps Progression Note: Increase weight when you can comfortably complete 8 reps for all sets with good form. Consider adding the 4th set from Week 5 onwards. Saturday: Active Recovery\nActivity Duration Intensity Notes Walk (e.g., 6k) or Light Cycling/Yoga 45-60 mins+ Low Focus on movement and recovery Sunday: Rest\nActivity Duration Intensity Notes Rest or Very Light Stroll - Very Low Allow body to fully recover ","permalink":"https://deepskandpal.github.io/fitness-log/workout-plan/","tags":["Plan"],"title":"Current Workout Plan"},{"categories":null,"content":"Problem Description rough nb\nThe count-and-say sequence is a sequence of digit strings defined by the recursive formula:\ncountAndSay(1) = \u0026quot;1\u0026quot; countAndSay(n) is the way you would \u0026ldquo;say\u0026rdquo; the digit string from countAndSay(n-1), which is then converted into a new digit string. To determine how you \u0026ldquo;say\u0026rdquo; a digit string, split it into the minimal number of substrings such that each substring contains exactly one unique digit. Then for each substring, say the number of digits, then say the digit. Finally, concatenate every said digit.\nFor example, the saying of the digit string \u0026quot;3322251\u0026quot; is \u0026quot;23321511\u0026quot;.\nGiven a positive integer n, return the nth term of the count-and-say sequence.\nExample 1:\nInput: n = 4 Output: \u0026#34;1211\u0026#34; Explanation: countAndSay(1) = \u0026#34;1\u0026#34; countAndSay(2) = say \u0026#34;1\u0026#34; = one 1 = \u0026#34;11\u0026#34; countAndSay(3) = say \u0026#34;11\u0026#34; = two 1s = \u0026#34;21\u0026#34; countAndSay(4) = say \u0026#34;21\u0026#34; = one 2 + one 1 = \u0026#34;12\u0026#34; + \u0026#34;11\u0026#34; = \u0026#34;1211\u0026#34; Solution Approach This problem is best solved iteratively. We start with the base case, n=1, which is \u0026quot;1\u0026quot;. Then, we generate the next term in the sequence based on the previous one, repeating this n-1 times.\nTo generate the next term from a given string s:\nInitialize an empty string or string builder for the new term. Iterate through the string s using a pointer or index, let\u0026rsquo;s call it i. At each position i, count the number of consecutive occurrences of the character s[i]. Let\u0026rsquo;s say it appears count times. Append the count and the character s[i] to our new string. Advance the pointer i by count to move to the next new character. Repeat until the entire string s is processed. The newly constructed string is the next term in the sequence. Let\u0026rsquo;s trace for n=4:\nn=1: res = \u0026quot;1\u0026quot; n=2: Generate from \u0026quot;1\u0026quot;. We see one \u0026lsquo;1\u0026rsquo;. New string is \u0026quot;11\u0026quot;. res = \u0026quot;11\u0026quot;. n=3: Generate from \u0026quot;11\u0026quot;. We see two \u0026lsquo;1\u0026rsquo;s. New string is \u0026quot;21\u0026quot;. res = \u0026quot;21\u0026quot;. n=4: Generate from \u0026quot;21\u0026quot;. We see one \u0026lsquo;2\u0026rsquo;, then one \u0026lsquo;1\u0026rsquo;. New string is \u0026quot;12\u0026quot; + \u0026quot;11\u0026quot; = \u0026quot;1211\u0026quot;. res = \u0026quot;1211\u0026quot;. This iterative approach builds the solution step-by-step.\nCode # Please provide your solution code. # I will add it here. Complexity Analysis Let L be the average length of the strings generated.\nTime: We iterate from 1 to n. In each iteration, we scan the previously generated string to create the new one. This scan takes time proportional to the length of the string. The length of the strings can grow, but it\u0026rsquo;s not a simple exponential. The complexity is roughly O(n * L), where L is the length of the terms, which can be large. Space: We only need to store the previous term to generate the next one. So, the space complexity is O(L) to store the result string of the (n-1)th iteration. Notes \u0026amp; Learnings This problem tests careful string manipulation and iteration. The description can be a bit tricky to understand at first, so working through an example is key. Using a string builder or a list of characters to build the new string is more efficient in many languages (like Python or Java) than repeated string concatenation.\n","permalink":"https://deepskandpal.github.io/dsa-log/count-and-say/","tags":["Iteration","Python"],"title":"Count And Say"},{"categories":null,"content":"Problem Description rough nb\nGiven a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent. Return the answer in any order.\nA mapping of digits to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters.\nExample 1:\nInput: digits = \u0026#34;23\u0026#34; Output: [\u0026#34;ad\u0026#34;,\u0026#34;ae\u0026#34;,\u0026#34;af\u0026#34;,\u0026#34;bd\u0026#34;,\u0026#34;be\u0026#34;,\u0026#34;bf\u0026#34;,\u0026#34;cd\u0026#34;,\u0026#34;ce\u0026#34;,\u0026#34;cf\u0026#34;] Example 2:\nInput: digits = \u0026#34;\u0026#34; Output: [] Solution Approach This problem is a classic example of a backtracking or recursion problem. We need to explore all possible combinations of letters for the given digits.\nThe main idea is to build the combinations character by character. We can define a recursive function that takes the current combination being built and the remaining digits to process.\nMapping: We\u0026rsquo;ll use a hash map (or an array) to store the mapping from each digit (\u0026lsquo;2\u0026rsquo; through \u0026lsquo;9\u0026rsquo;) to its corresponding letters. Base Case: The recursion stops when there are no more digits to process. At this point, the current combination we have built is a valid one, so we add it to our result list. Recursive Step: For the current digit we are considering, we get its corresponding letters (e.g., \u0026lsquo;2\u0026rsquo; -\u0026gt; \u0026ldquo;abc\u0026rdquo;). We then iterate through each of these letters. For each letter, we append it to our current combination and make a recursive call with the updated combination and the rest of the digits. Backtracking: After the recursive call returns, we need to \u0026ldquo;undo\u0026rdquo; our choice. This means removing the letter we just added from the current combination, so we can explore other possibilities for the current digit\u0026rsquo;s letters. This \u0026ldquo;undoing\u0026rdquo; step is the essence of backtracking. For an input like \u0026ldquo;23\u0026rdquo;:\nStart with an empty combination \u0026quot;\u0026quot; and digits \u0026quot;23\u0026quot;. Process \u0026lsquo;2\u0026rsquo;: Try \u0026lsquo;a\u0026rsquo;: Recurse with combination \u0026quot;a\u0026quot; and digits \u0026quot;3\u0026quot;. Process \u0026lsquo;3\u0026rsquo;: Try \u0026rsquo;d\u0026rsquo;: Recurse with \u0026quot;ad\u0026quot; and \u0026quot;\u0026quot;. Base case hit. Add \u0026ldquo;ad\u0026rdquo; to results. Backtrack. Try \u0026rsquo;e\u0026rsquo;: Recurse with \u0026quot;ae\u0026quot; and \u0026quot;\u0026quot;. Base case hit. Add \u0026ldquo;ae\u0026rdquo; to results. Backtrack. Try \u0026lsquo;f\u0026rsquo;: Recurse with \u0026quot;af\u0026quot; and \u0026quot;\u0026quot;. Base case hit. Add \u0026ldquo;af\u0026rdquo; to results. Backtrack. Backtrack (remove \u0026lsquo;a\u0026rsquo;). Try \u0026lsquo;b\u0026rsquo;: Recurse with \u0026quot;b\u0026quot; and \u0026quot;3\u0026quot;. \u0026hellip;and so on. This systematic exploration ensures we generate all possible combinations.\nComplexity Analysis Time: Let n be the number of digits in the input string. In the worst case, a digit maps to 4 letters (like \u0026lsquo;7\u0026rsquo; or \u0026lsquo;9\u0026rsquo;). The recursion tree will have a depth of n. At each level, the number of branches can be up to 4. So, the total number of combinations is roughly 4^n. For each combination, we do n appends to form the string. So the time complexity is O(4^n * n). Space: The space complexity is determined by the recursion depth and the storage for the output. The recursion depth will be n, so the call stack uses O(n) space. The output list will store O(4^n) combinations of length n, but this is usually excluded from space complexity analysis unless specified. So, the space complexity is O(n) for the recursion stack. Notes \u0026amp; Learnings This problem is a great introduction to backtracking. The key pattern is to make a choice, recurse, and then undo the choice (backtrack). This allows for a systematic exploration of all possible solutions. The use of an index to track progress through the input string is a common technique in these types of recursive problems.\n","permalink":"https://deepskandpal.github.io/dsa-log/letter-combinations-of-a-phone-number/","tags":["Backtracking","Python"],"title":"Letter Combinations Of A Phone Number"},{"categories":null,"content":"Problem Description The string PAYPALISHIRING is written in a zigzag pattern on a given number of rows like this:\nP A H N A P L S I I G Y I R And then read line by line: \u0026quot;PAHNAPLSIIGYIR\u0026quot;.\nWrite the code that will take a string and make this conversion given a number of rows.\nSolution Approach The problem asks us to reconstruct a string that is formed by reading a zigzag pattern row by row. We can solve this by simulating the placement of each character into its corresponding row.\nThe core idea is to maintain a list of strings, where each string represents a row in the zigzag pattern. We iterate through the input string, character by character, and append each character to the correct row.\nTo determine the correct row for each character, we can track the current row number and the direction of movement (down or up).\nInitialize an array of empty strings, with the size equal to numRows. Initialize a currentRow index to 0 and a step variable to 1 (indicating we are moving down). If numRows is 1, the zigzag pattern is just a single line, so the direction never changes. This is an important edge case to handle, where we can simply return the original string. Iterate through each character of the input string: a. Append the character to the string at rows[currentRow]. b. If we reach the top (currentRow == 0) or bottom (currentRow == numRows - 1) row, we must reverse the direction. This can be done by changing the sign of our step variable. c. Update currentRow by adding the step to it. After iterating through all characters, the rows array will contain the strings for each row of the pattern. Concatenate these strings to get the final result. This approach directly builds the rows of the zigzag pattern, making it straightforward to get the final output string.\nCode # The user has opted to not include a code block. # The solution involves iterating through the string once, # placing each character in the correct row based on the zigzag pattern. Complexity Analysis Time: O(n), where n is the length of the input string s. We iterate through the string once to place each character in its respective row. Concatenating the final rows also takes O(n) time. Space: O(n). We use additional space to store the characters in the numRows rows. In the worst case, the total space required for these rows is proportional to the length of the input string. Notes \u0026amp; Learnings This problem is a great example of simulation. The key is to find a simple rule to describe the zigzag movement. Using a step or direction variable simplifies the logic for moving up and down the rows. Handling edge cases like numRows = 1 is crucial for a correct solution. ","permalink":"https://deepskandpal.github.io/dsa-log/zigzag-conversion/","tags":["Simulation","Python"],"title":"Zigzag Conversion"},{"categories":["DSA Concepts"],"content":"Recursion and Backtracking This document covers the fundamental concepts of recursion and backtracking in the context of Data Structures and Algorithms (DSA).\nRecursion Recursion is a method of solving a computational problem where the solution depends on solutions to smaller instances of the same problem.\nKey Concepts Base Case: The condition under which the recursion stops. Recursive Step: The part of the function that calls itself. Example: Factorial function factorial(n) { // Base case if (n === 0) { return 1; } // Recursive step return n * factorial(n - 1); } Backtracking Backtracking is a general algorithmic technique that considers searching every possible combination in order to solve a computational problem. It is a refined brute force approach that incrementally builds candidates to the solutions, and abandons a candidate (\u0026ldquo;backtracks\u0026rdquo;) as soon as it determines that the candidate cannot possibly be completed to a valid solution.\nKey Concepts State Space Tree: The set of all possible states of a problem. Pruning: The process of discarding parts of the search space that will not lead to a solution. Example: N-Queens Problem A classic example of backtracking is the N-Queens problem, where the goal is to place N chess queens on an N×N chessboard so that no two queens threaten each other.\n","permalink":"https://deepskandpal.github.io/dsa-concepts/recursion-and-backtracking/","tags":["Recursion","Backtracking","DSA"],"title":"Recursion and Backtracking"},{"categories":["Interview-prep"],"content":"For this 45-minute scenario-based round, the key is not just having a story, but having the right story that is impactful, concise, and perfectly aligned with the value they\u0026rsquo;re probing.\nHere is your strategic playbook.\nGeneral Suggestions for This Round It\u0026rsquo;s a Conversation, Not an Interrogation: Your tone should be collaborative and reflective. You\u0026rsquo;re a senior professional discussing your experiences. Let your passion for solving hard problems come through. The \u0026ldquo;Why This Story\u0026rdquo; Matters: For each story, have a clear, one-sentence takeaway in your mind that connects it to the value. E.g., \u0026ldquo;This story shows my grit because I pursued a bug that everyone else had given up on for months.\u0026rdquo; Be Concise but Detailed in the \u0026lsquo;Action\u0026rsquo;: The Situation and Result should be brief. The Action is where you spend 70% of your time, detailing your specific thought process and steps. Prepare a Backup: Have a second story ready for each value. A common follow-up is, \u0026ldquo;That\u0026rsquo;s a great example. Can you tell me about another time when\u0026hellip;?\u0026rdquo; Listen to the Question: Don\u0026rsquo;t just jump to your prepared story. Listen to the nuance of their question and slightly tweak your narrative\u0026rsquo;s emphasis to match it perfectly. Your Best Stories Mapped to Loopio\u0026rsquo;s Values 1. Value: CURIOSITY (\u0026ldquo;The pursuit of the why\u0026rdquo;) Potential Trigger Questions:\n\u0026ldquo;Tell me about a time you had to solve a problem where the root cause was not obvious.\u0026rdquo; \u0026ldquo;Walk me through a situation where you felt an existing approach was flawed. What did you do?\u0026rdquo; \u0026ldquo;Describe a time your initial hypothesis about a problem was wrong.\u0026rdquo; Your Absolute Best Story: The Jio \u0026ldquo;Kirana Store\u0026rdquo; (Hinglish) Problem\nThis is your best \u0026ldquo;Curiosity\u0026rdquo; story because it shows you questioning a fundamental technical assumption (the utility of generic embeddings) based on a deep understanding of user behavior.\nS (Situation): \u0026ldquo;While building the \u0026lsquo;HelloJio\u0026rsquo; chatbot, our team had successfully implemented a classifier using pre-trained FastText embeddings. However, I observed a significant performance gap: the model failed on \u0026lsquo;Hinglish\u0026rsquo; queries—common in India—like \u0026lsquo;kirana store issue,\u0026rsquo; because \u0026lsquo;kirana\u0026rsquo; wasn\u0026rsquo;t in the generic English vocabulary.\u0026rdquo; T (Task): \u0026ldquo;My goal was not just to fix these specific errors, but to fundamentally understand why our NLP approach was failing for a huge segment of our users and to design a more robust, long-term solution.\u0026rdquo; A (Action): \u0026ldquo;My curiosity drove me to dig deeper than just patching the vocabulary. First, I quantified the problem by analyzing low-confidence predictions, confirming that out-of-vocabulary Hinglish terms were the primary culprit. This led me to question the core assumption: \u0026lsquo;Why are we using a generic, one-size-fits-all embedding model for our very specific, multi-language domain?\u0026rsquo; Driven by this question, I researched transfer learning and fine-tuning. I designed and implemented an experiment where we layered a trainable embedding layer on top of our LSTM. We initialized it with FastText but allowed it to fine-tune on our own labeled data. This created domain-specific embeddings where the model learned, from context, that \u0026lsquo;kirana\u0026rsquo; was semantically very close to \u0026lsquo;grocery\u0026rsquo;, solving the root problem.\u0026rdquo; R (Result): \u0026ldquo;The impact was a dramatic improvement in accuracy for our multilingual user base. But more importantly, it established a new pattern for our team: we learned that the highest-impact solutions come from being curious about our unique user context and tailoring our ML approach to it, rather than just applying standard models.\u0026rdquo; 2. Value: CANDOR (\u0026ldquo;Challenging ideas respectfully\u0026rdquo;) Potential Trigger Question:\n\u0026ldquo;Tell me about a time you disagreed with a technical solution proposed by your team.\u0026rdquo; \u0026ldquo;Walk me through a situation where you had to advocate for a more complex but better approach.\u0026rdquo; Your Story: Moving Beyond Top-K Sampling for Training Recommendations\nS (Situation): \u0026ldquo;At KnowBe4, we needed to add diversity to our training module recommendations. The initial solution proposed by the engineering team was to take the top 10 highest-scoring trainings for a given topic and then randomly sample one from that list. While simple, I immediately saw some fundamental limitations with this approach.\u0026rdquo;\nT (Task): \u0026ldquo;My goal was to respectfully challenge this \u0026rsquo;top-K\u0026rsquo; design. I believed that while it solved the immediate problem, it was a crude heuristic. I felt it was my responsibility to advocate for a more statistically robust and flexible system that would provide better long-term results and handle edge cases more gracefully.\u0026rdquo;\nA (Action): \u0026ldquo;I approached this with candor, backed by both logic and proactive work.\nFirst, in the design review meeting, I acknowledged the simplicity and merit of the proposed solution. I started by saying, \u0026lsquo;I agree this is a great, simple way to solve our immediate diversity problem.\u0026rsquo; Then, I respectfully raised several critical concerns, framing them as questions: \u0026lsquo;What happens if a topic has fewer than 10 trainings? How do we justify the hard cutoff at 10, ensuring the 11th best training never gets a chance? And should the #1 training really have the same random chance of being picked as the #10?\u0026rsquo; I then proposed an alternative solution I had been thinking about, which I called \u0026lsquo;Normal Distribution Sampling.\u0026rsquo; I explained the concept: we treat each training\u0026rsquo;s score as the mean of a Gaussian distribution and then sample from each distribution. This ensures higher-ranked trainings are more likely to be chosen, but lower-ranked ones still have a chance, with a smooth probability fall-off. The team was initially hesitant, concerned about the mathematical complexity. To address this head-on, I didn\u0026rsquo;t just argue; I built a simulation. I spent a day creating a Python script that ran hundreds of selection runs on real data from our system, visually demonstrating how my approach adapted gracefully to different scenarios, whereas the top-K method was rigid and had clear failure points.\u0026rdquo; R (Result): \u0026ldquo;The simulation was the key. Seeing the data made the benefits clear, and the team unanimously agreed to adopt my more sophisticated approach. We implemented the Normal Distribution Sampling method, which is now in production. It led to a more nuanced, effective, and fair recommendation system that doesn\u0026rsquo;t rely on arbitrary \u0026lsquo;magic numbers.\u0026rsquo; This experience solidified my belief in challenging initial designs with data-driven alternatives to build better products.\u0026rdquo;\nWhy This Story is Exceptional for \u0026ldquo;Candor\u0026rdquo;:\nRespectful Challenge: You start by acknowledging the merit of the other idea. Clear Logic: You articulate the flaws in the original plan with clear, logical questions. Proactive Solution: You don\u0026rsquo;t just poke holes; you come with a well-thought-out alternative. Data-Driven Persuasion: This is the most important part. You didn\u0026rsquo;t win by being the loudest voice; you won by proving your point with a simulation. This is the gold standard for technical candor. Better Outcome: The result was a technically superior system. 3. Value: GRIT (\u0026ldquo;Focus and perseverance\u0026rdquo;) Potential Trigger Questions:\n\u0026ldquo;Tell me about the most difficult technical problem you\u0026rsquo;ve ever solved.\u0026rdquo; \u0026ldquo;Describe a time you were completely stuck on a problem. What did you do?\u0026rdquo; \u0026ldquo;Walk me through a project that took a long time and required sustained effort to complete.\u0026rdquo; Your Absolute Best Story: The Farfetch \u0026ldquo;Stuck Experiments\u0026rdquo; Mystery\nThis is the quintessential grit story. It\u0026rsquo;s a technical detective narrative where you are the hero who solves the \u0026ldquo;unsolvable\u0026rdquo; problem through sheer perseverance.\nS (Situation): \u0026ldquo;The core A/B testing service at Farfetch, responsible for processing terabytes of data daily, was plagued by a mysterious bug where it would randomly \u0026lsquo;get stuck\u0026rsquo; indefinitely. This was a massive problem that had existed for months, and the team\u0026rsquo;s only solution was a costly workaround—running multiple instances and manually restarting them.\u0026rdquo; T (Task): \u0026ldquo;My mission was to find the true root cause and fix it permanently. It was understood to be a highly intermittent and difficult-to-reproduce issue that others had tried and failed to solve.\u0026rdquo; A (Action): \u0026ldquo;This was a long and frustrating investigation that tested my grit. For several weeks, I was in a cycle of adding more detailed logging, analyzing gigabytes of logs, and forming hypotheses that would turn out to be dead ends. The problem wouldn\u0026rsquo;t manifest for days at a time, making it impossible to debug conventionally. I refused to give up. I stuck with it, and my persistence eventually led me to a faint correlation between the \u0026lsquo;stuck\u0026rsquo; state and queries on our largest tables. This was the first real clue. This led me down a rabbit hole into the source code of our BigQuery client library. After hours of code tracing, I finally discovered the \u0026lsquo;smoking gun\u0026rsquo;: a flawed timeout logic. The code was supposed to time out after an hour but was instead waiting forever. I confirmed this with the platform team and found there was no server-side timeout either. It was this \u0026lsquo;perfect storm\u0026rsquo; of client and server misconfiguration that was causing the issue.\u0026rdquo; R (Result): \u0026ldquo;Finding and fixing that one line of flawed logic permanently solved a problem that had cost the company hundreds of engineering hours and had undermined trust in our platform. It taught me that the most critical problems sometimes require you to just keep digging, long after others have moved on.\u0026rdquo; 4. Value: MASTERY (\u0026ldquo;Striving to be better\u0026rdquo;) Potential Trigger Questions:\n\u0026ldquo;How do you stay up-to-date with new technologies?\u0026rdquo; \u0026ldquo;Tell me about something you\u0026rsquo;ve taught yourself recently.\u0026rdquo; \u0026ldquo;Describe a project you took on that was outside of your formal job description.\u0026rdquo; Your Absolute Best Story: Creating LangChef at KnowBe4\nThis is a perfect \u0026ldquo;Mastery\u0026rdquo; story because it was proactive, self-driven, and aimed at elevating the craft of the entire team. It\u0026rsquo;s the definition of owning your career and striving for excellence.\nS (Situation): \u0026ldquo;When my team at KnowBe4 began heavily using Generative AI, I saw we were falling into an inefficient pattern. Each engineer was building their own set of scripts for prompting, retrieval, and evaluation. Our experimentation was slow, hard to compare, and we were all reinventing the wheel.\u0026rdquo; T (Task): \u0026ldquo;I felt a personal responsibility to improve our craft. I tasked myself with creating a standardized, internal platform that would allow us to build and experiment with LLM workflows in a much more structured and efficient way.\u0026rdquo; A (Action): \u0026ldquo;This became a passion project that I drove from conception to adoption. I went beyond my assigned tickets and dedicated my own time to research the emerging LLMOps landscape. I decided to build a tool, \u0026lsquo;LangChef,\u0026rsquo; to solve our specific problems. I made deliberate architectural choices: using LangChain for its modularity, integrating Vector DBs for retrieval, and building a simple Streamlit UI so it was accessible to everyone, not just hard-core engineers. I built a V1 and didn\u0026rsquo;t just keep it for myself; I actively evangelized it, ran demos for my team, and incorporated their feedback to make it better. I then went a step further and open-sourced it.\u0026rdquo; R (Result): \u0026ldquo;LangChef became the standard way our team conducted LLM experiments, significantly accelerating our development cycles. For me, this project embodied \u0026lsquo;Mastery\u0026rsquo; because it wasn\u0026rsquo;t about completing a task; it was about identifying a better way for us to work, learning the necessary technologies, and building a tool that elevated the capabilities of our entire team.\u0026rdquo; 5. Value: SUPPORT (\u0026ldquo;Working as one team\u0026rdquo;) Potential Trigger Questions:\n\u0026ldquo;Tell me about a time you helped a teammate who was struggling.\u0026rdquo; \u0026ldquo;Describe a project where you had to work closely with a non-technical team.\u0026rdquo; \u0026ldquo;Tell me about a time you made a mistake and how you handled it.\u0026rdquo; Your Absolute Best Story: Automating the Accenture Quarterly Report\nThis is a fantastic \u0026ldquo;Support\u0026rdquo; story because its motivation was rooted in empathy for your teammates\u0026rsquo; struggles, and the outcome directly improved their work-life balance and job satisfaction.\nS (Situation): \u0026ldquo;When I was at Accenture, my team was responsible for a critical quarterly market-sizing report. The process was entirely manual, requiring several analysts to work long hours under intense pressure for 2-3 weeks, mostly in Excel. As a new member, I saw the stress and burnout this was causing my teammates.\u0026rdquo; T (Task): \u0026ldquo;My primary goal was to support my team. I saw that my technical skills in Python and APIs could be used to solve a major pain point for the non-engineering analysts, freeing them up for higher-value work.\u0026rdquo; A (Action): \u0026ldquo;I approached this as a support mission. I volunteered to my manager to take on this automation project, framing it as a way to improve team capacity and morale. Crucially, I didn\u0026rsquo;t work in a silo. I worked side-by-side with the analysts. I held sessions where they walked me through their exact Excel steps, and I translated their business logic and domain knowledge into my Python script. I treated them as the experts and my key collaborators, asking for their validation at each step to ensure the automated output was correct and trustworthy. This built a strong bridge between the technical and analytical functions of our team.\u0026rdquo; R (Result): \u0026ldquo;The script I built reduced a 3-week, high-stress process into a 2-hour automated job. The best result wasn\u0026rsquo;t just the time saved; it was the visible relief and gratitude from my teammates. We eliminated a major source of burnout and enabled them to focus on generating insights instead of manual data entry. It showed me how using my specific skills to support my colleagues can be one of the most impactful things I can do.\u0026rdquo; Of course. This is an excellent strategic move. Having well-prepared backup stories is crucial. It gives you flexibility and prevents you from sounding rehearsed on a single narrative. If the interviewer\u0026rsquo;s question doesn\u0026rsquo;t quite fit your primary story, you can pivot to a backup that\u0026rsquo;s a better match.\nBased on the full career history you\u0026rsquo;ve provided, I will now craft your \u0026ldquo;back stories.\u0026rdquo; These are still A-grade examples, but they provide a different angle or showcase a different aspect of your experience for each of Loopio\u0026rsquo;s values.\nYour Backup Stories: STAR Framework 1. Value: CURIOSITY (\u0026ldquo;The pursuit of the why\u0026rdquo;) Your Primary Story: The Jio \u0026ldquo;Kirana Store\u0026rdquo; (Hinglish) Problem. Your BEST Backup Story: The Farfetch \u0026ldquo;Stuck Experiments\u0026rdquo; Mystery\nWhy it works as a backup: This story showcases a different kind of curiosity—technical forensics. It\u0026rsquo;s about not accepting a workaround and being relentlessly curious about a systems-level problem, not just an ML model problem.\nS (Situation): \u0026ldquo;At Farfetch, our critical A/B testing platform was plagued by an intermittent bug where the daily batch job would hang indefinitely. The team\u0026rsquo;s accepted workaround was to scale the service horizontally on Kubernetes, treating the symptom rather than the disease.\u0026rdquo; T (Task): \u0026ldquo;While implementing the scaling solution was the immediate priority, my curiosity wouldn\u0026rsquo;t let the root cause go. I tasked myself with understanding why a modern service would just freeze without logging an error, something that seemed to violate basic principles of system behavior.\u0026rdquo; A (Action): \u0026ldquo;My investigation was driven by a series of \u0026lsquo;why\u0026rsquo; questions. I started by asking, \u0026lsquo;Why is this so hard to reproduce?\u0026rsquo; which led me to analyze weeks of logs, looking for patterns. I noticed a faint correlation with jobs that queried massive, multi-terabyte tables. This made me ask, \u0026lsquo;Why would a large query cause a hang instead of a timeout?\u0026rsquo; This question led me to stop looking at my team\u0026rsquo;s code and start investigating the BigQuery client library\u0026rsquo;s source code itself—an area no one had looked at before. That\u0026rsquo;s where I discovered the flawed timeout logic.\u0026rdquo; R (Result): \u0026ldquo;Uncovering that flawed logic explained the mystery perfectly. My curiosity led to a permanent fix that made the system fundamentally more reliable. It reinforced my belief that to solve the hardest problems, you have to be willing to question everything, even the behavior of the libraries you depend on.\u0026rdquo; 2. Value: CANDOR (\u0026ldquo;Challenging ideas respectfully\u0026rdquo;) Your Primary Story: Overhauling the Farfetch Deployment Pipeline. Your BEST Backup Story: Proposing UCB for the KnowBe4 Phishing Agent\nWhy it works as a backup: This shows candor in a design phase, not just a process phase. It\u0026rsquo;s about advocating for a more complex but technically superior solution, which is a common scenario for senior engineers.\nS (Situation): \u0026ldquo;While designing the architecture for the new autonomous phishing agent at KnowBe4, we needed to decide how the agent would select the difficulty of a phish for each user. One of the initial, simpler proposals on the table was to use a randomized difficulty level for each test.\u0026rdquo; T (Task): \u0026ldquo;Although a random approach was faster to implement, I felt it didn\u0026rsquo;t align with our goal of creating a truly personalized and effective training system. My task was to make a case for a more sophisticated but more impactful approach.\u0026rdquo; A (Action): \u0026ldquo;I approached this with respectful, data-driven candor. In a design review session, I first acknowledged the benefits of the random approach—it was simple and would get us an MVP faster. However, I then presented my alternative: using a multi-armed bandit algorithm like UCB. I explained that this would allow the agent to \u0026rsquo;learn\u0026rsquo; each user\u0026rsquo;s individual susceptibility level and dynamically adjust the difficulty to keep them challenged. I didn\u0026rsquo;t just propose the idea; I came prepared with a small simulation showing how a UCB agent would converge on optimal difficulty levels over time, versus a random agent which would not. This made the long-term benefits clear.\u0026rdquo; R (Result): \u0026ldquo;The team was convinced by the argument and the simulation data. We decided to incorporate the UCB algorithm into our final design. This act of candor elevated our product strategy from a simple content generator to a truly adaptive, personalized learning system.\u0026rdquo; 3. Value: GRIT (\u0026ldquo;Focus and perseverance\u0026rdquo;) Your Primary Story: The Farfetch \u0026ldquo;Stuck Experiments\u0026rdquo; Mystery. Your BEST Backup Story: The Jio Real-time Green Screen Removal on iOS\nWhy it works as a backup: This is a perfect story of deep, focused, technical grit. It\u0026rsquo;s about overcoming a specific, highly constrained technical hurdle through sheer determination and learning.\nS (Situation): \u0026ldquo;On the \u0026lsquo;HelloJio\u0026rsquo; chatbot team, we had a critical feature blocked by a technical limitation. We needed to display video responses with custom backgrounds, but the server was sending us videos with a raw green screen. We had to find a way to remove it in real-time, on the device, during streaming.\u0026rdquo; T (Task): \u0026ldquo;My task was to engineer a solution on iOS. This was incredibly challenging because real-time video processing is very performance-intensive, and any lag or battery drain would ruin the user experience.\u0026rdquo; A (Action): \u0026ldquo;This required pure technical perseverance. My initial CPU-based approaches failed miserably—they were too slow. I realized the only path forward was to learn and use Metal, Apple\u0026rsquo;s low-level GPU programming framework, which I had never used before. For over a week, I was completely immersed. I read documentation, worked through tutorials, and persistently experimented with custom shaders. It was a frustrating process of trial and error, but I was determined to make it work. I eventually engineered a solution that intercepted the video buffer and ran a highly efficient Metal shader on each frame to make the green transparent.\u0026rdquo; R (Result): \u0026ldquo;In the end, the solution worked perfectly. We shipped a feature that was previously thought to be impossible due to the server constraint. This project taught me that with enough focus and grit, I can learn any new technology required to solve a hard problem and see it through to a successful implementation.\u0026rdquo; 4. Value: MASTERY (\u0026ldquo;Striving to be better\u0026rdquo;) Your Primary Story: Creating LangChef at KnowBe4. Your BEST Backup Story: Automating the Accenture Quarterly Report\nWhy it works as a backup: While LangChef shows mastery of your core ML domain, this story shows your mastery of software engineering fundamentals (automation, APIs, scripting) and your drive to apply them to improve business processes, even when it\u0026rsquo;s not \u0026ldquo;ML.\u0026rdquo; It shows your versatility.\nS (Situation): \u0026ldquo;When I joined Accenture, I was introduced to a quarterly reporting process that was a huge, manual effort, taking a team of analysts 2-3 weeks of painstaking work in Excel. I was being trained to perform these same manual tasks.\u0026rdquo; T (Task): \u0026ldquo;I felt that as a technologist, simply accepting this manual process was not an option. I saw an opportunity to apply my craft—software engineering—to solve a business problem. My personal goal was to master this workflow and then automate it away completely.\u0026rdquo; A (Action): \u0026ldquo;On my own initiative, I dedicated time to this. I discovered the data providers had APIs, so I taught myself how to use them. I meticulously reverse-engineered the complex business logic from the Excel sheets and re-implemented it in a robust Python script. I added validation and error handling to ensure the quality was even higher than the manual process. I essentially treated this internal process with the same engineering rigor as a client-facing product.\u0026rdquo; R (Result): \u0026ldquo;The final script reduced a 3-week process to just 2 hours. This demonstrated to my team and leadership the power of applying engineering mastery to business operations. It proved my belief that striving to be the best at what I do means constantly looking for opportunities to replace manual toil with elegant, automated solutions.\u0026rdquo; 5. Value: SUPPORT (\u0026ldquo;Working as one team\u0026rdquo;) Your Primary Story: Automating the Accenture Quarterly Report (framed for support). Your BEST Backup Story: Wearing Multiple Hats at Jio\nWhy it works as a backup: This shows you supporting the team by being incredibly flexible and filling whatever role is necessary for the team to succeed. It\u0026rsquo;s a great \u0026ldquo;startup mentality\u0026rdquo; story.\nS (Situation): \u0026ldquo;The \u0026lsquo;HelloJio\u0026rsquo; team was a brand new, high-stakes project with an ambitious mission and a very small team. To succeed, we couldn\u0026rsquo;t afford to have rigid roles or work in silos.\u0026rdquo; T (Task): \u0026ldquo;My primary responsibility was to do whatever was necessary to help the team meet its goals. This meant I couldn\u0026rsquo;t just think of myself as an \u0026lsquo;iOS Developer\u0026rsquo; or an \u0026lsquo;ML Engineer\u0026rsquo; but as a problem-solver for the team.\u0026rdquo; A (Action): \u0026ldquo;I actively looked for areas where I could support my teammates. When I saw the backend team was getting bogged down by manual deployments, I jumped in to design and build the automated model loading system. When our NLP models were failing on certain queries, I stepped up to lead the data science investigation. When we had a tough UI problem on the client, I put my iOS hat back on. I was constantly context-switching to wherever the team\u0026rsquo;s biggest fire was, supporting my colleagues so they could stay focused on their core tasks.\u0026rdquo; R (Result): \u0026ldquo;This flexible, \u0026lsquo;all hands on deck\u0026rsquo; approach was critical to our success. We were able to move much faster and overcome obstacles that would have stalled a more rigid team. It taught me that sometimes the best way to support your team is to be willing to step outside your job description and solve the problem that most needs solving.\u0026rdquo; Framework for Answering Any Scenario Question Scenario-based questions for a senior role are designed to test your judgment, strategic thinking, and ability to handle future ambiguity. They often don\u0026rsquo;t have a single \u0026ldquo;right\u0026rdquo; answer. The interviewer is assessing your thought process and how you navigate complex situations.\nHere are some common new scenarios you might face, and a strategic framework for how to deal with them, using your own experience as the foundation for your answers.\nNo matter the scenario, use this 4-step mental framework:\nAcknowledge \u0026amp; Clarify (The \u0026ldquo;Seek to Understand\u0026rdquo; Step):\nStart by acknowledging the complexity of the situation (\u0026ldquo;That\u0026rsquo;s a great question, it\u0026rsquo;s a challenging scenario\u0026hellip;\u0026rdquo;). Ask clarifying questions to remove ambiguity. This shows you don\u0026rsquo;t rush to conclusions. \u0026ldquo;Before I propose a solution, could I clarify a few things? What\u0026rsquo;s the main business goal here? What are our latency requirements? What kind of data are we working with?\u0026rdquo; State Your Principles \u0026amp; Framework (The \u0026ldquo;Strategy\u0026rdquo; Step):\nOutline the principles that will guide your approach. This shows you have a structured way of thinking. \u0026ldquo;My approach here would be guided by a few key principles: start with the simplest baseline, focus on a core metric, and iterate quickly.\u0026rdquo; Propose a Solution \u0026amp; Discuss Trade-offs (The \u0026ldquo;Action\u0026rdquo; Step):\nProvide a concrete plan of action. This is where you connect the scenario to your past experiences. Crucially, discuss the trade-offs of your approach. This is the most important part for a senior role. \u0026ldquo;The advantage of this approach is X, but it comes with the trade-off of Y.\u0026rdquo; Define Success \u0026amp; Next Steps (The \u0026ldquo;Results\u0026rdquo; Step):\nExplain how you would measure success and what your next steps would be. \u0026ldquo;Success would be measured by a 10% lift in metric A. If the initial results are promising, my next step would be to\u0026hellip;\u0026rdquo; Potential New Scenarios \u0026amp; How to Handle Them Here are some new scenarios you might be asked, with a guide on how to apply the framework and leverage your experience.\nScenario 1: The Vague \u0026ldquo;Fix It\u0026rdquo; Problem Question: \u0026ldquo;Imagine you join a new team. The team has an existing ML model in production for ranking content, but user engagement is flat. Your manager says, \u0026lsquo;Your job is to improve this.\u0026rsquo; What are the first things you would do in your first 30-60 days?\u0026rdquo;\nYour Strategy: This tests your diagnostic and systematic problem-solving skills. Avoid jumping to \u0026ldquo;I\u0026rsquo;d build a new Transformer model!\u0026rdquo;\nHow to Answer:\nAcknowledge \u0026amp; Clarify: \u0026ldquo;That\u0026rsquo;s a great and very realistic challenge. My first priority wouldn\u0026rsquo;t be to change anything, but to deeply understand the current system. I\u0026rsquo;d ask: What is the exact business metric we\u0026rsquo;re trying to move? What is the model architecture? How is it evaluated offline? What does the data pipeline look like? Who are the key stakeholders?\u0026rdquo; State Your Principles: \u0026ldquo;My approach would be a systematic audit based on three pillars: the Data, the Model, and the System.\u0026rdquo; Propose a Solution (connecting to your experience): \u0026ldquo;For the Data: I\u0026rsquo;d start by doing a deep dive into the features. At Farfetch, I discovered our biggest issues were not in the model but in the data pipelines and processing. I would check for training-serving skew, analyze feature distributions, and look for data quality issues.\u0026rdquo; \u0026ldquo;For the Model: I\u0026rsquo;d establish a simple, strong baseline. Is the current model performing better than a simple logistic regression or a gradient-boosted tree? I\u0026rsquo;d also deeply analyze the offline metrics. At Accenture, we used a specific prioritization score; I\u0026rsquo;d want to understand if our current ranking metric truly correlates with online user engagement. I would also look at error analysis: where is the model failing most?\u0026rdquo; \u0026ldquo;For the System: I\u0026rsquo;d investigate the infrastructure. My experience at Jio and Farfetch taught me that deployment bottlenecks and system instability can kill a good model. I\u0026rsquo;d look at the model\u0026rsquo;s inference latency, deployment safety, and monitoring. Is there position bias that isn\u0026rsquo;t being accounted for?\u0026rdquo; Define Success \u0026amp; Next Steps: \u0026ldquo;My goal in the first 30 days wouldn\u0026rsquo;t be to have a new model, but to have a prioritized list of hypotheses (e.g., \u0026lsquo;I believe our feature X has data drift,\u0026rsquo; or \u0026lsquo;Our loss function is misaligned with our business goal\u0026rsquo;). My success would be presenting this data-driven list to the team. My next steps would be to design targeted A/B tests to validate these hypotheses.\u0026rdquo; Scenario 2: The \u0026ldquo;Build from Scratch\u0026rdquo; Problem (GenAI flavor) Question: \u0026ldquo;We want to build a new feature that allows our enterprise customers to ask natural language questions about their own business data (e.g., \u0026lsquo;How did our sales in the APAC region compare to EMEA last quarter?\u0026rsquo;). This involves complex, private data. How would you approach designing the V1 of this system?\u0026rdquo;\nYour Strategy: This tests your understanding of modern RAG systems, data privacy, and product scoping.\nHow to Answer:\nAcknowledge \u0026amp; Clarify: \u0026ldquo;This is a fantastic but very challenging problem, especially given the privacy and accuracy requirements. My first questions would be: What format is the customer\u0026rsquo;s data in (databases, documents, etc.)? What are the latency expectations for an answer? What\u0026rsquo;s the definition of a \u0026lsquo;correct\u0026rsquo; answer?\u0026rdquo; State Your Principles: \u0026ldquo;For a V1, my guiding principles would be Safety, Accuracy, and Simplicity. We must be absolutely sure we don\u0026rsquo;t leak data or provide incorrect answers. We should start with the simplest architecture that works.\u0026rdquo; Propose a Solution (connecting to your experience): \u0026ldquo;I would strongly advocate for a Retrieval-Augmented Generation (RAG) approach, not fine-tuning a model on customer data, to ensure factual grounding and data privacy. My experience building the policy quiz generator at KnowBe4 is directly applicable here.\u0026rdquo; \u0026ldquo;The V1 architecture would be: Data Ingestion \u0026amp; Chunking: A secure pipeline to ingest and chunk the customer\u0026rsquo;s documents or structured data. Just like at KnowBe4, we\u0026rsquo;d need intelligent chunking strategies. Embedding \u0026amp; Indexing: We\u0026rsquo;d use a high-quality embedding model to create vectors and store them in a secure Vector DB instance, strictly siloed per customer. Retrieval: When a user asks a question, we first convert it to an embedding and retrieve the top-K most relevant data chunks from their specific index. Generation: We then feed these retrieved chunks as context to a powerful LLM (like Claude or a fine-tuned open-source model) with a carefully engineered prompt that instructs it to answer the question only using the provided context.\u0026rdquo; Discuss Trade-offs: \u0026ldquo;The trade-off here is that this system can only answer questions based on the data it has access to. It won\u0026rsquo;t be able to \u0026lsquo;reason\u0026rsquo; beyond that, which is a good safety feature for V1. It will also have higher latency than a simple database query, which we need to manage.\u0026rdquo; Define Success \u0026amp; Next Steps: \u0026ldquo;Success for V1 would be answering a predefined set of \u0026lsquo;golden questions\u0026rsquo; with 99%+ accuracy and having zero data leakage incidents. The next steps would be to expand the range of data connectors and work on a more advanced \u0026lsquo;query decomposition\u0026rsquo; engine to handle more complex, multi-part questions.\u0026rdquo; Scenario 3: The Ethical Dilemma Question: \u0026ldquo;You\u0026rsquo;ve built a new recommendation model that shows a 5% lift in user engagement in an offline test. However, when you analyze the results, you notice that it disproportionately recommends sensational or \u0026lsquo;clickbait\u0026rsquo; content. The product manager is excited about the 5% lift and wants to ship it. What do you do?\u0026rdquo;\nYour Strategy: This tests your integrity, communication skills, and ability to think about long-term vs. short-term metrics. This is a classic \u0026ldquo;senior-level judgment\u0026rdquo; question.\nHow to Answer:\nAcknowledge \u0026amp; Clarify: \u0026ldquo;This is a critical situation where short-term metrics conflict with long-term user trust. My immediate action would be to raise this concern, backed by data.\u0026rdquo; State Your Principles: \u0026ldquo;My guiding principle here is that long-term user trust is more valuable than a short-term engagement lift. A metric increase that comes at the cost of product quality is not a true win.\u0026rdquo; Propose a Solution (connecting to your experience): \u0026ldquo;I would schedule a meeting with the PM and other stakeholders. I wouldn\u0026rsquo;t just say \u0026rsquo;this is bad\u0026rsquo;; I would come with a clear analysis. I\u0026rsquo;d present a breakdown showing which content categories are being over-represented and data suggesting that while these may get clicks, they often have lower session duration or higher \u0026rsquo;thumbs down\u0026rsquo; rates, indicating low-quality engagement.\u0026rdquo; \u0026ldquo;Next, I would propose a concrete plan to fix it. This relates to the diversification problem I worked on for the personalized content feed. I would suggest modifying our model\u0026rsquo;s objective function. We could add a penalty term for \u0026lsquo;clickbait-ness\u0026rsquo; (which we could define via a separate classifier or heuristics) or add a post-ranking diversification step to ensure a healthy mix of content topics.\u0026rdquo; Discuss Trade-offs: \u0026ldquo;The trade-off is that this fix might lower the overall engagement lift from 5% to, say, 2-3%. However, I would argue that a 3% lift in healthy, high-quality engagement is far more valuable for the long-term health of our platform than a 5% lift in clicks we\u0026rsquo;ll regret later.\u0026rdquo; Define Success \u0026amp; Next Steps: \u0026ldquo;Success is shipping a feature that provides a sustainable and healthy lift in user engagement. My next step would be to partner with the PM to define a more holistic \u0026lsquo;quality engagement\u0026rsquo; metric that we can optimize for in the future, moving beyond simple clicks.\u0026rdquo; ","permalink":"https://deepskandpal.github.io/tech-writings/my-work-behavioural-loopio/","tags":null,"title":"For Behavioural rounds: Loopio"},{"categories":null,"content":"Problem Description (Optional Summary) Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\nSolution Approach Use a hash map (dictionary in Python) to store numbers encountered so far and their indices. For each number, check if target - current_number exists in the hash map. If it does, we found the pair. Otherwise, add the current number and its index to the map.\nCode def twoSum(nums: list[int], target: int) -\u0026gt; list[int]: numMap = {} # val -\u0026gt; index for i, n in enumerate(nums): diff = target - n if diff in numMap: return [numMap[diff], i] numMap[n] = i return [] # Should not happen based on problem constraints Complexity Analysis Time: O(n) - We iterate through the list once. Hash map lookups/insertions are O(1) on average. Space: O(n) - In the worst case, the hash map stores almost all elements. Notes \u0026amp; Learnings Classic use case for a hash map to optimize search from O(n) (in a nested loop approach) to O(1). Remember to handle edge cases if the problem constraints allowed for no solution.\n","permalink":"https://deepskandpal.github.io/dsa-log/two-sum/","tags":["Python"],"title":"Two Sum"},{"categories":null,"content":"Introduction to Arrays Okay, I\u0026rsquo;ve reviewed the OCR of Chapter 5 (Arrays) from EPI that you provided. This is an excellent chapter with many fundamental array manipulations.\nBased on common interview patterns, the importance of the underlying techniques, and how well they illustrate core array concepts, here are my suggestions for Keystone Problems from this chapter to focus on with the 6-step (or revised time-constrained) process:\nKeystone Problems for Chapter 5 (Arrays):\n5.1 THE DUTCH NATIONAL FLAG PROBLEM (Page 39-42):\nWhy it\u0026rsquo;s Keystone: This is a classic partitioning problem. The single-pass O(N) time, O(1) space solution (the one with smaller, equal, and larger pointers/sections) is a beautiful and frequently asked pattern. It tests your ability to manipulate subarrays in place using multiple pointers and maintain invariants. Core Concepts: In-place modification, multiple pointers, partitioning, loop invariants. Focus: Definitely the single-pass, three-pointer solution (bottom of page 41, top of page 42). The earlier O(N^2) or two-pass O(N) solutions are good for understanding the evolution, but the one-pass is the target. 5.2 INCREMENT AN ARBITRARY-PRECISION INTEGER (Page 43):\nWhy it\u0026rsquo;s Keystone: Deals with representing large numbers using arrays and performing arithmetic. The \u0026ldquo;schoolbook\u0026rdquo; addition algorithm (right-to-left with carry) is fundamental. Handles edge cases like becoming. Core Concepts: Array as a number representation, right-to-left processing, carry propagation, handling array resizing (or prepending). Focus: The logic of iterating from the end, handling the carry, and the special case of an all-9s input that requires an extra digit. 5.6 BUY AND SELL A STOCK ONCE (Page 46-47):\nWhy it\u0026rsquo;s Keystone: A very common interview problem. The O(N) solution requires a clever insight about tracking the minimum price seen so far and calculating potential profit at each step. It\u0026rsquo;s simple once you see it, but a good test of problem-solving. Core Concepts: Iterating and maintaining state (min price, max profit), identifying the right greedy-like choice at each step. Focus: The single-pass O(N) solution. 5.9 ENUMERATE ALL PRIMES TO N (Sieve of Eratosthenes) (Page 49-50):\nWhy it\u0026rsquo;s Keystone: The Sieve of Eratosthenes is a classic algorithm with applications beyond just finding primes. It demonstrates efficient marking/elimination in an array (used as a boolean map). Core Concepts: Using an array as a boolean lookup, marking multiples, understanding why you only need to sieve up to sqrt(n) for primality testing of n (though here you sieve multiples of primes found). The optimization to start sieving from p^2 is also good. Focus: The basic Sieve logic first, then the optimization of starting to mark from p*p. 5.11 COMPUTE THE NEXT PERMUTATION (Page 52-53):\nWhy it\u0026rsquo;s Keystone: This is a more complex but very insightful array manipulation problem. It requires careful observation of patterns to find the \u0026ldquo;next lexicographically greater\u0026rdquo; permutation. The steps (find inversion point, find swap element, reverse suffix) are a specific algorithm worth understanding. Core Concepts: Lexicographical order, finding patterns from the right end of the array, in-place swaps and reversals. Focus: Understanding the 4 steps outlined in the solution. This one might take more time for the \u0026ldquo;hand-simulation\u0026rdquo; and \u0026ldquo;pseudocode\u0026rdquo; steps to really get the logic. Why these?\nThey cover a range of array manipulation techniques: partitioning, arithmetic, dynamic state tracking, using arrays as lookups/boolean maps, and complex in-place reordering. They are frequently asked or form the basis of variations asked in interviews. Understanding these deeply will give you a strong foundation for tackling other array problems. How to approach the \u0026ldquo;Scan\u0026rdquo; Problems from this chapter (after deep dives on keystones):\nArray boot camp (even_odd, Page 37): This is a simpler version of Dutch Flag (2-way partition). Good to see the connection. 5.3 Multiply Two Arbitrary-Precision Integers: Extends the idea from 5.2. Understand the schoolbook multiplication. The nested loops and sum accumulation in the result array are key. 5.4 Advancing Through an Array: Greedy approach. Understand why simple \u0026ldquo;jump as far as possible\u0026rdquo; might not work and why tracking furthest_reach_so_far is better. 5.5 Delete Duplicates from a Sorted Array: Classic two-pointer (read/write pointer) technique for in-place modification of sorted arrays. Very common pattern. 5.7 Buy and Sell a Stock Twice: This is a precursor to Dynamic Programming. The idea of splitting the problem and using precomputed results (max profit up to day i, max profit from day i onwards) is important. 5.8 Computing an Alternation: Interesting problem on local swaps. The O(N) solution without full sorting is clever. 5.12 Sample Offline Data (Random Sampling): Knuth-Fisher-Yates shuffle variant. Important for random selection. 5.17 Sudoku Checker Problem, 5.18 Spiral Ordering, 5.19 Rotate a 2D Array: These are good 2D array manipulation problems. Focus on how indices are transformed or how layers/boundaries are processed. For rotation, the layer-by-layer in-place swap is key. Start with the 5 keystone problems. If you find you have more time or want more practice, pick from the \u0026ldquo;scan\u0026rdquo; list based on areas you feel weaker in (e.g., 2D arrays, more two-pointer problems).\nThis should give you a solid and manageable plan for tackling the Arrays chapter effectively!\n","permalink":"https://deepskandpal.github.io/dsa-concepts/arrays/","tags":null,"title":"Arrays"},{"categories":null,"content":"","permalink":"https://deepskandpal.github.io/dsa-concepts/dsa/","tags":null,"title":"DSA"},{"categories":null,"content":"Introduction to Hashing Hashing is the process of converting an input item (key) into a fixed-size value, typically an integer index, using a hash function. This index is then used to place or locate the item in a data structure, most commonly a hash table (hash map or dictionary).\nKey Concepts Hash Function: A function that maps keys to indices. A good hash function should be fast to compute and distribute keys uniformly across the available indices. Hash Table: A data structure that uses a hash function to map keys to values for efficient lookups. Collisions: Occur when two different keys map to the same index. Collision Resolution: Strategies to handle collisions, such as: * Separate Chaining: Each index points to a linked list (or other structure) containing all keys that hash to that index. * Open Addressing (Probing): If an index is occupied, probe for the next available slot (linear probing, quadratic probing, double hashing). Common Operations \u0026amp; Complexity (Average Case for Hash Tables) Insertion: O(1) Deletion: O(1) Search: O(1) (Worst Case for all can be O(n) if collisions are poorly handled or hash function is bad) Use Cases Implementing dictionaries/hash maps. Database indexing. Caching. Checking for duplicates. ","permalink":"https://deepskandpal.github.io/dsa-concepts/hashing/","tags":null,"title":"Hashing"},{"categories":null,"content":"","permalink":"https://deepskandpal.github.io/dsa-concepts/linked-lists/","tags":null,"title":"Linked Lists"},{"categories":["RAG"],"content":"Paper: \u0026ldquo;Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing\u0026rdquo; Link: https://arxiv.org/abs/2502.12962\nDate: Based on arXiv pattern, likely February 2025 (very recent) Domain: Retrieval-Augmented Generation (RAG) and Long-Context Processing\n✅ Completed - This paper focuses on enhancing LLMs\u0026rsquo; long-context processing capabilities through improved attention and retrieval mechanisms using a sliding window approach with internal attention-based retrieval.\nWhy This Paper Matters Long-context processing remains one of the key challenges in modern LLMs. This work likely addresses:\nContext window limitations in current models Attention efficiency for very long sequences RAG enhancement through better retrieval mechanisms Information retention over extended contexts Key Insights from Paper Core Innovation: \u0026ldquo;Slide and Retrieve\u0026rdquo; Method Status: ✅ Completed\nUses LLM\u0026rsquo;s internal attention mechanism as the retrieval system Processes long documents through sliding window approach Maintains compressed cache of relevant information across chunks Eliminates need for external vector databases Technical Approach Status: ✅ Completed\nSequential chunk processing with context preservation Internal attention scores identify key sentences/phrases Compressed cache maintains narrative flow and document structure Real-time processing without pre-indexing requirements Advantages over Traditional RAG Status: ✅ Completed\nBetter contextual cohesion and document structure understanding Reduced infrastructure complexity No upfront indexing requirements Superior for single-document deep analysis tasks Current Insights Research Context Long-context processing is a critical bottleneck for LLMs Traditional attention has quadratic complexity with sequence length RAG systems offer promise but need better integration with base models Recent work on infinite attention and similar concepts gaining traction Expected Contributions Based on the title, this paper likely proposes:\nNovel attention mechanisms for long sequences Better retrieval-generation integration Improved context window utilization Enhanced information flow over long documents Questions to Investigate What specific attention enhancements are proposed? How does the retrieval mechanism integrate with attention? What are the computational complexity improvements? How does performance compare on long-context benchmarks? Implementation Summary Based on my practical implementation in the GitHub repository, the InfiniRetri approach demonstrates:\nPractical Implementation Sliding Window Processing: Long documents are chunked into manageable segments that fit within the model\u0026rsquo;s context window Attention-Based Retrieval: The LLM\u0026rsquo;s internal attention mechanism identifies and extracts the most relevant information from each chunk Compressed Caching: Key sentences and phrases are maintained in a compressed cache that carries forward context across chunks Sequential Processing: Unlike traditional RAG\u0026rsquo;s fragmented approach, this maintains document flow and narrative structure Key Benefits Observed Simplified Architecture: No need for external vector databases or embedding models Real-time Processing: Documents can be processed on-the-fly without pre-indexing Better Context Understanding: Maintains document structure and sequential relationships Reduced Infrastructure: Lower complexity compared to traditional RAG systems Trade-offs Higher Query Latency: Multiple LLM calls required for processing chunks Computational Cost: More expensive at query time vs. traditional RAG\u0026rsquo;s upfront indexing cost Scalability Limitations: Better suited for single-document analysis rather than multi-document knowledge bases Technical Analysis The core innovation lies in leveraging the LLM\u0026rsquo;s existing attention mechanism as both the retrieval and reasoning component. This eliminates the semantic gap between external retrievers and the generation model, resulting in more coherent long-context processing.\nSummary This paper represents a significant shift from traditional RAG architectures by using internal attention mechanisms for retrieval. While it introduces higher query-time costs, it offers superior contextual understanding and simpler infrastructure for single-document analysis tasks. The approach is particularly valuable for applications requiring deep document comprehension rather than broad knowledge base querying.\n","permalink":"https://deepskandpal.github.io/papershelf/read/infinit-retrevial-attention/","tags":null,"title":"Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing"},{"categories":["AI"],"content":"Website\nTLDR; They tackled long video generation by replacing expensive global attention with efficient local attention, and bridging the gaps between local segments using novel TTT layers. These TTT layers act like RNNs but have a much smarter, adaptive hidden state (a neural network that learns on-the-fly during generation). This allows them to capture long-range dependencies and complex dynamics better than traditional RNNs, leading to more coherent minute-long videos, albeit with some remaining artifacts and efficiency challenges.\nWhat\u0026rsquo;s the Problem they are trying to solve? they want to create long videos, like a full minute long, that tell a coherent story with multiple scenes and characters doing things (like Tom chasing Jerry). Think of generating a mini-cartoon episode from a text description.\nThe main hurdle? Memory and Computation.\n1. Transformers (The Standard Powerhouse): These models are amazing for many things, but they use something called \u0026ldquo;self-attention.\u0026rdquo; Imagine every frame in the video needing to look back and compare itself to every other frame that came before it. For a few seconds, this is fine. But for a minute-long video (which can be thousands of frames or tokens!), the computation becomes astronomically expensive – it scales quadratically (if you double the length, the cost goes up by four times!). It\u0026rsquo;s like trying to have a conversation where, before speaking, you have to re-read the entire transcript of everything said so far.\n2. RNNs / State-Space Models (Like Mamba, DeltaNet - The Efficient Alternatives) : These are designed to be more efficient for long sequences. Their cost scales linearly (double the length, double the cost). Think of them like reading a book: you maintain a \u0026ldquo;summary\u0026rdquo; or \u0026ldquo;hidden state\u0026rdquo; in your head that captures the story so far, and you update it as you read each new word. Much faster.\n2.1. The Catch: The \u0026ldquo;summary\u0026rdquo; these models keep (their hidden state) is usually quite simple, often just a matrix of numbers. While efficient, we found this simple summary isn\u0026rsquo;t expressive enough to capture the complex details needed for a dynamic, multi-scene story like Tom and Jerry. It struggles to remember specific details from long ago or juggle multiple interacting elements coherently. It\u0026rsquo;s like trying to summarize a complex movie plot in just one short sentence – you lose too much information. Core Idea: Test-Time Training (TTT) Layers So, we need the efficiency of RNNs but the expressiveness that approaches Transformers. How? We introduced Test-Time Training (TTT) Layers. Here\u0026rsquo;s the intuition:\nRNNs Revisited: Think of any RNN layer. It takes an input (xt) and its current \u0026ldquo;memory\u0026rdquo; or hidden state (Wt-1), updates that memory (Wt), and produces an output (zt). Making the Memory Smarter: What if, instead of the hidden state (Wt) being just a passive matrix of numbers, it was itself a small neural network (f)? A mini-brain, if you will. Learning On-The-Fly: Now, here\u0026rsquo;s the key TTT part: As each new piece of the video sequence (xt) comes in during generation (at \u0026ldquo;test time\u0026rdquo;), we actually perform a tiny bit of training on this mini-brain hidden state. We update the weights (W) of this internal network f based on the input xt it just saw (using a quick gradient descent step, Equation 1). Why \u0026ldquo;Test-Time Training\u0026rdquo;? Because this training/updating of the hidden state (W) happens while the model is generating the video (test time), not just during the initial, large-scale training phase.\n4.1 The Benefit : Because the hidden state is now an active, learning neural network (they used a simple 2-layer MLP, see Section 2.3), it can potentially store much richer, more complex information than a static matrix. It adapts and learns specifically for the sequence it\u0026rsquo;s currently processing. This gives it a better chance to remember long-range dependencies and handle complex scene changes. It\u0026rsquo;s like having a smarter, adaptive summary that actively learns as the story unfolds. Putting TTT into a Video Model (Section 3): Base Model: They didn\u0026rsquo;t start from scratch. We took a powerful pre-trained video generation model (CogVideo-X 5B) which was good at generating short (3-second) clips. The Problem: CogVideo-X uses standard Transformer attention, which is too slow for one minute. The Approach (Figure 3): Local Attention: We keep the original Transformer self-attention layers but make them local. They only look at frames within their own 3-second segment. This keeps them fast. Global TTT: We insert our TTT layers. Since TTT layers are efficient (linear cost), we run them globally across the entire one-minute sequence. They act as the connective tissue, passing information between the 3-second segments that the local attention layers can\u0026rsquo;t see. Gating (Eq 6): When you add new layers (like TTT) to a pre-trained model, you don\u0026rsquo;t want them to mess things up initially. We use a \u0026ldquo;gate\u0026rdquo; (a learnable multiplier tanh(α)) that starts close to zero. This means, at first, the TTT layer\u0026rsquo;s output is mostly ignored. As the model fine-tunes, it can learn to \u0026ldquo;open the gate\u0026rdquo; (α increases) if the TTT layer proves helpful. Bi-direction (Eq 7): Video models often need to understand context from both past and future frames. Standard RNNs/TTT only look backward. So, we use a standard trick: run one TTT layer forward, and another identical TTT layer (TTT\u0026rsquo;) backward over the sequence, then combine their results. This gives each position information flowing from both directions. Segmentation: We process the video in 3-second chunks for the local attention, but the TTT layers see the whole concatenated sequence of chunks. Text prompts are also associated with these chunks (Format 3 storyboard). Training: We fine-tuned this modified architecture on our Tom and Jerry dataset, starting with short clips and gradually increasing the length (multi-stage context extension, Section 3.3). Evaluation and Results (Section 4): They compared our TTT-MLP approach against strong baselines like Mamba 2, Gated DeltaNet (state-of-the-art RNNs), sliding-window attention, and a simpler TTT-Linear (where the hidden state f is just a linear layer, not an MLP). Key Finding: For long (one-minute) videos with complex stories, TTT-MLP performed significantly better than the baselines, especially in \u0026ldquo;Temporal Consistency\u0026rdquo; (making sense over time) and \u0026ldquo;Motion Naturalness\u0026rdquo; (realistic movement). It won by 34 Elo points on average in human evaluations (Table 1), which is a meaningful difference. Short Videos: Interestingly, for shorter 18-second videos (Table 3 in Appendix), Gated DeltaNet was slightly better, suggesting that the richer hidden state of TTT-MLP really shines when the context gets very long and complex. Limitations (Sec 4.4): It\u0026rsquo;s not perfect! Videos still have artifacts (weird physics, morphing objects - see Fig 7). It\u0026rsquo;s also computationally slower than Mamba or DeltaNet (Fig 6). The quality is also limited by the base CogVideo-X model. ","permalink":"https://deepskandpal.github.io/papershelf/read/ttt-layer-for-video-generation/","tags":null,"title":"One-Minute Video Generation with Test-Time Training"},{"categories":["GenAI"],"content":"Overview This is a brief summary of the example project, highlighting its main purpose or key feature. It\u0026rsquo;s used on the card display.\nDetails Here you can write a full description of the project.\nExplain the goals. Describe the technologies used. Discuss challenges faced and solutions implemented. Include code snippets or further images if relevant using Markdown. {{/* Example code snippet */}} ","permalink":"https://deepskandpal.github.io/creations/animal-gpt/","tags":["Web dev","RAG","LLMs"],"title":"Animal GPT"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-2/","tags":null,"title":"Harry Potter and the Chamber of Secrets"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-7/","tags":null,"title":"Harry Potter and the Deathly Hallows"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-6/","tags":null,"title":"Harry Potter and the Half-Blood Prince"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-5/","tags":null,"title":"Harry Potter and the Order of the Phoenix"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-3/","tags":null,"title":"Harry Potter and the Prisoner of Azkaban"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-4/","tags":null,"title":"Harry Potter and the Prisoner of Azkaban"},{"categories":["Biographies"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/elon-musk/","tags":null,"title":"Elon Musk: Tesla, SpaceX, and the Quest for a Fantastic Future"},{"categories":["Series: Harry Potter"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/hp-1/","tags":null,"title":"Harry Potter and the Philosopher's StoneS"},{"categories":["Biographies"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/shoe-dog/","tags":null,"title":"Shoe Dog"},{"categories":["Biographies"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/steve-jobs/","tags":null,"title":"STEVE JOBS"},{"categories":["Non Fiction"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/accelerating-india/","tags":null,"title":"Accelerating India's Development: A State-Led Roadmap for Effective Governance"},{"categories":["Business"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/good-to-great/","tags":null,"title":"Good To Great"},{"categories":["Machine Learning"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/interpretable-machine-learning/","tags":null,"title":"Interpretable Machine Learning"},{"categories":["Business"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/priceless/","tags":null,"title":"Priceless: the myth of fair value (and how to take advantage of it)"},{"categories":["Non Fiction"],"content":"","permalink":"https://deepskandpal.github.io/bookshelf/begin-infinity/","tags":null,"title":"The Beginning of Infinity"}]