<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Frameworks on 404EngineerNotFound</title><link>https://deepskandpal.github.io/tags/frameworks/</link><description>Recent content in Frameworks on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 19 Dec 2024 11:00:00 +0000</lastBuildDate><atom:link href="https://deepskandpal.github.io/tags/frameworks/index.xml" rel="self" type="application/rss+xml"/><item><title>GenAI Infrastructure &amp; Implementation</title><link>https://deepskandpal.github.io/tech-writings/genai-infrastructure/</link><pubDate>Thu, 19 Dec 2024 11:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/tech-writings/genai-infrastructure/</guid><description>&lt;h1 id="genai-infrastructure--implementation-knowledge-tree">GenAI Infrastructure &amp;amp; Implementation Knowledge Tree&lt;/h1>
&lt;p>This knowledge tree covers the technical infrastructure, hardware, frameworks, and implementation details required to build, train, and deploy GenAI systems at scale.&lt;/p>
&lt;h2 id="complete-infrastructure--implementation-overview">Complete Infrastructure &amp;amp; Implementation Overview&lt;/h2>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 60, &amp;#39;rankSpacing&amp;#39;: 100}}}%%
graph LR
 INFRA[&amp;#34;âš™ï¸ GenAI Infrastructure &amp;amp; Implementation&amp;#34;]
 
 INFRA --&amp;gt; HARDWARE[&amp;#34;ðŸ–¥ï¸ Hardware &amp;amp; Computing&amp;#34;]
 INFRA --&amp;gt; FRAMEWORKS[&amp;#34;ðŸ› ï¸ Software Frameworks&amp;#34;]
 INFRA --&amp;gt; SERVING[&amp;#34;ðŸš€ Model Serving&amp;#34;]
 INFRA --&amp;gt; DATA[&amp;#34;ðŸ’¾ Data Infrastructure&amp;#34;]
 INFRA --&amp;gt; PERFORMANCE[&amp;#34;âš¡ Performance &amp;amp; Optimization&amp;#34;]
 INFRA --&amp;gt; DEVOPS[&amp;#34;ðŸ”§ Development &amp;amp; DevOps&amp;#34;]
 
 style INFRA fill:#e1f5fe,stroke:#01579b,stroke-width:3px
 style HARDWARE fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
 style FRAMEWORKS fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
 style SERVING fill:#fff3e0,stroke:#e65100,stroke-width:2px
 style DATA fill:#fce4ec,stroke:#880e4f,stroke-width:2px
 style PERFORMANCE fill:#f1f8e9,stroke:#33691e,stroke-width:2px
 style DEVOPS fill:#e3f2fd,stroke:#0d47a1,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h2 id="hardware--computing-infrastructure">Hardware &amp;amp; Computing Infrastructure&lt;/h2>
&lt;h3 id="gpu-computing--acceleration">GPU Computing &amp;amp; Acceleration&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 50, &amp;#39;rankSpacing&amp;#39;: 80}}}%%
graph LR
 GPU[&amp;#34;ðŸŽ® GPU Computing&amp;#34;]
 
 GPU --&amp;gt; NVIDIA[&amp;#34;NVIDIA Stack&amp;#34;]
 GPU --&amp;gt; AMD[&amp;#34;AMD Stack&amp;#34;]
 GPU --&amp;gt; CUSTOM[&amp;#34;Custom Silicon&amp;#34;]
 GPU --&amp;gt; CLOUD[&amp;#34;Cloud GPUs&amp;#34;]
 
 NVIDIA --&amp;gt; A100[&amp;#34;A100/H100&amp;#34;]
 NVIDIA --&amp;gt; RTX[&amp;#34;RTX Series&amp;#34;]
 NVIDIA --&amp;gt; CUDA[&amp;#34;CUDA Programming&amp;#34;]
 NVIDIA --&amp;gt; TENSORRT[&amp;#34;TensorRT&amp;#34;]
 
 AMD --&amp;gt; MI[&amp;#34;MI Series&amp;#34;]
 AMD --&amp;gt; ROCM[&amp;#34;ROCm Platform&amp;#34;]
 AMD --&amp;gt; HIP[&amp;#34;HIP Programming&amp;#34;]
 
 CUSTOM --&amp;gt; TPU[&amp;#34;Google TPUs&amp;#34;]
 CUSTOM --&amp;gt; INFERENTIA[&amp;#34;AWS Inferentia&amp;#34;]
 CUSTOM --&amp;gt; TRAINIUM[&amp;#34;AWS Trainium&amp;#34;]
 CUSTOM --&amp;gt; HABANA[&amp;#34;Intel Habana&amp;#34;]
 
 CLOUD --&amp;gt; AWS_GPU[&amp;#34;AWS GPU Instances&amp;#34;]
 CLOUD --&amp;gt; GCP_GPU[&amp;#34;GCP GPU Instances&amp;#34;]
 CLOUD --&amp;gt; AZURE_GPU[&amp;#34;Azure GPU Instances&amp;#34;]
 
 style GPU fill:#e1f5fe,stroke:#01579b,stroke-width:3px
 style NVIDIA fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
 style AMD fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
 style CUSTOM fill:#fff3e0,stroke:#e65100,stroke-width:2px
 style CLOUD fill:#fce4ec,stroke:#880e4f,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h3 id="distributed-computing-architecture">Distributed Computing Architecture&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 50, &amp;#39;rankSpacing&amp;#39;: 80}}}%%
graph LR
 DISTRIBUTED[&amp;#34;ðŸŒ Distributed Computing&amp;#34;]
 
 DISTRIBUTED --&amp;gt; CLUSTER[&amp;#34;ðŸ–§ Cluster Management&amp;#34;]
 DISTRIBUTED --&amp;gt; NETWORKING[&amp;#34;ðŸ”— High-Speed Networking&amp;#34;]
 DISTRIBUTED --&amp;gt; STORAGE[&amp;#34;ðŸ’¾ Distributed Storage&amp;#34;]
 DISTRIBUTED --&amp;gt; ORCHESTRATION[&amp;#34;ðŸŽ¯ Orchestration&amp;#34;]
 
 CLUSTER --&amp;gt; SLURM[&amp;#34;SLURM&amp;#34;]
 CLUSTER --&amp;gt; PBS[&amp;#34;PBS/Torque&amp;#34;]
 CLUSTER --&amp;gt; K8S[&amp;#34;Kubernetes&amp;#34;]
 CLUSTER --&amp;gt; YARN[&amp;#34;YARN&amp;#34;]
 
 NETWORKING --&amp;gt; INFINIBAND[&amp;#34;InfiniBand&amp;#34;]
 NETWORKING --&amp;gt; RDMA[&amp;#34;RDMA&amp;#34;]
 NETWORKING --&amp;gt; NVLINK[&amp;#34;NVLink&amp;#34;]
 NETWORKING --&amp;gt; ETHERNET[&amp;#34;High-Speed Ethernet&amp;#34;]
 
 STORAGE --&amp;gt; LUSTRE[&amp;#34;Lustre FS&amp;#34;]
 STORAGE --&amp;gt; GPFS[&amp;#34;GPFS&amp;#34;]
 STORAGE --&amp;gt; CEPH[&amp;#34;Ceph&amp;#34;]
 STORAGE --&amp;gt; S3[&amp;#34;Object Storage&amp;#34;]
 
 ORCHESTRATION --&amp;gt; RAY[&amp;#34;Ray&amp;#34;]
 ORCHESTRATION --&amp;gt; DASK[&amp;#34;Dask&amp;#34;]
 ORCHESTRATION --&amp;gt; SPARK[&amp;#34;Apache Spark&amp;#34;]
 ORCHESTRATION --&amp;gt; HOROVOD[&amp;#34;Horovod&amp;#34;]
 
 style DISTRIBUTED fill:#e1f5fe,stroke:#01579b,stroke-width:3px
 style CLUSTER fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
 style NETWORKING fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
 style STORAGE fill:#fff3e0,stroke:#e65100,stroke-width:2px
 style ORCHESTRATION fill:#fce4ec,stroke:#880e4f,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h3 id="memory--storage-systems">Memory &amp;amp; Storage Systems&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 50, &amp;#39;rankSpacing&amp;#39;: 80}}}%%
graph LR
 MEMORY[&amp;#34;ðŸ’¾ Memory &amp;amp; Storage&amp;#34;]
 
 MEMORY --&amp;gt; RAM[&amp;#34;ðŸ”„ High-Bandwidth Memory&amp;#34;]
 MEMORY --&amp;gt; NVME[&amp;#34;âš¡ NVMe Storage&amp;#34;]
 MEMORY --&amp;gt; CACHE[&amp;#34;ðŸ—„ï¸ Caching Systems&amp;#34;]
 MEMORY --&amp;gt; TIERED[&amp;#34;ðŸ“š Tiered Storage&amp;#34;]
 
 RAM --&amp;gt; HBM[&amp;#34;HBM2/HBM3&amp;#34;]
 RAM --&amp;gt; DDR[&amp;#34;DDR4/DDR5&amp;#34;]
 RAM --&amp;gt; UNIFIED[&amp;#34;Unified Memory&amp;#34;]
 
 NVME --&amp;gt; GEN4[&amp;#34;PCIe Gen4&amp;#34;]
 NVME --&amp;gt; GEN5[&amp;#34;PCIe Gen5&amp;#34;]
 NVME --&amp;gt; OPTANE[&amp;#34;Intel Optane&amp;#34;]
 
 CACHE --&amp;gt; REDIS[&amp;#34;Redis&amp;#34;]
 CACHE --&amp;gt; MEMCACHED[&amp;#34;Memcached&amp;#34;]
 CACHE --&amp;gt; HAZELCAST[&amp;#34;Hazelcast&amp;#34;]
 
 TIERED --&amp;gt; HOT[&amp;#34;Hot Storage&amp;#34;]
 TIERED --&amp;gt; WARM[&amp;#34;Warm Storage&amp;#34;]
 TIERED --&amp;gt; COLD[&amp;#34;Cold Storage&amp;#34;]
 
 style MEMORY fill:#e1f5fe,stroke:#01579b,stroke-width:3px
 style RAM fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
 style NVME fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
 style CACHE fill:#fff3e0,stroke:#e65100,stroke-width:2px
 style TIERED fill:#fce4ec,stroke:#880e4f,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h2 id="software-frameworks--tools">Software Frameworks &amp;amp; Tools&lt;/h2>
&lt;h3 id="deep-learning-frameworks">Deep Learning Frameworks&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 50, &amp;#39;rankSpacing&amp;#39;: 80}}}%%
graph LR
 FRAMEWORKS[&amp;#34;ðŸ› ï¸ Deep Learning Frameworks&amp;#34;]
 
 FRAMEWORKS --&amp;gt; PYTORCH[&amp;#34;ðŸ”¥ PyTorch Ecosystem&amp;#34;]
 FRAMEWORKS --&amp;gt; TF[&amp;#34;ðŸ“Š TensorFlow Ecosystem&amp;#34;]
 FRAMEWORKS --&amp;gt; JAX[&amp;#34;ðŸ§® JAX Ecosystem&amp;#34;]
 FRAMEWORKS --&amp;gt; OTHER[&amp;#34;ðŸ”§ Other Frameworks&amp;#34;]
 
 PYTORCH --&amp;gt; TORCH[&amp;#34;PyTorch Core&amp;#34;]
 PYTORCH --&amp;gt; LIGHTNING[&amp;#34;PyTorch Lightning&amp;#34;]
 PYTORCH --&amp;gt; IGNITE[&amp;#34;PyTorch Ignite&amp;#34;]
 PYTORCH --&amp;gt; GEOMETRIC[&amp;#34;PyTorch Geometric&amp;#34;]
 
 TF --&amp;gt; TF_CORE[&amp;#34;TensorFlow Core&amp;#34;]
 TF --&amp;gt; KERAS[&amp;#34;Keras&amp;#34;]
 TF --&amp;gt; TF_SERVING[&amp;#34;TF Serving&amp;#34;]
 TF --&amp;gt; TF_LITE[&amp;#34;TensorFlow Lite&amp;#34;]
 
 JAX --&amp;gt; JAX_CORE[&amp;#34;JAX Core&amp;#34;]
 JAX --&amp;gt; FLAX[&amp;#34;Flax&amp;#34;]
 JAX --&amp;gt; HAIKU[&amp;#34;Haiku&amp;#34;]
 JAX --&amp;gt; OPTAX[&amp;#34;Optax&amp;#34;]
 
 OTHER --&amp;gt; MXNET[&amp;#34;Apache MXNet&amp;#34;]
 OTHER --&amp;gt; PADDLE[&amp;#34;PaddlePaddle&amp;#34;]
 OTHER --&amp;gt; ONEFLOW[&amp;#34;OneFlow&amp;#34;]
 OTHER --&amp;gt; MINDSPORE[&amp;#34;MindSpore&amp;#34;]
 
 style FRAMEWORKS fill:#e1f5fe,stroke:#01579b,stroke-width:3px
 style PYTORCH fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
 style TF fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
 style JAX fill:#fff3e0,stroke:#e65100,stroke-width:2px
 style OTHER fill:#fce4ec,stroke:#880e4f,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h3 id="genai-specific-libraries">GenAI-Specific Libraries&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 50, &amp;#39;rankSpacing&amp;#39;: 80}}}%%
graph LR
 GENAI_LIBS[&amp;#34;ðŸ¤– GenAI Libraries&amp;#34;]
 
 GENAI_LIBS --&amp;gt; HF[&amp;#34;ðŸ¤— Hugging Face&amp;#34;]
 GENAI_LIBS --&amp;gt; LANG[&amp;#34;ðŸ¦œ LangChain&amp;#34;]
 GENAI_LIBS --&amp;gt; DIFFUSION[&amp;#34;ðŸŽ¨ Diffusion&amp;#34;]
 GENAI_LIBS --&amp;gt; INFERENCE[&amp;#34;âš¡ Inference&amp;#34;]
 
 HF --&amp;gt; TRANSFORMERS[&amp;#34;Transformers&amp;#34;]
 HF --&amp;gt; DATASETS[&amp;#34;Datasets&amp;#34;]
 HF --&amp;gt; ACCELERATE[&amp;#34;Accelerate&amp;#34;]
 HF --&amp;gt; PEFT[&amp;#34;PEFT&amp;#34;]
 
 LANG --&amp;gt; LANGCHAIN[&amp;#34;LangChain&amp;#34;]
 LANG --&amp;gt; LLAMAINDEX[&amp;#34;LlamaIndex&amp;#34;]
 LANG --&amp;gt; SEMANTIC[&amp;#34;Semantic Kernel&amp;#34;]
 
 DIFFUSION --&amp;gt; DIFFUSERS[&amp;#34;Diffusers&amp;#34;]
 DIFFUSION --&amp;gt; STABLE[&amp;#34;Stable Diffusion&amp;#34;]
 DIFFUSION --&amp;gt; CONTROLNET[&amp;#34;ControlNet&amp;#34;]
 
 INFERENCE --&amp;gt; VLLM[&amp;#34;vLLM&amp;#34;]
 INFERENCE --&amp;gt; TEXTGEN[&amp;#34;Text Generation WebUI&amp;#34;]
 INFERENCE --&amp;gt; OLLAMA[&amp;#34;Ollama&amp;#34;]
 
 style GENAI_LIBS fill:#e1f5fe,stroke:#01579b,stroke-width:3px
 style HF fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
 style LANG fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
 style DIFFUSION fill:#fff3e0,stroke:#e65100,stroke-width:2px
 style INFERENCE fill:#fce4ec,stroke:#880e4f,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h3 id="development-environment--tools">Development Environment &amp;amp; Tools&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 50, &amp;#39;rankSpacing&amp;#39;: 80}}}%%
graph LR
 DEV_ENV[&amp;#34;ðŸ’» Development Environment&amp;#34;]
 
 DEV_ENV --&amp;gt; NOTEBOOKS[&amp;#34;ðŸ““ Interactive Development&amp;#34;]
 DEV_ENV --&amp;gt; IDES[&amp;#34;ðŸ–¥ï¸ IDEs &amp;amp; Editors&amp;#34;] 
 DEV_ENV --&amp;gt; CONTAINERS[&amp;#34;ðŸ“¦ Containerization&amp;#34;]
 DEV_ENV --&amp;gt; VERSION[&amp;#34;ðŸŒ¿ Version Control&amp;#34;]
 
 NOTEBOOKS --&amp;gt; JUPYTER[&amp;#34;Jupyter&amp;#34;]
 NOTEBOOKS --&amp;gt; COLAB[&amp;#34;Google Colab&amp;#34;]
 NOTEBOOKS --&amp;gt; KAGGLE[&amp;#34;Kaggle Notebooks&amp;#34;]
 NOTEBOOKS --&amp;gt; SAGEMAKER[&amp;#34;SageMaker Studio&amp;#34;]
 
 IDES --&amp;gt; VSCODE[&amp;#34;VS Code&amp;#34;]
 IDES --&amp;gt; PYCHARM[&amp;#34;PyCharm&amp;#34;]
 IDES --&amp;gt; CURSOR[&amp;#34;Cursor&amp;#34;]
 IDES --&amp;gt; VIM[&amp;#34;Vim/Neovim&amp;#34;]
 
 CONTAINERS --&amp;gt; DOCKER[&amp;#34;Docker&amp;#34;]
 CONTAINERS --&amp;gt; PODMAN[&amp;#34;Podman&amp;#34;]
 CONTAINERS --&amp;gt; SINGULARITY[&amp;#34;Singularity&amp;#34;]
 CONTAINERS --&amp;gt; NVIDIA_DOCKER[&amp;#34;NVIDIA Docker&amp;#34;]
 
 VERSION --&amp;gt; GIT[&amp;#34;Git&amp;#34;]
 VERSION --&amp;gt; DVC[&amp;#34;DVC - Data Version Control&amp;#34;]
 VERSION --&amp;gt; MLflow[&amp;#34;MLflow&amp;#34;]
 VERSION --&amp;gt; WANDB[&amp;#34;Weights &amp;amp; Biases&amp;#34;]
 
 style DEV_ENV fill:#e1f5fe,stroke:#01579b,stroke-width:3px
 style NOTEBOOKS fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
 style IDES fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
 style CONTAINERS fill:#fff3e0,stroke:#e65100,stroke-width:2px
 style VERSION fill:#fce4ec,stroke:#880e4f,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h2 id="model-serving--deployment">Model Serving &amp;amp; Deployment&lt;/h2>
&lt;h3 id="model-serving--deployment-infrastructure">Model Serving &amp;amp; Deployment Infrastructure&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 50, &amp;#39;rankSpacing&amp;#39;: 80}}}%%
graph LR
 SERVING[&amp;#34;ðŸš€ Model Serving&amp;#34;]
 
 SERVING --&amp;gt; INFERENCE[&amp;#34;âš¡ Inference Servers&amp;#34;]
 SERVING --&amp;gt; API[&amp;#34;ðŸ”Œ API Frameworks&amp;#34;]
 SERVING --&amp;gt; EDGE[&amp;#34;ðŸ“± Edge Deployment&amp;#34;]
 SERVING --&amp;gt; BATCH[&amp;#34;ðŸ“¦ Batch Processing&amp;#34;]
 
 INFERENCE --&amp;gt; TRITON[&amp;#34;NVIDIA Triton&amp;lt;br/&amp;gt;(Inference + Training Support)&amp;#34;]
 INFERENCE --&amp;gt; TORCHSERVE[&amp;#34;TorchServe&amp;#34;]
 INFERENCE --&amp;gt; TF_SERVE[&amp;#34;TensorFlow Serving&amp;#34;]
 INFERENCE --&amp;gt; BENTOML[&amp;#34;BentoML&amp;#34;]
 
 API --&amp;gt; FASTAPI[&amp;#34;FastAPI&amp;#34;]
 API --&amp;gt; FLASK[&amp;#34;Flask&amp;#34;]
 API --&amp;gt; DJANGO[&amp;#34;Django REST&amp;#34;]
 API --&amp;gt; GRADIO[&amp;#34;Gradio&amp;#34;]
 
 EDGE --&amp;gt; ONNX[&amp;#34;ONNX Runtime&amp;#34;]
 EDGE --&amp;gt; TFLITE[&amp;#34;TensorFlow Lite&amp;#34;]
 EDGE --&amp;gt; TENSORRT[&amp;#34;TensorRT&amp;#34;]
 EDGE --&amp;gt; OPENVINO[&amp;#34;OpenVINO&amp;#34;]
 
 BATCH --&amp;gt; AIRFLOW[&amp;#34;Apache Airflow&amp;#34;]
 BATCH --&amp;gt; PREFECT[&amp;#34;Prefect&amp;#34;]
 BATCH --&amp;gt; KUBEFLOW[&amp;#34;Kubeflow&amp;#34;]
 BATCH --&amp;gt; RAY_SERVE[&amp;#34;Ray Serve&amp;#34;]
 
 style SERVING fill:#e1f5fe,stroke:#01579b,stroke-width:3px
 style INFERENCE fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
 style API fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
 style EDGE fill:#fff3e0,stroke:#e65100,stroke-width:2px
 style BATCH fill:#fce4ec,stroke:#880e4f,stroke-width:2px
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Note&lt;/strong>: While primarily designed for inference, &lt;strong>NVIDIA Triton&lt;/strong> also supports training workflows including:&lt;/p></description></item></channel></rss>