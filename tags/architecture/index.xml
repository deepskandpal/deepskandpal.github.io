<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Architecture on 404EngineerNotFound</title><link>https://deepskandpal.github.io/tags/architecture/</link><description>Recent content in Architecture on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 26 Jan 2025 16:45:00 +0530</lastBuildDate><atom:link href="https://deepskandpal.github.io/tags/architecture/index.xml" rel="self" type="application/rss+xml"/><item><title>GenAI Core Architectures: Transformers, LLMs &amp; Generative Models</title><link>https://deepskandpal.github.io/tech-writings/genai/genai-architectures/</link><pubDate>Sun, 26 Jan 2025 16:45:00 +0530</pubDate><guid>https://deepskandpal.github.io/tech-writings/genai/genai-architectures/</guid><description>&lt;h1 id="genai-core-architectures-tree">GenAI Core Architectures Tree&lt;/h1>
&lt;p>The fundamental architectures that power modern generative AI systems. From attention mechanisms to large language models and diffusion processes.&lt;/p>
&lt;h2 id="core-architectures-knowledge-tree">Core Architectures Knowledge Tree&lt;/h2>
&lt;h3 id="complete-architecture-overview">Complete Architecture Overview&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 60, &amp;#39;rankSpacing&amp;#39;: 100}}}%%
graph LR
 ROOT[🏗️ GenAI Core Architectures]
 
 %% Main Architecture Families
 ROOT --&amp;gt; TRANSFORMERS[🔄 Transformers]
 ROOT --&amp;gt; LLM[🤖 Large Language Models]
 ROOT --&amp;gt; DIFFUSION[🎨 Diffusion Models]
 ROOT --&amp;gt; OTHER_GEN[🎭 Other Generative Models]
 
 %% Key Capabilities
 TRANSFORMERS --&amp;gt; T1[Attention Mechanisms]
 TRANSFORMERS --&amp;gt; T2[Architecture Variants]
 TRANSFORMERS --&amp;gt; T3[Core Components]
 
 LLM --&amp;gt; L1[Model Families]
 LLM --&amp;gt; L2[Scaling Laws]
 LLM --&amp;gt; L3[Training Paradigms]
 
 DIFFUSION --&amp;gt; D1[Theoretical Foundation]
 DIFFUSION --&amp;gt; D2[Model Variants]
 DIFFUSION --&amp;gt; D3[Conditioning]
 
 OTHER_GEN --&amp;gt; O1[GANs]
 OTHER_GEN --&amp;gt; O2[VAEs]
 OTHER_GEN --&amp;gt; O3[Flow Models]

 %% Styling
 style ROOT fill:#e1d5e7,stroke:#9673a6,stroke-width:4px
 style TRANSFORMERS fill:#d5e8d4,stroke:#82b366,stroke-width:3px
 style LLM fill:#dae8fc,stroke:#6c8ebf,stroke-width:3px
 style DIFFUSION fill:#f8cecc,stroke:#b85450,stroke-width:3px
 style OTHER_GEN fill:#fff2cc,stroke:#d6b656,stroke-width:3px
&lt;/code>&lt;/pre>&lt;h3 id="transformers-attention--architecture-design">Transformers: Attention &amp;amp; Architecture Design&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 60, &amp;#39;rankSpacing&amp;#39;: 80}}}%%
graph LR
 subgraph ATT[👁️ Attention Mechanisms]
 ATT1[Self-Attention&amp;lt;br/&amp;gt;Q-K-V, Scaled Dot-Product] --&amp;gt; ATT2[Multi-Head Attention&amp;lt;br/&amp;gt;Parallel Heads, Different Subspaces]
 ATT2 --&amp;gt; ATT3[Cross-Attention&amp;lt;br/&amp;gt;Encoder-Decoder, Conditioning]
 ATT3 --&amp;gt; ATT4[Sparse Attention&amp;lt;br/&amp;gt;Local, Sliding Window]
 ATT4 --&amp;gt; ATT5[Flash Attention&amp;lt;br/&amp;gt;Memory-Efficient, IO-Aware]
 end
 
 subgraph ARCH[🏛️ Architecture Variants]
 ARCH1[Encoder-Decoder&amp;lt;br/&amp;gt;Seq2Seq, Translation] --&amp;gt; ARCH2[Encoder-Only&amp;lt;br/&amp;gt;BERT, Understanding]
 ARCH2 --&amp;gt; ARCH3[Decoder-Only&amp;lt;br/&amp;gt;GPT, Generation]
 ARCH3 --&amp;gt; ARCH4[Hybrid&amp;lt;br/&amp;gt;Task-Specific Design]
 end
 
 subgraph COMP[⚙️ Core Components]
 COMP1[Position Embeddings&amp;lt;br/&amp;gt;Absolute, Relative, RoPE] --&amp;gt; COMP2[Layer Normalization&amp;lt;br/&amp;gt;Pre-norm, Post-norm]
 COMP2 --&amp;gt; COMP3[Feed-Forward Networks&amp;lt;br/&amp;gt;MLP, Gated Linear Units]
 COMP3 --&amp;gt; COMP4[Residual Connections&amp;lt;br/&amp;gt;Skip Connections, Gradient Flow]
 end

 style ATT fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
 style ARCH fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
 style COMP fill:#fff3e0,stroke:#ff9800,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h3 id="large-language-models-families--scaling">Large Language Models: Families &amp;amp; Scaling&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 50, &amp;#39;rankSpacing&amp;#39;: 70}}}%%
graph TD
 subgraph FAMILIES[👨‍👩‍👧‍👦 Model Families]
 GPT[GPT Family&amp;lt;br/&amp;gt;Decoder-Only] --&amp;gt; GPT_EVO[GPT-1 → GPT-2 → GPT-3 → GPT-4&amp;lt;br/&amp;gt;117M → 1.5B → 175B → Multimodal]
 BERT[BERT Family&amp;lt;br/&amp;gt;Encoder-Only] --&amp;gt; BERT_EVO[BERT → RoBERTa → ALBERT&amp;lt;br/&amp;gt;Bidirectional Understanding]
 T5[T5 Family&amp;lt;br/&amp;gt;Encoder-Decoder] --&amp;gt; T5_EVO[Text-to-Text Transfer&amp;lt;br/&amp;gt;Unified Framework]
 MODERN[Modern LLMs] --&amp;gt; MOD_EVO[LLaMA, Claude, Gemini&amp;lt;br/&amp;gt;Aquila2, DeepSeek-R1, Efficient, Aligned, Multimodal]
 end
 
 subgraph SCALING[📏 Scaling Dimensions]
 PARAM[Parameter Scaling&amp;lt;br/&amp;gt;Emergent Abilities] --- DATA[Data Scaling&amp;lt;br/&amp;gt;Quality vs Quantity]
 DATA --- COMPUTE[Compute Scaling&amp;lt;br/&amp;gt;Training FLOPs]
 COMPUTE --- LAWS[Scaling Laws&amp;lt;br/&amp;gt;Power Relationships]
 end
 
 subgraph TRAINING[🎯 Training Evolution]
 PRE[Pre-training&amp;lt;br/&amp;gt;Language Modeling] --&amp;gt; INST[Instruction Tuning&amp;lt;br/&amp;gt;Task Following]
 INST --&amp;gt; RLHF[RLHF&amp;lt;br/&amp;gt;Human Feedback]
 RLHF --&amp;gt; ALIGN[AI Alignment&amp;lt;br/&amp;gt;Constitutional AI]
 end

 FAMILIES --&amp;gt; SCALING
 SCALING --&amp;gt; TRAINING

 style FAMILIES fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
 style SCALING fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px
 style TRAINING fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Note on Agentic Architectures&lt;/strong>: While many models can be used in agentic systems, some are specifically designed or fine-tuned for reasoning and tool use (function calling). For a complete overview of how these models are used, see the &lt;a href="https://deepskandpal.github.io/tech-writings/genai/genai-applications/#-agentic-ai--autonomous-systems">Agentic AI &amp;amp; Autonomous Systems&lt;/a> hub.&lt;/p></description></item></channel></rss>