<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Mathematics on 404EngineerNotFound</title><link>https://deepskandpal.github.io/tags/mathematics/</link><description>Recent content in Mathematics on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 26 Jan 2025 16:30:00 +0530</lastBuildDate><atom:link href="https://deepskandpal.github.io/tags/mathematics/index.xml" rel="self" type="application/rss+xml"/><item><title>GenAI Foundations: Mathematical &amp; Deep Learning Prerequisites</title><link>https://deepskandpal.github.io/tech-writings/genai/genai-foundations/</link><pubDate>Sun, 26 Jan 2025 16:30:00 +0530</pubDate><guid>https://deepskandpal.github.io/tech-writings/genai/genai-foundations/</guid><description>&lt;h1 id="genai-foundations-tree">GenAI Foundations Tree&lt;/h1>
&lt;p>Essential mathematical and computational foundations that underpin all generative AI systems. Master these concepts to build a solid foundation for advanced GenAI topics.&lt;/p>
&lt;h2 id="foundations-knowledge-tree">Foundations Knowledge Tree&lt;/h2>
&lt;h3 id="complete-foundation-overview">Complete Foundation Overview&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 50, &amp;#39;rankSpacing&amp;#39;: 80}}}%%
graph LR
 ROOT[ðŸ“š GenAI Foundations]
 
 %% Mathematics Branch - Vertical
 ROOT --&amp;gt; MATH[ðŸ“ Mathematics]
 MATH --&amp;gt; MATH1[ðŸ“Š Linear Algebra]
 MATH --&amp;gt; MATH2[ðŸŽ² Probability &amp;amp; Statistics]
 MATH --&amp;gt; MATH3[ðŸ“¡ Information Theory]
 MATH --&amp;gt; MATH4[âš¡ Optimization]
 MATH --&amp;gt; MATH5[ðŸ“ˆ Calculus]
 
 %% Machine Learning Branch - Horizontal
 ROOT --&amp;gt; ML[ðŸ§  Machine Learning]
 ML --&amp;gt; ML1[ðŸ“‹ Supervised]
 ML --&amp;gt; ML2[ðŸ” Unsupervised] 
 ML --&amp;gt; ML3[ðŸŽ® Reinforcement]
 ML --&amp;gt; ML4[ðŸ“š Theory]
 ML --&amp;gt; ML5[ðŸ”„ Evaluation]
 
 %% Deep Learning Branch - Mixed
 ROOT --&amp;gt; DL[ðŸ”— Deep Learning]
 DL --&amp;gt; DL1[ðŸ§  Neural Networks]
 DL --&amp;gt; DL2[ðŸ”„ Training]
 DL --&amp;gt; DL3[âš¡ Activations]
 DL --&amp;gt; DL4[ðŸ›¡ï¸ Regularization]
 DL --&amp;gt; DL5[ðŸ–¼ï¸ CNNs]
 DL --&amp;gt; DL6[ðŸ”— RNNs]
 DL --&amp;gt; DL7[ðŸ’« Embeddings]

 %% Styling
 style ROOT fill:#e1d5e7,stroke:#9673a6,stroke-width:4px
 style MATH fill:#d5e8d4,stroke:#82b366,stroke-width:3px
 style ML fill:#dae8fc,stroke:#6c8ebf,stroke-width:3px
 style DL fill:#f8cecc,stroke:#b85450,stroke-width:3px
&lt;/code>&lt;/pre>&lt;h3 id="mathematics-core-areas-with-key-concepts">Mathematics: Core Areas with Key Concepts&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 40, &amp;#39;rankSpacing&amp;#39;: 60}}}%%
graph LR
 subgraph LA[ðŸ“Š Linear Algebra]
 M1A[Matrix Operations] --&amp;gt; M1B[Eigenvalues &amp;amp; Eigenvectors]
 M1B --&amp;gt; M1C[Vector Spaces]
 M1C --&amp;gt; M1D[Linear Transformations]
 M1D --&amp;gt; M1E[Decompositions: SVD, QR, LU]
 end
 
 subgraph PROB[ðŸŽ² Probability &amp;amp; Statistics]
 M2A[Distributions] --&amp;gt; M2B[Bayes Theorem]
 M2B --&amp;gt; M2C[Statistical Inference]
 M2C --&amp;gt; M2D[Random Variables]
 M2D --&amp;gt; M2E[Multivariate Statistics]
 end
 
 LA --&amp;gt; PROB
 
 subgraph INFO[ðŸ“¡ Information Theory]
 M3A[Entropy &amp;amp; Information] --&amp;gt; M3B[KL Divergence]
 M3B --&amp;gt; M3C[Mutual Information]
 M3C --&amp;gt; M3D[Channel Capacity]
 end
 
 PROB --&amp;gt; INFO

 style LA fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
 style PROB fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
 style INFO fill:#fff3e0,stroke:#ff9800,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h3 id="mathematics-optimization--calculus">Mathematics: Optimization &amp;amp; Calculus&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 40, &amp;#39;rankSpacing&amp;#39;: 60}}}%%
graph TD
 OPT[âš¡ Optimization Theory]
 CALC[ðŸ“ˆ Calculus &amp;amp; Analysis]
 
 OPT --&amp;gt; M4A[Convex Optimization&amp;lt;br/&amp;gt;Global Optima]
 OPT --&amp;gt; M4B[Gradient Descent&amp;lt;br/&amp;gt;Line Search]
 OPT --&amp;gt; M4C[Constrained Optimization&amp;lt;br/&amp;gt;Lagrange Multipliers]
 OPT --&amp;gt; M4D[Stochastic Optimization&amp;lt;br/&amp;gt;SGD, Mini-batch]
 
 CALC --&amp;gt; M5A[Multivariable Calculus&amp;lt;br/&amp;gt;Chain Rule, Jacobians]
 CALC --&amp;gt; M5B[Vector Calculus&amp;lt;br/&amp;gt;Gradients, Divergence]
 CALC --&amp;gt; M5C[Functional Analysis&amp;lt;br/&amp;gt;Function Spaces]
 CALC --&amp;gt; M5D[Differential Equations&amp;lt;br/&amp;gt;ODEs, PDEs]

 style OPT fill:#fce4ec,stroke:#e91e63,stroke-width:3px
 style CALC fill:#f3e5f5,stroke:#9c27b0,stroke-width:3px
&lt;/code>&lt;/pre>&lt;h3 id="machine-learning-learning-paradigms">Machine Learning: Learning Paradigms&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 40, &amp;#39;rankSpacing&amp;#39;: 70}}}%%
graph LR
 subgraph SUP[ðŸ“‹ Supervised Learning]
 ML1A[Classification]
 ML1B[Regression] 
 
 ML1A --&amp;gt; ML1A1[Binary Classification]
 ML1A --&amp;gt; ML1A2[Multi-class Classification]
 ML1A --&amp;gt; ML1A3[Imbalanced Learning]
 
 ML1B --&amp;gt; ML1B1[Linear Regression]
 ML1B --&amp;gt; ML1B2[Regularized Regression]
 ML1B --&amp;gt; ML1B3[Non-linear Regression]
 end
 
 subgraph UNSUP[ðŸ” Unsupervised Learning]
 ML2A[Clustering]
 ML2B[Dimensionality Reduction]
 
 ML2A --&amp;gt; ML2A1[Partitioning Methods]
 ML2A --&amp;gt; ML2A2[Hierarchical Methods]
 ML2A --&amp;gt; ML2A3[Density-based Methods]
 
 ML2B --&amp;gt; ML2B1[Linear Methods: PCA, LDA]
 ML2B --&amp;gt; ML2B2[Non-linear: t-SNE, UMAP]
 ML2B --&amp;gt; ML2B3[Manifold Learning]
 end
 
 SUP --&amp;gt; UNSUP

 style SUP fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
 style UNSUP fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h3 id="machine-learning-theory--evaluation">Machine Learning: Theory &amp;amp; Evaluation&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 50, &amp;#39;rankSpacing&amp;#39;: 60}}}%%
graph TD
 subgraph RL[ðŸŽ® Reinforcement Learning]
 ML3A[Policy Optimization&amp;lt;br/&amp;gt;Policy Gradient, Actor-Critic]
 ML3B[Value Functions&amp;lt;br/&amp;gt;Q-Learning, TD-Learning]
 ML3C[Exploration vs Exploitation&amp;lt;br/&amp;gt;Epsilon-Greedy, UCB]
 end
 
 subgraph THEORY[ðŸ“š Statistical Learning Theory]
 ML4A[PAC Learning&amp;lt;br/&amp;gt;Sample Complexity]
 ML4B[VC Dimension&amp;lt;br/&amp;gt;Shattering, Growth Function]
 ML4C[Generalization Bounds&amp;lt;br/&amp;gt;Rademacher Complexity]
 ML4D[Bias-Variance Tradeoff&amp;lt;br/&amp;gt;Model Complexity]
 end
 
 subgraph EVAL[ðŸ”„ Model Selection &amp;amp; Evaluation]
 ML5A[Cross-Validation&amp;lt;br/&amp;gt;K-Fold, Stratified]
 ML5B[Performance Metrics&amp;lt;br/&amp;gt;Accuracy, F1, AUC-ROC]
 ML5C[Hyperparameter Tuning&amp;lt;br/&amp;gt;Grid Search, Bayesian Opt]
 end

 RL --&amp;gt; THEORY
 THEORY --&amp;gt; EVAL

 style RL fill:#fff3e0,stroke:#ff9800,stroke-width:2px
 style THEORY fill:#fce4ec,stroke:#e91e63,stroke-width:2px
 style EVAL fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h3 id="deep-learning-neural-networks--training">Deep Learning: Neural Networks &amp;amp; Training&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 40, &amp;#39;rankSpacing&amp;#39;: 60}}}%%
graph LR
 subgraph NN[ðŸ§  Neural Networks &amp;amp; Training]
 DL1A[Perceptrons&amp;lt;br/&amp;gt;Single Layer] --&amp;gt; DL1B[Multi-Layer Perceptrons&amp;lt;br/&amp;gt;Hidden Layers]
 DL1B --&amp;gt; DL1C[Universal Approximation&amp;lt;br/&amp;gt;Function Approximation]
 DL1C --&amp;gt; DL1D[Network Architectures&amp;lt;br/&amp;gt;Feedforward, Skip Connections]
 end
 
 subgraph TRAIN[ðŸ”„ Training Process]
 DL2A[Chain Rule&amp;lt;br/&amp;gt;Gradient Computation] --&amp;gt; DL2B[Automatic Differentiation&amp;lt;br/&amp;gt;Computational Graphs]
 DL2B --&amp;gt; DL2C[Gradient Flow&amp;lt;br/&amp;gt;Vanishing/Exploding Gradients]
 DL2C --&amp;gt; DL2D[Implementation&amp;lt;br/&amp;gt;Memory Management]
 end
 
 subgraph OPT[ðŸ“ˆ Optimization Algorithms]
 DL5A[SGD Variants&amp;lt;br/&amp;gt;Momentum, Nesterov] --&amp;gt; DL5B[Adaptive Methods&amp;lt;br/&amp;gt;Adam, AdamW, RMSprop]
 DL5B --&amp;gt; DL5C[Learning Rate Scheduling&amp;lt;br/&amp;gt;Step Decay, Cosine]
 DL5C --&amp;gt; DL5D[Second-Order Methods&amp;lt;br/&amp;gt;Newton, L-BFGS]
 end
 
 NN --&amp;gt; TRAIN
 TRAIN --&amp;gt; OPT

 style NN fill:#ffebee,stroke:#f44336,stroke-width:2px
 style TRAIN fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
 style OPT fill:#fce4ec,stroke:#e91e63,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h3 id="deep-learning-functions--regularization">Deep Learning: Functions &amp;amp; Regularization&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 80, &amp;#39;rankSpacing&amp;#39;: 120}}}%%
graph LR
 subgraph ACT[âš¡ Activation Functions]
 direction TB
 DL3A[Classical&amp;lt;br/&amp;gt;Sigmoid, Tanh] --&amp;gt; DL3B[ReLU Family&amp;lt;br/&amp;gt;ReLU, Leaky ReLU]
 DL3B --&amp;gt; DL3C[Modern&amp;lt;br/&amp;gt;GELU, Mish]
 DL3C --&amp;gt; DL3D[Gating&amp;lt;br/&amp;gt;GLU, Swish Gate]
 end
 
 subgraph REG[ðŸ›¡ï¸ Regularization Techniques]
 direction TB
 DL4A[Dropout&amp;lt;br/&amp;gt;Random Neuron Dropping] --&amp;gt; DL4B[Batch Normalization&amp;lt;br/&amp;gt;Layer Normalization]
 DL4B --&amp;gt; DL4C[Weight Decay&amp;lt;br/&amp;gt;L1, L2 Regularization]
 DL4C --&amp;gt; DL4D[Early Stopping&amp;lt;br/&amp;gt;Validation Monitoring]
 DL4D --&amp;gt; DL4E[Data Augmentation&amp;lt;br/&amp;gt;Synthetic Data]
 end
 
 ACT ~~~ REG

 style ACT fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
 style REG fill:#fff3e0,stroke:#ff9800,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h3 id="deep-learning-advanced-architectures">Deep Learning: Advanced Architectures&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-mermaid" data-lang="mermaid">%%{init: {&amp;#39;flowchart&amp;#39;: {&amp;#39;nodeSpacing&amp;#39;: 40, &amp;#39;rankSpacing&amp;#39;: 60}}}%%
graph LR
 subgraph CNN[ðŸ–¼ï¸ Convolutional Networks]
 DL6A[Convolution Operation&amp;lt;br/&amp;gt;Filters, Feature Maps] --&amp;gt; DL6B[Pooling Layers&amp;lt;br/&amp;gt;Max, Average Pooling]
 DL6B --&amp;gt; DL6C[CNN Architectures&amp;lt;br/&amp;gt;LeNet, AlexNet, ResNet]
 DL6C --&amp;gt; DL6D[Advanced Techniques&amp;lt;br/&amp;gt;Dilated, Separable Conv]
 end
 
 subgraph RNN[ðŸ”— Recurrent Networks]
 DL7A[Vanilla RNN&amp;lt;br/&amp;gt;Hidden State] --&amp;gt; DL7B[LSTM Networks&amp;lt;br/&amp;gt;Gates &amp;amp; Cell State]
 DL7B --&amp;gt; DL7C[GRU Networks&amp;lt;br/&amp;gt;Simplified Architecture]
 DL7C --&amp;gt; DL7D[Sequence Modeling&amp;lt;br/&amp;gt;Many-to-Many, Seq2Seq]
 end
 
 subgraph EMB[ðŸ’« Embeddings]
 DL8A[Word Embeddings&amp;lt;br/&amp;gt;Word2Vec, GloVe] --&amp;gt; DL8B[Contextual Embeddings&amp;lt;br/&amp;gt;ELMo, BERT]
 DL8B --&amp;gt; DL8C[Positional Embeddings&amp;lt;br/&amp;gt;Absolute, Relative]
 DL8C --&amp;gt; DL8D[Embedding Techniques&amp;lt;br/&amp;gt;Negative Sampling]
 end
 
 CNN --&amp;gt; RNN
 RNN --&amp;gt; EMB

 style CNN fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px
 style RNN fill:#e0f2f1,stroke:#009688,stroke-width:2px
 style EMB fill:#fff8e1,stroke:#ffc107,stroke-width:2px
&lt;/code>&lt;/pre>&lt;h2 id="learning-path-recommendations">Learning Path Recommendations&lt;/h2>
&lt;h3 id="-quick-start-path-for-those-with-some-ml-background">ðŸš€ &lt;strong>Quick Start Path&lt;/strong> (For those with some ML background)&lt;/h3>
&lt;pre tabindex="0">&lt;code>Neural Networks â†’ Backpropagation â†’ Optimization â†’ Embeddings â†’ Advanced Topics
&lt;/code>&lt;/pre>&lt;h3 id="-comprehensive-path-from-ground-up">ðŸ“š &lt;strong>Comprehensive Path&lt;/strong> (From ground up)&lt;/h3>
&lt;pre tabindex="0">&lt;code>Linear Algebra â†’ Probability â†’ Optimization â†’ Supervised Learning â†’ Deep Learning â†’ Specialization
&lt;/code>&lt;/pre>&lt;h3 id="-research-oriented-path-for-advanced-learners">ðŸ”¬ &lt;strong>Research-Oriented Path&lt;/strong> (For advanced learners)&lt;/h3>
&lt;pre tabindex="0">&lt;code>Statistical Learning Theory â†’ Information Theory â†’ Advanced Optimization â†’ Modern Architectures
&lt;/code>&lt;/pre>&lt;h2 id="key-concepts-to-master">Key Concepts to Master&lt;/h2>
&lt;h3 id="mathematical-prerequisites">Mathematical Prerequisites&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Linear Algebra&lt;/strong>: Matrix operations, eigendecomposition, SVD&lt;/li>
&lt;li>&lt;strong>Probability&lt;/strong>: Distributions, Bayes&amp;rsquo; theorem, statistical inference&lt;/li>
&lt;li>&lt;strong>Optimization&lt;/strong>: Convex optimization, gradient descent, constrained optimization&lt;/li>
&lt;li>&lt;strong>Information Theory&lt;/strong>: Entropy, KL divergence, mutual information&lt;/li>
&lt;/ul>
&lt;h3 id="machine-learning-fundamentals">Machine Learning Fundamentals&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Supervised Learning&lt;/strong>: Classification, regression, model evaluation&lt;/li>
&lt;li>&lt;strong>Unsupervised Learning&lt;/strong>: Clustering, dimensionality reduction&lt;/li>
&lt;li>&lt;strong>Learning Theory&lt;/strong>: Bias-variance tradeoff, generalization bounds&lt;/li>
&lt;li>&lt;strong>Model Selection&lt;/strong>: Cross-validation, hyperparameter tuning&lt;/li>
&lt;/ul>
&lt;h3 id="deep-learning-essentials">Deep Learning Essentials&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Neural Networks&lt;/strong>: MLPs, universal approximation theorem&lt;/li>
&lt;li>&lt;strong>Training&lt;/strong>: Backpropagation, optimization algorithms&lt;/li>
&lt;li>&lt;strong>Regularization&lt;/strong>: Dropout, batch normalization, weight decay&lt;/li>
&lt;li>&lt;strong>Architectures&lt;/strong>: CNNs for vision, RNNs for sequences&lt;/li>
&lt;/ul>
&lt;h2 id="essential-resources-by-topic">Essential Resources by Topic&lt;/h2>
&lt;h3 id="-mathematics">ðŸ“ &lt;strong>Mathematics&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="http://localhost:1313/bookshelf/linear-done-right/">Linear Algebra Done Right&lt;/a>&lt;/strong> - Fundamental concepts â­&lt;/li>
&lt;li>&lt;strong>&lt;a href="http://localhost:1313/bookshelf/statistical-rethinking/">Statistical Rethinking&lt;/a>&lt;/strong> - Modern Bayesian approach â­&lt;/li>
&lt;li>&lt;strong>Elements of Information Theory&lt;/strong> - Cover &amp;amp; Thomas&lt;/li>
&lt;/ul>
&lt;h3 id="-machine-learning">ðŸ§  &lt;strong>Machine Learning&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="http://localhost:1313/bookshelf/elements/">The Elements of Statistical Learning&lt;/a>&lt;/strong> - Comprehensive reference â­&lt;/li>
&lt;li>&lt;strong>&lt;a href="http://localhost:1313/bookshelf/hands-on-ml/">Hands-On Machine Learning&lt;/a>&lt;/strong> - Practical implementation â­&lt;/li>
&lt;li>&lt;strong>Pattern Recognition and Machine Learning&lt;/strong> - Bishop&lt;/li>
&lt;/ul>
&lt;h3 id="-deep-learning">ðŸ”— &lt;strong>Deep Learning&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Deep Learning&lt;/strong> - Ian Goodfellow, Yoshua Bengio, Aaron Courville&lt;/li>
&lt;li>&lt;strong>Neural Networks and Deep Learning&lt;/strong> - Michael Nielsen (online)&lt;/li>
&lt;li>&lt;strong>Deep Learning Specialization&lt;/strong> - Andrew Ng (Coursera)&lt;/li>
&lt;/ul>
&lt;h2 id="common-pitfalls--tips">Common Pitfalls &amp;amp; Tips&lt;/h2>
&lt;h3 id="-mathematical-foundation-gaps">âš ï¸ &lt;strong>Mathematical Foundation Gaps&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Linear Algebra&lt;/strong>: Don&amp;rsquo;t skip eigenvalues - critical for PCA, attention&lt;/li>
&lt;li>&lt;strong>Probability&lt;/strong>: Master conditional probability - essential for Bayesian methods&lt;/li>
&lt;li>&lt;strong>Optimization&lt;/strong>: Understand convexity - affects convergence guarantees&lt;/li>
&lt;/ul>
&lt;h3 id="-learning-strategy">ðŸŽ¯ &lt;strong>Learning Strategy&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Theory + Practice&lt;/strong>: Balance mathematical understanding with implementation&lt;/li>
&lt;li>&lt;strong>Build Intuition&lt;/strong>: Visualize concepts before diving into equations&lt;/li>
&lt;li>&lt;strong>Progressive Complexity&lt;/strong>: Master simple cases before advanced variants&lt;/li>
&lt;/ul>
&lt;h2 id="connection-to-advanced-topics">Connection to Advanced Topics&lt;/h2>
&lt;p>These foundations directly enable understanding of:&lt;/p></description></item></channel></rss>