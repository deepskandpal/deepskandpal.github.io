<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><h1>Currently Reading</h1><p>This section contains papers that I am currently in the process of reading and analyzing. These represent active work-in-progress where I&rsquo;m building understanding, taking detailed notes, and working toward comprehensive summaries.</p><h2 id=reading-status>Reading Status</h2><p>Papers in this section are <strong>actively being studied</strong> and will show:</p><ul><li><strong>Initial impressions</strong> and key takeaways as I progress</li><li><strong>Section-by-section notes</strong> as I work through the paper</li><li><strong>Work-in-progress summaries</strong> that evolve over time</li><li><strong>Questions and areas for deeper investigation</strong></li></ul><h2 id=workflow>Workflow</h2><p>When I start reading a paper from the queue:</p><ol><li><strong>Move from to-read</strong> â†’ <strong>currently-reading</strong></li><li><strong>Set draft: false</strong> to make progress visible</li><li><strong>Add progressive notes</strong> as I work through sections</li><li><strong>Build comprehensive understanding</strong> over multiple sessions</li><li><strong>Move to read/</strong> when complete with full summary</li></ol><h2 id=current-focus>Current Focus</h2><p>The papers here represent my current learning priorities and research interests. They&rsquo;re visible to show what I&rsquo;m actively studying and the depth of analysis I bring to understanding academic work.</p><hr><section class=category-group><h2>Embeddings</h2><hr><ul class=posts-list><li class=post-item-simple><a href=/papershelf/currently-reading/vec2vec-embedding-translation/>Harnessing the Universal Geometry of Embeddings</a>
<time datetime=2025-07-22>(Jul 22, 2025)</time></li></ul></section><section class=category-group><h2>LLM</h2><hr><ul class=posts-list><li class=post-item-simple><a href=/papershelf/currently-reading/deepseek-r1/>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a>
<time datetime=2025-07-22>(Jul 22, 2025)</time></li><li class=post-item-simple><a href=/papershelf/currently-reading/kimi-k2-tech-report/>Kimi-K2 Technical Report</a>
<time datetime=2025-07-22>(Jul 22, 2025)</time></li></ul></section><section class=category-group><h2>RAG</h2><hr><ul class=posts-list><li class=post-item-simple><a href=/papershelf/currently-reading/infinit-retrevial-attention/>Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing</a>
<time datetime=2024-04-27>(Apr 27, 2024)</time></li></ul></section></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>