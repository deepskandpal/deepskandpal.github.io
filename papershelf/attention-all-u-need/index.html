<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><a href=#paper><strong>Paper: &ldquo;Attention Is All You Need&rdquo;</strong></a></li><li><a href=#abstract><strong>Abstract:</strong></a></li><li><a href=#1-introduction><strong>1. Introduction</strong></a></li><li><a href=#2-background><strong>2. Background</strong></a></li></ul></li><li><a href=#3-model-architecture><strong>3. Model Architecture</strong></a><ul><li><a href=#4-why-self-attention><strong>4. Why Self-Attention</strong></a></li><li><a href=#5-training><strong>5. Training</strong></a></li><li><a href=#6-results><strong>6. Results</strong></a></li><li><a href=#7-conclusion><strong>7. Conclusion</strong></a></li></ul></li></ul></nav></nav></aside><article class=paper-single><div class=paper-categories>Filed under:
<span class=category-pill>Transformers</span></div><h1>Attention Is All You Need</h1><span class=reading-time><em>11 min read</em></span><div class=paper-content><h2 id=paper><strong>Paper: &ldquo;Attention Is All You Need&rdquo;</strong></h2><p><strong>Title Itself:</strong> The title is a bold claim. It signals a departure from the dominant RNN/LSTM and CNN paradigms for sequence modeling at the time. It suggests that the attention mechanism, previously often an auxiliary component, could be the <em>core</em> and <em>sufficient</em> mechanism.</p><h2 id=abstract><strong>Abstract:</strong></h2><ul><li><strong>Introduces the Transformer:</strong> &ldquo;based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.&rdquo;<ul><li><strong>Goal:</strong> Superior quality, more parallelizable, less training time for sequence transduction (e.g., machine translation).</li></ul></li><li><strong>Key Results:</strong> State-of-the-art (SOTA) on WMT 2014 English-to-German (28.4 BLEU, >2 BLEU improvement) and English-to-French translation tasks, with significantly reduced training costs (e.g., 3.5 days on 8 P100 GPUs for a big model).</li><li><strong>Generalization:</strong> Shows it can be applied to other tasks like English constituency parsing.</li></ul><h2 id=1-introduction><strong>1. Introduction</strong></h2><ul><li><strong>Context:</strong> Recurrent Neural Networks (RNNs), especially LSTMs and Gated Recurrent Units (GRUs), were the SOTA for sequence modeling (language modeling, machine translation).<ul><li><strong>Problem with RNNs:</strong> They are inherently sequential. <code>h_t</code> depends on <code>h_{t-1}</code>. This &ldquo;sequential nature precludes parallelization within training examples.&rdquo; This becomes a bottleneck for longer sequences, as memory constraints limit batching across examples.</li></ul></li><li><strong>Attention as a Solution (Previously):</strong> Attention mechanisms were already being used with RNNs, allowing modeling of dependencies regardless of distance. However, they were typically used <em>in conjunction</em> with a recurrent network.</li><li><strong>The Paper&rsquo;s Proposal:</strong> &ldquo;In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.&rdquo;<ul><li><strong>Key Advantages Claimed:</strong> More parallelization, can reach new SOTA in translation quality with less training time.</li></ul></li></ul><h2 id=2-background><strong>2. Background</strong></h2><ul><li><strong>Motivation:</strong> Reduce sequential computation.</li><li><strong>Existing Alternatives to RNNs:</strong><ul><li><strong>Extended Neural GPU, ByteNet, ConvS2S:</strong> Used CNNs as basic building blocks, computing hidden representations in parallel.<ul><li><strong>Limitation:</strong> The number of operations to relate signals from two arbitrary positions grows with distance (linearly for ConvS2S, logarithmically for ByteNet). This makes learning distant dependencies harder.</li><li><strong>Transformer&rsquo;s Advantage:</strong> Reduces this to a <em>constant</em> number of operations (via self-attention), albeit at the cost of reduced &ldquo;effective resolution&rdquo; due to averaging attention-weighted positions (counteracted by Multi-Head Attention).</li></ul></li></ul></li><li><strong>Self-Attention (Intra-Attention):</strong> Defined as an attention mechanism relating different positions of a <em>single sequence</em> to compute a representation of that sequence. It had been used before successfully.</li><li><strong>End-to-End Memory Networks:</strong> Based on recurrent attention, showed good performance on simple QA.</li><li><strong>Transformer&rsquo;s Novelty Claim:</strong> &ldquo;the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.&rdquo;</li></ul><h1 id=3-model-architecture><strong>3. Model Architecture</strong></h1><p>This is the core of the paper.</p><ul><li><p><strong>Overall Structure:</strong> Most competitive neural sequence transduction models have an <strong>encoder-decoder structure</strong>.</p><ul><li><strong>Encoder:</strong> Maps an input sequence <code>(x1, ..., xn)</code> to continuous representations <code>z = (z1, ..., zn)</code>.</li><li><strong>Decoder:</strong> Given <code>z</code>, generates an output sequence <code>(y1, ..., ym)</code> one element at a time (auto-regressive). At each step, it consumes previously generated symbols as additional input.</li></ul></li><li><p><strong>Transformer&rsquo;s Specifics (Figure 1 is key here):</strong> Uses stacked self-attention and point-wise, fully connected layers for both encoder and decoder.</p><p><strong>3.1 Encoder and Decoder Stacks</strong></p><ul><li><strong>What it&rsquo;s ultimately trying to achieve:</strong> To build deep representations of the input (encoder) and to generate the output sequence token by token using these representations and what has been generated so far (decoder).</li><li><strong>Encoder:</strong><ul><li>A stack of <code>N=6</code> identical layers.</li><li>Each layer has two sub-layers:<ol><li><strong>Multi-Head Self-Attention Mechanism:</strong> Allows each position in the encoder to attend to all positions in the <em>previous layer of the encoder</em>. This is how the encoder builds contextualized representations of the input sequence.</li><li><strong>Simple, Position-wise Fully Connected Feed-Forward Network (FFN):</strong> Processes each position&rsquo;s representation independently.</li></ol></li><li><strong>Residual Connections & Layer Normalization:</strong> <code>LayerNorm(x + Sublayer(x))</code> around each of the two sub-layers.</li><li><code>d_model = 512</code> (dimensionality of embeddings and hidden layers).</li></ul></li><li><strong>Decoder:</strong><ul><li>Also a stack of <code>N=6</code> identical layers.</li><li>Each layer has <em>three</em> sub-layers:<ol><li><strong>Masked Multi-Head Self-Attention Mechanism:</strong> Allows each position in the decoder to attend to all positions <em>up to and including that position</em> in the decoder&rsquo;s input (i.e., previously generated output tokens). The &ldquo;masking&rdquo; is crucial to ensure the auto-regressive property (prediction for position <code>i</code> can only depend on known outputs at positions less than <code>i</code>).</li><li><strong>Multi-Head Encoder-Decoder Attention (Cross-Attention):</strong><ul><li><strong>Queries (Q)</strong> come from the <em>previous decoder sub-layer</em>.</li><li><strong>Keys (K) and Values (V)</strong> come from the <em>output of the encoder stack</em>.</li><li>This allows every position in the decoder to attend to all positions in the <em>input sequence</em>. This is how the decoder uses the information from the source sentence to generate the target sentence. This is the bridge between encoder and decoder.</li></ul></li><li><strong>Position-wise Fully Connected Feed-Forward Network (FFN):</strong> Same as in the encoder.</li></ol></li><li><strong>Residual Connections & Layer Normalization:</strong> Applied around each of the three sub-layers.</li><li>The modification to the self-attention sub-layer (masking) is highlighted.</li></ul></li></ul><p><strong>3.2 Attention</strong></p><ul><li><p><strong>What it&rsquo;s ultimately trying to achieve:</strong> To define a flexible mechanism for relating different parts of sequences, allowing the model to weigh the importance of different elements when computing a representation.</p></li><li><p><strong>General Definition:</strong> &ldquo;mapping a query and a set of key-value pairs to an output.&rdquo; Query, keys, values, output are all vectors. Output is a weighted sum of values, where weights are computed by a compatibility function of the query with corresponding keys.</p><p><strong>3.2.1 Scaled Dot-Product Attention (Figure 2, left)</strong></p><ul><li><strong>Inputs:</strong> Queries <code>Q</code>, Keys <code>K</code> (both of dimension <code>d_k</code>), Values <code>V</code> (of dimension <code>d_v</code>).</li><li><strong>Calculation:</strong> <code>Attention(Q, K, V) = softmax( (QK^T) / sqrt(d_k) ) * V</code> (Equation 1)</li><li><strong>Scaling <code>1/sqrt(d_k)</code>:</strong> Important! For large <code>d_k</code>, dot products <code>QK^T</code> can grow large, pushing softmax into regions with very small gradients. Scaling helps counteract this.</li><li><strong>Comparison to Additive Attention:</strong> Dot-product attention is faster and more space-efficient (uses optimized matrix multiplication). Additive attention (using an FFN to compute compatibility) might be better for very large <code>d_k</code> if scaling isn&rsquo;t used, but with scaling, dot-product is preferred.</li></ul><p><strong>3.2.2 Multi-Head Attention (Figure 2, right)</strong></p><ul><li><strong>What it&rsquo;s ultimately trying to achieve:</strong> To allow the model to <em>jointly</em> attend to information from different representation subspaces at different positions. A single attention head might be forced to average over different aspects; multiple heads can specialize.</li><li><strong>Mechanism:</strong><ol><li>Linearly project Q, K, V <code>h</code> times with different, learned linear projections to <code>d_k</code>, <code>d_k</code>, <code>d_v</code> dimensions respectively. (Here, <code>d_k = d_v = d_model / h = 64</code> for <code>h=8</code> heads and <code>d_model=512</code>).</li><li>Perform scaled dot-product attention in parallel for each of these <code>h</code> projected versions of Q, K, V, yielding <code>h</code> output vectors (head_i).</li><li>Concatenate these <code>h</code> output vectors.</li><li>Linearly project the concatenated vector again with another learned weight matrix <code>W^O</code> to get the final output.
<code>MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O</code>
<code>where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</code></li></ol></li><li><code>h=8</code> parallel attention layers (heads) are used. <code>d_k = d_v = d_model/h = 512/8 = 64</code>.</li><li>Total computational cost is similar to single-head attention with full dimensionality.</li></ul><p><strong>3.2.3 Applications of Attention in their Model</strong></p><ul><li><strong>Encoder-Decoder Attention (Cross-Attention):</strong> Q from previous decoder layer, K and V from encoder output. (This is the bridge).</li><li><strong>Encoder Self-Attention:</strong> Q, K, V all come from the output of the previous layer <em>in the encoder</em>. Each position attends to all positions in the previous encoder layer.</li><li><strong>Decoder Masked Self-Attention:</strong> Q, K, V all come from the output of the previous layer <em>in the decoder</em> (representing the generated sequence so far). Masking ensures leftward information flow only, preserving auto-regressive property.</li></ul></li></ul><p><strong>3.3 Position-wise Feed-Forward Networks (FFN)</strong></p><ul><li><strong>What it&rsquo;s ultimately trying to achieve:</strong> To provide additional non-linear processing capacity to each position&rsquo;s representation after attention has been applied.</li><li>Applied to each position separately and identically.</li><li>Two linear transformations with a ReLU activation in between: <code>FFN(x) = max(0, xW1 + b1)W2 + b2</code> (Equation 2).</li><li><code>d_model = 512</code> (input/output dimensionality).</li><li>Inner-layer dimensionality <code>d_ff = 2048</code>. (This expansion and contraction is a common pattern).</li></ul><p><strong>3.4 Embeddings and Softmax</strong></p><ul><li>Learned embeddings convert input/output tokens to vectors of dimension <code>d_model</code>.</li><li>A standard learned linear transformation and softmax function convert the decoder output to predicted next-token probabilities.</li><li><strong>Weight Sharing:</strong> The weight matrix between the two embedding layers (input and output token embeddings) and the pre-softmax linear transformation are shared.</li><li>In embedding layers, weights are multiplied by <code>sqrt(d_model)</code>.</li></ul><p><strong>3.5 Positional Encoding (PE)</strong></p><ul><li><strong>What it&rsquo;s ultimately trying to achieve:</strong> To inject information about the relative or absolute position of tokens, since the model has no recurrence or convolution.</li><li>Added to input embeddings at the bottoms of encoder and decoder stacks.</li><li>Same dimension <code>d_model</code> as embeddings.</li><li><strong>Sinusoidal Functions (Fixed, Not Learned):</strong>
<code>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</code>
<code>PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</code>
where <code>pos</code> is position, <code>i</code> is dimension.<ul><li>Wavelengths form a geometric progression from <code>2π</code> to <code>10000 * 2π</code>.</li><li>Hypothesized to allow the model to learn to attend by relative positions, as <code>PE_{pos+k}</code> can be represented as a linear function of <code>PE_{pos}</code>.</li><li>Chosen because it might allow extrapolation to longer sequences. Experimentally, it performed similarly to learned PEs.</li></ul></li></ul></li></ul><h2 id=4-why-self-attention><strong>4. Why Self-Attention</strong></h2><ul><li><strong>Desiderata for sequence mapping:</strong><ol><li>Total computational complexity per layer.</li><li>Amount of computation that can be parallelized (minimum number of sequential operations).</li><li>Path length between long-range dependencies.</li></ol></li><li><strong>Comparison (Table 1 is key):</strong><ul><li><strong>Self-Attention:</strong><ul><li>Complexity: <code>O(n^2 * d)</code> (n=sequence length, d=representation dimension). Quadratic in <code>n</code>.</li><li>Sequential Ops: <code>O(1)</code> (highly parallelizable).</li><li>Max Path Length: <code>O(1)</code> (direct connection between any two tokens).</li></ul></li><li><strong>Recurrent (RNN):</strong><ul><li>Complexity: <code>O(n * d^2)</code>.</li><li>Sequential Ops: <code>O(n)</code> (inherently sequential).</li><li>Max Path Length: <code>O(n)</code>.</li></ul></li><li><strong>Convolutional (CNN):</strong><ul><li>Complexity: <code>O(k * n * d^2)</code> (k=kernel size).</li><li>Sequential Ops: <code>O(1)</code> (if k &lt; n).</li><li>Max Path Length: <code>O(n/k)</code> for contiguous kernels, <code>O(log_k(n))</code> for dilated.</li></ul></li></ul></li><li><strong>Self-Attention Advantages:</strong><ul><li>Constant path length for dependencies.</li><li>Highly parallelizable per layer.</li><li>When <code>n &lt; d</code> (common for sentence representations like word-piece embeddings), self-attention is faster than RNNs.</li></ul></li><li><strong>Addressing <code>n^2</code> Complexity for Very Long Sequences:</strong><ul><li><strong>Restricted Self-Attention:</strong> Consider only a neighborhood <code>r</code> around the output position. Max path <code>O(n/r)</code>. (This hints at later work like Longformer, BigBird).</li></ul></li><li><strong>Interpretability:</strong> Self-attention distributions can be inspected, often showing behavior related to syntactic/semantic structure (Appendix figures show this).</li></ul><h2 id=5-training><strong>5. Training</strong></h2><pre><code>**5.1 Training Data and Batching**
*   WMT 2014 English-German (4.5M sentence pairs), English-French (36M pairs).
*   Byte-Pair Encoding (BPE) for shared source-target vocabulary.
*   Batched sentence pairs by approximate sequence length. Each batch ~25000 source/target tokens.

**5.2 Hardware and Schedule**
*   8 NVIDIA P100 GPUs. Base models: 0.4s/step, 100k steps (12 hours). Big models: 1.0s/step, 300k steps (3.5 days).

**5.3 Optimizer**
*   Adam optimizer with `β1 = 0.9, β2 = 0.98, ε = 10^-9`.
*   Learning rate varied according to a formula: increase linearly for `warmup_steps` (e.g., 4000), then decrease proportionally to `1/sqrt(step_num)`. (Equation 3). This is a common learning rate schedule for Transformers.

**5.4 Regularization**
*   **Residual Dropout (`P_drop = 0.1` for base):** Applied to the output of each sub-layer *before* it's added to the sub-layer input (residual) and normalized. Also applied to sums of embeddings and PEs.
*   **Label Smoothing (`ε_ls = 0.1`):** During training, instead of using hard 0/1 targets for cross-entropy, the target for the correct class is slightly less than 1 (e.g., 0.9) and the remaining probability mass (0.1) is distributed among incorrect classes. This makes the model &quot;less sure&quot; but improves accuracy and BLEU, and helps with perplexity calibration.
</code></pre><h2 id=6-results><strong>6. Results</strong></h2><pre><code>**6.1 Machine Translation**
*   **Table 2 is key:** Transformer (big) achieves new SOTA on En-De (28.4 BLEU) and En-Fr (41.8 BLEU for their re-run, which was a new single-model SOTA then). Outperforms previous SOTA ensembles with much less training cost.
*   Base model also surpasses all previous models at a fraction of cost.
*   Beam search (size 4) and length penalty used for inference.

**6.2 Model Variations (Table 3 is key - Ablation Studies)**
*   **Number of Attention Heads (A):** Single head worse than 8. Too many (16, 32 with fixed computation) also hurts. 8 heads with `d_k=64` was good.
*   **Key Size `d_k` (B):** Reducing `d_k` (attention key/value dimension per head) hurts performance, suggesting dot-product might not be sophisticated enough if `d_k` is too small.
*   **Model Size (C):** Bigger models (larger `d_model`, `d_ff`, more layers - not explicitly varied here but implied by &quot;big&quot; model) are better.
*   **Dropout (D):** Very helpful in avoiding overfitting.
*   **Positional Encoding (E):** Learned PEs gave nearly identical results to sinusoidal ones.

**6.3 English Constituency Parsing**
*   Shows Transformer can generalize to other tasks beyond translation.
*   Performs surprisingly well even with limited task-specific tuning. Outperforms most previous models on WSJ dataset, except a highly tuned Recurrent Neural Network Grammar.
</code></pre><h2 id=7-conclusion><strong>7. Conclusion</strong></h2><ul><li>Reiterates the proposal: Transformer, first sequence transduction model based entirely on attention, replacing RNNs/CNNs.</li><li>Faster to train for MT, achieved SOTA.</li><li>Excited about future of attention-based models, plans to extend to other tasks and modalities, and investigate restricted attention for large inputs/outputs.</li></ul></div><hr><div class=paper-reference><p><strong>Read the original paper:</strong>
<a href=https://arxiv.org/abs/1706.03762 target=_blank rel="noopener noreferrer">Attention Is All You Need</a></p><p class=paper-disclaimer><em>The content presented here is a collection of my personal notes and explanations based on the paper. This is by no means an exhaustive explanation, and I strongly encourage you to read the actual paper for a comprehensive understanding.</em></p></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>