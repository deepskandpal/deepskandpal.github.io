<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><ul><li><a href=#paper><strong>Paper: &ldquo;Training Compute-Optimal Large Language Models&rdquo;</strong></a></li><li><a href=#abstract--key-contributions><strong>Abstract & Key Contributions:</strong></a></li><li><a href=#1-main-findings><strong>1. Main Findings</strong></a></li><li><a href=#2-methodology><strong>2. Methodology</strong></a></li><li><a href=#3-chinchilla-model-results><strong>3. Chinchilla Model Results</strong></a></li><li><a href=#4-dataset-and-training><strong>4. Dataset and Training</strong></a></li><li><a href=#5-impact-and-implications><strong>5. Impact and Implications</strong></a></li><li><a href=#6-key-takeaways><strong>6. Key Takeaways</strong></a></li></ul></li></ul></nav></nav></aside><article class=paper-single><div class=paper-categories>Filed under:
<span class=category-pill>AI</span></div><h1>Training Compute-Optimal Large Language Models</h1><span class=reading-time><em>4 min read</em></span><div class=paper-content><h2 id=paper><strong>Paper: &ldquo;Training Compute-Optimal Large Language Models&rdquo;</strong></h2><p><strong>Authors:</strong> Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre</p><p><strong>Also Known As:</strong> The &ldquo;Chinchilla&rdquo; paper (named after their 70B parameter model)</p><h2 id=abstract--key-contributions><strong>Abstract & Key Contributions:</strong></h2><p>This seminal paper from DeepMind challenges the conventional wisdom about how to scale large language models optimally. The key findings are:</p><ul><li><p><strong>Scaling Laws Revision:</strong> Previous work (notably GPT-3/Kaplan et al.) suggested that model size should grow much faster than training data. This paper shows that <strong>model size and training data should be scaled equally</strong> for compute-optimal training.</p></li><li><p><strong>Chinchilla Model:</strong> They train Chinchilla, a 70B parameter model on 1.4 trillion tokens, which substantially outperforms larger models like GPT-3 (175B) and Gopher (280B) that were trained on fewer tokens.</p></li><li><p><strong>Compute Budget Allocation:</strong> For a given compute budget, it&rsquo;s better to train a smaller model on more data rather than a larger model on less data.</p></li></ul><h2 id=1-main-findings><strong>1. Main Findings</strong></h2><p><strong>Optimal Scaling Relationship:</strong></p><ul><li><strong>Model parameters (N)</strong> and <strong>training tokens (D)</strong> should scale proportionally: N ∝ D</li><li>For every doubling of compute budget, both model size and training data should increase by approximately √2 (41%)</li></ul><p><strong>Practical Implications:</strong></p><ul><li>Most large models (GPT-3, Gopher, etc.) are <strong>over-parameterized and under-trained</strong></li><li>Training smaller models on more data is more compute-efficient</li><li>A 70B model trained on 1.4T tokens outperforms 175B+ models trained on 300B tokens</li></ul><h2 id=2-methodology><strong>2. Methodology</strong></h2><p><strong>Three Approaches to Derive Scaling Laws:</strong></p><ol><li><strong>Approach 1:</strong> Fix model sizes, vary training FLOPs by changing number of training tokens</li><li><strong>Approach 2:</strong> IsoFLOP profiles - train models of different sizes for same compute budget</li><li><strong>Approach 3:</strong> Parametric fits to loss as function of model size and data</li></ol><p><strong>Key Insight:</strong> All three approaches converge on the same scaling relationship, providing strong evidence for the N ∝ D scaling law.</p><h2 id=3-chinchilla-model-results><strong>3. Chinchilla Model Results</strong></h2><p><strong>Performance Highlights:</strong></p><ul><li><strong>70B parameters, 1.4T training tokens</strong></li><li>Outperforms GPT-3 (175B) on virtually all benchmarks</li><li>Uses 4× less compute for inference than Gopher (280B)</li><li>Achieves state-of-the-art results on many language understanding tasks</li></ul><p><strong>Training Details:</strong></p><ul><li>AdamW optimizer with β₁ = 0.9, β₂ = 0.95</li><li>Learning rate: 2.0 × 10⁻⁴ with cosine decay</li><li>Batch size: 1.5M tokens</li><li>Training time: Approximately 5 days on 2048 TPU v3 cores</li></ul><h2 id=4-dataset-and-training><strong>4. Dataset and Training</strong></h2><p><strong>MassiveText Dataset:</strong></p><ul><li><strong>1.4 trillion tokens</strong> (much larger than GPT-3&rsquo;s ~300B tokens)</li><li>Web pages, books, news articles, Wikipedia, GitHub code</li><li>Careful filtering and deduplication</li><li>Multiple languages but English-heavy</li></ul><p><strong>Training Efficiency:</strong></p><ul><li>Gradient checkpointing for memory efficiency</li><li>Mixed precision training (bfloat16)</li><li>Sequence length: 2048 tokens</li><li>Vocabulary: 32,000 SentencePiece tokens</li></ul><h2 id=5-impact-and-implications><strong>5. Impact and Implications</strong></h2><p><strong>For the Field:</strong></p><ul><li>Shifted focus from &ldquo;bigger models&rdquo; to &ldquo;better data scaling&rdquo;</li><li>Influenced subsequent models (PaLM, LaMDA, etc.) to use more training data</li><li>Established new best practices for compute allocation in LLM training</li></ul><p><strong>Scaling Law Formula:</strong>
For compute-optimal training:</p><pre tabindex=0><code>N_opt ∝ C^a  where a ≈ 0.50
D_opt ∝ C^b  where b ≈ 0.50
</code></pre><p>Where C is the compute budget, N is model parameters, D is training tokens.</p><p><strong>Practical Rule:</strong>
For every 10× increase in compute:</p><ul><li>Increase model size by ~3.2×</li><li>Increase training data by ~3.2×</li></ul><h2 id=6-key-takeaways><strong>6. Key Takeaways</strong></h2><ol><li><strong>Data is as important as parameters</strong> for language model performance</li><li><strong>Most existing large models are undertrained</strong> relative to their parameter count</li><li><strong>Compute-optimal scaling</strong> requires balanced growth of both model size and training data</li><li><strong>Smaller, well-trained models</strong> can outperform larger, undertrained ones</li><li><strong>Training costs dominate</strong> inference costs for most use cases</li></ol><p>This paper fundamentally changed how the field thinks about scaling language models and directly influenced the training strategies for models like GPT-4, PaLM-2, and Claude.</p></div><hr><div class=paper-reference><p><strong>Read the original paper:</strong>
<a href=https://arxiv.org/pdf/2203.15556 target=_blank rel="noopener noreferrer">Training Compute-Optimal Large Language Models</a></p><p class=paper-disclaimer><em>The content presented here is a collection of my personal notes and explanations based on the paper. This is by no means an exhaustive explanation, and I strongly encourage you to read the actual paper for a comprehensive understanding.</em></p></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>