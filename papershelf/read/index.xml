<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Read Papers on 404EngineerNotFound</title><link>https://deepskandpal.github.io/papershelf/read/</link><description>Recent content in Read Papers on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 08 Jun 2025 01:01:23 +0530</lastBuildDate><atom:link href="https://deepskandpal.github.io/papershelf/read/index.xml" rel="self" type="application/rss+xml"/><item><title>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title><link>https://deepskandpal.github.io/papershelf/read/illusion-of-thinking/</link><pubDate>Sun, 08 Jun 2025 01:01:23 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/read/illusion-of-thinking/</guid><description>&lt;p>At its heart, &amp;ldquo;The Illusion of Thinking,&amp;rdquo; is about trying to genuinely understand how well these new &amp;ldquo;Large Reasoning Models&amp;rdquo; (LRMs) actually &lt;em>reason&lt;/em>. You&amp;rsquo;ve probably heard about models that show their &amp;ldquo;thinking steps&amp;rdquo; before giving an answer, like with Chain-of-Thought. They often do better on benchmarks, which is exciting. But we felt that just looking at the final answer on standard math or coding tests wasn&amp;rsquo;t telling the whole story.&lt;/p></description></item><item><title>The Matrix Calculus You Need For Deep Learning</title><link>https://deepskandpal.github.io/papershelf/read/maths/</link><pubDate>Sun, 27 Apr 2025 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/read/maths/</guid><description>&lt;h1 id="the-big-picture-goal">&lt;strong>The Big Picture Goal:&lt;/strong>&lt;/h1>
&lt;p>We want our machine learning models to make good predictions. To do this, we define a &lt;strong>loss function&lt;/strong> (or cost function) that measures how &amp;ldquo;bad&amp;rdquo; our model&amp;rsquo;s predictions are compared to the true values.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Training = Minimizing Loss:&lt;/strong> Training a model means finding the model parameters (like weights &lt;code>w&lt;/code> and biases &lt;code>b&lt;/code>) that make this loss function as small as possible.&lt;/li>
&lt;li>&lt;strong>Calculus as a Tool for Minimization:&lt;/strong> Derivatives (and their extensions like gradients) tell us the &lt;strong>rate of change&lt;/strong> or the &lt;strong>slope&lt;/strong> of a function.
&lt;ul>
&lt;li>If we know the slope of our loss function with respect to our model parameters, we know which &amp;ldquo;direction&amp;rdquo; to tweak those parameters to &lt;em>decrease&lt;/em> the loss. This is the essence of &lt;strong>Gradient Descent&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>So, all this calculus is ultimately about finding an efficient way to &amp;ldquo;walk downhill&amp;rdquo; on our loss function surface to find the parameter values that give the lowest error.&lt;/p></description></item><item><title>One-Minute Video Generation with Test-Time Training</title><link>https://deepskandpal.github.io/papershelf/read/ttt-layer-for-video-generation/</link><pubDate>Mon, 15 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/read/ttt-layer-for-video-generation/</guid><description>&lt;p>&lt;a href="https://test-time-training.github.io/video-dit/">Website&lt;/a>&lt;/p>
&lt;h2 id="tldr">TLDR;&lt;/h2>
&lt;p>They tackled long video generation by replacing expensive global attention with efficient local attention, and bridging the gaps between local segments using novel TTT layers. These TTT layers act like RNNs but have a much smarter, adaptive hidden state (a neural network that learns on-the-fly during generation). This allows them to capture long-range dependencies and complex dynamics better than traditional RNNs, leading to more coherent minute-long videos, albeit with some remaining artifacts and efficiency challenges.&lt;/p></description></item></channel></rss>