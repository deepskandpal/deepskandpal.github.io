<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><a href=#our-approach-puzzles><strong>Our Approach: Puzzles!</strong></a></li><li><a href=#what-we-found-the><strong>What We Found (The &ldquo;Illusion of Thinking&rdquo; Part):</strong></a></li><li><a href=#so-what><strong>So, What&rsquo;s the &ldquo;Illusion&rdquo;?</strong></a></li><li><a href=#why-does-this-matter><strong>Why Does This Matter?</strong></a></li></ul></nav></nav></aside><article class=paper-single><div class=paper-categories>Filed under:
<span class=category-pill>AI</span></div><h1>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</h1><span class=reading-time><em>5 min read</em></span><div class=paper-content><p>At its heart, &ldquo;The Illusion of Thinking,&rdquo; is about trying to genuinely understand how well these new &ldquo;Large Reasoning Models&rdquo; (LRMs) actually <em>reason</em>. You&rsquo;ve probably heard about models that show their &ldquo;thinking steps&rdquo; before giving an answer, like with Chain-of-Thought. They often do better on benchmarks, which is exciting. But we felt that just looking at the final answer on standard math or coding tests wasn&rsquo;t telling the whole story.</p><h1 id=the-big-problem-we-saw><strong>The Big Problem We Saw:</strong></h1><p>Imagine you&rsquo;re teaching a student. If they just spit out the right answer, you don&rsquo;t know if they truly understood the <em>method</em> or just got lucky, or maybe even saw the answer somewhere before (which is a bit like &ldquo;data contamination&rdquo; in AI benchmarks). We wanted to look <em>inside</em> the &ldquo;thinking process&rdquo; and also test these models in a way where we could be sure they hadn&rsquo;t just memorized the solutions.</p><h2 id=our-approach-puzzles><strong>Our Approach: Puzzles!</strong></h2><p>Instead of standard benchmarks, we turned to classic puzzles – things like the <strong>Tower of Hanoi</strong>, <strong>Checkers Jumping</strong>, <strong>River Crossing</strong>, and <strong>Blocks World</strong> (you can see these in Figure 3 on page 6). Why puzzles?</p><ol><li><strong>Controllable Complexity:</strong> We can make these puzzles a little harder or a little easier just by changing a small thing (like adding one more disk in Tower of Hanoi). This lets us see exactly when and how the models start to struggle.</li><li><strong>No Cheating:</strong> It&rsquo;s highly unlikely these models have seen the exact step-by-step solutions to, say, a 10-disk Tower of Hanoi problem in their training data, especially in the specific format we used.</li><li><strong>Clear Rules, Clear Steps:</strong> The logic is all there. They don&rsquo;t need outside knowledge, just the rules we give them. This helps us see if they can follow algorithmic steps.</li><li><strong>We Can Check Their Work:</strong> We can use simulators to see if every single step in their &ldquo;thinking&rdquo; is correct, not just the final answer. (See the top part of Figure 1 on page 2 for how we analyze both the &ldquo;thoughts&rdquo; and the final answer).</li></ol><h2 id=what-we-found-the><strong>What We Found (The &ldquo;Illusion of Thinking&rdquo; Part):</strong></h2><ol><li><p><strong>There&rsquo;s a Wall – Complete Accuracy Collapse:</strong> As we made the puzzles harder, every LRM we tested eventually hit a wall. Beyond a certain complexity, their accuracy just plummeted to zero. They couldn&rsquo;t solve it at all, no matter how many tries we gave them (within a generous token budget). (You can see this in Figure 4 on page 7, where accuracy drops off a cliff for all puzzles as complexity increases).</p></li><li><p><strong>They Stop Trying So Hard (Counter-intuitive Scaling):</strong> This was really interesting. You&rsquo;d think that as a problem gets harder, the model would &ldquo;think&rdquo; more (i.e., use more computational steps or &ldquo;tokens&rdquo;). They do, up to a point. But right around where they start to fail catastrophically, they actually start <em>reducing</em> their reasoning effort, even if they have plenty of &ldquo;thinking time&rdquo; (token budget) left. It’s like they get overwhelmed and just give up prematurely. (Look at Figure 1, bottom middle graph on page 2, or Figure 6 on page 9 – the token usage goes up, then down).</p></li><li><p><strong>The Three Regimes (When &ldquo;Thinking&rdquo; Helps, and When It Doesn&rsquo;t):</strong></p><ul><li><strong>Easy Problems:</strong> Surprisingly, standard LLMs (without the fancy &ldquo;thinking&rdquo; steps) often did <em>better</em> or were more efficient than the LRMs. It seems the extra &ldquo;thinking&rdquo; by LRMs was sometimes unnecessary or even confusing for simple tasks. (See Figure 5 on page 8, the &ldquo;low complexity&rdquo; section).</li><li><strong>Medium Problems:</strong> This is where LRMs shone. The extra &ldquo;thinking&rdquo; steps helped them solve problems that the standard LLMs struggled with. (Figure 5, &ldquo;medium complexity&rdquo;).</li><li><strong>Hard Problems:</strong> Both the LRMs and the standard LLMs eventually failed completely. The &ldquo;thinking&rdquo; could delay the failure, but it couldn&rsquo;t prevent it. (Figure 5, &ldquo;high complexity&rdquo;).</li></ul></li><li><p><strong>Peeking Inside the &ldquo;Thoughts&rdquo; (Figure 1, bottom right, and Figure 7a on page 10):</strong></p><ul><li><strong>Overthinking Simple Stuff:</strong> For easy puzzles, we saw models find the correct solution path early in their &ldquo;thoughts&rdquo; but then continue to explore, sometimes making mistakes later or just wasting effort.</li><li><strong>Struggling with Harder Stuff:</strong> For more complex problems (but before they completely collapsed), they&rsquo;d often explore a lot of wrong paths before stumbling upon the correct one, if they found it at all.</li><li><strong>Giving Up:</strong> For the really hard problems, they just couldn&rsquo;t find a correct solution path, no matter how long their &ldquo;thoughts&rdquo; were.</li></ul></li><li><p><strong>They Can&rsquo;t Always Follow Directions:</strong> Even when we gave the models the <em>exact algorithm</em> to solve a puzzle (like Tower of Hanoi, see Figure 8a and 8b on page 11), they still failed around the same complexity level! This tells us they have trouble just executing a known series of logical steps, not just <em>finding</em> the steps.</p></li><li><p><strong>Inconsistent Reasoning:</strong> A model might be pretty good at Tower of Hanoi up to a certain number of disks, which involves many steps. But then it might fail miserably at a River Crossing problem that actually requires fewer steps but has different types of constraints (Figure 8c and 8d on page 11). This suggests they aren&rsquo;t learning a general, abstract reasoning skill but are perhaps more sensitive to the specific structure or &ldquo;feel&rdquo; of a problem, maybe due to what they saw in training.</p></li></ol><h2 id=so-what><strong>So, What&rsquo;s the &ldquo;Illusion&rdquo;?</strong></h2><p>The &ldquo;illusion&rdquo; is that while these models <em>appear</em> to be thinking and reasoning like humans when they produce step-by-step traces, their underlying capabilities have hard limits. Their reasoning isn&rsquo;t as robust or generalizable as it might seem from their performance on some benchmarks. It seems they are very good at pattern matching and interpolating from things they&rsquo;ve seen, but when faced with problems that require them to systematically apply rules in novel or increasingly complex ways (algorithmic complexity), they break down.</p><h2 id=why-does-this-matter><strong>Why Does This Matter?</strong></h2><p>Our findings suggest we need to be cautious about how much we trust the &ldquo;reasoning&rdquo; of current LRMs, especially for complex, high-stakes tasks. It also points towards areas where these models need to improve, like developing more robust algorithmic execution and generalization capabilities.</p></div><hr><div class=paper-reference><p><strong>Read the original paper:</strong>
<a href=https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf target=_blank rel="noopener noreferrer">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></p><p class=paper-disclaimer><em>The content presented here is a collection of my personal notes and explanations based on the paper. This is by no means an exhaustive explanation, and I strongly encourage you to read the actual paper for a comprehensive understanding.</em></p></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>