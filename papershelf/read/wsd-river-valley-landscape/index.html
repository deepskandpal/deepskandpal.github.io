<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><a href=#paper><strong>Paper: &ldquo;Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective&rdquo;</strong></a></li><li><a href=#abstract--key-contributions><strong>Abstract & Key Contributions:</strong></a></li><li><a href=#1-the-wsd-learning-rate-schedule><strong>1. The WSD Learning Rate Schedule</strong></a></li><li><a href=#2-the-river-valley-loss-landscape><strong>2. The River Valley Loss Landscape</strong></a></li><li><a href=#3-theoretical-analysis><strong>3. Theoretical Analysis</strong></a></li><li><a href=#4-wsd-s-simplified-algorithm><strong>4. WSD-S: Simplified Algorithm</strong></a></li><li><a href=#5-empirical-results><strong>5. Empirical Results</strong></a></li><li><a href=#6-key-insights-and-implications><strong>6. Key Insights and Implications</strong></a></li><li><a href=#7-technical-details><strong>7. Technical Details</strong></a></li><li><a href=#8-broader-impact><strong>8. Broader Impact</strong></a></li></ul></nav></nav></aside><article class=paper-single><div class=paper-categories>Filed under:
<span class=category-pill>Deep Learning</span></div><h1>Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective</h1><span class=reading-time><em>4 min read</em></span><div class=paper-content><h2 id=paper><strong>Paper: &ldquo;Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective&rdquo;</strong></h2><p><strong>Authors:</strong> Kaiyue Wen, David Hall, Zhiyuan Li, Jason Wang, Percy Liang, Tengyu Ma (Stanford University & TTIC)</p><h2 id=abstract--key-contributions><strong>Abstract & Key Contributions:</strong></h2><p>This paper provides a theoretical explanation for the effectiveness of the Warmup-Stable-Decay (WSD) learning rate schedule in language model pretraining. The key contributions include:</p><ul><li><strong>River Valley Loss Landscape Theory:</strong> Introduces a novel conceptual framework where the loss landscape resembles a deep valley with a river at its bottom</li><li><strong>WSD Mechanism Explanation:</strong> Shows why WSD&rsquo;s non-traditional loss curve (high loss during stable phase, sharp drop during decay) leads to superior performance</li><li><strong>WSD-S Algorithm:</strong> Proposes WSD-Simplified, an improved variant that reuses checkpoints and maintains a single main branch</li></ul><h2 id=1-the-wsd-learning-rate-schedule><strong>1. The WSD Learning Rate Schedule</strong></h2><p><strong>Traditional Problem:</strong><br>Cosine learning rate schedules require pre-determining a fixed compute budget, making it difficult to adapt to additional compute or data.</p><p><strong>WSD Solution:</strong></p><ul><li><strong>Warmup Phase:</strong> Standard learning rate warmup</li><li><strong>Stable Phase:</strong> Maintains constant learning rate indefinitely (main branch)</li><li><strong>Decay Phase:</strong> Branches out with rapidly decaying learning rate to create checkpoints</li></ul><p><strong>Key Advantage:</strong> Can continue training without pre-specified compute budget and obtain multiple checkpoints from a single run.</p><h2 id=2-the-river-valley-loss-landscape><strong>2. The River Valley Loss Landscape</strong></h2><p><strong>Core Metaphor:</strong>
The loss landscape resembles a deep valley with steep hillsides and a winding river at the bottom.</p><p><strong>Two-Phase Dynamics:</strong></p><p><strong>Stable Phase (High Learning Rate):</strong></p><ul><li>Iterate oscillates between steep hillsides due to large learning rate</li><li>Makes swift progress along the river direction</li><li>Loss appears elevated due to oscillations</li><li><strong>Key Insight:</strong> Progress occurs along the &ldquo;river&rdquo; (main optimization direction)</li></ul><p><strong>Decay Phase (Rapidly Decreasing Learning Rate):</strong></p><ul><li>Minimizes oscillations by reducing learning rate</li><li>Moves iterate closer to the river bottom</li><li>Reveals true optimization progress with sharp loss drop</li><li><strong>Key Insight:</strong> Progress occurs in &ldquo;mountain&rdquo; directions (orthogonal to river)</li></ul><h2 id=3-theoretical-analysis><strong>3. Theoretical Analysis</strong></h2><p><strong>River Valley Landscape Definition:</strong></p><ul><li><strong>River Direction:</strong> Low curvature, allows fast progress with high learning rate</li><li><strong>Mountain Directions:</strong> High curvature, require low learning rate for stability</li><li><strong>Mathematical Formulation:</strong> Uses eigenvalue decomposition of Hessian to characterize landscape</li></ul><p><strong>Key Theoretical Results:</strong></p><ol><li><strong>Progress Decomposition:</strong> Total progress = River progress + Mountain progress</li><li><strong>Phase Effectiveness:</strong> Stable phase optimizes river direction, decay phase optimizes mountain directions</li><li><strong>Landscape Emergence:</strong> Shows river valley structure naturally emerges from pretraining on simple bigram datasets</li></ol><h2 id=4-wsd-s-simplified-algorithm><strong>4. WSD-S: Simplified Algorithm</strong></h2><p><strong>Key Innovation:</strong> Instead of maintaining multiple branches, WSD-S:</p><ul><li>Resumes training from previous decay phase checkpoints</li><li>Uses constant learning rate from decayed checkpoints</li><li>Maintains single main branch while reusing previous decay benefits</li></ul><p><strong>Algorithm Steps:</strong></p><ol><li>Train with constant learning rate (stable phase)</li><li>Apply decay phase to create checkpoint</li><li>Resume from decayed checkpoint with constant learning rate</li><li>Repeat process for multiple compute budgets</li></ol><h2 id=5-empirical-results><strong>5. Empirical Results</strong></h2><p><strong>Model Scales Tested:</strong> 0.1B to 1.2B parameters (LLaMA and GPT-2 architectures)</p><p><strong>Performance Comparisons:</strong></p><ul><li><strong>WSD-S vs WSD:</strong> Consistently outperforms original WSD</li><li><strong>WSD-S vs Cosine:</strong> Superior final performance with more flexibility</li><li><strong>WSD-S vs Cyclic-Cosine:</strong> Better checkpoint quality across compute budgets</li></ul><p><strong>Training Efficiency:</strong></p><ul><li>Single run produces multiple high-quality checkpoints</li><li>No need to restart training for different compute budgets</li><li>Maintains performance while simplifying the branching mechanism</li></ul><h2 id=6-key-insights-and-implications><strong>6. Key Insights and Implications</strong></h2><p><strong>For Learning Rate Scheduling:</strong></p><ol><li><strong>Dual-Purpose Design:</strong> High learning rate for river progress, low learning rate for mountain progress</li><li><strong>Landscape-Aware Optimization:</strong> Understanding loss geometry informs better scheduling</li><li><strong>Flexible Compute Budgets:</strong> Enables adaptive training without restarting</li></ol><p><strong>For Understanding Optimization:</strong></p><ol><li><strong>Non-Monotonic Progress:</strong> Elevated loss doesn&rsquo;t necessarily indicate poor optimization</li><li><strong>Directional Progress:</strong> Different learning rates optimize different aspects of the landscape</li><li><strong>Checkpoint Reusability:</strong> Decayed checkpoints provide good initialization points</li></ol><p><strong>Practical Applications:</strong></p><ul><li><strong>Continual Learning:</strong> Resume training from any checkpoint efficiently</li><li><strong>Resource Planning:</strong> Adapt to available compute without waste</li><li><strong>Model Development:</strong> Test multiple configurations from single training run</li></ul><h2 id=7-technical-details><strong>7. Technical Details</strong></h2><p><strong>River Valley Conditions:</strong></p><ul><li>Large eigenvalue gaps in loss Hessian</li><li>Low-dimensional &ldquo;river&rdquo; subspace with small eigenvalues</li><li>High-dimensional &ldquo;mountain&rdquo; subspace with large eigenvalues</li></ul><p><strong>Theoretical Guarantees:</strong></p><ul><li>Convergence analysis for both stable and decay phases</li><li>Progress bounds in river and mountain directions</li><li>Landscape emergence conditions for realistic datasets</li></ul><h2 id=8-broader-impact><strong>8. Broader Impact</strong></h2><p>This work bridges the gap between empirical optimization practices and theoretical understanding, providing:</p><ul><li><strong>Conceptual Foundation:</strong> River valley metaphor for understanding LLM training dynamics</li><li><strong>Practical Algorithm:</strong> WSD-S for efficient multi-checkpoint training</li><li><strong>Future Directions:</strong> Framework for analyzing other learning rate schedules</li></ul><p>The paper demonstrates how geometric intuition about loss landscapes can lead to both theoretical insights and practical algorithmic improvements in large language model training.</p></div><hr><div class=paper-reference><p><strong>Read the original paper:</strong>
<a href=https://arxiv.org/pdf/2410.05192 target=_blank rel="noopener noreferrer">Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective</a></p><p class=paper-disclaimer><em>The content presented here is a collection of my personal notes and explanations based on the paper. This is by no means an exhaustive explanation, and I strongly encourage you to read the actual paper for a comprehensive understanding.</em></p></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>