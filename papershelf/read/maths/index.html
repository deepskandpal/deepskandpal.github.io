<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.31/dist/flexsearch.bundle.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js></script></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul><div class=search-container><input type=search id=search-input placeholder=Search...>
<i class="fa fa-search"></i></div></nav><div id=search-results-container><ul id=search-results></ul></div></header><main><div class=single-content-wrapper><aside class=article-sidebar><nav><h4>On this page</h4><nav id=TableOfContents><ul><li><a href=#section-41-generalization-of-the-jacobian-page-7-of-the-parr--howard-paper><strong>Section 4.1: Generalization of the Jacobian</strong> (page 7 of the Parr & Howard paper).</a></li><li><a href=#section-42-derivatives-of-vector-element-wise-binary-operators-pages-9-11><strong>Section 4.2: Derivatives of vector element-wise binary operators</strong> (Pages 9-11).</a></li><li><a href=#section-43-derivatives-involving-scalar-expansion-pages-11-12><strong>Section 4.3: Derivatives involving scalar expansion</strong> (Pages 11-12).</a></li><li><a href=#section-44-vector-sum-reduction-pages-12-13><strong>Section 4.4: Vector sum reduction</strong> (Pages 12-13).</a></li><li><a href=#section-45-the-chain-rules><strong>Section 4.5: The Chain Rules</strong></a></li></ul></nav></nav></aside><article class=paper-single><div class=paper-categories>Filed under:
<span class=category-pill>Maths</span></div><h1>The Matrix Calculus You Need For Deep Learning</h1><span class=reading-time><em>53 min read</em></span><div class=paper-content><h1 id=the-big-picture-goal><strong>The Big Picture Goal:</strong></h1><p>We want our machine learning models to make good predictions. To do this, we define a <strong>loss function</strong> (or cost function) that measures how &ldquo;bad&rdquo; our model&rsquo;s predictions are compared to the true values.</p><ul><li><strong>Training = Minimizing Loss:</strong> Training a model means finding the model parameters (like weights <code>w</code> and biases <code>b</code>) that make this loss function as small as possible.</li><li><strong>Calculus as a Tool for Minimization:</strong> Derivatives (and their extensions like gradients) tell us the <strong>rate of change</strong> or the <strong>slope</strong> of a function.<ul><li>If we know the slope of our loss function with respect to our model parameters, we know which &ldquo;direction&rdquo; to tweak those parameters to <em>decrease</em> the loss. This is the essence of <strong>Gradient Descent</strong>.</li></ul></li></ul><p>So, all this calculus is ultimately about finding an efficient way to &ldquo;walk downhill&rdquo; on our loss function surface to find the parameter values that give the lowest error.</p><p>Now, let&rsquo;s look at the paper.</p><h1 id=section-1---introduction><strong>(Section 1 - Introduction)</strong></h1><ul><li><p>The paper acknowledges that derivatives are critical for ML, especially deep neural networks, which are trained by optimizing a loss function.</p></li><li><p>It mentions that while modern libraries have &ldquo;automatic differentiation,&rdquo; understanding the underlying matrix calculus helps to &ldquo;grok academic papers&rdquo; and understand what&rsquo;s happening &ldquo;under the hood.&rdquo;</p></li><li><p>It gives an example of a single neuron&rsquo;s activation:</p><ul><li><code>z(x) = w · x + b</code> (affine function: dot product of weights <code>w</code> with input <code>x</code>, plus a bias <code>b</code>).</li><li>Followed by an activation function, e.g., <code>max(0, z(x))</code> (ReLU).</li></ul></li><li><p><strong>Key statement on training:</strong> &ldquo;Training this neuron means choosing weights <code>w</code> and bias <code>b</code> so that we get the desired output for all N inputs <code>x</code>. To do that, we minimize a loss function&mldr; All of those [gradient descent methods] require the partial derivative (the gradient) of activation(x) with respect to the model parameters <code>w</code> and <code>b</code>. Our goal is to gradually tweak <code>w</code> and <code>b</code> so that the overall loss function keeps getting smaller&mldr;&rdquo;</p><ul><li><em>What this is ultimately trying to achieve:</em> Use the gradient (which calculus helps us find) to iteratively adjust <code>w</code> and <code>b</code> to reduce the model&rsquo;s error.</li></ul></li><li><p>The introduction to the paper&rsquo;s goal: &ldquo;&mldr;rederive and rediscover some key matrix calculus rules in an effort to explain them&mldr; There aren’t dozens of new rules to learn; just a couple of key concepts.&rdquo; This is encouraging!</p></li></ul><p>Let&rsquo;s proceed to <strong>Section 2: Review: Scalar derivative rules</strong> (Page 4 of the paper). This should be a good refresher. Are you ready to look at these basic rules? We can take it slow and make sure each one makes sense.</p><p>Alright, let&rsquo;s dive into <strong>Section 2: Review: Scalar derivative rules</strong> from the Parr & Howard paper (page 4, extending to the table on page 5).</p><p>This section is a quick refresher of the basic derivative rules you likely encountered in a first calculus course. The core idea of a derivative for a function of a single variable, <code>f(x)</code>, is to find its <strong>instantaneous rate of change</strong> or the <strong>slope of the tangent line</strong> at any given point <code>x</code>.</p><p><em>What we are ultimately trying to achieve with derivatives (in an ML context):</em> We want to know how a small change in an input (like a model parameter <code>θ</code> or an input feature <code>x</code>) will affect the output of a function (like the loss function <code>J(θ)</code> or an activation function <code>a(z)</code>). If the derivative is large and positive, a small increase in the input leads to a large increase in the output. If it&rsquo;s negative, an increase in input leads to a decrease in output. If it&rsquo;s zero, the function is momentarily flat at that point (which could indicate a minimum, maximum, or a plateau).</p><p>Let&rsquo;s look at the rules presented in the table on page 5:</p><ol><li><p><strong>Constant Rule:</strong></p><ul><li>Rule: <code>f(x) = c</code> (where <code>c</code> is a constant)</li><li>Scalar derivative notation: <code>d/dx (c) = 0</code></li><li>Example: <code>d/dx (99) = 0</code></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> A constant function doesn&rsquo;t change, no matter what <code>x</code> is. So, its rate of change (slope) is always zero. Think of a flat horizontal line – its slope is 0.</li></ul></li><li><p><strong>Multiplication by Constant Rule:</strong></p><ul><li>Rule: <code>f(x) = cf(x)</code> (actually, it should be <code>g(x) = c * f(x)</code>)</li><li>Scalar derivative notation: <code>d/dx (c * f(x)) = c * (df/dx)</code></li><li>Example: <code>d/dx (3x) = 3 * (d/dx (x)) = 3 * 1 = 3</code></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> If you scale a function by a constant, its rate of change (slope) at any point is also scaled by that same constant. The shape of the change is the same, just amplified or shrunk.</li></ul></li><li><p><strong>Power Rule:</strong></p><ul><li>Rule: <code>f(x) = xⁿ</code></li><li>Scalar derivative notation: <code>d/dx (xⁿ) = nxⁿ⁻¹</code></li><li>Example: <code>d/dx (x³) = 3x³⁻¹ = 3x²</code></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> This rule tells us how functions involving powers of <code>x</code> change. For <code>x³</code>, the slope isn&rsquo;t constant; it changes depending on <code>x</code>. At <code>x=1</code>, slope is 3. At <code>x=2</code>, slope is <code>3*(2)² = 12</code>.</li></ul></li><li><p><strong>Sum Rule:</strong></p><ul><li>Rule: <code>h(x) = f(x) + g(x)</code></li><li>Scalar derivative notation: <code>d/dx (f(x) + g(x)) = (df/dx) + (dg/dx)</code></li><li>Example: <code>d/dx (x² + 3x) = (d/dx x²) + (d/dx 3x) = 2x + 3</code></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> The rate of change of a sum of functions is the sum of their individual rates of change. If one part is changing quickly and another slowly, the sum changes according to the combined effect.</li></ul></li><li><p><strong>Difference Rule:</strong></p><ul><li>Rule: <code>h(x) = f(x) - g(x)</code></li><li>Scalar derivative notation: <code>d/dx (f(x) - g(x)) = (df/dx) - (dg/dx)</code></li><li>Example: <code>d/dx (x² - 3x) = (d/dx x²) - (d/dx 3x) = 2x - 3</code></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> Similar to the sum rule, the rate of change of a difference is the difference of the rates of change.</li></ul></li><li><p><strong>Product Rule:</strong></p><ul><li>Rule: <code>h(x) = f(x)g(x)</code></li><li>Scalar derivative notation: <code>d/dx (f(x)g(x)) = f(x)(dg/dx) + (df/dx)g(x)</code></li><li>Example: <code>d/dx (x² * x)</code> (where <code>f(x)=x²</code>, <code>g(x)=x</code>)<ul><li><code>= x² * (d/dx x) + (d/dx x²) * x</code></li><li><code>= x² * 1 + 2x * x = x² + 2x² = 3x²</code></li><li>(This is consistent with <code>d/dx (x³) = 3x²</code>)</li></ul></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> When two functions are multiplied, a change in <code>x</code> affects both. The overall rate of change depends on how <code>g(x)</code> changes (weighted by <code>f(x)</code>) <em>plus</em> how <code>f(x)</code> changes (weighted by <code>g(x)</code>).</li></ul></li><li><p><strong>Chain Rule (for single variable nested functions):</strong></p><ul><li>Rule: <code>h(x) = f(g(x))</code> (a function <code>f</code> applied to the output of another function <code>g</code>)</li><li>Scalar derivative notation: <code>d/dx f(g(x)) = (df/du) * (du/dx)</code>, where <code>u = g(x)</code>.<ul><li>The paper writes this as <code>df(u)/du * du/dx</code>.</li></ul></li><li>Example: <code>d/dx ln(x²)</code><ul><li>Let <code>u = g(x) = x²</code>. Then <code>f(u) = ln(u)</code>.</li><li><code>du/dx = d/dx (x²) = 2x</code>.</li><li><code>df/du = d/du (ln(u)) = 1/u</code>.</li><li>So, <code>d/dx ln(x²) = (1/u) * (2x) = (1/x²) * (2x) = 2x/x² = 2/x</code>.</li></ul></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> It tells us how to find the rate of change of a composite function. How much does the <em>outer function</em> <code>f</code> change with respect to its input <code>u</code> (<code>df/du</code>), multiplied by how much that <em>inner input <code>u</code></em> changes with respect to the original variable <code>x</code> (<code>du/dx</code>). It &ldquo;chains&rdquo; the rates of change together. This rule is <em>extremely</em> important for neural networks because they are essentially deeply nested functions, and backpropagation is an application of the chain rule.</li></ul></li></ol><p>The paper also mentions (bottom of page 4, top of page 5):</p><ul><li>The notation <code>d/dx</code> can be thought of as an <strong>operator</strong> that takes a function <code>f(x)</code> and gives you its derivative <code>df/dx</code>.</li><li>This operator view is helpful because the operator is <strong>distributive</strong> (<code>d/dx (f+g) = df/dx + dg/dx</code>) and lets you <strong>pull out constants</strong> (<code>d/dx (cf) = c * df/dx</code>). This simplifies taking derivatives of complex expressions, like their example:
<code>d/dx (9(x + x²)) = 9 * d/dx (x + x²) = 9 * (d/dx x + d/dx x²) = 9 * (1 + 2x) = 9 + 18x</code>.</li></ul><p>This review of scalar derivative rules is foundational. For deep learning and training models with Gradient Descent, we are constantly asking: &ldquo;If I wiggle this weight/bias a tiny bit, how much does my loss function change?&rdquo; Derivatives give us the answer to that question.</p><hr><h1 id=section-3-introduction-to-vector-calculus-and-partial-derivatives><strong>Section 3: Introduction to vector calculus and partial derivatives</strong></h1><p>This section transitions us from functions of a single variable (<code>f(x)</code>) to functions of <em>multiple</em> variables (e.g., <code>f(x, y)</code>). Neural network layers often have many inputs (features) and many parameters (weights, biases), so we need to be able to talk about how the output changes when we wiggle <em>one specific input or parameter</em>, while keeping others constant.</p><ul><li><p><strong>Functions of Multiple Parameters:</strong></p><ul><li>Instead of <code>f(x)</code>, we now consider <code>f(x, y)</code>. For example, the product <code>xy</code>.</li><li>If we want to know its derivative, the question is &ldquo;derivative with respect to what?&rdquo; With respect to <code>x</code>, or with respect to <code>y</code>? The change in <code>xy</code> will be different depending on which variable we &ldquo;wiggle.&rdquo;</li></ul></li><li><p><strong>Partial Derivatives:</strong></p><ul><li>When we have a function of multiple variables, we compute <strong>partial derivatives</strong>.</li><li>The notation changes from <code>d/dx</code> to <code>∂/∂x</code> (using a stylized &rsquo;d&rsquo;, often called &ldquo;del&rdquo; or &ldquo;curly d&rdquo;).</li><li><strong><code>∂f(x,y)/∂x</code></strong>: This is the partial derivative of <code>f(x,y)</code> with respect to <code>x</code>.<ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> It tells us the rate of change of the function <code>f</code> if we change <code>x</code> by a tiny amount, <em>while holding all other variables (in this case, <code>y</code>) constant</em>. You treat <code>y</code> as if it were just a number.</li></ul></li><li><strong><code>∂f(x,y)/∂y</code></strong>: This is the partial derivative of <code>f(x,y)</code> with respect to <code>y</code>.<ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> It tells us the rate of change of <code>f</code> if we change <code>y</code> by a tiny amount, <em>while holding <code>x</code> constant</em>.</li></ul></li></ul></li><li><p><strong>Example from the paper:</strong> <code>f(x, y) = 3x²y</code></p><ul><li><strong>Partial derivative with respect to <code>x</code> (<code>∂f/∂x</code>):</strong><ul><li>Treat <code>y</code> (and <code>3</code>) as constants.</li><li><code>∂/∂x (3x²y) = 3y * (∂/∂x x²) = 3y * (2x) = 6yx</code>.</li><li><em>Intuition:</em> If <code>y</code> is fixed, say <code>y=2</code>, then <code>f(x,2) = 6x²</code>. The derivative <code>d/dx (6x²) = 12x</code>. Our partial derivative <code>6yx</code> gives <code>6(2)x = 12x</code>. It matches!</li></ul></li><li><strong>Partial derivative with respect to <code>y</code> (<code>∂f/∂y</code>):</strong><ul><li>Treat <code>x</code> (and <code>3x²</code>) as constants.</li><li><code>∂/∂y (3x²y) = 3x² * (∂/∂y y) = 3x² * 1 = 3x²</code>.</li><li><em>Intuition:</em> If <code>x</code> is fixed, say <code>x=1</code>, then <code>f(1,y) = 3y</code>. The derivative <code>d/dy (3y) = 3</code>. Our partial derivative <code>3x²</code> gives <code>3(1)² = 3</code>. It matches!</li></ul></li></ul></li><li><p><strong>The Gradient (∇f):</strong></p><ul><li>For a function of multiple variables, like <code>f(x,y)</code>, we can organize all its partial derivatives into a <strong>vector</strong>. This vector is called the <strong>gradient</strong> of <code>f</code>, denoted by <code>∇f</code> (nabla f).</li><li>For <code>f(x,y) = 3x²y</code>, the paper writes:
<code>∇f(x, y) = [∂f/∂x, ∂f/∂y] = [6yx, 3x²]</code></li><li><em>What the gradient is ultimately trying to achieve:</em> The gradient vector <code>∇f</code> at a specific point <code>(x₀, y₀)</code> points in the direction of the <strong>steepest ascent</strong> (fastest increase) of the function <code>f</code> at that point. Its magnitude <code>||∇f||</code> tells you the rate of increase in that direction.</li><li>In machine learning, our loss function <code>J(θ)</code> depends on many parameters <code>θ₁, θ₂, ..., θₙ</code>. The gradient <code>∇J(θ)</code> will be a vector <code>[∂J/∂θ₁, ∂J/∂θ₂, ..., ∂J/∂θₙ]</code>.</li><li><strong>Gradient Descent</strong> uses this: it calculates <code>∇J(θ)</code> and then takes a step in the <em>opposite</em> direction (<code>-∇J(θ)</code>) to go &ldquo;downhill&rdquo; and reduce the loss.</li></ul></li></ul><p>This section essentially extends the concept of a derivative (rate of change/slope) from single-variable functions to multi-variable functions by considering the rate of change with respect to each variable <em>individually</em>, holding others constant. The gradient then bundles all these partial rates of change into a single vector that gives us the &ldquo;overall&rdquo; direction of steepest increase.</p><hr><h1 id=section-4-matrix-calculus><strong>Section 4: Matrix calculus</strong></h1><p>This is where things get a bit more generalized. So far:</p><ul><li>Scalar derivative: <code>df/dx</code> (function of one variable, derivative is a scalar).</li><li>Vector calculus (gradient): <code>∇f(x,y) = [∂f/∂x, ∂f/∂y]</code> (function of multiple scalar variables <code>x, y</code>, output is a scalar <code>f</code>, gradient is a vector of partials).</li></ul><p><strong>Matrix calculus</strong> deals with derivatives when:</p><ol><li>The input is a vector (e.g., <code>x = [x₁, x₂]</code>).</li><li>The output can also be a vector (e.g., <code>y = [f₁(x), f₂(x)]</code>).</li></ol><ul><li><strong>From One Function to Many Functions:</strong><ul><li>The paper keeps <code>f(x, y) = 3x²y</code> from the previous section.</li><li>It introduces another function <code>g(x, y) = 2x + y⁸</code>.</li><li>We can find the gradient of <code>g</code> just like we did for <code>f</code>:<ul><li><code>∂g/∂x = ∂/∂x (2x + y⁸) = 2</code> (treating <code>y⁸</code> as constant)</li><li><code>∂g/∂y = ∂/∂y (2x + y⁸) = 8y⁷</code> (treating <code>2x</code> as constant)</li><li>So, <code>∇g(x, y) = [2, 8y⁷]</code>.</li></ul></li></ul></li></ul><figure><img src=/papershelf/assets/maths/jacobian-matrix.png alt=image width=750></figure><ul><li><p><strong>The Jacobian Matrix (J):</strong></p><ul><li><p>When we have multiple functions (say, <code>f</code> and <code>g</code>), each of which can depend on multiple input variables (say, <code>x</code> and <code>y</code>), we can organize their gradients into a <strong>matrix</strong>. This matrix is called the <strong>Jacobian matrix</strong> (or just the Jacobian).</p></li><li><p>If we have two functions <code>f(x,y)</code> and <code>g(x,y)</code>, the Jacobian <code>J</code> (using the paper&rsquo;s &ldquo;numerator layout&rdquo; where gradients are rows) is formed by stacking their gradient vectors:
<code>J = [ ∇f(x, y) ] = [ ∂f/∂x ∂f/∂y ]</code>
<code>[ ∇g(x, y) ] [ ∂g/∂x ∂g/∂y ]</code></p><p>So for <code>f(x,y) = 3x²y</code> and <code>g(x,y) = 2x + y⁸</code>, the Jacobian is:
<code>J = [ 6yx 3x² ]</code>
<code>[ 2 8y⁷ ]</code></p></li><li><p><em>What the Jacobian is ultimately trying to achieve:</em> It captures all the first-order partial derivatives of a vector-valued function (a function that outputs a vector of values) with respect to a vector of input variables.</p><ul><li>Each row <code>i</code> tells you how function <code>fᵢ</code> changes with respect to each input variable.</li><li>Each column <code>j</code> tells you how all output functions change with respect to input variable <code>xⱼ</code>.</li><li>Essentially, if you have <code>m</code> functions and <code>n</code> input variables, the Jacobian is an <code>m x n</code> matrix where the entry <code>Jᵢⱼ = ∂fᵢ/∂xⱼ</code>.</li></ul></li></ul></li><li><p><strong>Layouts (Numerator vs. Denominator):</strong></p><ul><li>The paper notes: &ldquo;there are multiple ways to represent the Jacobian.&rdquo; They use &ldquo;numerator layout&rdquo; (where <code>∇f</code> is a row vector, and the Jacobian stacks these row vectors).</li><li>Other sources might use &ldquo;denominator layout,&rdquo; which is essentially the <em>transpose</em> of the numerator layout Jacobian. It&rsquo;s important to be aware of this when consulting different texts, as the shapes of matrices in equations will change accordingly. The paper sticks to numerator layout.
Example given (page 7, top): Transpose of our Jacobian above is <code>[ 6yx 2 ]</code>
<code>[ 3x² 8y⁷ ]</code></li></ul></li></ul><p>This Jacobian matrix is a fundamental tool when dealing with transformations between vector spaces in calculus, and it plays a key role in the chain rule for vector functions, which is vital for backpropagation in neural networks with multiple layers and multiple neurons per layer.</p><p>The main idea here is organization:</p><ul><li><strong>Gradient:</strong> Organizes partial derivatives of a <em>single scalar-output function</em> with respect to its <em>multiple inputs</em> into a vector.</li><li><strong>Jacobian:</strong> Organizes gradients of <em>multiple scalar-output functions</em> (or equivalently, partial derivatives of a <em>single vector-output function</em>) into a matrix.</li></ul><p>Great! Understanding the Jacobian as an organized collection of partial derivatives is a key step.</p><h2 id=section-41-generalization-of-the-jacobian-page-7-of-the-parr--howard-paper><strong>Section 4.1: Generalization of the Jacobian</strong> (page 7 of the Parr & Howard paper).</h2><p>This section formalizes the Jacobian for a general case where you have a vector of functions, <code>y = f(x)</code>, where <code>x</code> is an input vector and <code>y</code> is an output vector.</p><ul><li><p><strong>Vector Notation:</strong></p><ul><li>Input vector <code>x</code> has <code>n</code> elements: <code>x = [x₁, x₂, ..., xₙ]ᵀ</code> (they assume column vectors by default).</li><li>Output vector <code>y</code> has <code>m</code> scalar-valued functions: <code>y = [f₁(x), f₂(x), ..., fₘ(x)]ᵀ</code>. Each <code>fᵢ(x)</code> takes the whole vector <code>x</code> as input and returns a scalar.</li></ul></li><li><p><strong>The Jacobian Matrix <code>∂y/∂x</code>:</strong>
The paper defines the Jacobian as a stack of <code>m</code> gradients (one for each output function <code>fᵢ</code>). Since they use numerator layout (where each gradient <code>∇fᵢ(x)</code> is a row vector of partials with respect to the components of <code>x</code>), the Jacobian becomes an <code>m x n</code> matrix:</p><p><code>∂y/∂x = [ ∇f₁(x) ] = [ ∂f₁/∂x₁ ∂f₁/∂x₂ ... ∂f₁/∂xₙ ]</code>
<code>[ ∇f₂(x) ] [ ∂f₂/∂x₁ ∂f₂/∂x₂ ... ∂f₂/∂xₙ ]</code>
<code>[ ... ] [ ... ... ... ... ]</code>
<code>[ ∇fₘ(x) ] [ ∂fₘ/∂x₁ ∂fₘ/∂x₂ ... ∂fₘ/∂xₙ ]</code></p><ul><li><strong>Rows:</strong> Each row <code>i</code> of the Jacobian corresponds to one output function <code>fᵢ(x)</code> and contains all the partial derivatives of <em>that specific output function</em> with respect to <em>each of the input variables</em> <code>x₁, x₂, ..., xₙ</code>. So, row <code>i</code> is <code>∇fᵢ(x)</code>.</li><li><strong>Columns:</strong> Each column <code>j</code> of the Jacobian corresponds to one input variable <code>xⱼ</code> and contains all the partial derivatives of <em>each of the output functions</em> <code>f₁, f₂, ..., fₘ</code> with respect to <em>that specific input variable <code>xⱼ</code></em>.</li></ul><p><em>What the Jacobian <code>∂y/∂x</code> is ultimately trying to achieve:</em> It describes how each component of the output vector <code>y</code> changes in response to a small change in each component of the input vector <code>x</code>. It&rsquo;s a complete map of all the first-order sensitivities between the inputs and outputs.</p></li><li><p><strong>Visualizing Jacobian Shapes (diagram on page 8):</strong>
This is a handy diagram to remember the dimensions based on the nature of input <code>x</code> and output <code>f</code> (or <code>y</code>):</p><ul><li><strong>Scalar input <code>x</code>, Scalar output <code>f</code>:</strong> Derivative <code>∂f/∂x</code> is a scalar. (This is standard Calc 1).</li><li><strong>Vector input <code>x</code>, Scalar output <code>f</code>:</strong> Derivative <code>∂f/∂x</code> (which is <code>∇f</code>) is a row vector (1 x n) in numerator layout.</li><li><strong>Scalar input <code>x</code>, Vector output <code>f</code>:</strong> Derivative <code>∂f/∂x</code> is a column vector (m x 1). (Each <code>∂fᵢ/∂x</code> is a scalar, stacked up).</li><li><strong>Vector input <code>x</code>, Vector output <code>f</code>:</strong> Derivative <code>∂f/∂x</code> (the Jacobian) is an <code>m x n</code> matrix.</li></ul></li><li><p><strong>Example: Jacobian of the Identity Function (page 8):</strong></p><ul><li>If <code>y = f(x) = x</code>, then each output component <code>yᵢ</code> is just equal to the corresponding input component <code>xᵢ</code> (so <code>fᵢ(x) = xᵢ</code>). Here, <code>m=n</code>.</li><li>Let&rsquo;s find <code>∂fᵢ/∂xⱼ = ∂xᵢ/∂xⱼ</code>:<ul><li>If <code>i = j</code>, then <code>∂xᵢ/∂xᵢ = 1</code> (the derivative of <code>x₁</code> with respect to <code>x₁</code> is 1).</li><li>If <code>i ≠ j</code>, then <code>∂xᵢ/∂xⱼ = 0</code> (e.g., <code>x₁</code> does not change when <code>x₂</code> changes, because they are independent input components, so <code>∂x₁/∂x₂ = 0</code>).</li></ul></li><li>When you assemble these into the Jacobian matrix, you get 1s on the main diagonal and 0s everywhere else. This is the <strong>Identity Matrix (I)</strong>.</li><li><code>∂x/∂x = I</code></li><li><em>What this ultimately means:</em> If you wiggle an input <code>xⱼ</code> by a small amount, only the corresponding output <code>yⱼ</code> wiggles by that same amount, and other outputs <code>yᵢ</code> (where <code>i≠j</code>) don&rsquo;t change at all. This makes perfect sense for the identity function.</li></ul></li></ul><p>This section firmly establishes the Jacobian matrix as the way to represent the derivative of a vector function with respect to a vector input. It&rsquo;s the matrix that holds all the individual &ldquo;slopes&rdquo; that connect changes in each input dimension to changes in each output dimension.</p><p>This general form of the Jacobian will be crucial when we get to the vector chain rule later, which is how backpropagation efficiently calculates gradients through multiple layers of a neural network, where each layer can be seen as a vector function taking a vector input.</p><hr><h2 id=section-42-derivatives-of-vector-element-wise-binary-operators-pages-9-11><strong>Section 4.2: Derivatives of vector element-wise binary operators</strong> (Pages 9-11).</h2><p><strong>The Big Picture of This Section:</strong></p><p>Neural networks involve many operations on vectors: adding an input vector to a bias vector, multiplying activations by weights, etc. Often, these operations are <strong>element-wise</strong> – meaning the operation is applied independently to corresponding elements of the input vectors to produce an element of the output vector.</p><ul><li>Example: If <code>w = [w₁, w₂]</code> and <code>x = [x₁, x₂]</code>, then <code>w + x = [w₁ + x₁, w₂ + x₂]</code>. The first element of the output only depends on the first elements of the inputs, and so on.</li></ul><p>This section aims to figure out:</p><ul><li>If we have an output vector <code>y</code> that&rsquo;s a result of an element-wise operation between two input vectors <code>w</code> and <code>x</code> (e.g., <code>y = w + x</code> or <code>y = w * x</code>), how does <code>y</code> change if we wiggle <code>w</code>? (This gives us the Jacobian <code>∂y/∂w</code>).</li><li>And how does <code>y</code> change if we wiggle <code>x</code>? (This gives us the Jacobian <code>∂y/∂x</code>).</li></ul><p><em>What we are ultimately trying to achieve here is to find simplified forms for these Jacobians, because for element-wise operations, many of the terms in the full Jacobian matrix will turn out to be zero.</em></p><p><strong>Breaking it Down (Page 9):</strong></p><p>The paper starts with the general form: <code>y = f(w) ⊙ g(x)</code>.</p><ul><li><code>w</code> and <code>x</code> are input vectors.</li><li><code>f(w)</code> and <code>g(x)</code> are functions that produce vectors of the same size.</li><li><code>⊙</code> represents <em>any</em> element-wise binary operator (like <code>+</code>, <code>-</code>, element-wise <code>*</code>, element-wise <code>/</code>).</li><li><code>y</code> is the output vector.
So, <code>yᵢ = fᵢ(w) ⊙ gᵢ(x)</code>. The <code>i</code>-th element of <code>y</code> depends only on the <code>i</code>-th element processing of <code>w</code> and <code>x</code>.</li></ul><p>The full Jacobian <code>∂y/∂w</code> would be a matrix where element <code>(i,j)</code> is <code>∂yᵢ/∂wⱼ</code> (how the <code>i</code>-th output element <code>yᵢ</code> changes with respect to the <code>j</code>-th input element <code>wⱼ</code> from vector <code>w</code>).
Similarly for <code>∂y/∂x</code>.</p><p><strong>The &ldquo;Furball&rdquo; and the Simplification (Page 10):</strong></p><p>The general Jacobian matrix for such an operation (shown at the bottom of page 9) looks complicated – a &ldquo;furball,&rdquo; as the paper says.</p><p>However, because the operations are <strong>element-wise</strong>, there&rsquo;s a huge simplification:</p><ul><li><code>yᵢ</code> (the <code>i</code>-th element of the output) <em>only</em> depends on <code>wᵢ</code> (the <code>i</code>-th element of <code>w</code>) and <code>xᵢ</code> (the <code>i</code>-th element of <code>x</code>).</li><li><code>yᵢ</code> does <em>not</em> depend on <code>wⱼ</code> if <code>j ≠ i</code>.</li><li><code>yᵢ</code> does <em>not</em> depend on <code>xⱼ</code> if <code>j ≠ i</code>.</li></ul><p><strong>What does this mean for the Jacobian <code>∂y/∂w</code>?</strong>
Consider <code>∂yᵢ/∂wⱼ</code>:</p><ul><li>If <code>j ≠ i</code> (we are looking at an off-diagonal element of the Jacobian): Since <code>yᵢ</code> does not depend on <code>wⱼ</code>, its derivative <code>∂yᵢ/∂wⱼ</code> must be <strong>zero</strong>.</li><li>If <code>j = i</code> (we are looking at a diagonal element of the Jacobian): Then <code>∂yᵢ/∂wᵢ</code> will generally be non-zero, and it&rsquo;s just the derivative of the <code>i</code>-th scalar operation <code>fᵢ(wᵢ) ⊙ gᵢ(xᵢ)</code> with respect to <code>wᵢ</code> (treating <code>xᵢ</code> as a constant for this partial derivative).</li></ul><p><strong>The Result: Diagonal Jacobians!</strong>
This means that for element-wise operations, the Jacobian matrices <code>∂y/∂w</code> and <code>∂y/∂x</code> are <strong>diagonal matrices</strong>. A diagonal matrix has non-zero values only along its main diagonal, and zeros everywhere else.</p><p>The paper introduces the &ldquo;element-wise diagonal condition&rdquo;: <code>fᵢ(w)</code> should only access <code>wᵢ</code> and <code>gᵢ(x)</code> should only access <code>xᵢ</code>. This is precisely what happens in simple element-wise vector operations.</p><p>So, the Jacobian <code>∂y/∂w</code> simplifies to (middle of page 10):
<code>∂y/∂w = diag( ∂/∂w₁ (f₁(w₁) ⊙ g₁(x₁)), ∂/∂w₂ (f₂(w₂) ⊙ g₂(x₂)), ..., ∂/∂wₙ (fₙ(wₙ) ⊙ gₙ(xₙ)) )</code>
Each term on the diagonal is just a <em>scalar derivative</em> of the <code>i</code>-th component operation.</p><p><em>What this simplification is ultimately trying to achieve:</em> It makes calculating these Jacobians much easier. Instead of a full matrix of derivatives, we only need to calculate <code>n</code> scalar derivatives for the diagonal.</p><p><strong>Special Case: <code>f(w) = w</code> (Page 10-11)</strong></p><p>Very often in neural networks, one of the functions in the element-wise operation is just the identity. For example, vector addition <code>y = w + x</code>. Here <code>f(w) = w</code> (so <code>fᵢ(wᵢ) = wᵢ</code>) and <code>g(x) = x</code> (so <code>gᵢ(xᵢ) = xᵢ</code>).</p><p>Let&rsquo;s look at <code>y = w + x</code>, so <code>yᵢ = wᵢ + xᵢ</code>.</p><ul><li><strong><code>∂y/∂w</code>:</strong><ul><li>The <code>i</code>-th diagonal element is <code>∂/∂wᵢ (wᵢ + xᵢ)</code>. Treating <code>xᵢ</code> as a constant, this derivative is <code>1</code>.</li><li>So, <code>∂(w+x)/∂w = diag(1, 1, ..., 1) = I</code> (the identity matrix).</li></ul></li><li><strong><code>∂y/∂x</code>:</strong><ul><li>The <code>i</code>-th diagonal element is <code>∂/∂xᵢ (wᵢ + xᵢ)</code>. Treating <code>wᵢ</code> as a constant, this derivative is <code>1</code>.</li><li>So, <code>∂(w+x)/∂x = diag(1, 1, ..., 1) = I</code> (the identity matrix).</li></ul></li></ul><p>The paper then lists Jacobians for common element-wise binary operations where <code>f(w) = w</code> (so <code>fᵢ(wᵢ) = wᵢ</code>):</p><ul><li><p><strong>Addition: <code>y = w + x</code></strong></p><ul><li><code>∂y/∂w = I</code></li><li><code>∂y/∂x = I</code></li><li><em>Intuition:</em> If you wiggle <code>w₁</code> by a small amount <code>Δ</code>, <code>y₁</code> changes by <code>Δ</code>, and no other <code>yⱼ</code> changes. Same for <code>x</code>.</li></ul></li><li><p><strong>Subtraction: <code>y = w - x</code></strong></p><ul><li><code>∂y/∂w = I</code></li><li><code>∂y/∂x = -I</code> (because <code>∂/∂xᵢ (wᵢ - xᵢ) = -1</code>)</li><li><em>Intuition:</em> If you wiggle <code>x₁</code> by <code>Δ</code>, <code>y₁</code> changes by <code>-Δ</code>.</li></ul></li><li><p><strong>Element-wise Multiplication (Hadamard Product): <code>y = w ⊗ x</code> (so <code>yᵢ = wᵢ * xᵢ</code>)</strong></p><ul><li><code>∂y/∂w</code>: <code>i</code>-th diagonal element is <code>∂/∂wᵢ (wᵢ * xᵢ) = xᵢ</code>. So, <code>∂y/∂w = diag(x)</code>.</li><li><code>∂y/∂x</code>: <code>i</code>-th diagonal element is <code>∂/∂xᵢ (wᵢ * xᵢ) = wᵢ</code>. So, <code>∂y/∂x = diag(w)</code>.</li><li><em>Intuition for ∂y/∂w:</em> If you wiggle <code>w₁</code> by <code>Δ</code>, <code>y₁</code> changes by <code>x₁ * Δ</code>. The change in <code>y₁</code> depends on the value of <code>x₁</code>.</li></ul></li><li><p><strong>Element-wise Division: <code>y = w / x</code> (so <code>yᵢ = wᵢ / xᵢ</code>)</strong></p><ul><li><code>∂y/∂w</code>: <code>i</code>-th diagonal element is <code>∂/∂wᵢ (wᵢ / xᵢ) = 1/xᵢ</code>. So, <code>∂y/∂w = diag(1/x₁, 1/x₂, ...)</code>.</li><li><code>∂y/∂x</code>: <code>i</code>-th diagonal element is <code>∂/∂xᵢ (wᵢ / xᵢ) = -wᵢ / xᵢ²</code>. So, <code>∂y/∂x = diag(-w₁/x₁², ...)</code>.</li></ul></li></ul><p><strong>Key Takeaway from Section 4.2:</strong>
When dealing with element-wise operations between two vectors <code>w</code> and <code>x</code> to produce <code>y</code>, the Jacobians <code>∂y/∂w</code> and <code>∂y/∂x</code> are <strong>diagonal matrices</strong>. This is a huge simplification. The values on the diagonal are found by simply taking the scalar derivative of the <code>i</code>-th component operation with respect to the <code>i</code>-th component of the input vector. This section provides the rules for common operations like addition, subtraction, and element-wise multiplication/division.</p><p>This means that when we&rsquo;re backpropagating gradients through such an element-wise layer, the calculations become much simpler than if we had to deal with full Jacobian matrices. We&rsquo;re essentially just scaling the incoming gradients by these diagonal terms.</p><hr><h2 id=section-43-derivatives-involving-scalar-expansion-pages-11-12><strong>Section 4.3: Derivatives involving scalar expansion</strong> (Pages 11-12).</h2><p><strong>The Big Picture of This Section:</strong></p><p>In neural networks, we often perform operations between a vector and a <em>scalar</em>. For example:</p><ul><li>Adding a scalar bias <code>b</code> to every element of a vector <code>z</code>: <code>y = z + b</code>.</li><li>Multiplying every element of a vector <code>z</code> by a scalar learning rate <code>η</code>: <code>y = η * z</code>.</li></ul><p>This section explains how to find the derivatives for these types of operations. The trick is to realize that these operations can be viewed as <strong>implicit element-wise operations</strong> where the scalar is &ldquo;expanded&rdquo; or &ldquo;broadcast&rdquo; into a vector of the same size as the other vector.</p><p><em>What we are ultimately trying to achieve here is to get rules for how the output vector <code>y</code> changes if we wiggle the input vector <code>x</code>, and how <code>y</code> changes if we wiggle the input scalar <code>z</code>.</em></p><p><strong>Breaking it Down (Page 11-12):</strong></p><p>The paper uses the example <code>y = x + z</code>, where <code>x</code> is a vector and <code>z</code> is a scalar.</p><ul><li><strong>Implicit Expansion:</strong> This is really <code>y = f(x) + g(z)</code> where <code>f(x) = x</code> and <code>g(z) = 1z</code> (a vector where every element is <code>z</code>).</li><li>So, <code>yᵢ = xᵢ + z</code>. Each element <code>yᵢ</code> is the sum of the corresponding <code>xᵢ</code> and the <em>same</em> scalar <code>z</code>.</li></ul><p><strong>1. Derivative with respect to the vector <code>x</code>: <code>∂y/∂x</code></strong></p><ul><li>This fits the &ldquo;element-wise diagonal condition&rdquo; we just discussed.<ul><li><code>fᵢ(x) = xᵢ</code> only depends on <code>xᵢ</code>.</li><li><code>gᵢ(z) = z</code> (for the i-th component) only depends on the scalar <code>z</code> (which is considered independent of <code>x</code> for this partial derivative).</li></ul></li><li>The <code>i</code>-th diagonal element of the Jacobian <code>∂y/∂x</code> is <code>∂/∂xᵢ (xᵢ + z)</code>.</li><li>Since <code>z</code> is treated as a constant when differentiating with respect to <code>xᵢ</code>, <code>∂z/∂xᵢ = 0</code>.</li><li>So, <code>∂/∂xᵢ (xᵢ + z) = ∂xᵢ/∂xᵢ + ∂z/∂xᵢ = 1 + 0 = 1</code>.</li><li>Therefore, <code>∂y/∂x = diag(1, 1, ..., 1) = I</code> (the identity matrix).</li><li><strong>Intuition:</strong> If you wiggle <code>xᵢ</code> by a small amount <code>Δ</code>, only <code>yᵢ</code> changes, and it changes by <code>Δ</code>. This is the definition of the identity matrix&rsquo;s effect.</li><li><em>What this is ultimately trying to achieve:</em> It confirms that adding a scalar to a vector shifts all elements equally, so the rate of change of each output element <code>yᵢ</code> with respect to its corresponding input <code>xᵢ</code> is 1, and there&rsquo;s no cross-influence.</li></ul><p><strong>2. Derivative with respect to the scalar <code>z</code>: <code>∂y/∂z</code></strong>
This is different! We are differentiating a <em>vector output</em> <code>y</code> with respect to a <em>scalar input</em> <code>z</code>.
Based on the table from page 8 (or just thinking about it):</p><ul><li>Input: scalar <code>z</code></li><li>Output: vector <code>y</code></li><li>The derivative <code>∂y/∂z</code> should be a <strong>column vector</strong> (or a vertical vector as the paper terms it). Each element of this column vector will be <code>∂yᵢ/∂z</code>.</li><li>Let&rsquo;s find <code>∂yᵢ/∂z</code> for <code>yᵢ = xᵢ + z</code>.</li><li>When differentiating with respect to <code>z</code>, we treat <code>xᵢ</code> as a constant. So, <code>∂xᵢ/∂z = 0</code>.</li><li><code>∂yᵢ/∂z = ∂/∂z (xᵢ + z) = ∂xᵢ/∂z + ∂z/∂z = 0 + 1 = 1</code>.</li><li>Since this is true for <em>every</em> <code>i</code> (from 1 to <code>n</code>, the dimension of <code>y</code> and <code>x</code>), then <code>∂y/∂z</code> is a column vector of all ones.</li><li>The paper writes this as <code>∂/∂z (x + z) = 1</code> (where <code>1</code> is the vector of ones of appropriate length).</li><li><strong>Intuition:</strong> If you wiggle the single scalar <code>z</code> by <code>Δ</code>, <em>every</em> element <code>yᵢ</code> of the output vector changes by <code>Δ</code> because <code>z</code> is added to every <code>xᵢ</code>.</li><li><em>What this is ultimately trying to achieve:</em> It shows how a single scalar change propagates to all elements of the output vector when scalar addition is involved.</li></ul><p><strong>Now for Scalar Multiplication: <code>y = xz</code> (or <code>y = zx</code>)</strong>
This is treated as element-wise multiplication: <code>y = x ⊗ 1z</code>.
So, <code>yᵢ = xᵢ * z</code>.</p><p><strong>1. Derivative with respect to the vector <code>x</code>: <code>∂y/∂x</code></strong></p><ul><li>Again, element-wise diagonal condition holds.</li><li>The <code>i</code>-th diagonal element is <code>∂/∂xᵢ (xᵢ * z)</code>.</li><li>Treat <code>z</code> as constant: <code>∂/∂xᵢ (xᵢ * z) = z * ∂xᵢ/∂xᵢ = z * 1 = z</code>.</li><li>So, <code>∂y/∂x = diag(z, z, ..., z) = zI</code> (scalar <code>z</code> times the identity matrix).</li><li>The paper writes this as <code>∂/∂x (xz) = diag(1z) = Iz</code>. (Here, <code>1z</code> in <code>diag(1z)</code> means a vector of <code>z</code>s, which makes more sense. If <code>diag(z)</code> was meant for a scalar <code>z</code>, it would just be <code>z</code> itself, not a matrix. So <code>diag(1z)</code> is the clearer way to express <code>zI</code>). My interpretation: <code>diag(1z)</code> means a diagonal matrix with <code>z</code> on every diagonal element. This is <code>z * I</code>.</li><li><strong>Intuition:</strong> If you wiggle <code>xᵢ</code> by <code>Δ</code>, <code>yᵢ</code> changes by <code>z * Δ</code>. The effect is scaled by <code>z</code>.</li></ul><p><strong>2. Derivative with respect to the scalar <code>z</code>: <code>∂y/∂z</code></strong></p><ul><li><code>∂y/∂z</code> will be a column vector. The <code>i</code>-th element is <code>∂yᵢ/∂z</code>.</li><li><code>yᵢ = xᵢ * z</code>. Treat <code>xᵢ</code> as constant.</li><li><code>∂yᵢ/∂z = ∂/∂z (xᵢ * z) = xᵢ * ∂z/∂z = xᵢ * 1 = xᵢ</code>.</li><li>So, <code>∂y/∂z</code> is a column vector whose elements are <code>[x₁, x₂, ..., xₙ]ᵀ</code>, which is just the vector <code>x</code>.</li><li>The paper writes this as <code>∂/∂z (xz) = x</code>.</li><li><strong>Intuition:</strong> If you wiggle the scalar <code>z</code> by <code>Δ</code>, the <code>i</code>-th output <code>yᵢ</code> changes by <code>xᵢ * Δ</code>. The amount <code>yᵢ</code> changes depends on the value of <code>xᵢ</code>.</li><li><em>What this is ultimately trying to achieve:</em> It shows how a single scalar change <code>z</code> propagates to the output vector <code>y</code>, with each element <code>yᵢ</code> being affected proportionally to the corresponding <code>xᵢ</code>.</li></ul><p><strong>Key Takeaway from Section 4.3:</strong>
Operations involving a vector and a scalar (like adding a scalar to a vector, or multiplying a vector by a scalar) can be understood by &ldquo;expanding&rdquo; the scalar into a vector and then applying the rules for element-wise vector operations.</p><ul><li>When differentiating with respect to the <strong>vector input</strong>, the Jacobian is still <strong>diagonal</strong>.</li><li>When differentiating with respect to the <strong>scalar input</strong>, the result is a <strong>vector</strong> (not a diagonal matrix), because a single change in the scalar affects all components of the output vector.</li></ul><p>This section helps us build up the rules needed for things like <code>Wx + b</code> in a neural network layer, where <code>b</code> might be a bias <em>vector</em> added element-wise, or a scalar bias broadcasted. The logic applies similarly.</p><hr><h2 id=section-44-vector-sum-reduction-pages-12-13><strong>Section 4.4: Vector sum reduction</strong> (Pages 12-13).</h2><p><strong>The Big Picture of This Section:</strong></p><p>In many machine learning contexts, especially when defining loss functions, we often need to <strong>reduce a vector to a single scalar value</strong>. The most common way to do this is by <strong>summing up all the elements of the vector</strong>.</p><ul><li>Example: The Mean Squared Error (MSE) involves summing the squared errors for each instance. If we have a vector of squared errors, we sum them up to get a total squared error, then average.</li></ul><p>This section focuses on finding the derivative of such a sum:</p><ul><li>If <code>y = sum(f(x))</code>, where <code>x</code> is an input vector and <code>f(x)</code> produces an output vector whose elements are then summed to get the scalar <code>y</code>, how does this scalar <code>y</code> change if we wiggle the input vector <code>x</code>? This will give us <code>∂y/∂x</code>, which will be a <strong>gradient</strong> (a row vector, according to the paper&rsquo;s convention for derivative of a scalar w.r.t. a vector).</li></ul><p><em>What we are ultimately trying to achieve here is a rule for how to differentiate through a summation operation. This is critical for backpropagation, as the total loss is often a sum of losses from individual components or instances.</em></p><p><strong>Breaking it Down (Page 13):</strong></p><p>Let <code>y = sum(f(x)) = Σᵢ fᵢ(x)</code>.</p><ul><li><code>x</code> is an input vector <code>[x₁, x₂, ..., xₙ]</code>.</li><li><code>f(x)</code> is a vector-valued function, producing an output vector <code>[f₁(x), f₂(x), ..., fₚ(x)]</code>. (Note: the paper uses <code>n</code> as the dimension of <code>x</code> and also <code>n</code> for the number of terms in the sum, implying <code>p=n</code>. Let&rsquo;s assume the output vector of <code>f(x)</code> has <code>p</code> elements).</li><li>Each <code>fᵢ(x)</code> could potentially depend on <em>all</em> elements of <code>x</code> (not just <code>xᵢ</code>). This is an important distinction from the element-wise operations earlier.</li><li><code>y</code> is the scalar sum of all elements of the vector <code>f(x)</code>.</li></ul><p>We want to find the gradient <code>∂y/∂x</code>. This will be a row vector:
<code>∂y/∂x = [∂y/∂x₁, ∂y/∂x₂, ..., ∂y/∂xₙ]</code></p><p>Let&rsquo;s consider one component of this gradient, <code>∂y/∂xⱼ</code> (how the sum <code>y</code> changes with respect to one input element <code>xⱼ</code>):
<code>∂y/∂xⱼ = ∂/∂xⱼ ( Σᵢ fᵢ(x) )</code></p><p>Because the derivative operator is distributive over a sum, we can move it inside:
<code>∂y/∂xⱼ = Σᵢ ( ∂fᵢ(x)/∂xⱼ )</code></p><p><em>What this means:</em> To find how the total sum <code>y</code> changes when <code>xⱼ</code> changes, we sum up how <em>each individual term</em> <code>fᵢ(x)</code> in the sum changes when <code>xⱼ</code> changes.</p><p>So, the gradient vector <code>∂y/∂x</code> becomes:
<code>∂y/∂x = [ Σᵢ(∂fᵢ/∂x₁), Σᵢ(∂fᵢ/∂x₂), ..., Σᵢ(∂fᵢ/∂xₙ) ]</code></p><p><strong>Example 1: <code>y = sum(x)</code> (Simple sum of input vector elements)</strong>
Here, <code>f(x) = x</code>, so <code>fᵢ(x) = xᵢ</code>.</p><ul><li><code>∂y/∂xⱼ = Σᵢ (∂xᵢ/∂xⱼ)</code><ul><li><code>∂xᵢ/∂xⱼ</code> is 1 if <code>i=j</code>, and 0 if <code>i≠j</code>.</li><li>So, in the sum <code>Σᵢ (∂xᵢ/∂xⱼ)</code>, only one term is non-zero: when <code>i=j</code>, the term is 1.</li><li>Therefore, <code>∂y/∂xⱼ = 1</code> for all <code>j</code>.</li></ul></li><li>This means <code>∂y/∂x = [1, 1, ..., 1]</code>.</li><li>The paper writes this as <code>∇y = 1ᵀ</code> (a row vector of ones).</li><li><strong>Intuition:</strong> If you have <code>y = x₁ + x₂ + ... + xₙ</code>, and you wiggle <code>x₁</code> by a small amount <code>Δ</code>, <code>y</code> changes by <code>Δ</code>. If you wiggle <code>x₂</code> by <code>Δ</code>, <code>y</code> also changes by <code>Δ</code>, and so on. The rate of change of <code>y</code> with respect to any <code>xⱼ</code> is 1.</li><li><em>What this result ultimately achieves:</em> It gives us a very simple rule: the derivative of the sum of a vector&rsquo;s elements, with respect to that vector itself, is a vector of ones (transposed to match the gradient convention).</li></ul><p><strong>Example 2: <code>y = sum(xz)</code> (Sum of a vector multiplied by a scalar)</strong>
Here, <code>x</code> is a vector, <code>z</code> is a scalar.
Let <code>f(x,z)</code> be the vector <code>xz</code> (meaning <code>fᵢ(x,z) = xᵢz</code>).
Then <code>y = sum(f(x,z)) = Σᵢ (xᵢz)</code>.</p><ol><li><p><strong>Derivative with respect to the vector <code>x</code>: <code>∂y/∂x</code></strong></p><ul><li>The <code>j</code>-th component of the gradient is <code>∂y/∂xⱼ = Σᵢ (∂(xᵢz)/∂xⱼ)</code>.</li><li><code>∂(xᵢz)/∂xⱼ</code>:<ul><li>If <code>i = j</code>: <code>∂(xⱼz)/∂xⱼ = z</code> (treating <code>z</code> as constant).</li><li>If <code>i ≠ j</code>: <code>∂(xᵢz)/∂xⱼ = 0</code> (because <code>xᵢz</code> doesn&rsquo;t depend on <code>xⱼ</code>).</li></ul></li><li>So, in the sum <code>Σᵢ (∂(xᵢz)/∂xⱼ)</code>, only the term where <code>i=j</code> survives, and it is <code>z</code>.</li><li>Therefore, <code>∂y/∂xⱼ = z</code> for all <code>j</code>.</li><li>This means <code>∂y/∂x = [z, z, ..., z]</code>.</li><li><strong>Intuition:</strong> If <code>y = x₁z + x₂z + ... + xₙz</code>, and you wiggle <code>xⱼ</code> by <code>Δ</code>, then <code>y</code> changes by <code>zΔ</code>. The rate of change is <code>z</code>.</li></ul></li><li><p><strong>Derivative with respect to the scalar <code>z</code>: <code>∂y/∂z</code></strong> (This is a scalar derivative of a scalar w.r.t a scalar)</p><ul><li><code>y = Σᵢ (xᵢz) = z * (Σᵢ xᵢ)</code>.</li><li><code>∂y/∂z = ∂/∂z ( z * sum(x) )</code>.</li><li>Since <code>sum(x)</code> is treated as a constant when differentiating w.r.t. <code>z</code>,</li><li><code>∂y/∂z = sum(x)</code>.</li><li><strong>Intuition:</strong> If <code>y = z * (x₁ + x₂ + ... + xₙ)</code>, and you wiggle <code>z</code> by <code>Δ</code>, then <code>y</code> changes by <code>(x₁ + ... + xₙ)Δ</code>. The rate of change is <code>sum(x)</code>.</li></ul></li></ol><p><strong>Key Takeaway from Section 4.4:</strong>
When you differentiate a scalar sum <code>y = sum(f(x))</code> with respect to the input vector <code>x</code>, the <code>j</code>-th component of the resulting gradient <code>∂y/∂x</code> is the sum of how each term <code>fᵢ(x)</code> in the original sum changes with respect to <code>xⱼ</code>.</p><ul><li>For <code>y = sum(x)</code>, this simplifies to <code>∂y/∂x = 1ᵀ</code>.</li><li>This rule is fundamental for backpropagation because the overall loss of a network is often a sum (or average) of losses per instance, or a sum of terms in a complex loss function. When we take the derivative of this total loss with respect to weights in earlier layers, we&rsquo;ll be implicitly using this idea of differentiating through a sum.</li></ul><p>The paper uses the notation <code>∇y</code> when the output <code>y</code> is a scalar and the differentiation is with respect to a vector, resulting in a row vector (the gradient). It uses <code>∂y/∂x</code> for the more general Jacobian when <code>y</code> could also be a vector.</p><hr><h2 id=section-45-the-chain-rules><strong>Section 4.5: The Chain Rules</strong></h2><p><strong>The Big Picture of This Section:</strong></p><p>Neural networks are essentially deeply nested functions. The output of one layer becomes the input to the next, which then feeds into another, and so on, until we get the final output, and then a loss function is applied.
<code>loss = L( activation_L( ... activation_2( activation_1(X, w₁, b₁), w₂, b₂) ... ), y_true )</code></p><p>To train the network using gradient descent, we need to calculate how the final <code>loss</code> changes with respect to <em>every single weight and bias</em> in the entire network, even those in the very first layers. This is a monumental task if done naively.</p><p>The <strong>Chain Rule</strong> is the mathematical tool that allows us to do this efficiently.</p><ul><li><strong>What it&rsquo;s ultimately trying to achieve:</strong> It provides a systematic way to calculate the derivative of a composite (nested) function by breaking it down into simpler derivatives of its constituent parts and then &ldquo;chaining&rdquo; them together (usually by multiplication).</li></ul><p>The paper discusses three variants, from simple to more general:</p><ol><li>Single-variable chain rule (scalar function of a scalar variable).</li><li>Single-variable total-derivative chain rule (scalar function of a scalar variable, but with intermediate multivariate functions).</li><li>Vector chain rule (vector function of a vector variable). This is the most general one for neural networks.</li></ol><p><strong>4.5.1 Single-variable chain rule (Pages 14-17)</strong></p><p>This is the chain rule you likely learned in basic calculus.</p><ul><li><strong>Scenario:</strong> You have a function nested within another, like <code>y = f(g(x))</code>. For example, <code>y = sin(x²)</code>.<ul><li>Here, <code>g(x) = x²</code> (the inner function).</li><li>And <code>f(u) = sin(u)</code> (the outer function, where <code>u = g(x)</code>).</li></ul></li><li><strong>The Rule:</strong> <code>dy/dx = dy/du * du/dx</code><ul><li><code>du/dx</code>: How the inner function <code>u</code> changes with respect to <code>x</code>. For <code>u=x²</code>, <code>du/dx = 2x</code>.</li><li><code>dy/du</code>: How the outer function <code>y</code> changes with respect to its direct input <code>u</code>. For <code>y=sin(u)</code>, <code>dy/du = cos(u)</code>.</li><li>Then <code>dy/dx = cos(u) * 2x</code>. Substitute back <code>u=x²</code> to get <code>cos(x²) * 2x</code>.</li></ul></li><li><strong>The Process (as recommended by the paper):</strong><ol><li><strong>Introduce intermediate variables:</strong> Break down the nested expression.
For <code>y = sin(x²)</code>, let <code>u = x²</code>. Then <code>y = sin(u)</code>.</li><li><strong>Compute derivatives of intermediate variables:</strong>
<code>du/dx = 2x</code>
<code>dy/du = cos(u)</code></li><li><strong>Combine (multiply) the derivatives:</strong>
<code>dy/dx = (dy/du) * (du/dx) = cos(u) * 2x</code></li><li><strong>Substitute back:</strong>
<code>dy/dx = cos(x²) * 2x</code></li></ol></li><li><strong>Units Analogy (page 15):</strong> The paper gives a nice analogy: if <code>y</code> is miles, <code>u</code> is gallons, and <code>x</code> is tank level, then <code>miles/tank = (miles/gallon) * (gallons/tank)</code>. The intermediate unit &ldquo;gallon&rdquo; cancels out.</li><li><strong>Dataflow Diagram / Abstract Syntax Tree (page 16):</strong> Visualizing the chain of operations helps. Changes in <code>x</code> bubble up to <code>u</code>, then to <code>y</code>. The chain rule traces this path.</li><li><strong>Condition for Single-Variable Chain Rule:</strong> There&rsquo;s a <em>single dataflow path</em> from <code>x</code> to <code>y</code>. Intermediate functions (<code>u(x)</code>, <code>y(u)</code>) have only one parameter.</li><li><strong>Deeply Nested Expressions (page 17):</strong> The process extends. For <code>y = f₄(f₃(f₂(f₁(x))))</code>, let <code>u₁=f₁(x)</code>, <code>u₂=f₂(u₁)</code> etc.
Then <code>dy/dx = (dy/du₃) * (du₃/du₂) * (du₂/du₁) * (du₁/dx)</code>.
Example: <code>y = ln(sin(x³)²)</code><ol><li><code>u₁ = x³</code>
<code>u₂ = sin(u₁)</code>
<code>u₃ = u₂²</code>
<code>y = u₄ = ln(u₃)</code></li><li><code>du₁/dx = 3x²</code>
<code>du₂/du₁ = cos(u₁)</code>
<code>du₃/du₂ = 2u₂</code>
<code>dy/du₃ = 1/u₃</code></li><li><code>dy/dx = (1/u₃) * (2u₂) * (cos(u₁)) * (3x²)</code></li><li>Substitute back: <code>(1/sin(x³)² ) * (2sin(x³)) * (cos(x³)) * (3x²) = 6x²cos(x³)/sin(x³)</code></li></ol></li><li><em>What the single-variable chain rule is ultimately trying to achieve:</em> It provides a recipe to find the overall rate of change of a nested scalar function by multiplying the rates of change of its individual components along the chain of dependency.</li></ul><p><strong>4.5.2 Single-variable total-derivative chain rule (Pages 18-20)</strong></p><p>This handles a more complex scenario. What if an intermediate variable depends on the original input <code>x</code> <em>and</em> other intermediate variables that also depend on <code>x</code>?</p><ul><li><p><strong>Scenario:</strong> <code>y = f(x) = x + x²</code>.
The paper rewrites this using intermediate variables to illustrate:
<code>u₁(x) = x²</code>
<code>y = u₂(x, u₁) = x + u₁</code>
Here, <code>y</code> (which is <code>u₂</code>) depends directly on <code>x</code> <em>and</em> it depends on <code>u₁</code>, which <em>also</em> depends on <code>x</code>. So, <code>x</code> influences <code>y</code> through <em>two paths</em>:</p><ol><li>Directly (the <code>x</code> term in <code>x + u₁</code>).</li><li>Indirectly (through <code>u₁</code> which is <code>x²</code>).</li></ol></li><li><p>The simple chain rule <code>dy/dx = (∂y/∂u₁) * (du₁/dx)</code> would give <code>1 * 2x = 2x</code>, which is wrong (the derivative of <code>x+x²</code> is <code>1+2x</code>).</p></li><li><p><strong>The &ldquo;Law&rdquo; of Total Derivatives (page 19):</strong>
To get <code>dy/dx</code> (the <em>total</em> derivative of <code>y</code> with respect to <code>x</code>), you need to sum up <em>all possible contributions</em> from changes in <code>x</code> to the change in <code>y</code>.
For <code>y = u₂(x, u₁(x))</code>:
<code>dy/dx = ∂u₂/∂x + (∂u₂/∂u₁) * (du₁/dx)</code>
Let&rsquo;s break this down:</p><ul><li><code>∂u₂/∂x</code>: This is the partial derivative of <code>u₂(x,u₁)</code> with respect to <code>x</code>, <em>treating <code>u₁</code> as if it were an independent constant for this term</em>. For <code>u₂ = x + u₁</code>, <code>∂u₂/∂x = 1</code>. (How <code>y</code> changes if <code>x</code> wiggles but <code>u₁</code> is held fixed).</li><li><code>∂u₂/∂u₁</code>: Partial derivative of <code>u₂(x,u₁)</code> with respect to <code>u₁</code>, <em>treating <code>x</code> as constant</em>. For <code>u₂ = x + u₁</code>, <code>∂u₂/∂u₁ = 1</code>. (How <code>y</code> changes if <code>u₁</code> wiggles but <code>x</code> is held fixed).</li><li><code>du₁/dx</code>: Total derivative of <code>u₁(x)</code> with respect to <code>x</code>. For <code>u₁ = x²</code>, <code>du₁/dx = 2x</code>. (How <code>u₁</code> changes if <code>x</code> wiggles).</li><li>So, <code>dy/dx = 1 + (1 * 2x) = 1 + 2x</code>. Correct!</li></ul></li><li><p><strong>General Single-Variable Total-Derivative Chain Rule (page 19):</strong>
If <code>y = f(x, u₁, u₂, ..., uₙ)</code>, and each <code>uᵢ</code> is also a function of <code>x</code> (<code>uᵢ(x)</code>):
<code>dy/dx (total) = ∂f/∂x (direct) + Σᵢ (∂f/∂uᵢ) * (duᵢ/dx (total))</code></p><ul><li><code>∂f/∂x</code>: Partial derivative of <code>f</code> w.r.t <code>x</code>, holding all <code>uᵢ</code> constant (measures direct effect of <code>x</code> on <code>f</code>).</li><li><code>∂f/∂uᵢ</code>: Partial derivative of <code>f</code> w.r.t <code>uᵢ</code>, holding <code>x</code> and other <code>uⱼ</code> constant (how <code>f</code> changes if <em>only</em> <code>uᵢ</code> changes).</li><li><code>duᵢ/dx</code>: Total derivative of <code>uᵢ</code> w.r.t <code>x</code> (how <code>uᵢ</code> itself changes when <code>x</code> changes, which might involve its own chain of dependencies).</li><li><em>What this rule is ultimately trying to achieve:</em> It correctly accounts for <em>all paths</em> through which a change in <code>x</code> can affect the final output <code>y</code>, summing up the contributions from the direct path and all indirect paths through intermediate variables <code>uᵢ</code>.</li></ul></li><li><p><strong>Simplified Final Form (page 20):</strong>
The paper cleverly shows that if you introduce <code>x</code> itself as another intermediate variable (e.g., <code>uₙ₊₁ = x</code>), then the &ldquo;direct&rdquo; <code>∂f/∂x</code> term can be absorbed into the sum.
If <code>y = f(u₁, ..., uₙ, uₙ₊₁)</code> where <code>uᵢ = uᵢ(x)</code> for all <code>i</code>, and <code>uₙ₊₁ = x</code> (so <code>duₙ₊₁/dx = 1</code>),
Then <code>dy/dx = Σᵢ (∂f/∂uᵢ) * (duᵢ/dx)</code> (summing over all <code>n+1</code> intermediate variables).
<em>This foreshadows the vector chain rule where the sum looks like a dot product.</em></p></li><li><p><strong>Caution on Terminology:</strong> The paper notes that what they call the &ldquo;single-variable total-derivative chain rule&rdquo; is often just called the &ldquo;multivariable chain rule&rdquo; in calculus discussions, which can be misleading because the overall function <code>f(x)</code> is still a scalar function of a single scalar <code>x</code>.</p></li></ul><p><strong>4.5.3 Vector chain rule (Pages 21-22) - THE PAYOFF!</strong></p><p>Now for the most general case, relevant to neural networks:</p><ul><li><p><code>g(x)</code>: A function that takes a vector <code>x</code> and outputs an intermediate vector <code>u = g(x)</code>.</p></li><li><p><code>f(u)</code>: A function that takes the vector <code>u</code> and outputs a final vector <code>y = f(u)</code>.</p></li><li><p>We want to find <code>∂y/∂x</code>, the Jacobian of the composite function <code>y = f(g(x))</code> with respect to <code>x</code>.</p></li><li><p><strong>The Rule (beautifully simple in vector/matrix form):</strong>
<code>∂y/∂x = (∂y/∂u) * (∂u/∂x)</code>
Where:</p><ul><li><code>∂u/∂x</code>: Is the Jacobian of <code>g</code> with respect to <code>x</code>. If <code>x</code> is <code>n x 1</code> and <code>u</code> is <code>k x 1</code>, this is a <code>k x n</code> matrix.</li><li><code>∂y/∂u</code>: Is the Jacobian of <code>f</code> with respect to <code>u</code>. If <code>u</code> is <code>k x 1</code> and <code>y</code> is <code>m x 1</code>, this is an <code>m x k</code> matrix.</li><li><code>*</code>: This is <strong>matrix multiplication</strong>.</li><li>The result <code>∂y/∂x</code> will be an <code>m x n</code> matrix, which is the correct shape for the Jacobian of <code>y</code> (m-dim) w.r.t. <code>x</code> (n-dim).</li></ul></li><li><p><strong>How the paper rediscovers it (page 21):</strong>
It starts with a simpler case: <code>y = f(g(x))</code> where <code>x</code> is a <em>scalar</em>, <code>g(x)</code> is a vector <code>[g₁(x), g₂(x)]ᵀ</code>, and <code>f(g)</code> is a vector <code>[f₁(g), f₂(g)]ᵀ</code> (where <code>f₁</code> might depend on <code>g₁</code> and <code>g₂</code>, and <code>f₂</code> might also depend on <code>g₁</code> and <code>g₂</code>).</p><ul><li><code>∂y/∂x</code> will be a column vector <code>[∂y₁/∂x, ∂y₂/∂x]ᵀ</code>.</li><li>Using the single-variable total-derivative chain rule for each component <code>yᵢ</code>:
<code>∂y₁/∂x = (∂f₁/∂g₁) * (dg₁/dx) + (∂f₁/∂g₂) * (dg₂/dx)</code>
<code>∂y₂/∂x = (∂f₂/∂g₁) * (dg₁/dx) + (∂f₂/∂g₂) * (dg₂/dx)</code></li><li>This can be written in matrix form:
<code>[ ∂y₁/∂x ] = [ ∂f₁/∂g₁ ∂f₁/∂g₂ ] * [ dg₁/dx ]</code>
<code>[ ∂y₂/∂x ] [ ∂f₂/∂g₁ ∂f₂/∂g₂ ] [ dg₂/dx ]</code>
This is exactly <code>∂y/∂x = (∂y/∂g) * (∂g/∂x)</code>! (Here <code>u</code> is called <code>g</code>).</li></ul></li><li><p><strong>The Beauty of the Vector Chain Rule (page 22):</strong>
It <em>automatically</em> takes care of the total derivative aspect (summing over all intermediate paths) because of how matrix multiplication is defined (sum of products).
The full Jacobian components are shown, illustrating the <code>m x k</code> matrix <code>∂f/∂g</code> multiplying the <code>k x n</code> matrix <code>∂g/∂x</code> to give an <code>m x n</code> result.</p></li><li><p><strong>Simplification for Element-wise Operations (page 22, bottom):</strong>
If <code>f</code> operates element-wise on <code>g</code> (i.e., <code>yᵢ = fᵢ(gᵢ)</code>) AND <code>g</code> operates element-wise on <code>x</code> (i.e., <code>gᵢ = gᵢ(xᵢ)</code>), then:</p><ul><li><code>∂f/∂g</code> becomes <code>diag(∂fᵢ/∂gᵢ)</code></li><li><code>∂g/∂x</code> becomes <code>diag(∂gᵢ/∂xᵢ)</code></li><li>Then <code>∂y/∂x = diag(∂fᵢ/∂gᵢ) * diag(∂gᵢ/∂xᵢ) = diag( (∂fᵢ/∂gᵢ) * (∂gᵢ/∂xᵢ) )</code>.</li><li>The Jacobian is diagonal, and each diagonal element is just the result of the single-variable chain rule applied to the components. This connects back to the earlier sections.</li></ul></li></ul><p><strong>Key Takeaway from Section 4.5:</strong>
The chain rule, in its various forms, is <em>the</em> fundamental mechanism for calculating derivatives of complex, nested functions.</p><ul><li>For a simple chain <code>y(u(x))</code>, it&rsquo;s <code>dy/dx = (dy/du)(du/dx)</code>.</li><li>If <code>x</code> can affect <code>y</code> through multiple paths via intermediate variables <code>uᵢ(x)</code>, the total derivative involves summing the contributions from each path.</li><li>For vector functions of vector variables <code>y(u(x))</code>, the Jacobian <code>∂y/∂x = (∂y/∂u)(∂u/∂x)</code> (matrix multiplication) elegantly captures all these interactions. This is the version most directly applicable to backpropagation in neural networks, where <code>∂y/∂u</code> is the gradient from the layer above, and <code>∂u/∂x</code> is the local gradient of the current layer.</li></ul><p>This vector chain rule is what allows backpropagation to efficiently compute the gradient of the final loss with respect to every single weight in a deep network by &ldquo;chaining&rdquo; these Jacobian multiplications backward through the layers. Each Jacobian <code>∂(layer_output)/∂(layer_input)</code> tells us how a layer transforms incoming gradient signals.</p><h1 id=section-5-the-gradient-of-neuron-activation><strong>Section 5: The gradient of neuron activation</strong></h1><p><strong>The Big Picture of This Section:</strong></p><p>In the previous section (4.5.3 Vector Chain Rule), the paper showed how to calculate <code>∂y/∂x = (∂y/∂u) * (∂u/∂x)</code> where <code>u=g(x)</code> and <code>y=f(u)</code>.
This section is going to calculate one of these crucial Jacobians: the <code>∂u/∂x</code> part, specifically for a typical neuron&rsquo;s &ldquo;affine function&rdquo; (the weighted sum + bias) <em>before</em> the non-linear activation. Then, it will combine it with the derivative of an activation function (like <code>max(0,z)</code>) using the chain rule.</p><p>Essentially, we want to answer:</p><ul><li>If a neuron calculates <code>z = w·x + b</code> (affine part) and then <code>activation = A(z)</code> (activation part), how does this <code>activation</code> change if we wiggle the inputs <code>x</code>, the weights <code>w</code>, or the bias <code>b</code>?</li></ul><p>This is a fundamental building block for backpropagation. When we have the gradient of the loss with respect to a neuron&rsquo;s <em>activation</em> (this would be like <code>∂L/∂activation</code>, coming from layers above), we&rsquo;ll need <code>∂activation/∂w</code> and <code>∂activation/∂b</code> to find out how to update that neuron&rsquo;s weights and bias.</p><p><em>What we are ultimately trying to achieve here is to find the specific derivative expressions (gradients) for a single neuron&rsquo;s output with respect to its inputs and its own parameters (weights and bias). These will be the local gradients used in the backpropagation algorithm.</em></p><p><strong>Breaking it Down:</strong></p><p>The typical neuron computation is:</p><ol><li><strong>Affine function:</strong> <code>z(w, b, x) = w · x + b</code> (a scalar output if we consider one neuron for now)</li><li><strong>Activation function:</strong> <code>activation(z) = A(z)</code>. The paper uses <code>A(z) = max(0, z)</code> (ReLU) as the example.</li></ol><p>So, the overall neuron activation is <code>activation(x) = max(0, w · x + b)</code>.</p><p><strong>Step 1: Focus on the Affine Part <code>z = w · x + b</code> (Page 24, bottom half)</strong></p><p>Let <code>y = w · x + b</code>. (The paper uses <code>y</code> here for the affine part before <code>max</code>, which is a bit confusing as <code>y</code> is often the final output. Let&rsquo;s stick to <code>z</code> for the affine part as the paper does in <code>max(0,z)</code>).
So, <code>z = w · x + b</code>.
We need <code>∂z/∂w</code> and <code>∂z/∂b</code>.</p><ul><li><p><strong><code>∂z/∂w</code> (Derivative of <code>w·x + b</code> with respect to vector <code>w</code>):</strong></p><ul><li><code>∂/∂w (w·x)</code>: This is the derivative of a dot product. The paper cleverly rephrases <code>w·x</code> as <code>sum(w ⊗ x)</code> where <code>⊗</code> is element-wise multiplication.</li><li>Let <code>u = w ⊗ x</code>. We know from Section 4.2 that <code>∂u/∂w = diag(x)</code>.</li><li>Let <code>z' = sum(u)</code>. We know from Section 4.4 that <code>∂z'/∂u = 1ᵀ</code> (a row vector of ones).</li><li>Using the vector chain rule for <code>∂z'/∂w = (∂z'/∂u) * (∂u/∂w)</code>:
<code>∂(w·x)/∂w = 1ᵀ * diag(x) = xᵀ</code> (a row vector).</li><li><code>∂/∂w (b)</code>: Since <code>b</code> is a scalar bias and doesn&rsquo;t depend on <code>w</code>, this derivative is <code>0ᵀ</code> (a row vector of zeros).</li><li>So, <code>∂z/∂w = ∂(w·x + b)/∂w = xᵀ + 0ᵀ = xᵀ</code>.</li><li><strong>Intuition:</strong> How does <code>z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b</code> change if <code>wⱼ</code> changes? It changes by <code>xⱼ</code>. So the gradient vector is <code>[x₁, x₂, ..., xₙ]</code>, which is <code>xᵀ</code>.</li><li><em>What this is trying to achieve:</em> It tells us that the sensitivity of the pre-activation value <code>z</code> to a change in a weight <code>wⱼ</code> is simply the value of the corresponding input <code>xⱼ</code>. Larger inputs mean that their corresponding weights have a bigger impact.</li></ul></li><li><p><strong><code>∂z/∂b</code> (Derivative of <code>w·x + b</code> with respect to scalar <code>b</code>):</strong></p><ul><li><code>∂/∂b (w·x)</code>: This is 0, as <code>w·x</code> doesn&rsquo;t depend on <code>b</code>.</li><li><code>∂/∂b (b)</code>: This is 1.</li><li>So, <code>∂z/∂b = 0 + 1 = 1</code>.</li><li><strong>Intuition:</strong> If you change the bias <code>b</code> by a small amount <code>Δ</code>, <code>z</code> changes by <code>Δ</code>. The rate of change is 1.</li><li><em>What this is trying to achieve:</em> It tells us that the pre-activation value <code>z</code> changes one-to-one with changes in the bias <code>b</code>.</li></ul></li></ul><p><strong>Step 2: Tackle the <code>max(0, z)</code> Activation Function (ReLU) (Page 24, bottom, and Page 25, top)</strong></p><p>Let <code>activation = A(z) = max(0, z)</code>. We need <code>∂A/∂z</code>.
The <code>max(0,z)</code> function is piecewise:</p><ul><li>If <code>z ≤ 0</code>, then <code>A(z) = 0</code>. The derivative <code>dA/dz = 0</code>.</li><li>If <code>z > 0</code>, then <code>A(z) = z</code>. The derivative <code>dA/dz = 1</code>.
The paper writes this as: <code>∂/∂z max(0, z) = { 0 if z ≤ 0; 1 if z > 0 }</code>.
(Note: Technically, it&rsquo;s not differentiable at <code>z=0</code>, but in practice, we can assign a subderivative of 0 or 1).</li></ul><p>The paper then briefly mentions &ldquo;broadcasting&rdquo; for <code>max(0, x)</code> if <code>x</code> is a vector, meaning <code>max</code> is applied element-wise. The derivative <code>∂/∂xᵢ max(0, xᵢ)</code> would be 0 if <code>xᵢ ≤ 0</code> and 1 if <code>xᵢ > 0</code>.</p><p><strong>Step 3: Combine using the Chain Rule for <code>activation(x) = max(0, w · x + b)</code> (Page 25)</strong></p><p>Let <code>z(w, b, x) = w · x + b</code> (intermediate scalar variable representing the affine part).
Let <code>activation(z) = max(0, z)</code>.</p><p>We want <code>∂activation/∂w</code> and <code>∂activation/∂b</code>.
Using the vector chain rule (or its scalar adaptation here since <code>z</code> and <code>activation</code> are scalars for a single neuron):</p><ul><li><p><strong><code>∂activation/∂w = (∂activation/∂z) * (∂z/∂w)</code></strong></p><ul><li><code>∂activation/∂z</code> is <code>0</code> if <code>z ≤ 0</code> and <code>1</code> if <code>z > 0</code>.</li><li><code>∂z/∂w</code> is <code>xᵀ</code>.</li><li>So, <code>∂activation/∂w = { 0ᵀ if z ≤ 0 (i.e., w·x+b ≤ 0) }</code>
<code>{ xᵀ if z > 0 (i.e., w·x+b > 0) }</code></li><li><em>What this is ultimately trying to achieve:</em> If the neuron is &ldquo;off&rdquo; (pre-activation <code>z</code> is negative or zero, so ReLU output is 0), then small changes to the weights <code>w</code> have <em>no effect</em> on the output (gradient is 0). If the neuron is &ldquo;on&rdquo; (<code>z > 0</code>, ReLU output is <code>z</code>), then the way the output changes with respect to <code>w</code> is the same as how <code>z</code> changes with respect to <code>w</code>, which is <code>xᵀ</code>.</li></ul></li><li><p><strong><code>∂activation/∂b = (∂activation/∂z) * (∂z/∂b)</code></strong></p><ul><li><code>∂activation/∂z</code> is <code>0</code> if <code>z ≤ 0</code> and <code>1</code> if <code>z > 0</code>.</li><li><code>∂z/∂b</code> is <code>1</code>.</li><li>So, <code>∂activation/∂b = { 0 if z ≤ 0 (i.e., w·x+b ≤ 0) }</code>
<code>{ 1 if z > 0 (i.e., w·x+b > 0) }</code></li><li><em>What this is ultimately trying to achieve:</em> If the neuron is &ldquo;off&rdquo;, changes to <code>b</code> have no effect on the output. If the neuron is &ldquo;on&rdquo;, changes to <code>b</code> have a one-to-one effect on the output.</li></ul></li></ul><p><strong>Key Takeaway from Section 5:</strong>
This section derived the gradients of a single ReLU neuron&rsquo;s activation with respect to its weights <code>w</code> and its bias <code>b</code>.</p><ul><li><code>∂(ReLU(w·x+b))/∂w = xᵀ</code> if <code>w·x+b > 0</code>, and <code>0ᵀ</code> otherwise.</li><li><code>∂(ReLU(w·x+b))/∂b = 1</code> if <code>w·x+b > 0</code>, and <code>0</code> otherwise.</li></ul><p>These are the <em>local gradients</em> for a single neuron. In backpropagation, these local gradients will be multiplied by the gradient of the loss coming from the layer above (using the chain rule again!) to find out how much that specific neuron&rsquo;s weights and bias contributed to the overall network error. This tells us how to update <code>w</code> and <code>b</code> for <em>that neuron</em>.</p><h1 id=section-6-the-gradient-of-the-neural-network-loss-function><strong>Section 6: The gradient of the neural network loss function</strong></h1><p><strong>The Big Picture of This Section:</strong></p><p>This is where everything we&rsquo;ve learned about matrix calculus (gradients, Jacobians, chain rules) comes together to explain how a neural network learns by minimizing a loss function.</p><p>We have:</p><ol><li>A <strong>model (our neural network)</strong> with parameters (weights <code>w</code> and biases <code>b</code> for all neurons).</li><li>A <strong>loss function <code>C(w, b, X, y)</code></strong> that measures how &ldquo;bad&rdquo; the model&rsquo;s predictions are on a set of training data <code>X</code> with true labels <code>y</code>. For example, Mean Squared Error for regression.</li><li>An <strong>optimization algorithm (Gradient Descent)</strong> that needs the gradient of this loss function <code>C</code> with respect to <em>all</em> the model&rsquo;s parameters (<code>w</code>&rsquo;s and <code>b</code>&rsquo;s) to update them and reduce the loss.</li></ol><p><em>What we are ultimately trying to achieve in this section is to derive the formulas for <code>∂C/∂w</code> and <code>∂C/∂b</code> for a simple neuron, using the chain rule and the neuron activation gradients we found in Section 5. This will show the complete calculation for one neuron&rsquo;s parameter updates.</em></p><p><strong>Setting the Stage (Page 25, bottom):</strong></p><p>The paper considers training a single neuron.</p><ul><li><strong>Input:</strong> <code>X</code> is a matrix where each row <code>xᵢ</code> is an input vector for one training instance. <code>N</code> is the number of instances.</li><li><strong>Target:</strong> <code>y</code> is a vector where each <code>yᵢ</code> is the scalar target output for instance <code>xᵢ</code>.</li><li><strong>Loss Function (Mean Squared Error):</strong>
<code>C(w, b, X, y) = (1/N) * Σᵢ (yᵢ - activation(xᵢ))²</code>
where <code>activation(xᵢ) = max(0, w · xᵢ + b)</code>.</li></ul><p><strong>Following the Chain Rule Process (Page 26):</strong></p><p>To find <code>∂C/∂w</code> (and later <code>∂C/∂b</code>), we introduce intermediate variables to break down the loss function calculation for a <em>single instance</em> <code>xᵢ</code> first, and then we&rsquo;ll average over all instances.</p><p>Let&rsquo;s consider the contribution of one instance <code>x</code> (dropping the <code>i</code> superscript for a moment) to the loss: <code>(y_true - activation(x))²</code>.
The paper uses these intermediate variables:</p><ol><li><code>u(w, b, x) = max(0, w · x + b)</code> (This is the neuron&rsquo;s activation. Let&rsquo;s call it <code>a</code> for activation to avoid confusion with <code>u</code> often being an intermediate in chain rule). So, <code>a = max(0, w·x+b)</code>.</li><li><code>v(y_true, a) = y_true - a</code> (This is the error for this instance).</li><li><code>L_instance(v) = v²</code> (The squared error for this instance. The total loss <code>C</code> will be the average of these <code>L_instance</code> values).</li></ol><p><strong>6.1 The gradient with respect to the weights <code>w</code> (Pages 26-27)</strong></p><p>We want <code>∂C/∂w</code>. Since <code>C</code> is an average, <code>∂C/∂w = (1/N) Σᵢ (∂L_instanceᵢ / ∂w)</code>.
Let&rsquo;s find <code>∂L_instance / ∂w</code> for one instance.
<code>L_instance = v²</code>
<code>v = y_true - a</code>
<code>a = max(0, w·x + b)</code></p><p>Using the chain rule: <code>∂L_instance/∂w = (∂L_instance/∂v) * (∂v/∂a) * (∂a/∂w)</code></p><ol><li><code>∂L_instance/∂v = ∂(v²)/∂v = 2v</code></li><li><code>∂v/∂a = ∂(y_true - a)/∂a = -1</code></li><li><code>∂a/∂w = ∂(max(0, w·x+b))/∂w</code>. We found this in Section 5!<ul><li>It&rsquo;s <code>xᵀ</code> if <code>w·x+b > 0</code> (neuron is active).</li><li>It&rsquo;s <code>0ᵀ</code> if <code>w·x+b ≤ 0</code> (neuron is not active).</li></ul></li></ol><p>Combining these for one instance:
<code>∂L_instance/∂w = (2v) * (-1) * (xᵀ or 0ᵀ)</code>
<code>∂L_instance/∂w = -2v * (xᵀ or 0ᵀ)</code>
Substitute <code>v = y_true - a = y_true - max(0, w·x+b)</code>:
<code>∂L_instance/∂w = -2(y_true - max(0, w·x+b)) * (xᵀ or 0ᵀ)</code></p><p>The paper simplifies this into a conditional expression (top of page 26, and then expanded middle of page 27):</p><ul><li>If <code>w·x + b ≤ 0</code> (neuron not active, <code>max(0,...) = 0</code>):
Then <code>∂L_instance/∂w = -2(y_true - 0) * 0ᵀ = 0ᵀ</code>. (Makes sense, if neuron output is 0, weights don&rsquo;t affect it in this region).</li><li>If <code>w·x + b > 0</code> (neuron active, <code>max(0,...) = w·x+b</code>):
Then <code>∂L_instance/∂w = -2(y_true - (w·x+b)) * xᵀ = 2((w·x+b) - y_true) * xᵀ</code>.</li></ul><p>Now, average over all <code>N</code> instances <code>xᵢ</code>:
<code>∂C/∂w = (1/N) Σᵢ 2 * ( (w·xᵢ+b) - yᵢ ) * xᵢᵀ</code> (This sum is only for instances where <code>w·xᵢ+b > 0</code>).
Or, more compactly, as shown in the paper before the &ldquo;To interpret that equation&mldr;&rdquo; part:
<code>∂C/∂w = (2/N) Σᵢ eᵢ xᵢᵀ</code> (where <code>eᵢ = (w·xᵢ+b) - yᵢ</code> is the error, and this sum is for the &ldquo;nonzero activation case,&rdquo; i.e., where <code>w·xᵢ+b > 0</code>).</p><ul><li><strong>Interpretation (Page 27):</strong><ul><li><code>∂C/∂w</code> is a <strong>weighted average of the input vectors <code>xᵢᵀ</code></strong> (transposed to be row vectors, which when summed give the gradient row vector).</li><li>The <strong>weights for this average are the error terms <code>eᵢ</code></strong> (the difference between the neuron&rsquo;s affine output <code>w·xᵢ+b</code> and the target <code>yᵢ</code>).</li><li><em>What this is ultimately trying to achieve:</em><ul><li>If the error <code>eᵢ</code> for instance <code>xᵢ</code> is large and positive (model predicted too high), the gradient <code>∂C/∂w</code> will be pushed in the direction of <code>xᵢᵀ</code>. To reduce the error via gradient descent (which subtracts the gradient), the weights <code>w</code> will be adjusted <em>against</em> <code>xᵢᵀ</code>, effectively reducing the output <code>w·xᵢ+b</code>.</li><li>If the error <code>eᵢ</code> is large and negative (model predicted too low), the gradient <code>∂C/∂w</code> will be pushed in the direction <em>opposite</em> to <code>xᵢᵀ</code>. Gradient descent will adjust <code>w</code> <em>towards</em> <code>xᵢᵀ</code>, increasing the output.</li><li>Instances <code>xᵢ</code> that result in larger errors <code>eᵢ</code> have more influence on the gradient (they &ldquo;pull&rdquo; the weights harder).</li><li>This is exactly what we want for learning! The gradient points in a direction that tells us how to change <code>w</code> to reduce the average error.</li></ul></li></ul></li></ul><p><strong>Gradient Descent Update (Page 27, bottom):</strong>
<code>w_next = w_current - η * (∂C/∂w)</code>
This is the standard gradient descent update rule. Because the gradient <code>∂C/∂w</code> points towards <em>higher</em> cost, we subtract a small amount of it (scaled by learning rate <code>η</code>) from the current weights to move towards <em>lower</em> cost.</p><p><strong>6.2 The derivative with respect to the bias <code>b</code> (Pages 27-28)</strong></p><p>The process is very similar. We want <code>∂C/∂b</code>.
<code>∂C/∂b = (1/N) Σᵢ (∂L_instanceᵢ / ∂b)</code>.
Using the chain rule: <code>∂L_instance/∂b = (∂L_instance/∂v) * (∂v/∂a) * (∂a/∂b)</code></p><ol><li><code>∂L_instance/∂v = 2v</code> (same as before)</li><li><code>∂v/∂a = -1</code> (same as before)</li><li><code>∂a/∂b = ∂(max(0, w·x+b))/∂b</code>. We found this in Section 5!<ul><li>It&rsquo;s <code>1</code> if <code>w·x+b > 0</code> (neuron is active).</li><li>It&rsquo;s <code>0</code> if <code>w·x+b ≤ 0</code> (neuron is not active).</li></ul></li></ol><p>Combining these for one instance:
<code>∂L_instance/∂b = (2v) * (-1) * (1 or 0)</code>
<code>∂L_instance/∂b = -2v * (1 or 0)</code>
Substitute <code>v = y_true - max(0, w·x+b)</code>:</p><ul><li>If <code>w·x + b ≤ 0</code>:
<code>∂L_instance/∂b = -2(y_true - 0) * 0 = 0</code>.</li><li>If <code>w·x + b > 0</code>:
<code>∂L_instance/∂b = -2(y_true - (w·x+b)) * 1 = 2((w·x+b) - y_true)</code>.</li></ul><p>Averaging over all <code>N</code> instances <code>xᵢ</code>:
<code>∂C/∂b = (1/N) Σᵢ 2 * ( (w·xᵢ+b) - yᵢ )</code> (This sum is only for instances where <code>w·xᵢ+b > 0</code>).
Or, as the paper shows:
<code>∂C/∂b = (2/N) Σᵢ eᵢ</code> (where <code>eᵢ = (w·xᵢ+b) - yᵢ</code>, summed for the &ldquo;nonzero activation case&rdquo;).</p><ul><li><strong>Interpretation (Page 28):</strong><ul><li>The partial derivative <code>∂C/∂b</code> is just the <strong>average error <code>eᵢ</code></strong> (for activated neurons), scaled by 2.</li><li><em>What this is ultimately trying to achieve:</em><ul><li>If, on average, the neuron is predicting too high (average <code>eᵢ</code> is positive), the gradient <code>∂C/∂b</code> is positive. Gradient descent <code>b_next = b - η(∂C/∂b)</code> will decrease <code>b</code>, reducing the neuron&rsquo;s output.</li><li>If, on average, the neuron is predicting too low (average <code>eᵢ</code> is negative), the gradient <code>∂C/∂b</code> is negative. Gradient descent will increase <code>b</code>, increasing the neuron&rsquo;s output.</li></ul></li></ul></li></ul><p><strong>Combining <code>w</code> and <code>b</code> (Page 29, top):</strong>
The paper mentions that in practice, it&rsquo;s convenient to combine <code>w</code> and <code>b</code> into a single parameter vector <code>ŵ = [wᵀ, b]ᵀ</code> and augment the input vector <code>x</code> with a 1: <code>✰ = [xᵀ, 1]ᵀ</code>. Then the affine function is just <code>ŵ · ✰</code>. This simplifies some notation but the underlying derivative calculations are the same.</p><p><strong>Section 7: Summary (Page 29)</strong>
This section of the paper concludes that we now have the two partials (<code>∂C/∂w</code> and <code>∂C/∂b</code>) necessary to perform gradient descent and optimize the parameters of a single neuron. It also reminds us that the next step in understanding full backpropagation for a multi-layer network would be to learn about partial derivatives of matrices, not just vectors.</p><p><strong>Key Takeaway from Section 6:</strong>
This section ties everything together for a single neuron. By applying the chain rule, we derived how the overall Mean Squared Error loss changes with respect to the neuron&rsquo;s weights <code>w</code> and its bias <code>b</code>.</p><ul><li><code>∂C/∂w</code> is an average of input vectors <code>xᵢᵀ</code>, weighted by the prediction errors <code>eᵢ</code>.</li><li><code>∂C/∂b</code> is an average of the prediction errors <code>eᵢ</code>.
These gradients tell gradient descent <em>how to adjust</em> <code>w</code> and <code>b</code> to make the neuron&rsquo;s predictions better on average for the given training data. This is the fundamental step of learning for one neuron. Backpropagation is essentially applying this logic layer by layer, from the output back to the input, using the chain rule at each step.</li></ul></div><hr><div class=paper-reference><p><strong>Read the original paper:</strong>
<a href=https://arxiv.org/pdf/1802.01528 target=_blank rel="noopener noreferrer">The Matrix Calculus You Need For Deep Learning</a></p><p class=paper-disclaimer><em>The content presented here is a collection of my personal notes and explanations based on the paper. This is by no means an exhaustive explanation, and I strongly encourage you to read the actual paper for a comprehensive understanding.</em></p></div></article></div></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer><a id=scrollTopBtn title="Go to top"><i class="fa-solid fa-arrow-up"></i></a>
<script src=/js/search.js></script><script>var mybutton=document.getElementById("scrollTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.classList.add("show"):mybutton.classList.remove("show")}mybutton.onclick=function(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("code.language-mermaid");e.forEach(function(e,t){const n=document.createElement("div");n.className="mermaid",n.textContent=e.textContent,n.id="mermaid-"+t,e.parentNode.parentNode.replaceChild(n,e.parentNode)}),mermaid.initialize({startOnLoad:!0,theme:"default",themeVariables:{primaryColor:"#4a90e2",primaryTextColor:"#333",primaryBorderColor:"#4a90e2",lineColor:"#333"}}),mermaid.init()})</script></body></html>