<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>404EngineerNotFound</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin=anonymous referrerpolicy=no-referrer></head><body>\<header><nav><div class=logo><a href=/>404EngineerNotFound</a></div><ul class=main-nav><li class="nav-item has-dropdown"><a href=#>Writings <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/stories/>Stories</a></li><li class=dropdown-item><a href=/thoughts/>Thoughts</a></li><li class=dropdown-item><a href=/fitness-log/>Fitness Log</a></li></ul></li><li class="nav-item has-dropdown"><a href=#>Tech Lab <i class="fas fa-caret-down fa-xs"></i></a><ul class=dropdown-menu><li class=dropdown-item><a href=/papershelf/>Papershelf</a></li><li class=dropdown-item><a href=/creations/>Creations</a></li><li class=dropdown-item><a href=/dsa-log/>DSA Log</a></li><li class=dropdown-item><a href=/tech-writings/>Technical Writings</a></li></ul></li><li class=nav-item><a href=/bookshelf/>Bookshelf</a></li><li class=nav-item><a href=/about/>About</a></li></ul></nav></header><main><article class=paper-single><div class=paper-categories>Filed under:
<span class=category-pill>AI</span></div><h1>The Matrix Calculus You Need For Deep Learning</h1><span class=reading-time><em>27 min read</em></span><div class=paper-content><h2 id=the-big-picture-goal><strong>The Big Picture Goal:</strong></h2><p>We want our machine learning models to make good predictions. To do this, we define a <strong>loss function</strong> (or cost function) that measures how &ldquo;bad&rdquo; our model&rsquo;s predictions are compared to the true values.</p><ul><li><strong>Training = Minimizing Loss:</strong> Training a model means finding the model parameters (like weights <code>w</code> and biases <code>b</code>) that make this loss function as small as possible.</li><li><strong>Calculus as a Tool for Minimization:</strong> Derivatives (and their extensions like gradients) tell us the <strong>rate of change</strong> or the <strong>slope</strong> of a function.<ul><li>If we know the slope of our loss function with respect to our model parameters, we know which &ldquo;direction&rdquo; to tweak those parameters to <em>decrease</em> the loss. This is the essence of <strong>Gradient Descent</strong>.</li></ul></li></ul><p>So, all this calculus is ultimately about finding an efficient way to &ldquo;walk downhill&rdquo; on our loss function surface to find the parameter values that give the lowest error.</p><p>Now, let&rsquo;s look at the paper.</p><h2 id=page-3-of-the-paper-section-1---introduction><strong>(Page 3 of the paper: Section 1 - Introduction)</strong></h2><ul><li><p>The paper acknowledges that derivatives are critical for ML, especially deep neural networks, which are trained by optimizing a loss function.</p></li><li><p>It mentions that while modern libraries have &ldquo;automatic differentiation,&rdquo; understanding the underlying matrix calculus helps to &ldquo;grok academic papers&rdquo; and understand what&rsquo;s happening &ldquo;under the hood.&rdquo;</p></li><li><p>It gives an example of a single neuron&rsquo;s activation:</p><ul><li><code>z(x) = w · x + b</code> (affine function: dot product of weights <code>w</code> with input <code>x</code>, plus a bias <code>b</code>).</li><li>Followed by an activation function, e.g., <code>max(0, z(x))</code> (ReLU).</li></ul></li><li><p><strong>Key statement on training:</strong> &ldquo;Training this neuron means choosing weights <code>w</code> and bias <code>b</code> so that we get the desired output for all N inputs <code>x</code>. To do that, we minimize a loss function&mldr; All of those [gradient descent methods] require the partial derivative (the gradient) of activation(x) with respect to the model parameters <code>w</code> and <code>b</code>. Our goal is to gradually tweak <code>w</code> and <code>b</code> so that the overall loss function keeps getting smaller&mldr;&rdquo;</p><ul><li><em>What this is ultimately trying to achieve:</em> Use the gradient (which calculus helps us find) to iteratively adjust <code>w</code> and <code>b</code> to reduce the model&rsquo;s error.</li></ul></li><li><p>The introduction to the paper&rsquo;s goal: &ldquo;&mldr;rederive and rediscover some key matrix calculus rules in an effort to explain them&mldr; There aren’t dozens of new rules to learn; just a couple of key concepts.&rdquo; This is encouraging!</p></li></ul><p>Let&rsquo;s proceed to <strong>Section 2: Review: Scalar derivative rules</strong> (Page 4 of the paper). This should be a good refresher. Are you ready to look at these basic rules? We can take it slow and make sure each one makes sense.</p><p>Alright, let&rsquo;s dive into <strong>Section 2: Review: Scalar derivative rules</strong> from the Parr & Howard paper (page 4, extending to the table on page 5).</p><p>This section is a quick refresher of the basic derivative rules you likely encountered in a first calculus course. The core idea of a derivative for a function of a single variable, <code>f(x)</code>, is to find its <strong>instantaneous rate of change</strong> or the <strong>slope of the tangent line</strong> at any given point <code>x</code>.</p><p><em>What we are ultimately trying to achieve with derivatives (in an ML context):</em> We want to know how a small change in an input (like a model parameter <code>θ</code> or an input feature <code>x</code>) will affect the output of a function (like the loss function <code>J(θ)</code> or an activation function <code>a(z)</code>). If the derivative is large and positive, a small increase in the input leads to a large increase in the output. If it&rsquo;s negative, an increase in input leads to a decrease in output. If it&rsquo;s zero, the function is momentarily flat at that point (which could indicate a minimum, maximum, or a plateau).</p><p>Let&rsquo;s look at the rules presented in the table on page 5:</p><ol><li><p><strong>Constant Rule:</strong></p><ul><li>Rule: <code>f(x) = c</code> (where <code>c</code> is a constant)</li><li>Scalar derivative notation: <code>d/dx (c) = 0</code></li><li>Example: <code>d/dx (99) = 0</code></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> A constant function doesn&rsquo;t change, no matter what <code>x</code> is. So, its rate of change (slope) is always zero. Think of a flat horizontal line – its slope is 0.</li></ul></li><li><p><strong>Multiplication by Constant Rule:</strong></p><ul><li>Rule: <code>f(x) = cf(x)</code> (actually, it should be <code>g(x) = c * f(x)</code>)</li><li>Scalar derivative notation: <code>d/dx (c * f(x)) = c * (df/dx)</code></li><li>Example: <code>d/dx (3x) = 3 * (d/dx (x)) = 3 * 1 = 3</code></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> If you scale a function by a constant, its rate of change (slope) at any point is also scaled by that same constant. The shape of the change is the same, just amplified or shrunk.</li></ul></li><li><p><strong>Power Rule:</strong></p><ul><li>Rule: <code>f(x) = xⁿ</code></li><li>Scalar derivative notation: <code>d/dx (xⁿ) = nxⁿ⁻¹</code></li><li>Example: <code>d/dx (x³) = 3x³⁻¹ = 3x²</code></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> This rule tells us how functions involving powers of <code>x</code> change. For <code>x³</code>, the slope isn&rsquo;t constant; it changes depending on <code>x</code>. At <code>x=1</code>, slope is 3. At <code>x=2</code>, slope is <code>3*(2)² = 12</code>.</li></ul></li><li><p><strong>Sum Rule:</strong></p><ul><li>Rule: <code>h(x) = f(x) + g(x)</code></li><li>Scalar derivative notation: <code>d/dx (f(x) + g(x)) = (df/dx) + (dg/dx)</code></li><li>Example: <code>d/dx (x² + 3x) = (d/dx x²) + (d/dx 3x) = 2x + 3</code></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> The rate of change of a sum of functions is the sum of their individual rates of change. If one part is changing quickly and another slowly, the sum changes according to the combined effect.</li></ul></li><li><p><strong>Difference Rule:</strong></p><ul><li>Rule: <code>h(x) = f(x) - g(x)</code></li><li>Scalar derivative notation: <code>d/dx (f(x) - g(x)) = (df/dx) - (dg/dx)</code></li><li>Example: <code>d/dx (x² - 3x) = (d/dx x²) - (d/dx 3x) = 2x - 3</code></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> Similar to the sum rule, the rate of change of a difference is the difference of the rates of change.</li></ul></li><li><p><strong>Product Rule:</strong></p><ul><li>Rule: <code>h(x) = f(x)g(x)</code></li><li>Scalar derivative notation: <code>d/dx (f(x)g(x)) = f(x)(dg/dx) + (df/dx)g(x)</code></li><li>Example: <code>d/dx (x² * x)</code> (where <code>f(x)=x²</code>, <code>g(x)=x</code>)<ul><li><code>= x² * (d/dx x) + (d/dx x²) * x</code></li><li><code>= x² * 1 + 2x * x = x² + 2x² = 3x²</code></li><li>(This is consistent with <code>d/dx (x³) = 3x²</code>)</li></ul></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> When two functions are multiplied, a change in <code>x</code> affects both. The overall rate of change depends on how <code>g(x)</code> changes (weighted by <code>f(x)</code>) <em>plus</em> how <code>f(x)</code> changes (weighted by <code>g(x)</code>).</li></ul></li><li><p><strong>Chain Rule (for single variable nested functions):</strong></p><ul><li>Rule: <code>h(x) = f(g(x))</code> (a function <code>f</code> applied to the output of another function <code>g</code>)</li><li>Scalar derivative notation: <code>d/dx f(g(x)) = (df/du) * (du/dx)</code>, where <code>u = g(x)</code>.<ul><li>The paper writes this as <code>df(u)/du * du/dx</code>.</li></ul></li><li>Example: <code>d/dx ln(x²)</code><ul><li>Let <code>u = g(x) = x²</code>. Then <code>f(u) = ln(u)</code>.</li><li><code>du/dx = d/dx (x²) = 2x</code>.</li><li><code>df/du = d/du (ln(u)) = 1/u</code>.</li><li>So, <code>d/dx ln(x²) = (1/u) * (2x) = (1/x²) * (2x) = 2x/x² = 2/x</code>.</li></ul></li><li><em>What it&rsquo;s ultimately trying to achieve:</em> It tells us how to find the rate of change of a composite function. How much does the <em>outer function</em> <code>f</code> change with respect to its input <code>u</code> (<code>df/du</code>), multiplied by how much that <em>inner input <code>u</code></em> changes with respect to the original variable <code>x</code> (<code>du/dx</code>). It &ldquo;chains&rdquo; the rates of change together. This rule is <em>extremely</em> important for neural networks because they are essentially deeply nested functions, and backpropagation is an application of the chain rule.</li></ul></li></ol><p>The paper also mentions (bottom of page 4, top of page 5):</p><ul><li>The notation <code>d/dx</code> can be thought of as an <strong>operator</strong> that takes a function <code>f(x)</code> and gives you its derivative <code>df/dx</code>.</li><li>This operator view is helpful because the operator is <strong>distributive</strong> (<code>d/dx (f+g) = df/dx + dg/dx</code>) and lets you <strong>pull out constants</strong> (<code>d/dx (cf) = c * df/dx</code>). This simplifies taking derivatives of complex expressions, like their example:
<code>d/dx (9(x + x²)) = 9 * d/dx (x + x²) = 9 * (d/dx x + d/dx x²) = 9 * (1 + 2x) = 9 + 18x</code>.</li></ul><p>This review of scalar derivative rules is foundational. For deep learning and training models with Gradient Descent, we are constantly asking: &ldquo;If I wiggle this weight/bias a tiny bit, how much does my loss function change?&rdquo; Derivatives give us the answer to that question.</p><h2 id=section-3-introduction-to-vector-calculus-and-partial-derivatives-in-the-parr--howard-paper-page-5><strong>Section 3: Introduction to vector calculus and partial derivatives</strong> in the Parr & Howard paper (page 5).</h2><p>This section transitions us from functions of a single variable (<code>f(x)</code>) to functions of <em>multiple</em> variables (e.g., <code>f(x, y)</code>). Neural network layers often have many inputs (features) and many parameters (weights, biases), so we need to be able to talk about how the output changes when we wiggle <em>one specific input or parameter</em>, while keeping others constant.</p><ul><li><p><strong>Functions of Multiple Parameters:</strong></p><ul><li>Instead of <code>f(x)</code>, we now consider <code>f(x, y)</code>. For example, the product <code>xy</code>.</li><li>If we want to know its derivative, the question is &ldquo;derivative with respect to what?&rdquo; With respect to <code>x</code>, or with respect to <code>y</code>? The change in <code>xy</code> will be different depending on which variable we &ldquo;wiggle.&rdquo;</li></ul></li><li><p><strong>Partial Derivatives:</strong></p><ul><li>When we have a function of multiple variables, we compute <strong>partial derivatives</strong>.</li><li>The notation changes from <code>d/dx</code> to <code>∂/∂x</code> (using a stylized &rsquo;d&rsquo;, often called &ldquo;del&rdquo; or &ldquo;curly d&rdquo;).</li><li><strong><code>∂f(x,y)/∂x</code></strong>: This is the partial derivative of <code>f(x,y)</code> with respect to <code>x</code>.<ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> It tells us the rate of change of the function <code>f</code> if we change <code>x</code> by a tiny amount, <em>while holding all other variables (in this case, <code>y</code>) constant</em>. You treat <code>y</code> as if it were just a number.</li></ul></li><li><strong><code>∂f(x,y)/∂y</code></strong>: This is the partial derivative of <code>f(x,y)</code> with respect to <code>y</code>.<ul><li><em>What it&rsquo;s ultimately trying to achieve:</em> It tells us the rate of change of <code>f</code> if we change <code>y</code> by a tiny amount, <em>while holding <code>x</code> constant</em>.</li></ul></li></ul></li><li><p><strong>Example from the paper:</strong> <code>f(x, y) = 3x²y</code></p><ul><li><strong>Partial derivative with respect to <code>x</code> (<code>∂f/∂x</code>):</strong><ul><li>Treat <code>y</code> (and <code>3</code>) as constants.</li><li><code>∂/∂x (3x²y) = 3y * (∂/∂x x²) = 3y * (2x) = 6yx</code>.</li><li><em>Intuition:</em> If <code>y</code> is fixed, say <code>y=2</code>, then <code>f(x,2) = 6x²</code>. The derivative <code>d/dx (6x²) = 12x</code>. Our partial derivative <code>6yx</code> gives <code>6(2)x = 12x</code>. It matches!</li></ul></li><li><strong>Partial derivative with respect to <code>y</code> (<code>∂f/∂y</code>):</strong><ul><li>Treat <code>x</code> (and <code>3x²</code>) as constants.</li><li><code>∂/∂y (3x²y) = 3x² * (∂/∂y y) = 3x² * 1 = 3x²</code>.</li><li><em>Intuition:</em> If <code>x</code> is fixed, say <code>x=1</code>, then <code>f(1,y) = 3y</code>. The derivative <code>d/dy (3y) = 3</code>. Our partial derivative <code>3x²</code> gives <code>3(1)² = 3</code>. It matches!</li></ul></li></ul></li><li><p><strong>The Gradient (∇f):</strong></p><ul><li>For a function of multiple variables, like <code>f(x,y)</code>, we can organize all its partial derivatives into a <strong>vector</strong>. This vector is called the <strong>gradient</strong> of <code>f</code>, denoted by <code>∇f</code> (nabla f).</li><li>For <code>f(x,y) = 3x²y</code>, the paper writes:
<code>∇f(x, y) = [∂f/∂x, ∂f/∂y] = [6yx, 3x²]</code></li><li><em>What the gradient is ultimately trying to achieve:</em> The gradient vector <code>∇f</code> at a specific point <code>(x₀, y₀)</code> points in the direction of the <strong>steepest ascent</strong> (fastest increase) of the function <code>f</code> at that point. Its magnitude <code>||∇f||</code> tells you the rate of increase in that direction.</li><li>In machine learning, our loss function <code>J(θ)</code> depends on many parameters <code>θ₁, θ₂, ..., θₙ</code>. The gradient <code>∇J(θ)</code> will be a vector <code>[∂J/∂θ₁, ∂J/∂θ₂, ..., ∂J/∂θₙ]</code>.</li><li><strong>Gradient Descent</strong> uses this: it calculates <code>∇J(θ)</code> and then takes a step in the <em>opposite</em> direction (<code>-∇J(θ)</code>) to go &ldquo;downhill&rdquo; and reduce the loss.</li></ul></li></ul><p>This section essentially extends the concept of a derivative (rate of change/slope) from single-variable functions to multi-variable functions by considering the rate of change with respect to each variable <em>individually</em>, holding others constant. The gradient then bundles all these partial rates of change into a single vector that gives us the &ldquo;overall&rdquo; direction of steepest increase.</p><h2 id=section-4-matrix-calculus-in-the-parr--howard-paper-page-6><strong>Section 4: Matrix calculus</strong> in the Parr & Howard paper (page 6).</h2><p>This is where things get a bit more generalized. So far:</p><ul><li>Scalar derivative: <code>df/dx</code> (function of one variable, derivative is a scalar).</li><li>Vector calculus (gradient): <code>∇f(x,y) = [∂f/∂x, ∂f/∂y]</code> (function of multiple scalar variables <code>x, y</code>, output is a scalar <code>f</code>, gradient is a vector of partials).</li></ul><p><strong>Matrix calculus</strong> deals with derivatives when:</p><ol><li>The input is a vector (e.g., <code>x = [x₁, x₂]</code>).</li><li>The output can also be a vector (e.g., <code>y = [f₁(x), f₂(x)]</code>).</li></ol><ul><li><strong>From One Function to Many Functions:</strong><ul><li>The paper keeps <code>f(x, y) = 3x²y</code> from the previous section.</li><li>It introduces another function <code>g(x, y) = 2x + y⁸</code>.</li><li>We can find the gradient of <code>g</code> just like we did for <code>f</code>:<ul><li><code>∂g/∂x = ∂/∂x (2x + y⁸) = 2</code> (treating <code>y⁸</code> as constant)</li><li><code>∂g/∂y = ∂/∂y (2x + y⁸) = 8y⁷</code> (treating <code>2x</code> as constant)</li><li>So, <code>∇g(x, y) = [2, 8y⁷]</code>.</li></ul></li></ul></li></ul><figure><img src=/papershelf/assets/maths/jacobian-matrix.png alt=image width=750></figure><ul><li><p><strong>The Jacobian Matrix (J):</strong></p><ul><li><p>When we have multiple functions (say, <code>f</code> and <code>g</code>), each of which can depend on multiple input variables (say, <code>x</code> and <code>y</code>), we can organize their gradients into a <strong>matrix</strong>. This matrix is called the <strong>Jacobian matrix</strong> (or just the Jacobian).</p></li><li><p>If we have two functions <code>f(x,y)</code> and <code>g(x,y)</code>, the Jacobian <code>J</code> (using the paper&rsquo;s &ldquo;numerator layout&rdquo; where gradients are rows) is formed by stacking their gradient vectors:
<code>J = [ ∇f(x, y) ] = [ ∂f/∂x ∂f/∂y ]</code>
<code>[ ∇g(x, y) ] [ ∂g/∂x ∂g/∂y ]</code></p><p>So for <code>f(x,y) = 3x²y</code> and <code>g(x,y) = 2x + y⁸</code>, the Jacobian is:
<code>J = [ 6yx 3x² ]</code>
<code>[ 2 8y⁷ ]</code></p></li><li><p><em>What the Jacobian is ultimately trying to achieve:</em> It captures all the first-order partial derivatives of a vector-valued function (a function that outputs a vector of values) with respect to a vector of input variables.</p><ul><li>Each row <code>i</code> tells you how function <code>fᵢ</code> changes with respect to each input variable.</li><li>Each column <code>j</code> tells you how all output functions change with respect to input variable <code>xⱼ</code>.</li><li>Essentially, if you have <code>m</code> functions and <code>n</code> input variables, the Jacobian is an <code>m x n</code> matrix where the entry <code>Jᵢⱼ = ∂fᵢ/∂xⱼ</code>.</li></ul></li></ul></li><li><p><strong>Layouts (Numerator vs. Denominator):</strong></p><ul><li>The paper notes: &ldquo;there are multiple ways to represent the Jacobian.&rdquo; They use &ldquo;numerator layout&rdquo; (where <code>∇f</code> is a row vector, and the Jacobian stacks these row vectors).</li><li>Other sources might use &ldquo;denominator layout,&rdquo; which is essentially the <em>transpose</em> of the numerator layout Jacobian. It&rsquo;s important to be aware of this when consulting different texts, as the shapes of matrices in equations will change accordingly. The paper sticks to numerator layout.
Example given (page 7, top): Transpose of our Jacobian above is <code>[ 6yx 2 ]</code>
<code>[ 3x² 8y⁷ ]</code></li></ul></li></ul><p>This Jacobian matrix is a fundamental tool when dealing with transformations between vector spaces in calculus, and it plays a key role in the chain rule for vector functions, which is vital for backpropagation in neural networks with multiple layers and multiple neurons per layer.</p><p>The main idea here is organization:</p><ul><li><strong>Gradient:</strong> Organizes partial derivatives of a <em>single scalar-output function</em> with respect to its <em>multiple inputs</em> into a vector.</li><li><strong>Jacobian:</strong> Organizes gradients of <em>multiple scalar-output functions</em> (or equivalently, partial derivatives of a <em>single vector-output function</em>) into a matrix.</li></ul><p>Great! Understanding the Jacobian as an organized collection of partial derivatives is a key step.</p><h2 id=section-41-generalization-of-the-jacobian-page-7-of-the-parr--howard-paper><strong>Section 4.1: Generalization of the Jacobian</strong> (page 7 of the Parr & Howard paper).</h2><p>This section formalizes the Jacobian for a general case where you have a vector of functions, <code>y = f(x)</code>, where <code>x</code> is an input vector and <code>y</code> is an output vector.</p><ul><li><p><strong>Vector Notation:</strong></p><ul><li>Input vector <code>x</code> has <code>n</code> elements: <code>x = [x₁, x₂, ..., xₙ]ᵀ</code> (they assume column vectors by default).</li><li>Output vector <code>y</code> has <code>m</code> scalar-valued functions: <code>y = [f₁(x), f₂(x), ..., fₘ(x)]ᵀ</code>. Each <code>fᵢ(x)</code> takes the whole vector <code>x</code> as input and returns a scalar.</li></ul></li><li><p><strong>The Jacobian Matrix <code>∂y/∂x</code>:</strong>
The paper defines the Jacobian as a stack of <code>m</code> gradients (one for each output function <code>fᵢ</code>). Since they use numerator layout (where each gradient <code>∇fᵢ(x)</code> is a row vector of partials with respect to the components of <code>x</code>), the Jacobian becomes an <code>m x n</code> matrix:</p><p><code>∂y/∂x = [ ∇f₁(x) ] = [ ∂f₁/∂x₁ ∂f₁/∂x₂ ... ∂f₁/∂xₙ ]</code>
<code>[ ∇f₂(x) ] [ ∂f₂/∂x₁ ∂f₂/∂x₂ ... ∂f₂/∂xₙ ]</code>
<code>[ ... ] [ ... ... ... ... ]</code>
<code>[ ∇fₘ(x) ] [ ∂fₘ/∂x₁ ∂fₘ/∂x₂ ... ∂fₘ/∂xₙ ]</code></p><ul><li><strong>Rows:</strong> Each row <code>i</code> of the Jacobian corresponds to one output function <code>fᵢ(x)</code> and contains all the partial derivatives of <em>that specific output function</em> with respect to <em>each of the input variables</em> <code>x₁, x₂, ..., xₙ</code>. So, row <code>i</code> is <code>∇fᵢ(x)</code>.</li><li><strong>Columns:</strong> Each column <code>j</code> of the Jacobian corresponds to one input variable <code>xⱼ</code> and contains all the partial derivatives of <em>each of the output functions</em> <code>f₁, f₂, ..., fₘ</code> with respect to <em>that specific input variable <code>xⱼ</code></em>.</li></ul><p><em>What the Jacobian <code>∂y/∂x</code> is ultimately trying to achieve:</em> It describes how each component of the output vector <code>y</code> changes in response to a small change in each component of the input vector <code>x</code>. It&rsquo;s a complete map of all the first-order sensitivities between the inputs and outputs.</p></li><li><p><strong>Visualizing Jacobian Shapes (diagram on page 8):</strong>
This is a handy diagram to remember the dimensions based on the nature of input <code>x</code> and output <code>f</code> (or <code>y</code>):</p><ul><li><strong>Scalar input <code>x</code>, Scalar output <code>f</code>:</strong> Derivative <code>∂f/∂x</code> is a scalar. (This is standard Calc 1).</li><li><strong>Vector input <code>x</code>, Scalar output <code>f</code>:</strong> Derivative <code>∂f/∂x</code> (which is <code>∇f</code>) is a row vector (1 x n) in numerator layout.</li><li><strong>Scalar input <code>x</code>, Vector output <code>f</code>:</strong> Derivative <code>∂f/∂x</code> is a column vector (m x 1). (Each <code>∂fᵢ/∂x</code> is a scalar, stacked up).</li><li><strong>Vector input <code>x</code>, Vector output <code>f</code>:</strong> Derivative <code>∂f/∂x</code> (the Jacobian) is an <code>m x n</code> matrix.</li></ul></li><li><p><strong>Example: Jacobian of the Identity Function (page 8):</strong></p><ul><li>If <code>y = f(x) = x</code>, then each output component <code>yᵢ</code> is just equal to the corresponding input component <code>xᵢ</code> (so <code>fᵢ(x) = xᵢ</code>). Here, <code>m=n</code>.</li><li>Let&rsquo;s find <code>∂fᵢ/∂xⱼ = ∂xᵢ/∂xⱼ</code>:<ul><li>If <code>i = j</code>, then <code>∂xᵢ/∂xᵢ = 1</code> (the derivative of <code>x₁</code> with respect to <code>x₁</code> is 1).</li><li>If <code>i ≠ j</code>, then <code>∂xᵢ/∂xⱼ = 0</code> (e.g., <code>x₁</code> does not change when <code>x₂</code> changes, because they are independent input components, so <code>∂x₁/∂x₂ = 0</code>).</li></ul></li><li>When you assemble these into the Jacobian matrix, you get 1s on the main diagonal and 0s everywhere else. This is the <strong>Identity Matrix (I)</strong>.</li><li><code>∂x/∂x = I</code></li><li><em>What this ultimately means:</em> If you wiggle an input <code>xⱼ</code> by a small amount, only the corresponding output <code>yⱼ</code> wiggles by that same amount, and other outputs <code>yᵢ</code> (where <code>i≠j</code>) don&rsquo;t change at all. This makes perfect sense for the identity function.</li></ul></li></ul><p>This section firmly establishes the Jacobian matrix as the way to represent the derivative of a vector function with respect to a vector input. It&rsquo;s the matrix that holds all the individual &ldquo;slopes&rdquo; that connect changes in each input dimension to changes in each output dimension.</p><p>This general form of the Jacobian will be crucial when we get to the vector chain rule later, which is how backpropagation efficiently calculates gradients through multiple layers of a neural network, where each layer can be seen as a vector function taking a vector input.</p><h2 id=section-42-derivatives-of-vector-element-wise-binary-operators-pages-9-11><strong>Section 4.2: Derivatives of vector element-wise binary operators</strong> (Pages 9-11).</h2><p><strong>The Big Picture of This Section:</strong></p><p>Neural networks involve many operations on vectors: adding an input vector to a bias vector, multiplying activations by weights, etc. Often, these operations are <strong>element-wise</strong> – meaning the operation is applied independently to corresponding elements of the input vectors to produce an element of the output vector.</p><ul><li>Example: If <code>w = [w₁, w₂]</code> and <code>x = [x₁, x₂]</code>, then <code>w + x = [w₁ + x₁, w₂ + x₂]</code>. The first element of the output only depends on the first elements of the inputs, and so on.</li></ul><p>This section aims to figure out:</p><ul><li>If we have an output vector <code>y</code> that&rsquo;s a result of an element-wise operation between two input vectors <code>w</code> and <code>x</code> (e.g., <code>y = w + x</code> or <code>y = w * x</code>), how does <code>y</code> change if we wiggle <code>w</code>? (This gives us the Jacobian <code>∂y/∂w</code>).</li><li>And how does <code>y</code> change if we wiggle <code>x</code>? (This gives us the Jacobian <code>∂y/∂x</code>).</li></ul><p><em>What we are ultimately trying to achieve here is to find simplified forms for these Jacobians, because for element-wise operations, many of the terms in the full Jacobian matrix will turn out to be zero.</em></p><p><strong>Breaking it Down (Page 9):</strong></p><p>The paper starts with the general form: <code>y = f(w) ⊙ g(x)</code>.</p><ul><li><code>w</code> and <code>x</code> are input vectors.</li><li><code>f(w)</code> and <code>g(x)</code> are functions that produce vectors of the same size.</li><li><code>⊙</code> represents <em>any</em> element-wise binary operator (like <code>+</code>, <code>-</code>, element-wise <code>*</code>, element-wise <code>/</code>).</li><li><code>y</code> is the output vector.
So, <code>yᵢ = fᵢ(w) ⊙ gᵢ(x)</code>. The <code>i</code>-th element of <code>y</code> depends only on the <code>i</code>-th element processing of <code>w</code> and <code>x</code>.</li></ul><p>The full Jacobian <code>∂y/∂w</code> would be a matrix where element <code>(i,j)</code> is <code>∂yᵢ/∂wⱼ</code> (how the <code>i</code>-th output element <code>yᵢ</code> changes with respect to the <code>j</code>-th input element <code>wⱼ</code> from vector <code>w</code>).
Similarly for <code>∂y/∂x</code>.</p><p><strong>The &ldquo;Furball&rdquo; and the Simplification (Page 10):</strong></p><p>The general Jacobian matrix for such an operation (shown at the bottom of page 9) looks complicated – a &ldquo;furball,&rdquo; as the paper says.</p><p>However, because the operations are <strong>element-wise</strong>, there&rsquo;s a huge simplification:</p><ul><li><code>yᵢ</code> (the <code>i</code>-th element of the output) <em>only</em> depends on <code>wᵢ</code> (the <code>i</code>-th element of <code>w</code>) and <code>xᵢ</code> (the <code>i</code>-th element of <code>x</code>).</li><li><code>yᵢ</code> does <em>not</em> depend on <code>wⱼ</code> if <code>j ≠ i</code>.</li><li><code>yᵢ</code> does <em>not</em> depend on <code>xⱼ</code> if <code>j ≠ i</code>.</li></ul><p><strong>What does this mean for the Jacobian <code>∂y/∂w</code>?</strong>
Consider <code>∂yᵢ/∂wⱼ</code>:</p><ul><li>If <code>j ≠ i</code> (we are looking at an off-diagonal element of the Jacobian): Since <code>yᵢ</code> does not depend on <code>wⱼ</code>, its derivative <code>∂yᵢ/∂wⱼ</code> must be <strong>zero</strong>.</li><li>If <code>j = i</code> (we are looking at a diagonal element of the Jacobian): Then <code>∂yᵢ/∂wᵢ</code> will generally be non-zero, and it&rsquo;s just the derivative of the <code>i</code>-th scalar operation <code>fᵢ(wᵢ) ⊙ gᵢ(xᵢ)</code> with respect to <code>wᵢ</code> (treating <code>xᵢ</code> as a constant for this partial derivative).</li></ul><p><strong>The Result: Diagonal Jacobians!</strong>
This means that for element-wise operations, the Jacobian matrices <code>∂y/∂w</code> and <code>∂y/∂x</code> are <strong>diagonal matrices</strong>. A diagonal matrix has non-zero values only along its main diagonal, and zeros everywhere else.</p><p>The paper introduces the &ldquo;element-wise diagonal condition&rdquo;: <code>fᵢ(w)</code> should only access <code>wᵢ</code> and <code>gᵢ(x)</code> should only access <code>xᵢ</code>. This is precisely what happens in simple element-wise vector operations.</p><p>So, the Jacobian <code>∂y/∂w</code> simplifies to (middle of page 10):
<code>∂y/∂w = diag( ∂/∂w₁ (f₁(w₁) ⊙ g₁(x₁)), ∂/∂w₂ (f₂(w₂) ⊙ g₂(x₂)), ..., ∂/∂wₙ (fₙ(wₙ) ⊙ gₙ(xₙ)) )</code>
Each term on the diagonal is just a <em>scalar derivative</em> of the <code>i</code>-th component operation.</p><p><em>What this simplification is ultimately trying to achieve:</em> It makes calculating these Jacobians much easier. Instead of a full matrix of derivatives, we only need to calculate <code>n</code> scalar derivatives for the diagonal.</p><p><strong>Special Case: <code>f(w) = w</code> (Page 10-11)</strong></p><p>Very often in neural networks, one of the functions in the element-wise operation is just the identity. For example, vector addition <code>y = w + x</code>. Here <code>f(w) = w</code> (so <code>fᵢ(wᵢ) = wᵢ</code>) and <code>g(x) = x</code> (so <code>gᵢ(xᵢ) = xᵢ</code>).</p><p>Let&rsquo;s look at <code>y = w + x</code>, so <code>yᵢ = wᵢ + xᵢ</code>.</p><ul><li><strong><code>∂y/∂w</code>:</strong><ul><li>The <code>i</code>-th diagonal element is <code>∂/∂wᵢ (wᵢ + xᵢ)</code>. Treating <code>xᵢ</code> as a constant, this derivative is <code>1</code>.</li><li>So, <code>∂(w+x)/∂w = diag(1, 1, ..., 1) = I</code> (the identity matrix).</li></ul></li><li><strong><code>∂y/∂x</code>:</strong><ul><li>The <code>i</code>-th diagonal element is <code>∂/∂xᵢ (wᵢ + xᵢ)</code>. Treating <code>wᵢ</code> as a constant, this derivative is <code>1</code>.</li><li>So, <code>∂(w+x)/∂x = diag(1, 1, ..., 1) = I</code> (the identity matrix).</li></ul></li></ul><p>The paper then lists Jacobians for common element-wise binary operations where <code>f(w) = w</code> (so <code>fᵢ(wᵢ) = wᵢ</code>):</p><ul><li><p><strong>Addition: <code>y = w + x</code></strong></p><ul><li><code>∂y/∂w = I</code></li><li><code>∂y/∂x = I</code></li><li><em>Intuition:</em> If you wiggle <code>w₁</code> by a small amount <code>Δ</code>, <code>y₁</code> changes by <code>Δ</code>, and no other <code>yⱼ</code> changes. Same for <code>x</code>.</li></ul></li><li><p><strong>Subtraction: <code>y = w - x</code></strong></p><ul><li><code>∂y/∂w = I</code></li><li><code>∂y/∂x = -I</code> (because <code>∂/∂xᵢ (wᵢ - xᵢ) = -1</code>)</li><li><em>Intuition:</em> If you wiggle <code>x₁</code> by <code>Δ</code>, <code>y₁</code> changes by <code>-Δ</code>.</li></ul></li><li><p><strong>Element-wise Multiplication (Hadamard Product): <code>y = w ⊗ x</code> (so <code>yᵢ = wᵢ * xᵢ</code>)</strong></p><ul><li><code>∂y/∂w</code>: <code>i</code>-th diagonal element is <code>∂/∂wᵢ (wᵢ * xᵢ) = xᵢ</code>. So, <code>∂y/∂w = diag(x)</code>.</li><li><code>∂y/∂x</code>: <code>i</code>-th diagonal element is <code>∂/∂xᵢ (wᵢ * xᵢ) = wᵢ</code>. So, <code>∂y/∂x = diag(w)</code>.</li><li><em>Intuition for ∂y/∂w:</em> If you wiggle <code>w₁</code> by <code>Δ</code>, <code>y₁</code> changes by <code>x₁ * Δ</code>. The change in <code>y₁</code> depends on the value of <code>x₁</code>.</li></ul></li><li><p><strong>Element-wise Division: <code>y = w / x</code> (so <code>yᵢ = wᵢ / xᵢ</code>)</strong></p><ul><li><code>∂y/∂w</code>: <code>i</code>-th diagonal element is <code>∂/∂wᵢ (wᵢ / xᵢ) = 1/xᵢ</code>. So, <code>∂y/∂w = diag(1/x₁, 1/x₂, ...)</code>.</li><li><code>∂y/∂x</code>: <code>i</code>-th diagonal element is <code>∂/∂xᵢ (wᵢ / xᵢ) = -wᵢ / xᵢ²</code>. So, <code>∂y/∂x = diag(-w₁/x₁², ...)</code>.</li></ul></li></ul><p><strong>Key Takeaway from Section 4.2:</strong>
When dealing with element-wise operations between two vectors <code>w</code> and <code>x</code> to produce <code>y</code>, the Jacobians <code>∂y/∂w</code> and <code>∂y/∂x</code> are <strong>diagonal matrices</strong>. This is a huge simplification. The values on the diagonal are found by simply taking the scalar derivative of the <code>i</code>-th component operation with respect to the <code>i</code>-th component of the input vector. This section provides the rules for common operations like addition, subtraction, and element-wise multiplication/division.</p><p>This means that when we&rsquo;re backpropagating gradients through such an element-wise layer, the calculations become much simpler than if we had to deal with full Jacobian matrices. We&rsquo;re essentially just scaling the incoming gradients by these diagonal terms.</p><h2 id=section-43-derivatives-involving-scalar-expansion-pages-11-12><strong>Section 4.3: Derivatives involving scalar expansion</strong> (Pages 11-12).</h2><p><strong>The Big Picture of This Section:</strong></p><p>In neural networks, we often perform operations between a vector and a <em>scalar</em>. For example:</p><ul><li>Adding a scalar bias <code>b</code> to every element of a vector <code>z</code>: <code>y = z + b</code>.</li><li>Multiplying every element of a vector <code>z</code> by a scalar learning rate <code>η</code>: <code>y = η * z</code>.</li></ul><p>This section explains how to find the derivatives for these types of operations. The trick is to realize that these operations can be viewed as <strong>implicit element-wise operations</strong> where the scalar is &ldquo;expanded&rdquo; or &ldquo;broadcast&rdquo; into a vector of the same size as the other vector.</p><p><em>What we are ultimately trying to achieve here is to get rules for how the output vector <code>y</code> changes if we wiggle the input vector <code>x</code>, and how <code>y</code> changes if we wiggle the input scalar <code>z</code>.</em></p><p><strong>Breaking it Down (Page 11-12):</strong></p><p>The paper uses the example <code>y = x + z</code>, where <code>x</code> is a vector and <code>z</code> is a scalar.</p><ul><li><strong>Implicit Expansion:</strong> This is really <code>y = f(x) + g(z)</code> where <code>f(x) = x</code> and <code>g(z) = 1z</code> (a vector where every element is <code>z</code>).</li><li>So, <code>yᵢ = xᵢ + z</code>. Each element <code>yᵢ</code> is the sum of the corresponding <code>xᵢ</code> and the <em>same</em> scalar <code>z</code>.</li></ul><p><strong>1. Derivative with respect to the vector <code>x</code>: <code>∂y/∂x</code></strong></p><ul><li>This fits the &ldquo;element-wise diagonal condition&rdquo; we just discussed.<ul><li><code>fᵢ(x) = xᵢ</code> only depends on <code>xᵢ</code>.</li><li><code>gᵢ(z) = z</code> (for the i-th component) only depends on the scalar <code>z</code> (which is considered independent of <code>x</code> for this partial derivative).</li></ul></li><li>The <code>i</code>-th diagonal element of the Jacobian <code>∂y/∂x</code> is <code>∂/∂xᵢ (xᵢ + z)</code>.</li><li>Since <code>z</code> is treated as a constant when differentiating with respect to <code>xᵢ</code>, <code>∂z/∂xᵢ = 0</code>.</li><li>So, <code>∂/∂xᵢ (xᵢ + z) = ∂xᵢ/∂xᵢ + ∂z/∂xᵢ = 1 + 0 = 1</code>.</li><li>Therefore, <code>∂y/∂x = diag(1, 1, ..., 1) = I</code> (the identity matrix).</li><li><strong>Intuition:</strong> If you wiggle <code>xᵢ</code> by a small amount <code>Δ</code>, only <code>yᵢ</code> changes, and it changes by <code>Δ</code>. This is the definition of the identity matrix&rsquo;s effect.</li><li><em>What this is ultimately trying to achieve:</em> It confirms that adding a scalar to a vector shifts all elements equally, so the rate of change of each output element <code>yᵢ</code> with respect to its corresponding input <code>xᵢ</code> is 1, and there&rsquo;s no cross-influence.</li></ul><p><strong>2. Derivative with respect to the scalar <code>z</code>: <code>∂y/∂z</code></strong>
This is different! We are differentiating a <em>vector output</em> <code>y</code> with respect to a <em>scalar input</em> <code>z</code>.
Based on the table from page 8 (or just thinking about it):</p><ul><li>Input: scalar <code>z</code></li><li>Output: vector <code>y</code></li><li>The derivative <code>∂y/∂z</code> should be a <strong>column vector</strong> (or a vertical vector as the paper terms it). Each element of this column vector will be <code>∂yᵢ/∂z</code>.</li><li>Let&rsquo;s find <code>∂yᵢ/∂z</code> for <code>yᵢ = xᵢ + z</code>.</li><li>When differentiating with respect to <code>z</code>, we treat <code>xᵢ</code> as a constant. So, <code>∂xᵢ/∂z = 0</code>.</li><li><code>∂yᵢ/∂z = ∂/∂z (xᵢ + z) = ∂xᵢ/∂z + ∂z/∂z = 0 + 1 = 1</code>.</li><li>Since this is true for <em>every</em> <code>i</code> (from 1 to <code>n</code>, the dimension of <code>y</code> and <code>x</code>), then <code>∂y/∂z</code> is a column vector of all ones.</li><li>The paper writes this as <code>∂/∂z (x + z) = 1</code> (where <code>1</code> is the vector of ones of appropriate length).</li><li><strong>Intuition:</strong> If you wiggle the single scalar <code>z</code> by <code>Δ</code>, <em>every</em> element <code>yᵢ</code> of the output vector changes by <code>Δ</code> because <code>z</code> is added to every <code>xᵢ</code>.</li><li><em>What this is ultimately trying to achieve:</em> It shows how a single scalar change propagates to all elements of the output vector when scalar addition is involved.</li></ul><p><strong>Now for Scalar Multiplication: <code>y = xz</code> (or <code>y = zx</code>)</strong>
This is treated as element-wise multiplication: <code>y = x ⊗ 1z</code>.
So, <code>yᵢ = xᵢ * z</code>.</p><p><strong>1. Derivative with respect to the vector <code>x</code>: <code>∂y/∂x</code></strong></p><ul><li>Again, element-wise diagonal condition holds.</li><li>The <code>i</code>-th diagonal element is <code>∂/∂xᵢ (xᵢ * z)</code>.</li><li>Treat <code>z</code> as constant: <code>∂/∂xᵢ (xᵢ * z) = z * ∂xᵢ/∂xᵢ = z * 1 = z</code>.</li><li>So, <code>∂y/∂x = diag(z, z, ..., z) = zI</code> (scalar <code>z</code> times the identity matrix).</li><li>The paper writes this as <code>∂/∂x (xz) = diag(1z) = Iz</code>. (Here, <code>1z</code> in <code>diag(1z)</code> means a vector of <code>z</code>s, which makes more sense. If <code>diag(z)</code> was meant for a scalar <code>z</code>, it would just be <code>z</code> itself, not a matrix. So <code>diag(1z)</code> is the clearer way to express <code>zI</code>). My interpretation: <code>diag(1z)</code> means a diagonal matrix with <code>z</code> on every diagonal element. This is <code>z * I</code>.</li><li><strong>Intuition:</strong> If you wiggle <code>xᵢ</code> by <code>Δ</code>, <code>yᵢ</code> changes by <code>z * Δ</code>. The effect is scaled by <code>z</code>.</li></ul><p><strong>2. Derivative with respect to the scalar <code>z</code>: <code>∂y/∂z</code></strong></p><ul><li><code>∂y/∂z</code> will be a column vector. The <code>i</code>-th element is <code>∂yᵢ/∂z</code>.</li><li><code>yᵢ = xᵢ * z</code>. Treat <code>xᵢ</code> as constant.</li><li><code>∂yᵢ/∂z = ∂/∂z (xᵢ * z) = xᵢ * ∂z/∂z = xᵢ * 1 = xᵢ</code>.</li><li>So, <code>∂y/∂z</code> is a column vector whose elements are <code>[x₁, x₂, ..., xₙ]ᵀ</code>, which is just the vector <code>x</code>.</li><li>The paper writes this as <code>∂/∂z (xz) = x</code>.</li><li><strong>Intuition:</strong> If you wiggle the scalar <code>z</code> by <code>Δ</code>, the <code>i</code>-th output <code>yᵢ</code> changes by <code>xᵢ * Δ</code>. The amount <code>yᵢ</code> changes depends on the value of <code>xᵢ</code>.</li><li><em>What this is ultimately trying to achieve:</em> It shows how a single scalar change <code>z</code> propagates to the output vector <code>y</code>, with each element <code>yᵢ</code> being affected proportionally to the corresponding <code>xᵢ</code>.</li></ul><p><strong>Key Takeaway from Section 4.3:</strong>
Operations involving a vector and a scalar (like adding a scalar to a vector, or multiplying a vector by a scalar) can be understood by &ldquo;expanding&rdquo; the scalar into a vector and then applying the rules for element-wise vector operations.</p><ul><li>When differentiating with respect to the <strong>vector input</strong>, the Jacobian is still <strong>diagonal</strong>.</li><li>When differentiating with respect to the <strong>scalar input</strong>, the result is a <strong>vector</strong> (not a diagonal matrix), because a single change in the scalar affects all components of the output vector.</li></ul><p>This section helps us build up the rules needed for things like <code>Wx + b</code> in a neural network layer, where <code>b</code> might be a bias <em>vector</em> added element-wise, or a scalar bias broadcasted. The logic applies similarly.</p></div><hr><div class=paper-reference><p><strong>Read the original paper:</strong>
<a href=https://arxiv.org/pdf/1802.01528 target=_blank rel="noopener noreferrer">The Matrix Calculus You Need For Deep Learning</a></p><p class=paper-disclaimer><em>The content presented here is a collection of my personal notes and explanations based on the paper. This is by no means an exhaustive explanation, and I strongly encourage you to read the actual paper for a comprehensive understanding.</em></p></div></article></main><footer><p>&copy; 2025 Deepanshu Kandpal</p></footer></body></html>