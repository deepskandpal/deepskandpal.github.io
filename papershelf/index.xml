<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Papershelf on 404EngineerNotFound</title><link>https://deepskandpal.github.io/papershelf/</link><description>Recent content in Papershelf on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 17 Jun 2025 09:57:39 +0530</lastBuildDate><atom:link href="https://deepskandpal.github.io/papershelf/index.xml" rel="self" type="application/rss+xml"/><item><title>MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core</title><link>https://deepskandpal.github.io/papershelf/moe/</link><pubDate>Tue, 17 Jun 2025 09:57:39 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/moe/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Qwen3 Technical Report</title><link>https://deepskandpal.github.io/papershelf/qwen3/</link><pubDate>Thu, 12 Jun 2025 18:07:27 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/qwen3/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention</title><link>https://deepskandpal.github.io/papershelf/hardware-attention/</link><pubDate>Mon, 09 Jun 2025 22:02:04 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/hardware-attention/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title><link>https://deepskandpal.github.io/papershelf/illusion-of-thinking/</link><pubDate>Sun, 08 Jun 2025 01:01:23 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/illusion-of-thinking/</guid><description>&lt;p>At its heart, &amp;ldquo;The Illusion of Thinking,&amp;rdquo; is about trying to genuinely understand how well these new &amp;ldquo;Large Reasoning Models&amp;rdquo; (LRMs) actually &lt;em>reason&lt;/em>. You&amp;rsquo;ve probably heard about models that show their &amp;ldquo;thinking steps&amp;rdquo; before giving an answer, like with Chain-of-Thought. They often do better on benchmarks, which is exciting. But we felt that just looking at the final answer on standard math or coding tests wasn&amp;rsquo;t telling the whole story.&lt;/p></description></item><item><title>Understanding the difficulty of training deep feedforward neural networks</title><link>https://deepskandpal.github.io/papershelf/understand-training-diff-deep-nn/</link><pubDate>Fri, 06 Jun 2025 16:11:01 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/understand-training-diff-deep-nn/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 â€“ LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY</title><link>https://deepskandpal.github.io/papershelf/disciplined-nn/</link><pubDate>Fri, 06 Jun 2025 15:57:54 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/disciplined-nn/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Multi-Head Attention: Collaborate Instead of Concatenate</title><link>https://deepskandpal.github.io/papershelf/multihead-attention/</link><pubDate>Wed, 04 Jun 2025 23:24:48 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/multihead-attention/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title><link>https://deepskandpal.github.io/papershelf/flash-attention/</link><pubDate>Wed, 04 Jun 2025 23:24:37 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/flash-attention/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link>https://deepskandpal.github.io/papershelf/bert/</link><pubDate>Wed, 04 Jun 2025 23:24:31 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/bert/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Improving Language Understanding by Generative Pre-Training</title><link>https://deepskandpal.github.io/papershelf/gpt-1/</link><pubDate>Tue, 03 Jun 2025 11:01:51 +0530</pubDate><guid>https://deepskandpal.github.io/papershelf/gpt-1/</guid><description>&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>(Your summary and notes about the paper go here)&lt;/p></description></item><item><title>Sequential Test and Adaptive Experimental Design</title><link>https://deepskandpal.github.io/papershelf/msprt/</link><pubDate>Sun, 27 Apr 2025 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/msprt/</guid><description/></item><item><title>The Matrix Calculus You Need For Deep Learning</title><link>https://deepskandpal.github.io/papershelf/maths/</link><pubDate>Sun, 27 Apr 2025 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/maths/</guid><description>&lt;h1 id="the-big-picture-goal">&lt;strong>The Big Picture Goal:&lt;/strong>&lt;/h1>
&lt;p>We want our machine learning models to make good predictions. To do this, we define a &lt;strong>loss function&lt;/strong> (or cost function) that measures how &amp;ldquo;bad&amp;rdquo; our model&amp;rsquo;s predictions are compared to the true values.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Training = Minimizing Loss:&lt;/strong> Training a model means finding the model parameters (like weights &lt;code>w&lt;/code> and biases &lt;code>b&lt;/code>) that make this loss function as small as possible.&lt;/li>
&lt;li>&lt;strong>Calculus as a Tool for Minimization:&lt;/strong> Derivatives (and their extensions like gradients) tell us the &lt;strong>rate of change&lt;/strong> or the &lt;strong>slope&lt;/strong> of a function.
&lt;ul>
&lt;li>If we know the slope of our loss function with respect to our model parameters, we know which &amp;ldquo;direction&amp;rdquo; to tweak those parameters to &lt;em>decrease&lt;/em> the loss. This is the essence of &lt;strong>Gradient Descent&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>So, all this calculus is ultimately about finding an efficient way to &amp;ldquo;walk downhill&amp;rdquo; on our loss function surface to find the parameter values that give the lowest error.&lt;/p></description></item><item><title>Training Compute-Optimal Large Language Models</title><link>https://deepskandpal.github.io/papershelf/chinchilla-scaling-laws/</link><pubDate>Mon, 30 Dec 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/chinchilla-scaling-laws/</guid><description>&lt;h2 id="paper">&lt;strong>Paper: &amp;ldquo;Training Compute-Optimal Large Language Models&amp;rdquo;&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Authors:&lt;/strong> Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre&lt;/p>
&lt;p>&lt;strong>Also Known As:&lt;/strong> The &amp;ldquo;Chinchilla&amp;rdquo; paper (named after their 70B parameter model)&lt;/p>
&lt;h2 id="abstract--key-contributions">&lt;strong>Abstract &amp;amp; Key Contributions:&lt;/strong>&lt;/h2>
&lt;p>This seminal paper from DeepMind challenges the conventional wisdom about how to scale large language models optimally. The key findings are:&lt;/p></description></item><item><title>Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective</title><link>https://deepskandpal.github.io/papershelf/wsd-river-valley-landscape/</link><pubDate>Mon, 30 Dec 2024 12:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/wsd-river-valley-landscape/</guid><description>&lt;h2 id="paper">&lt;strong>Paper: &amp;ldquo;Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective&amp;rdquo;&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Authors:&lt;/strong> Kaiyue Wen, David Hall, Zhiyuan Li, Jason Wang, Percy Liang, Tengyu Ma (Stanford University &amp;amp; TTIC)&lt;/p>
&lt;h2 id="abstract--key-contributions">&lt;strong>Abstract &amp;amp; Key Contributions:&lt;/strong>&lt;/h2>
&lt;p>This paper provides a theoretical explanation for the effectiveness of the Warmup-Stable-Decay (WSD) learning rate schedule in language model pretraining. The key contributions include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>River Valley Loss Landscape Theory:&lt;/strong> Introduces a novel conceptual framework where the loss landscape resembles a deep valley with a river at its bottom&lt;/li>
&lt;li>&lt;strong>WSD Mechanism Explanation:&lt;/strong> Shows why WSD&amp;rsquo;s non-traditional loss curve (high loss during stable phase, sharp drop during decay) leads to superior performance&lt;/li>
&lt;li>&lt;strong>WSD-S Algorithm:&lt;/strong> Proposes WSD-Simplified, an improved variant that reuses checkpoints and maintains a single main branch&lt;/li>
&lt;/ul>
&lt;h2 id="1-the-wsd-learning-rate-schedule">&lt;strong>1. The WSD Learning Rate Schedule&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Traditional Problem:&lt;/strong>&lt;br>
Cosine learning rate schedules require pre-determining a fixed compute budget, making it difficult to adapt to additional compute or data.&lt;/p></description></item><item><title>Recommending What Video to Watch Next: A Multitask Ranking System</title><link>https://deepskandpal.github.io/papershelf/youtube-multitask-ranking/</link><pubDate>Thu, 19 Dec 2024 10:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/youtube-multitask-ranking/</guid><description>&lt;h2 id="paper">&lt;strong>Paper: &amp;ldquo;Recommending What Video to Watch Next: A Multitask Ranking System&amp;rdquo;&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Authors:&lt;/strong> Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee Kumthekar, Maheswaran Sathiamoorthy, Xinyang Yi, Ed Chi (Google)&lt;/p>
&lt;p>&lt;strong>Conference:&lt;/strong> RecSys 2019&lt;/p>
&lt;h2 id="abstract">&lt;strong>Abstract:&lt;/strong>&lt;/h2>
&lt;p>This paper presents YouTube&amp;rsquo;s large-scale multi-objective ranking system for &amp;ldquo;watch next&amp;rdquo; recommendations, serving over one billion users. The system simultaneously optimizes for multiple engagement objectives (clicks, watch time, shares, etc.) using a Multi-gate Mixture-of-Experts (MMoE) architecture. Key innovations include techniques to handle selection bias in training data and methods to balance multiple objectives in a production environment.&lt;/p></description></item><item><title>Deep Neural Networks for YouTube Recommendations</title><link>https://deepskandpal.github.io/papershelf/youtube-dnn-recommendations/</link><pubDate>Thu, 19 Dec 2024 10:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/youtube-dnn-recommendations/</guid><description>&lt;h2 id="paper">&lt;strong>Paper: &amp;ldquo;Deep Neural Networks for YouTube Recommendations&amp;rdquo;&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Authors:&lt;/strong> Paul Covington, Jay Adams, Emre Sargin (Google)&lt;/p>
&lt;p>&lt;strong>Conference:&lt;/strong> RecSys 2016&lt;/p>
&lt;h2 id="abstract">&lt;strong>Abstract:&lt;/strong>&lt;/h2>
&lt;p>This paper presents YouTube&amp;rsquo;s recommendation system architecture, which serves recommendations to over one billion users through a two-stage approach: candidate generation and ranking. The system uses deep neural networks to handle the massive scale (billions of videos, millions of users, real-time serving requirements), addressing challenges like scalability, freshness, and noise in implicit feedback data.&lt;/p></description></item><item><title>Step-by-Step Diffusion: An Elementary Tutorial</title><link>https://deepskandpal.github.io/papershelf/step-by-step-diffusion/</link><pubDate>Fri, 26 Jul 2024 20:00:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/step-by-step-diffusion/</guid><description>&lt;h2 id="paper">&lt;strong>Paper: &amp;ldquo;Step-by-Step Diffusion: An Elementary Tutorial&amp;rdquo;&lt;/strong>&lt;/h2>
&lt;h2 id="abstract">&lt;strong>Abstract:&lt;/strong>&lt;/h2>
&lt;p>We present an accessible first course on diffusion models and flow matching for machine learning, aimed at a technical audience with no diffusion experience. We try to simplify the mathematical details as much as possible (sometimes heuristically), while retaining enough precision to derive correct algorithms.&lt;/p></description></item><item><title>Attention Is All You Need</title><link>https://deepskandpal.github.io/papershelf/attention-all-u-need/</link><pubDate>Sat, 27 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/attention-all-u-need/</guid><description>&lt;h2 id="paper">&lt;strong>Paper: &amp;ldquo;Attention Is All You Need&amp;rdquo;&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Title Itself:&lt;/strong> The title is a bold claim. It signals a departure from the dominant RNN/LSTM and CNN paradigms for sequence modeling at the time. It suggests that the attention mechanism, previously often an auxiliary component, could be the &lt;em>core&lt;/em> and &lt;em>sufficient&lt;/em> mechanism.&lt;/p>
&lt;h2 id="abstract">&lt;strong>Abstract:&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Introduces the Transformer:&lt;/strong> &amp;ldquo;based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.&amp;rdquo;
&lt;ul>
&lt;li>&lt;strong>Goal:&lt;/strong> Superior quality, more parallelizable, less training time for sequence transduction (e.g., machine translation).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Key Results:&lt;/strong> State-of-the-art (SOTA) on WMT 2014 English-to-German (28.4 BLEU, &amp;gt;2 BLEU improvement) and English-to-French translation tasks, with significantly reduced training costs (e.g., 3.5 days on 8 P100 GPUs for a big model).&lt;/li>
&lt;li>&lt;strong>Generalization:&lt;/strong> Shows it can be applied to other tasks like English constituency parsing.&lt;/li>
&lt;/ul>
&lt;h2 id="1-introduction">&lt;strong>1. Introduction&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Context:&lt;/strong> Recurrent Neural Networks (RNNs), especially LSTMs and Gated Recurrent Units (GRUs), were the SOTA for sequence modeling (language modeling, machine translation).
&lt;ul>
&lt;li>&lt;strong>Problem with RNNs:&lt;/strong> They are inherently sequential. &lt;code>h_t&lt;/code> depends on &lt;code>h_{t-1}&lt;/code>. This &amp;ldquo;sequential nature precludes parallelization within training examples.&amp;rdquo; This becomes a bottleneck for longer sequences, as memory constraints limit batching across examples.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Attention as a Solution (Previously):&lt;/strong> Attention mechanisms were already being used with RNNs, allowing modeling of dependencies regardless of distance. However, they were typically used &lt;em>in conjunction&lt;/em> with a recurrent network.&lt;/li>
&lt;li>&lt;strong>The Paper&amp;rsquo;s Proposal:&lt;/strong> &amp;ldquo;In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.&amp;rdquo;
&lt;ul>
&lt;li>&lt;strong>Key Advantages Claimed:&lt;/strong> More parallelization, can reach new SOTA in translation quality with less training time.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="2-background">&lt;strong>2. Background&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Motivation:&lt;/strong> Reduce sequential computation.&lt;/li>
&lt;li>&lt;strong>Existing Alternatives to RNNs:&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Extended Neural GPU, ByteNet, ConvS2S:&lt;/strong> Used CNNs as basic building blocks, computing hidden representations in parallel.
&lt;ul>
&lt;li>&lt;strong>Limitation:&lt;/strong> The number of operations to relate signals from two arbitrary positions grows with distance (linearly for ConvS2S, logarithmically for ByteNet). This makes learning distant dependencies harder.&lt;/li>
&lt;li>&lt;strong>Transformer&amp;rsquo;s Advantage:&lt;/strong> Reduces this to a &lt;em>constant&lt;/em> number of operations (via self-attention), albeit at the cost of reduced &amp;ldquo;effective resolution&amp;rdquo; due to averaging attention-weighted positions (counteracted by Multi-Head Attention).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Self-Attention (Intra-Attention):&lt;/strong> Defined as an attention mechanism relating different positions of a &lt;em>single sequence&lt;/em> to compute a representation of that sequence. It had been used before successfully.&lt;/li>
&lt;li>&lt;strong>End-to-End Memory Networks:&lt;/strong> Based on recurrent attention, showed good performance on simple QA.&lt;/li>
&lt;li>&lt;strong>Transformer&amp;rsquo;s Novelty Claim:&lt;/strong> &amp;ldquo;the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;h1 id="3-model-architecture">&lt;strong>3. Model Architecture&lt;/strong>&lt;/h1>
&lt;p>This is the core of the paper.&lt;/p></description></item><item><title>Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing</title><link>https://deepskandpal.github.io/papershelf/infinit-retrevial-attention/</link><pubDate>Sat, 27 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/infinit-retrevial-attention/</guid><description/></item><item><title>One-Minute Video Generation with Test-Time Training</title><link>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</link><pubDate>Mon, 15 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</guid><description>&lt;p>&lt;a href="https://test-time-training.github.io/video-dit/">Website&lt;/a>&lt;/p>
&lt;h1 id="tldr">TLDR;&lt;/h1>
&lt;p>They tackled long video generation by replacing expensive global attention with efficient local attention, and bridging the gaps between local segments using novel TTT layers. These TTT layers act like RNNs but have a much smarter, adaptive hidden state (a neural network that learns on-the-fly during generation). This allows them to capture long-range dependencies and complex dynamics better than traditional RNNs, leading to more coherent minute-long videos, albeit with some remaining artifacts and efficiency challenges.&lt;/p></description></item></channel></rss>