<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Papershelves on 404EngineerNotFound</title><link>https://deepskandpal.github.io/papershelf/</link><description>Recent content in Papershelves on 404EngineerNotFound</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 15 Apr 2024 19:30:00 +0000</lastBuildDate><atom:link href="https://deepskandpal.github.io/papershelf/index.xml" rel="self" type="application/rss+xml"/><item><title>One-Minute Video Generation with Test-Time Training</title><link>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</link><pubDate>Mon, 15 Apr 2024 19:30:00 +0000</pubDate><guid>https://deepskandpal.github.io/papershelf/ttt-layer-for-video-generation/</guid><description>&lt;p>&lt;a href="https://test-time-training.github.io/video-dit/">Website&lt;/a>&lt;/p>
&lt;h2 id="tldr">TLDR;&lt;/h2>
&lt;p>They tackled long video generation by replacing expensive global attention with efficient local attention, and bridging the gaps between local segments using novel TTT layers. These TTT layers act like RNNs but have a much smarter, adaptive hidden state (a neural network that learns on-the-fly during generation). This allows them to capture long-range dependencies and complex dynamics better than traditional RNNs, leading to more coherent minute-long videos, albeit with some remaining artifacts and efficiency challenges.&lt;/p></description></item></channel></rss>